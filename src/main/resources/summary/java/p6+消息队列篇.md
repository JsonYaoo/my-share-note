# 六、消息队列篇

### 1.1. 什么是消息队列？

#### 概念

- **消息**：Message，指在应用间传送的数据，消息可以非常简单，比如只包含文本字符串，也可以更复杂，可能包含嵌入对象。
- **消息队列**：Message Queue，MQ，是一种应用间的通信方式，消息发送后可以立即返回，由消息系统来确保消息的可靠传递。
  1. 消息发布者只管把消息发布到 MQ 中，而不用管谁来取。
  2. 消息使用者只管从 MQ 中取消息而不管是谁发布的。
  3. 这样发布者和使用者都不用知道对方的存在，解除了上下游调用的依赖关系，实现异步和解耦。
  4. MQ 作为高并发系统的核心组件之一，能够帮助业务系统解构提升开发效率和系统稳定性。
  5. 目前主流的消息队列有 ActiveMQ、Kafka、RocketMQ、RabbitMQ 等。

#### 用途

- **异步处理**：把消息写入消息队列，使得**非必要**的业务逻辑能够以异步的方式运行，从而加快响应速度。

  ![1633838988599](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633838988599.png)

- **系统解耦**：解决不同重要程度、不同能力级别系统之间依赖导致一死全死的问题。

  ![1633838915140](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633838915140.png)

- **削峰填谷**：主要解决瞬时写压力大于应用服务能力，把瞬时的大数据量放到后面慢速消费掉，从而导致消息丢失、系统奔溃等问题。

- **蓄流压测**：线上有些链路不好压测，可以通过堆积一定量消息再放开来压测。

- **其他用途**：广播订阅、RPC 调用、日志收集、数据同步、数据采集、分布式事务支持等。

#### 局限

- **系统可用性降低**：本来系统运行好好的，如果加个消息队列进去，那消息队列挂了，系统可用性就可能会降低。
- **系统复杂性增加**：增加消息队列组件，要多考虑很多方面的问题，系统复杂性增大，比如一致性问题、如何保证消息不被重复消费、如何保证保证消息可靠传输等。

#### 技术选型

##### 基础对比

| 比较点     | Kafka                                                        | RocketMQ                                          | RabbitMQ                          |
| ---------- | ------------------------------------------------------------ | ------------------------------------------------- | --------------------------------- |
| 设计定位   | 系统间的数据流管道，实时数据处理                             | 非日志的可靠性传输                                | 可靠消息传输                      |
| 使用场景   | 常规的消息系统、网站活性跟踪、监控数据、日志收集、处理等     | 订单、交易、充值、流计算、消息推送、binlog 分发等 | 类似于RocketMQ                    |
| 成熟度     | 日志领域成熟                                                 | 成熟                                              | 成熟                              |
| 所属社区   | Apache                                                       | Alibaba 开发，现已加入到 Apache下                 | Mozilla Public License            |
| 社区活跃度 | 高                                                           | 中                                                | 高                                |
| API完备性  | 高                                                           | 高                                                | 高                                |
| 文档完备性 | 高                                                           | 高                                                | 高                                |
| 开发语言   | Scala                                                        | Java                                              | Erlang                            |
| 支持协议   | 一套自行设计的基于TCP的二进制协议                            | 自定义的一套非JMS协议                             | AMQP                              |
| 客户端语言 | C/C++、Python、Go、Erlang、.NET、Ruby、Node.js、PHP、Java 等 | Java                                              | Java、C、C++、Python、PHP、Perl等 |

##### 功能对比

|                | Kafka                                        | RocketMQ                                                     | RabbitMQs                                                    |
| -------------- | -------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 生产端负载均衡 | 可自由指定                                   | 可自由指定                                                   | 需要单独的Load balancer支持                                  |
| 生产端批量发送 | 支持，默认Producer缓存、压缩，然后批量发送   | 不支持                                                       | 不支持                                                       |
| 消费 失败重试  | 支持，offset存储在ZK中                       | 支持，offset存储在broker中                                   | 支持                                                         |
| 消费方式       | Consumer Pull                                | Consumer pull、Broker push                                   | Broker push                                                  |
| 消费并行度     | 消费并行度和分区数一致                       | 顺序消费：消费并行度和分区数一致；乱序消费：消费服务器的消费线程数之和 | 镜像模式下其实等价于从Master消费                             |
| 顺序消费       | 支持，但是一台broker宕机后，就会产生消息乱序 | 支持，在顺序消息场景下，消费失败时消息队列将会暂定           | 支持，但是如果一个消费失败，消息的顺序会被打乱               |
| 消息重新消费   | 支持，通过修改offset来重新消费               | 支持，按照时间重新消费                                       | -                                                            |
| 定时消息       | 不支持                                       | 开源版本仅支持定时Level                                      | 不支持                                                       |
| 事务消息       | 不支持                                       | 支持                                                         | 不支持                                                       |
| 消息过滤       | 不支持                                       | 支持，通过tag过滤，类似于子topic                             | 不支持                                                       |
| 消息查询       | 不支持                                       | 支持，根据MessageId查询，支持根据MessageKey查询信息          | 不支持                                                       |
| 消息清理       | 指定文件保存时间，过期删除                   | 指定文件保存时间，过期删除                                   | 默认可用内存40%触发GC，GC时找到相邻的两个文件，合并right文件到left |

##### 高可用对比

|            | Kafka                                                        | RocketMQ                                                     | RabbitMQ                                                     |
| ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 存储方式   | 磁盘文件                                                     | 磁盘文件                                                     | 内存、文件                                                   |
| 部署依赖   | Zookeeper                                                    | NameServer                                                   | Erlang环境                                                   |
| 部署方式   | 单机、集群                                                   | 单机、集群                                                   | 单机、集群                                                   |
| 集群管理   | Zookeeper                                                    | NameServer                                                   | -                                                            |
| 选主方式   | 从 ISR 中自动选举一个 Leader                                 | 不支持自动选主，brokername相同，brokerid = 0时为Master了，其他为Slave | 最早加入集群的broker                                         |
| 主从切换   | 支持，一主多备，Master失效后自动从ISR中选择一个主            | 不支持，Master失效以后不能消费，Consumer默认30s可以感知此事，此后从Slave消费，如果Master无法恢复，异步复制时可能会丢失部分消息 | 支持，最早加入集群的Slave会成为Master，由于新加入的Slave不会同步Master之前的数据，所以可能会丢失部分数据 |
| 复制备份   | 消息写入到Leader的log，Followers从Leader中pull，pull到数据以后先ack Leader，然后写入log中，ISR中维护与Leader同步的列表，落后太多的Follower会被删除掉 | 同步双写、异步复制（Slave启动线程从Master中拉数据）          | 普通模式下不复制；镜像模式下，消息先到Master，然后镜像到Slave上，入集群之前的消息不会被复制到新的Slave上 |
| 可用性     | 非常高，分布式、主从                                         | 非常高，分布式、主从                                         | 高，主从、镜像模式（数据量大时可能产生性能瓶颈）             |
| 数据可靠性 | 很好，支持Producer单条发送、同步复制，但这种场景下性能明显下降 | 很好，Producer单条发送，broker端支持同步刷盘、异步刷盘、同步双写、异步复制 | 好，Producer支持同步、异步ACK，支持队列数据持久化，镜像模式中支持主从同步 |

##### 运维对比

|              | Kafka                                                        | RocketMQ                                                     | RabbitMQ                                                 |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | -------------------------------------------------------- |
| 系统维护     | Scala语言开发，维护成本高                                    | Java语言开发，维护成本低                                     | Erlang语言开发，维护成本高                               |
| 访问权限控制 | 无                                                           | 无                                                           | 支持配置用户名和密码                                     |
| 管理后台     | 官网不提供，第三方开源管理工具可供使用，不用重新开发         | 官方提供，rocketmq-console                                   | 官方提供，rabbitmqadmin                                  |
| 管理后台功能 | broker、topic、partition、logsize、consumer groups、offset、cluster | cluster、topic、connection、nameserv、message、broker、offset、consumer | overview、connection、channels、exchanges、queues、admin |

#### 技术选型总结

|                  | Kafka                                                        | RocketMQ                                                     | RabbitMQ                                                     | Java                      |
| ---------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------- |
| 开发语言         | Scala                                                        | Java                                                         | Erlang                                                       | Java                      |
| 单机吞吐量       | 10w级别                                                      | 10w级别                                                      | 1w级别                                                       | 1w级别                    |
| 消息写入性能     | 非常好，每条10个字节测试下，100w/s                           | 很好，每条10个字节测试下，单机单broker约7w/s，单机3个broker约12w/s | RAM模式约为RocketMQ的1/2，Disk模式性能约为RAM模式的1/3       | -                         |
| 消息投递实时性   | 毫秒级，具体由Consumer轮询间隔时间决定                       | 毫秒级，支持pull、push两种模式，延时通常在毫秒级             | 毫秒级                                                       | 毫秒级                    |
| 单机支持的队列数 | 单机超过64个队列/分区，Load会发生明显的飙高，队列越多，Load越高，发送消息的响应时间越长 | 单机支持最高5w个队列，并且Load不会发生明显变化               | 依赖于内存                                                   | -                         |
| 堆积能力         | 非常好，消息存储在log中，每个分区一个log文件                 | 非常好，所有消息存储在同一个commit log中                     | 一般，生产者、消费者正常时，性能表现稳定；消费者不消费时，性能不稳定 | -                         |
| 性能稳定性       | 队列/分区多时，性能不稳定、明显下降；消息堆积时，性能稳定    | 队列较多、消息堆积时，性能都稳定                             | 消息堆积时，性能不稳定、明显下降                             | -                         |
| 可用性           | 非常高，分布式、主从                                         | 非常高，分布式、主从                                         | 高，主从、镜像模式                                           | 高，Master-Slave、Network |
| 数据可靠性       | 很好，支持Producer单条发送、同步复制，但这种场景下性能明显下降 | 很好，Producer单条发送，broker端支持同步刷盘、异步刷盘、同步双写、异步复制 | 好，Producer支持同步、异步ACK，支持队列数据持久化，镜像模式中支持主从同步 | 有较低概率丢失数据        |

##### 为什么使用 Kafka？

高可用，几乎所有相关的开源软件都支持，满足大多数的应用场景，尤其是**大数据和流计算**领域。

- **优势**：
  - 高性能，高吞吐，消息持久化，可伸缩，支持分区、副本和容错。
  - 对批处理和异步处理做了大量的设计，因此可以得到非常高的性能，每秒处理几十万异步消息，如果开启了压缩，最终可以达到每秒处理 2000w 消息的级别。
- **局限**：
  - 由于是异步的和批处理的，延迟也会高，不适合电商场景。
  - 不支持事务。
  - 消费集群数目受 Partition 数目限制。
  - 单机 Topic 多时，性能会明显降低。
- **使用建议**：
  1. Kafka 提供的消息中间件的功能明显较少一些，相对上述几款 MQ 中间件要少很多。
  2. 但是 Kafka 优势在于，专为超高吞吐量的实时日志采集、实时数据同步、实时数据计算等场景来设计。
  3. 因此，Kafka在大数据领域中配合实时计算技术（比如Spark Streaming、Storm、Flink）使用的较多，在传统的 MQ 中间件使用场景中较少采用。
  4. 如果是大数据领域的**实时计算、日志采集**等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。

##### 为什么使用 RocketMQ？

借鉴了Kafka的设计并做了很多改进，几乎具备了消息队列应该具备的**所有特性和功能**。

- **优势**：
  - 主要用于有序，事务，流计算，消息推送，日志流处理，binlog分发等场景。
  - 对电商领域的响应延迟做了很多优化，每秒处理**几十万**的消息，同时响应在毫秒级，如果应用很关注响应时间，可以使用RocketMQ，性能比RabbitMQ高一个数量级，同时，经过了历次的双11考验，性能，稳定性可靠性没的说。
  - Java开发，阅读源代码、扩展、二次开发很方便。
- **局限**：
  - 消息堆积、吞吐量上不如Kafka。
  - 不支持主从自动切换，master失效后，消费者需要一定的时间才能感知。
  - 客户端只支持Java，跟周边系统的整合和兼容不是很好。
- **使用建议**：
  1. RocketMQ，是阿里开源的，经过阿里的生产环境的超高并发、高吞吐的考验，性能卓越，同时还支持分布式事务等特殊场景。
  2. 而且RocketMQ是基于Java语言开发的，适合深入阅读源码，有需要可以站在源码层面解决线上生产问题，包括源码的二次开发和改造。
  3. RocketMQ，确实很不错，但社区活跃度其实不算高，可能有突然黄掉的风险，如果对公司技术实力有绝对自信的，推荐用 RocketMQ，否则回去老老实实用 RabbitMQ 吧，毕竟 RabbitMQ 有活跃的开源社区，绝对不会黄，所以**大型公司，基础架构研发实力较强**，用 RocketMQ 是很好的选择。

##### 为什么使用 RabbitMQ？

RabbitMQ开始是用在电信业务的可靠通信的，也是少有的几款**支持AMQP**协议的产品之一。

- **优势**：
  - 轻量级、低延迟，快速，部署使用方便。
  - 支持灵活的路由配置，在生产者和队列之间有一个交换器模块，根据配置的路由规则，生产者发送的消息可以发送到不同的队列中，路由规则很灵活，还可以自己实现。
  - 客户端支持大多数的编程语言，支持**AMQP**协议。
- **局限**：
  - 消息吞吐能力有限，如果有大量消息堆积在队列中，性能会急剧下降，每秒处理**几万到几十万**的消息，如果应用要求高的性能，注意不要选择 RabbitMQ。
  - 不支持事务。
  - 集群不支持动态扩展，横向扩展能力不是那么好。
  - RabbitMQ是Erlang开发的，功能扩展和二次开发代价很高。
- **使用建议**：
  1. 好处在于可以支撑高并发、高吞吐、性能很高，同时有非常完善便捷的后台管理界面可以使用，还支持集群化、高可用部署架构、消息高可靠支持，功能较为完善。
  2. 经过调研，国内各大互联网公司落地大规模 RabbitMQ 集群支撑自身业务的 case 较多，国内各种中小型互联网公司使用 RabbitMQ 的实践也比较多，同时开源社区很活跃，较高频率的迭代版本，来修复发现的bug以及进行各种优化，因此综合考虑过后，可以采取 RabbitMQ。
  3. 但是 RabbitMQ 也有一点缺陷，就是它自身是基于 Erlang 语言开发，导致较为难以分析里面的源码，也较难进行深层次的源码定制和改造，毕竟需要较为扎实的 Erlang 语言功底才可以。
  4. RabbitMQ 的 Erlang 语言阻止了大量的 Java 工程师去深入研究和掌控它，对公司而言，几乎处于不可控的状态，但是确实人家是开源的，比较稳定的支持，活跃度也高，管理功能很丰富，所以**中小型公司，技术实力较为一般，技术挑战不是特别高**，用 RabbitMQ 是不错的选择。

##### 为什么使用 ActiveMQ？

ActiveMQ，是老牌的消息中间件，国内很多公司过去运用的还是非常广泛的，功能很强大。

- **使用建议**：
  1. 由于 ActiveMQ 无法支撑互联网公司的高并发、高负载以及高吞吐的复杂场景，因此，在国内互联网公司落地较少，使用较多的还是一些**传统企业**，用 ActiveMQ 进行异步调用和系统解耦。
  2. 一般的业务系统最早都用 ActiveMQ，但是现在确实用的不多了，而且没经过大规模吞吐量场景的验证，社区也不是很活跃，所以还是算了吧，不推荐使用。

#### 如何设计消息队列？

##### 生产端可靠性投递

如果涉及的是金融相关的业务， 需要保证消息一定不能丢失，做到生产端发出消息时跟数据库操作保持一个原子性操作。

##### 幂等性消费

由于生产者需要做到可靠性投递，可能会出现重复的消息，如果重复的消息被消费者消费了两次或者多次，可能会导致数据的不一致，所以，此时消费端一定要做到一个幂等性的验证。

##### 高可用性

如果MQ一个Broker挂掉了， 应该如何保证服务的高可用？ => HA

##### 低延迟

在MQ面对巨量的流量冲压下，应该如何消息写入的低延迟？是否会给系统带来瓶颈？

##### 可靠性

在消息落到MQ，应该如何保障肯定不会丢失？比如磁盘损坏 => 副本的方式

##### 堆积能力

某个业务场景下，到底有多少的数据量，大体预估消息高峰期会堆积到什么程度？MQ能不能抗住流量的冲击？

##### 可扩展性

MQ能否天然的支持无感知横向扩容？

### 1.2. 什么是 JMS？

#### 概念

- JMS，Java Message Service，Java 消息服务，定义了 Java 中访问消息中间件接口的规范，只是接口并没有给予实现，而实现 JMS 接口的消息中间件称为 JMS Provider。
- 目前知名的MOM（Message Oriented Middleware，消息中间件）包括： ActiveMQ、RocketMQ、Kafka 以及 RabbitMQ，他们都是**基本遵循**或者**参考**JMS规范，都有自己的特点和优势。

| 专业术语          | 释义                                                         |
| ----------------- | ------------------------------------------------------------ |
| JMS               | Java Message Service，实现 JMS 接口的消息中间件              |
| Provider          | Message Provider，消息的生产者                               |
| Consumer          | Message Consumer，消息的消费者                               |
| PTP               | Point to Point，点对点的消息模型                             |
| Pub/Sub           | Publish/Subscribe，发布/订阅模型                             |
| Queue             | 消息队列，一般都是会真正的进行物理存储                       |
| Topic             | 主题目标                                                     |
| ConnectionFactory | 连接工厂，JMS 用它来创建连接                                 |
| Connection        | JMS 客户端到 JMS Provider 的连接                             |
| Destination       | 消息的目的地，指对于生产者来说是消息发送目标，对于消息消费者来说是消息来源 |
| Session           | 会话，一个发送或接收消息的线程，好比Mybatis的Session         |

#### 消息投递模式

##### PTP模式

P2P模式，点对点模式，包含三个角色：消息队列（Queue），发送者（Sender），接收者（Receiver），每个消息都被发送到一个特定的队列，接收者从队列中获取消息，在这期间，队列保留着消息，直到他们被消费或超时。

- **特点**：
  - 每个消息只有一个消费者，即一旦被消费，消息就不再在消息队列中。
  - 发送者和接收者之间在时间上没有依赖性，也就是说当发送者发送了消息之后，不管接收者有没有正在运行，它不会影响到消息被发送到队列。
  - 接收者在成功接收消息之后需向队列应答成功。
- **适用场景**：如果希望发送的**每个消息**都会被成功处理的话，那么需要P2P模式。

![1633184806569](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633184806569.png)

##### Pub/Sub模式

Pub/Sub模式，发布订阅模式，包含三个角色：主题（Topic），发布者（Publisher），订阅者（Subscriber） ，多个发布者将消息发送到Topic，系统将这些消息传递给多个订阅者。

- **特点**：
  - 每个消息可以有多个消费者。
  - 发布者和订阅者之间有时间上的依赖性，针对某个主题的订阅者，它必须创建一个订阅者之后，才能消费发布者的消息。
  - 为了消费消息，订阅者必须保持运行的状态，另外，为了缓和这样严格的时间相关性，JMS允许订阅者创建一个可持久化的订阅，使得即使订阅者没有在运行，它也能接收到发布者的消息。
- **适用场景**：如果希望发送的消息可以**不被做任何处理**、或者**只被一个消息者处理**、或者可**以被多个消费者处理**的话，那么可以采用Pub/Sub模型。

![1633185288754](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633185288754.png)

### 1.3. 详细介绍ActiveMQ？

##### 概念

- ActiveMQ，Apache出品，是当前最流行的、能力强劲的开源消息总线。
- ActiveMQ，是一个完全支持JMS1.1和J2EE 1.4规范的 JMS Provider实现，尽管JMS规范出台已经是很久的事情了，但是JMS在当今的J2EE应用中间仍然扮演着特殊的地位。
- 如果想对大规模、高并发应用服务做消息中间件技术选型，譬如淘宝、京东这种大型的电商网站，尤其是双 11 这种特殊时间，ActiveMQ 可能就显得力不从心了。

##### 特点

1. 多种语言和协议编写客户端。
   - 语言：Java、C、C++、C#、Ruby、Perl、Python、PHP。
   - 应用协议：OpenWire、Stomp REST、WS Notification、XMPP、AMQP.
2. 完全支持 JMS1.1 和 J2EE 1.4 规范 
   - 持久化，XA消息，事务。
3. 对Spring的支持：
   - ActiveMQ可以很容易内嵌到使用Spring的系统里面去，而且也支持Spring2.0的特性。
4. 通过了常见 J2EE 服务器（如 Geronimo、JBoss 4、GlassFish、WebLogic）的测试：
   - 其中通过 JCA 1.5 resource adaptors 的配置，可以让 ActiveMQ可以自动的部署到任何兼容 J2EE 1.4 商业服务器上。
5. 支持多种传送协议：
   - in-VM、TCP、SSL、NIO、UDP、JGroups、JXTA。
6. 支持通过 JDBC 和 journal 提供高速的消息持久化。
7. 从设计上保证了高性能的集群：
   - 客户端-服务器，点对点。
8. 支持Ajax。
9. 支持与Axis的整合。
10. 可以很容易得调用内嵌 JMS provider，进行测试。

##### 指标

衡量一个 MOM 消息中间件，主要从三方面考虑即可，即服务性能、存储堆积能力（数据存储）和可扩展性（集群架构）。

- **服务性能**： ActiveMQ 的性能一般，在早期传统行业为王的时代比较流行，但现如今面对高并发、大数据的业务场景，往往力不从心。
- **数据存储**：默认采用 Kahadb（索引文件形式）存储，也可以使用高性能的 google leveldb（内存数据库存储），或者可以使用 MySQL、Oracle 关系型数据库进行消息存储。
- **集群架构**：ActiveMQ 可以与 Zookeeper 构建成主备集群模型，并且多套的主备模型直接可以采用 Network 的方式构建分布式集群。

##### 集群架构模式

ActiveMQ 最经典的两种集群架构模式为：Master-Slave、Network。

- **Master-Slave**：

  - **概念**：顾名思义，即主从，或者主备方式（双机热备机制），是目前 ActiveMQ 推荐的高可靠性和容错的解决方案。
  - **原理**：如果消息被复制到 Slave Broker后，即使 Master Broker 遇到了像硬件故障之类的错误，也可以立即切换到 Slave Broker 而不丢失任何消息。
  - **缺点**：做不到分布式的 topic 和 queue，当消息量巨大时，MQ 集群压力就会过大，没办法满足分布式的需求。

  ![1633244456251](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633244456251.png)

- **Network**：

  - **概念**：可以理解为网络通信方式，也可以叫 Network of Brokers，其消息会进行均衡，真正解决了分布式消息存储、故障转移和 Broker 切换的问题。
  - **原理**：一个 Broker 会相同对待所有的订阅，不管他们是来自本地的客户连接，还是来自远程 Broker，都会递送有关的消息拷贝到每个订阅，远程 Broker 得到这个消息拷贝后，会依次把它递送到其内部的本地连接上。
  - **缺点**：
    1. 部署非常麻烦，需要两套或者多套集群直接相互交叉配置，相互间能够感知到彼此的存在。
    2. 虽然解决了分布式消息队列的问题，但还有许多潜在的问题，比如资源浪费问题。
       - 通常采用 Master-Slave 模型是传统型互联网公司的首选，而互联网公司往往会选择开箱即用的消息中间件，从运维、部署、使用各个方面都要优于 ActiveMQ。

  ![1633244714673](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633244714673.png)

### 1.4. 什么是AMQP？

AMQP，Advanced Message Queuing Protocol，高级消息队列协议，是具有现代特征的二进制协议，是一个提供统一消息服务的应用层标准高级消息队列协议，是应用层协议的一个**开放标准**，为面向消息的中间件设计。

- **Publisher application**：生产者应用程序，与 Exchage 是多对多的关系。
- **Consumer application**：消费者应用程序，与 Queue 是多对多的关系。
- **Server**：消息队列服务器，物理机。
- **Virtual host**：虚拟主机，用于划分模型域。
- **Exchange**：交换机，类似于主题，是一个逻辑的概念，需要跟 Queue 绑定起来，与 Queue 是多对多的关系。
  - 生产者可以向 Exchange 投递消息，Exchange 会根据一定规则把消息路由到某几个队列中。
- **Message Queue**：消息队列，是一个逻辑的概念，与 Exchange 是多对多的关系。
  - 消费者可以从 Queue 消费消息。

![1633250094430](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633250094430.png)

### 1.5. 详细介绍RabbitMQ？

#### 概念

RabbitMQ，是一个开源的、基于Erlang 语言编写的、实现了 AMQP 协议的分布式消息队列系统，采用经典和新颖的消息传递方式，进行跨应用共享数据，其消息路由的强大（广播、直连、主题、首部交换机等）以及容易使用（只需添加和删除消费者即可完成扩展和缩小），使得能够在 MQ 中脱颖而出。

- **通用用途**：异步处理、系统解耦、削峰填谷。
- **特色用途**：广播订阅、RPC 调用。

| 专业术语    | 释义                                                         |
| ----------- | ------------------------------------------------------------ |
| Server      | 又称 Broker， 指消息队列服务器实体，用于接受客户端的连接     |
| Connection  | 连接，应用程序与 Broker 建立的网络连接                       |
| Channel     | 网络信道，几乎所有的操作都在 Channel 中进行，是进行消息读写的通道（通过 Connection是发送不了消息的），客户端可建立多个 Channel，每个 Channel 代表一个会话任务 |
| Message     | 消息，服务器和应用程序之间传递的数据，由 Properties 和 Body 组成，Properties 可以对消息进行修饰，比如消息的优先级、延迟等高级特性，Body 则是消息体的内容 |
| VHost       | Virtual host，虚拟地址，用于进行**逻辑隔离**，是最上层的消息路由，一个 Vhost 里面可以有若干个 Exchange 和 Queue，但它们的名称不能相同；Vhost 拥有独立的权限系统，可以做到 Vhost 范围的用户控制。 |
| Exchange    | 交换机，接收生产者投递的消息，生产者可以在发送消息时带上 Routing Key，交换机会根据 Routing Key 路由消息到绑定的队列 |
| Binding     | Exchange 和 Queue 之间的虚拟连接，可以包含 Routing Key，把 Exchange 和 Queue按照Routing Key 绑定起来 |
| Routing Key | 路由关键字，一个路由规则，虚拟机可以用来确定如何路由一个消息 |
| Queue       | 又称 Message Queue，消息队列，保存消息并将它们转发给消费者，每个消息可能会被路由到一个或者多个队列 |
| Producer    | 消息生产者，指投递消息的程序                                 |
| Consumer    | 消息消费者，指消费消息的程序                                 |

#### 原理

![1633680788174](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633680788174.png)

1. 一方面，生产者向 Exchange 发送消息。
2. Exchange （比如根据 RoutingKey ）把消息路由到 Queue 或者其他 Exchange中。
3. RabbitMQ Broker 在收到消息时，向生产者发送确认。
4. 另一方面，消费者与 RabbitMQ 保持 TCP 长连接，并声明所消费的 Queue。
5. 消息路由到 Queue 后，RabbitMQ 会向消费者**推送消息**。
   - **为什么推而不是拉？**
     1. 推比拉消息分布更均匀，更适合低延迟。
     2. 当某单个队列出现竞争消费者时，如果每个消费者都拉取消息，由于每个消息者拉取的数量不同，消息分布可能会变得非常不均匀。
     3. 消息分布越不均匀，延迟就越多，消费时消息排序的丢失就越严重。
6. 消息消费完毕，消费者会把消费结果（成功或者失败）返回给 RabbitMQ Broker。
7. RabbitMQ Broker 一旦发现消费者消费成功，就会把消息就从 Queue 中移除。

#### API

##### 原生 POM 依赖

```xml
<dependency>
    <groupId>com.rabbitmq</groupId>
    <artifactId>amqp-client</artifactId>
    <version>3.6.5</version>
</dependency>
```

##### 生产者 | HelloWorld

```java
public static void main(String[] args) throws IOException, TimeoutException {
    // 1. 创建ConnectionFactory
    ConnectionFactory connectionFactory = new ConnectionFactory();
    connectionFactory.setHost(HOST);
    connectionFactory.setPort(PORT);
    connectionFactory.setVirtualHost(VIRTUAL_HOST);

    // 2. 创建Connection
    Connection connection = connectionFactory.newConnection();

    // 3. 创建Channel
    Channel channel = connection.createChannel();

    // 4. 创建Queue: 一般不在Java中创建, 而是提前维护好队列
    // Queue.DeclareOk queueDeclare(String queue, boolean durable, boolean exclusive, boolean autoDelete, Map<String, Object> arguments) throws IOException;
    channel.queueDeclare(ROUTING_KEY, false, false, false, null);

    // 5. 构建消息
    AMQP.BasicProperties props = new AMQP.BasicProperties.Builder()
        .deliveryMode(2)
        .contentEncoding("UTF-8")
        .headers(new HashMap<String, Object>())
        .build();

    // 6. 发送消息到Queue中
    for(int i = 0; i < 5; i++){
        String msg = "Hello World RabbitMQ" + i;
        // 注意, 这里EXCHANGE_NAME为"", 表示采用了默认的交换机(Direct模式), 因此, 表示把消息路由到名称为ROUTING_KEY的Queue上
        // void basicPublish(String exchange, String routingKey, BasicProperties props, byte[] body) throws IOException;
        channel.basicPublish("", ROUTING_KEY, props, msg.getBytes());
        System.out.println("发送消息完毕...");
    }
}
```

##### 消费者 | HelloWorld

```java
public static void main(String[] args) throws IOException, TimeoutException, InterruptedException {
    // 1. 创建ConnectionFactory
    ConnectionFactory connectionFactory = new ConnectionFactory();
    connectionFactory.setHost(HOST);
    connectionFactory.setPort(PORT);
    connectionFactory.setVirtualHost(VIRTUAL_HOST);
    connectionFactory.setAutomaticRecoveryEnabled(true);
    connectionFactory.setNetworkRecoveryInterval(3000);

    // 2. 创建Connection
    Connection connection = connectionFactory.newConnection();

    // 3. 创建Channel
    Channel channel = connection.createChannel();

    // 4. 创建Queue: 生产者创建了消费者就不用创建了
    // channel.queueDeclare(ROUTING_KEY, false, false, false, null);

    // 5. 创建Consumer
    QueueingConsumer queueingConsumer = new QueueingConsumer(channel);
    // 由于采用了默认的交换机(Direct模式), 因此ROUTING_KEY的消息会路由到名为ROUTING_KEY的Queue中, 所以消费者去ROUTING_KEY队列中拿消息
    // String basicConsume(String queue, boolean autoAck, Consumer callback) throws IOException;
    channel.basicConsume(ROUTING_KEY, true, queueingConsumer);

    // 6. 拉取 & 监听消息
    System.out.println("开始监听消息...");
    while (true){
        QueueingConsumer.Delivery delivery = queueingConsumer.nextDelivery();
        String msg = new String(delivery.getBody());
        System.out.println("收到消息: " + msg);
    }
}
```

##### Exchange 属性

Exchange，交换机，接收消息，并根据 ROUTING_KEY 转发消息所绑定的队列。

| 属性        | 释义                                                         |
| ----------- | ------------------------------------------------------------ |
| Name        | 交换机名称                                                   |
| Type        | 交换机类型， direct、topic、fanout、headers                  |
| Durability  | Exchange 是否需要持久化，true表示需要持久化，一般生产都是需要持久化的 |
| Auto Delete | 当最后一个绑定到 Exchange 上的队列被删除后，是否自动删除该 Exchange，不常用，一般生产都是不需要自动删除的 |
| Internal    | 当前 Exchange 是否用于 RabbitMQ 内部使用，外部无法访问；默认为 False，代表外部使用，外部可以访问 |
| Arguments   | 扩展参数，用于扩展 AMQP 协议，来自动化使用，比如特殊功能、或者带延迟功能的交换机 |

###### Direct Exchange

![1633266272323](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633266272323.png)

Direct Exchange，直连交换机，所有发送到 Direct Exchange 的消息，都会被转发到 RoutingKey 指定的 Queue 中。

- 直连模式下，可以不需要对 Exchange 进行任何绑定 binding 操作，比如此时 Exchange 为 ""，则默认使用 RabbitMQ 自带的 Exchange（**AMQP.default**），此时 RoutingKey 为 Queue 的名称。
- 在其他消息传递时，发送时指定的 RoutingKey 必须和 Queue 名称完全匹配时，该消息才会被 Queue 接收，否则该消息会被抛弃。

```java
// 生产者 - 发送消息msg到EXCHANGE_NAME
channel.basicPublish(EXCHANGE_NAME, ROUTING_KEY, null, msg.getBytes());

// 消费者 - Queue绑定Exchange: 表示交换机EXCHANGE_NAME上ROUTING_KEY的消息会路由到QUEUE_NAME中
channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, ROUTING_KEY);
// 消费者 - 绑定消费队列
// String basicConsume(String queue, boolean autoAck, Consumer callback) throws IOException;
channel.basicConsume(QUEUE_NAME, true, queueingConsumer);
// 消费者 - 开始消费队列
QueueingConsumer.Delivery delivery = queueingConsumer.nextDelivery();
```

###### Topic Exchange

![1633329219405](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633329219405.png)

Topic Exchange，主题交换机，所有发送到 Topic Exchange 的消息，都会被转发到所有关心该 RoutingKey #Topic 的 Queue上。

- Queue 通过绑定一个 Topic，如果 Exchange 发现某个 RoutingKe 能够与该 Topic 进行**模糊匹配**，则会把消息投递到该 Queue 中。
  - `#` 表示匹配一个或多个词，比如 "log.info.oa" 能够匹配 `log.#`，则 RoutingKey 为 "log.info.oa" 的消息，将会被 Exchange 投递到 Topic 为 `log.#` 的 Queue 中。
  - `*` 表示只能匹配一个词，比如 "log.erro" 能够匹配 `log.*`，则 RoutingKey 为 "log.erro" 的消息，将会被 Exchange 投递到 Topic 为 `log.*` 的 Queue 中。
- Queue 和 Exchange 是多对多的关系，但如果业务上使用多对多的关系，会导致消息投递逻辑比较乱，建议一类消息只搞一种规则（比如 `test.#`），只投递到一个 Exchange 中，一个 Exchange 绑定多个 Queue，然后消费者从 Queue 消费消息。

```java
// 生产者 - topic类型交换机: 实际Routing_key
String routingKey1 = "user.save";
String routingKey2 = "user.update";
String routingKey3 = "user.delete.abc";
// 生产者 - 发送消息msg到EXCHANGE_NAME
channel.basicPublish(EXCHANGE_NAME, routingKey1, null, msg.getBytes());
channel.basicPublish(EXCHANGE_NAME, routingKey2, null, msg.getBytes());
channel.basicPublish(EXCHANGE_NAME, routingKey3, null, msg.getBytes());

// 消费者1 - 声明交换机(一般手动创建，不在Java代码中创建)
channel.exchangeDeclare(EXCHANGE_NAME, "topic", true, false, false,null);
// 消费者1 - 可以匹配"user.save"、"user.update"、"user.delete.abc"的消息
String routingKey = "user.#"; 
// 消费者1 - Queue绑定Exchange: 表示EXCHANGE匹配"user.*"的消息会路由到QUEUE中
channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, routingKey);
// 消费者1 - 绑定消费队列
channel.basicConsume(QUEUE_NAME, true, queueingConsumer);
// 消费者1 - 开始消费队列
QueueingConsumer.Delivery delivery = queueingConsumer.nextDelivery();

// 消费者2 - 声明交换机(一般手动创建，不在Java代码中创建)
channel.exchangeDeclare(EXCHANGE_NAME, "topic", true, false, false,null);
// 消费者2 - 可以匹配"user.save"、"user.update"的消息
String routingKey2 = "user.*"; 
// 消费者2 - Queue绑定Exchange: 表示EXCHANGE匹配"user.*"的消息会路由到QUEUE中
channel.queueBind(QUEUE_NAME_2, EXCHANGE_NAME, routingKey2);
// 消费者2 - 绑定消费队列
channel.basicConsume(QUEUE_NAME_2, true, queueingConsumer);
// 消费者2 - 开始消费队列
QueueingConsumer.Delivery delivery = queueingConsumer.nextDelivery();
```

###### Fanout Exchange

![1633329283915](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633329283915.png)

Fanout Exchange，广播交换机，只需要简单的将 Queue 绑定到 Exchange 上，不处理任何 RoutingKey，发送到 Exchange 的消息，都会被转发到与该 Exchange **绑定**的所有 Queue 上，其消息转发效率是**最高**的。

- 从效率来讲，由于 key 匹配速度不同，Fanout Exchange > Direct Exchange > Topic Exchange，所以 Exchange 的选用应该尽量简单，没必要把时间花在 key 的解析和匹配上。

```java
// 生产者 - Fanout类型的交换机不走路由键，所以设不设置Routing_key都不起作用
String routingKey = "";
// 生产者 - 发送消息msg到EXCHANGE_NAME
channel.basicPublish(EXCHANGE_NAME, routingKey, null, msg.getBytes());

// 消费者 - 声明交换机(一般手动创建，不在Java代码中创建)
channel.exchangeDeclare(EXCHANGE_NAME, "fanout", true, false, false,null);
// 消费者 - Fanout类型的交换机不走路由键，所以设不设置Routing_key都不起作用
String routingKey = "";
// 消费者 - Fanout交换机，只需要Queue绑定到Exchange上即可
channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, routingKey);
// 消费者 - 绑定消费队列
channel.basicConsume(QUEUE_NAME, true, queueingConsumer);
// 消费者 - 开始消费队列
QueueingConsumer.Delivery delivery = queueingConsumer.nextDelivery();
```

###### Headers Exchange

Headers Exchange，首部模式，不处理任何 RoutingKey，而是根据发送消息内容中的 `headers` 属性进行匹配。

**其工作原理为**：

1. 在绑定 Queue 与 Exchange 时，指定一组 Key-Value 作为 Header，其值可以是任意类型，而不像 fanout，direct，topic 的 RoutingKey 那样只能是字符串形式。
2. 当消息发送到 RabbitMQ 时，会取到该消息的 `headers` 与 Exchange 绑定时指定的 Key-Value 进行匹配。
3. 如果匹配，则消息会路由到该 Queue，否则不会路由到该 Queue，其匹配规则 `x-match` 有下列两种类型：
   - **x-match = all** ：表示所有的 Key-Value 全部匹配，才能接收到消息。
   - **x-match = any** ：表示只要有 Key-Value 存在匹配，就能接收到消息。

```java
// 生产者 - x-match = all，根据用户设置去通知用户，设置email的用户只接收email的消息，设置sms的用户只接收sms的消息，设置两种类型都设置的则两种消息都能收到。
String message = "email inform to user" + i;
Map<String,Object> headers =  new Hashtable<String, Object>();
headers.put("inform_type", "email");// 匹配email消费者绑定的header
// 生产者 -  headers.put("inform_type", "sms");// 匹配sms消费者绑定的header
AMQP.BasicProperties.Builder properties = new AMQP.BasicProperties.Builder();
properties.headers(headers);
// 生产者 - email通知
channel.basicPublish(EXCHANGE_NAME, "", properties.build(), message.getBytes());

// 消费者 - 声明交换机(一般手动创建，不在Java代码中创建)
channel.exchangeDeclare(EXCHANGE_HEADERS_INFORM, BuiltinExchangeType.HEADERS);
// 消费者 - headers.put("inform_type", "sms");// 匹配sms消费者绑定的header
Map<String, Object> headers_email = new Hashtable<String, Object>();
headers_email.put("inform_email", "email");
// 消费者 - Headers交换机，不匹配任何RountingKey，根据Headers进行匹配
channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, "", headers_email);
// 消费者 - 绑定消费队列
channel.basicConsume(QUEUE_NAME, true, queueingConsumer);
// 消费者 - 开始消费队列
QueueingConsumer.Delivery delivery = queueingConsumer.nextDelivery();
```

###### x-consistent-hash Exchange (>= 3.6.0版本)

![1633699257907](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633699257907.png)

x-consistent-hash Exchange，一致性哈希交换机，根据 RoutingKey 计算出一个 hash 值，然后使用**一致性哈希算法**，根据这个 hash 值，把消息分发到绑定在该交换机下的队列中。因此，如果没有交换机和队列发生绑定更改，具有相同 RoutingKey 的消息，具有相同的 hash 值，将被路由到同一个队列中。

- **背景**：x-consistent-hash Exchange，是在从 RabbitMQ 3.6.0 版本开始，才整合到 RabbitMQ 发布包中的，对于之前的版本，需要手动下载插件去安装。
- **权重**：绑定键 BindingKey 使用一个**数字字符串**，表示所绑定的权重，数字越大，绑定队列的权重就越大，分发消息时能接收到的消息就越多。
  - 绑定键 BindingKey 决定队列的权重。
  - 路由键 RoutingKey 决定消息的分发。
- **局限**：
  - **数据不均匀**：在节点太少时，容易出现节点分布不均匀，从而导致数据倾斜。

##### 常见工作模式

###### Point to Point | 点对点模式

![1633702087552](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633702087552.png)

- 实现方法：Direct Exchange 直连交换机 +  Exchange  = "" + RountingKey = Queue_name + 一个消费者消费一个 Queue。

###### Work Queues |  工作队列模式

![1633700659483](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633700659483.png)

- **实现方法**：Direct Exchange 直连交换机 + Exchange  = "" + RountingKey = Queue_name + 多个消费者消费同一个队列 Queue。

###### Publish/Subscribe | 发布订阅模式

![1633700920376](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633700920376.png)

- **实现方法**：Fanout Exchange 广播交换机 + Exchange != "" + RountingKey = "" + 绑定 Exchange & Queue + 多个消费者消费同一个/多个 Queue。

###### Routing | 路由模式

![1633701310453](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633701310453.png)

- **实现方法**：Direct Exchange 直连交换机 + Exchange != "" + 多个 RountingKey + 多个 Queue 分别通过 BiningKey = RountingKey 绑定Exchange + 多个消费者分别消费不同的 Queue。

###### Topics | 主题模式

![1633701632932](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633701632932.png)

- **实现方法**：Topic Exchange 主题交换机 + Exchange != "" + 多个 RoutingKey + 多个 Queue 分别通过BiningKey（模糊匹配）绑定Exchange + 多个消费者分别消费不同的 Queue。

###### Headers | 首部模式

![1633702207503](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633702207503.png)

- **实现方法**：Headers Exchange 首部交换机 + Exchange != "" + RountingKey = "" + 多个 Queue 分别通过 Headers 绑定Exchange + 多个消费者消费不同的 Queue。

###### RPC 模式

![1633702404965](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633702404965.png)

- **实现方法**：RPC 模式，指客户端远程调用服务端的方法 ，使用 MQ 可以实现 RPC 的异步调用，基于 Direct Exchange **直连交换机**实现，流程如下：
  1. 客户端既是生产者也是消费者，它会向 RPC 请求队列发送 RPC 调用消息，同时监听 RPC 响应队列。
  2. 服务端监听 RPC 请求队列的消息，收到消息后执行服务端的方法，得到方法返回的结果后，再将 RPC 结果发送到 RPC 响应队列。
  3. 由于客户端（RPC 调用方）监听 RPC 响应队列，所以此时会接收到 RPC 调用结果，完成一次 MQ 的 RPC 异步调用。

###### 工作模式总结

- Exchange 交换机，类似于反向代理用的 Nginx 服务器，不过 Nginx 负责请求的转发， 而 Exchagne 负责消息的转发。
- RabbitMQ 消息的传递，整体流程为『 生产者 -> 交换机 -> 队列 -> 消费者 』的这么一个模式，其中的点对点模式和工作队列模式，可以理解成是一个**匿名的交换机**进行投递队列，此时 RoutingKey = Queue_name。

| 工作模式     | 应用场景                                           |
| ------------ | -------------------------------------------------- |
| 点对点模式   | 把消息固定投放到一个队列，且只需一个消费者         |
| 工作队列模式 | 把消息固定投放到一个队列，但需要多个消费端加快消费 |
| 发布订阅模式 | 把消息投递同时投递到多个队列一起消费               |
| 路由模式     | 把消息固定投放到多个队列                           |
| 主题模式     | 按照一定规则，把消息投递到多个队列                 |
| 首部模式     | 不处理任何 RoutingKey，而是根据 `headers` 进行匹配 |
| RPC 模式     | 发起异步的 RPC 调用                                |

##### Queue 属性

Queue，消息队列，实际存储消息数据。 

| 属性        | 释义                                                         |
| ----------- | ------------------------------------------------------------ |
| Durability  | Queue 是否需要持久化，Durable：是， Transient：否            |
| Auto Delete | Queue 是否自动删除，yes 代表是，当最后一个监听器被移除之后，该 Queue 会自动被删除 |

##### Message 属性

Message，服务器和应用程序之间传送的数据，本质上是一段数据，由 Properties 和 Payload（相当于 Body）组成。

| 属性               | 释义                                                         |
| ------------------ | ------------------------------------------------------------ |
| Delivery mode      | 1 代表非持久化，2 代表持久，一些客户端库将此属性公开为布尔值或枚举。 |
| Type               | 用于特定应用程序的消息类型，例如“orders.created”             |
| Headers            | Map类型，带有 Name-Value 的自定义映射                        |
| Content type       | 内容类型，例如“application/json”，由应用程序使用，而不是 RabbitMQ |
| Content encoding   | 内容编码，例如“gzip”，由应用程序使用，而不是 RabbitMQ        |
| Message ID         | 消息 ID（唯一 ID）                                           |
| **Correlation ID** | 操作 ID（唯一 ID），用于将请求与响应相关联，消费者可用于进行幂等性校验 |
| Reply To           | 携带响应队列名称，用于 RPC 模式                              |
| **Expiration**     | 每条消息的 TTL 过期时间                                      |
| Timestamp          | 应用程序提供的时间戳                                         |
| User ID            | 用户ID，如果设置则需要验证                                   |
| App ID             | 应用名称                                                     |

##### SpringBoot POM 依赖

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-amqp</artifactId>
</dependency>
```

##### SpringBoot 生产者 | 配置

```properties
# RabbitMQ通用配置
spring.rabbitmq.addresses=192.168.1.111:5672,192.168.1.112:5672,192.168.1.113:5672
spring.rabbitmq.username=guest
spring.rabbitmq.password=guest
spring.rabbitmq.virtual-host=/
spring.rabbitmq.connection-timeout=15000ms

# RabbitMQ生产者配置
# 启动消息Confirm机制, 不需要和mandatory一起使用
spring.rabbitmq.publisher-confirms=true
# 启动消息Return机制, 需要和mandatory一起使用
#spring.rabbitmq.publisher-returns=true
#spring.rabbitmq.template.mandatory=true
```

##### SpringBoot 生产者 | HelloWorld

```java
/**
 * RabbitMQ: Producer
 */
@Component
public class RabbitSender {

    @Autowired
    private RabbitTemplate rabbitTemplate;

    /**
     * 消息Confirm机制
     */
    final RabbitTemplate.ConfirmCallback confirmCallback = new RabbitTemplate.ConfirmCallback() {
        @Override
        public void confirm(CorrelationData correlationData, boolean ack, String cause) {
            System.out.println("消息ACK结果:" + ack + ", correlationData: " + correlationData.getId());
        }
    };

    /**
     * 对外发送消息
     * @param message
     * @param properties
     */
    public void send(Object message, Map<String, Object> properties){
        // 封装Message
        MessageHeaders messageHeaders = new MessageHeaders(properties);
        Message<Object> msg = MessageBuilder.createMessage(message, messageHeaders);

        // 指定业务唯一ID
        CorrelationData correlationData = new CorrelationData(UUID.randomUUID().toString());

        // 设置消息发送后置处理器
        MessagePostProcessor messagePostProcessor = new MessagePostProcessor() {
            @Override
            public org.springframework.amqp.core.Message postProcessMessage(org.springframework.amqp.core.Message message) throws AmqpException {
                System.out.println("post to do: " + message);
                return message;
            }

            @Override
            public org.springframework.amqp.core.Message postProcessMessage(org.springframework.amqp.core.Message message, Correlation correlation) {
                System.out.println("post to do " + message + " and correlation: " + correlation);
                return message;
            }
        };

        // 设置消息Confirm回调函数
        rabbitTemplate.setConfirmCallback(confirmCallback);

        // 发送消息，"springboot.rabbit"作为RoutingKey，匹配消费者的"springboot.*"
        // 	public void convertAndSend(String exchange, String routingKey, final Object message, final MessagePostProcessor messagePostProcessor, @Nullable CorrelationData correlationData) throws AmqpException
        rabbitTemplate.convertAndSend("exchange-1", "springboot.rabbit", msg, messagePostProcessor, correlationData);
    }
}
```

##### SpringBoot 消费者 | 配置

```properties
# RabbitMQ通用配置
spring.rabbitmq.addresses=192.168.1.111:5672,192.168.1.112:5672,192.168.1.113:5672
spring.rabbitmq.username=guest
spring.rabbitmq.password=guest
spring.rabbitmq.virtual-host=/
spring.rabbitmq.connection-timeout=15000ms

# RabbitMQ消费者配置
spring.rabbitmq.listener.simple.acknowledge-mode=manual
spring.rabbitmq.listener.simple.concurrency=1
spring.rabbitmq.listener.simple.max-concurrency=5
spring.rabbitmq.listener.simple.prefetch=1
```

##### SpringBoot 消费者 | HelloWorld

```java
/**
 * RabbitMQ: Receiver
 */
@Component
public class RabbitReceiver {

    @RabbitListener(bindings = @QueueBinding(
            value = @Queue(value = "queue-1", durable = "true"),
            exchange = @Exchange(name = "exchange-1", durable = "true", type = "topic", ignoreDeclarationExceptions = "true"),
            key = "springboot.*"
    ))
    @RabbitHandler
    public void onMessage(Message message, Channel channel) throws IOException {
        // 1. 收到业务以后进行业务端消费处理
        System.out.println("-----------------------");
        System.out.println("消费消息:" + message.getPayload());

        // 2. 处理成功后进行手工ACK
        Long deliveryTag = (Long) message.getHeaders().get(AmqpHeaders.DELIVERY_TAG);
        // void basicAck(long deliveryTag, boolean multiple) throws IOException;
        channel.basicAck(deliveryTag, false);
    }
}
```

#### 高级特性

##### 生产端 Confirm 机制

![1633418219083](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633418219083.png)

消息确认，指生产者投递消息后，如果 Broker 收到消息，则会给生产者一个应答，生产者接收应答，可以用于确定该消息是否已经正常地发送到了 Broker，是可靠性投递的核心保障。

- **原理**：

  1. RabbitMQ 不会在收到消息时立马把消息写入磁盘，而是在数百毫秒的范围内，定期写入消息。
  2. 当队列被镜像时，只有在所有镜像都把将消息副本写入磁盘时，才会发送 ACK 给生产者。
  3. 这意味着 Confirm 机制会增加更多延迟，但如果数据需要高可靠性，那么这是必要的。

- **实现方法**：

  1. 在 Channel 上开启确认模式 `channel.confirmSelect()`。
  2. 在 Channel 上添加监听 `addConfirmListener`，去监听消息投递结果（成功和失败），根据具体的结果对消息进行重新发送，或者记录日志等其他后续处理。

  ```java
  // 生产者 - 开启Confirm消息机制
  channel.confirmSelect();
  // 生产者 - 绑定Confirm监听器 => 异步监听
  channel.addConfirmListener(new ConfirmListener() {
      @Override
      public void handleAck(long deliveryTag, boolean multiple) throws IOException {
          System.out.println("------- ok ---------" + deliveryTag);
      }
  
      @Override
      public void handleNack(long deliveryTag, boolean multiple) throws IOException {
          System.err.println("------- error ---------" + deliveryTag);
      }
  });
  // 生产者 - 发送消息msg到EXCHANGE_NAME
  String routingKey1 = "confirm.save";
  channel.basicPublish(EXCHANGE_NAME, routingKey1, null, msg.getBytes());
  ```

##### 生产端  Return 机制

![1633419001204](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633419001204.png)

Return Listener，可以用于处理一些**不可路由的消息**，比如 RoutingKey 规则不对不可路由时，如果不做任何监听，RabbitMQ 会默认直接丢弃掉，而此时可以使用 Return Listener 进行监听、处理这些不可达的消息。

- **实现方法**：设置 `Mandatory`，如果为 true，则监听器会接收到路由不可达的消息，然后进行后续处理；如果为 false，则Broker 会自动删除该消息。

  ```java
  // 生产者 - 开启Return消息机制
  channel.addReturnListener(new ReturnListener() {
      @Override
      public void handleReturn(int replyCode,
                               String replyText,
                               String exchange,
                               String routingKey,
                               AMQP.BasicProperties properties,
                               byte[] body) throws IOException {
          System.out.println("**************handleReturn**********");
          System.out.println("replyCode: " + replyCode);
          System.out.println("replyText: " + replyText);
          System.out.println("exchange: " + exchange);
          System.out.println("routingKey: " + routingKey);
          System.out.println("body: " + new String(body));
      }
  });
  // 生产者 - 投递时，routingKey1路由不到任何队列，触发Return机制
  channel.basicPublish(EXCHANGE_NAME, routingKey1, false, null, msg.getBytes());
  ```

##### 消费端 QoS 限流

- **背景**：假设有一个场景，RabbitMQ 服务器有上万条未处理的消息堆积，巨量的消息瞬间全部推送过来，单个客户端无法同时处理它们。

- **概念**：RabbitMQ 提供了一种 QoS（Quality of Service，服务质量保证）功能，即在非自动确认消息的前提下，如果一定数目的消息未被 `ACK` 前，则不消费新的消息。

- **参数**：

  - **prefetchSize**：报文大小，RabbitMQ 没有相应的实现。
  - **prefetchCount**：一次性从 Broker 中获取 N 个消息，但不能多于 N 个消息，即消费者一旦有 N 个消息仍未 ACK，则该消费者将不再消费任何消息，直到有消息 ACK 掉。
  - **global**：是否把 QoS 设置应用于 Channel 级别，RabbitMQ 没有相应的实现。
    - **true**：代表应用于 Channel 级别，即 Channel 下面所有的消费者都会使用该配置。
    - **false**：代表应用于 Consumer 级别，即只有当前 Consumer 才有效。

- **原理**：

  1. 首先，`basic.qos` 是通过 Channel 进行设置的，即只有在 Channel 建立之后，才能发送 `basic.qos` 信令。
  2. 在 RabbitMQ 实现中，每个 Channel 对应一个 rabbit_limiter 进程，当收到 `basic.qos` 信令后，会记录信令中 `prefetch_count` 的值，以及该 Channel 未 ACK 的消息个数。
  3. 当 RabbitMQ 要将 Queue 中的一条消息投递给 Consumer 时，会先遍历该 Queue 上的 Consumer 列表，然后选出一个合适的 Consumer，再把消息投递出去。
  4. 其中挑选 Consumer 的一个依据就是，看 Consumer 对应的 Channel 上未 ACK 的消息数是否已经达到了设置的 `prefetch_count` 。
  5. 如果未 ACK 的消息数已经达到了 `prefetch_count` ，则该消费者不符合要求，继续遍历挑选，此时该消费者将不再收到消息的投递，直到它发生了消息 ACK。
  6. 当挑选到合适的消费者后，RabbitMQ 会中断后续的遍历挑选操作，把消息投递到该消费者中去。

- **用途**： 用于控制消息发送给消费者的速度，让 Consumer 能够保持饱和的工作状态。

  1. 如果没有设置 QoS，那么 RabbitMQ 会把队列所有的消息，都按照**网络和客户端允许的速度**，推送给客户端，Consumer 把所有接收到的消息都缓存在自己的内存中，从而导致该 Consumer 内存占用飞速上涨。

  2. 可见，没有设置 Qos 会为 Consumer 提供无限的缓冲区，导致不良行为和不良性能的发生，因此，设置合理的 Qos 对性能的提升十分重要。

     - **目的**：为了让 Consumer 能够保持**饱和的工作状态**，减少 Consumer 缓冲区大小，使得更多消息留在 Queue 而不是 Consumer 内存中，方便在添加新 Consumer 时能够接收到消息推送。

     - **合理的 Qos**：当网络正常时，合理的 Qos 应该等于比值 `q =（消息往返两次所花的时间 + 消息被处理所花的时间） / 消息被处理所花的时间`，以保证在 Qos 最后一个消息被消费完毕时，最早消息的 ACK 已经回到 Broker，并且 Broker 也已经把新消息投递到 Consumer 内存中，使得 Consumer 工作永远饱和，而不至于阻塞等待消息的接收。

       | 异常情况           | 影响                                                         |
       | ------------------ | ------------------------------------------------------------ |
       | 网络正常，Qos 过大 | 会导致堆积在 Consumer 内存中的消息过多，消费出现大量额外的延迟 |
       | 网络正常，Qos 过小 | 会导致 Consumer 出现阻塞等待消息接收的情况，浪费 Consumer 工作时间 |
       | 网络变慢，Qos 正常 | 消息往返时间变长，q 增大，此时 Qos 偏小，会导致 Consumer 出现阻塞等待消息接收的情况，浪费 Consumer 工作时间 |
       | 消费变慢，Qos 正常 | 消息被处理所花时间变长，q 减小，此时 Qos 偏大，会导致堆积在 Consumer 内存中的消息过多，消费出现大量额外的延迟 |

- **实现方法**：

  ```java
  // void basicQos(int prefetchSize, int prefetchCount, boolean global) throws IOException;
  // 消费者 - 开启消费限流 报文大小、限流阈值、是否设置为Channel级别
  channel.basicQos(0, 1, false);
  
  // 消费者 - 消费消息，只有使用手工ACK才可以进行消费流控
  while (true){
      QueueingConsumer.Delivery delivery = queueingConsumer.nextDelivery();
      String msg = new String(delivery.getBody());
      // void basicAck(long deliveryTag, boolean multiple) throws IOException;
      channel.basicAck(delivery.getEnvelope().getDeliveryTag(), false);
  }
  ```

##### 消费端 ACK/NACK 与消息重回队列

消费端进行消费时，如果消费端返回 `NACK`，可以进行日志记录 + 失败补偿，**千万不要把消息重回队列**；而由于服务器宕机等问题，需要消费端进行手工 `ACK`，保证消费成功，一般不会选择自动 `ACK`。

- **消息重回队列**：指把那些没有处理成功的（即返回 `NACK` 的）消息，重新投递给 Broker。

  - **缺点**：如果该消息一直消费失败，会导致无限制地被重新投递、重新消费...，非常浪费性能，甚至可能会搞挂 MQ，所以在使用中，一般都会关闭消息重回队列的功能。

- **实现方法**：

  ```java
  // 消费者 - NACK时开启消息重回队列，一般不采用, 弄不好会搞挂MQ
  // void basicNack(long deliveryTag, boolean multiple, boolean requeue) throws IOException;
  channel.basicNack(delivery.getEnvelope().getDeliveryTag(), false, true);
  ```

##### TTL 消息与 TTL 队列

TTL，Time to Live，指消息或者队列的生存时间，即过期时间。

- **TTL 消息**：

  - 指超过过期时间后，消息仍未被消费，则会被 RabbitMQ 删除。
  - RabbitMQ 支持 TTL 消息，通过在消息发送时进行指定。

- **TTL 队列**：

  - 指队列中**所有消息**都为 TTL 消息，且过期时间为指定的队列 TTL。
  - RabbitMQ 支持 TTL 队列，从**消息入队开始**计算，只要超过了队列的超时时间配置，那么消息会自动被删除。

- **实现方法**：

  ```java
  // 生产者 - 设置TTL队列参数
  HashMap<String, Object> queueArguments = new HashMap<>();
  queueArguments.put("x-message-ttl", 6000);// 6s过期时间
  // 生产者 - 声明交换机
  channel.exchangeDeclare(EXCHANGE_NAME, EXCHANGE_TYPE, true, false, false, null);
  // 生产者 - 声明队列（由于有TTL参数，所以为TTL队列）
  channel.queueDeclare(QUEUE_NAME, false, false, false, queueArguments);
  // 生产者 - Queue绑定Exchange: 表示交换机EXCHANGE_NAME上ROUTING_KEY的消息会路由到QUEUE_NAME中
  String routingKey = ROUTING_KEY;
  channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, routingKey);
  
  // 生产者 - 或者可以通过设置BasicProperties，来设置TTL消息
  AMQP.BasicProperties properties = new AMQP.BasicProperties.Builder()
      .deliveryMode(2)
      .contentEncoding("UTF-8")
      // TTL消息
      // .expiration("6000")
      .headers(headers)
      .build();
  ```

##### 死信交换机与死信队列

RabbitMQ 中有死信交换机的概念，Dead Letter Exchange，DLX，当一个队列中的消息变成**死信**（Dead Message）之后，它能被重新投递到另一个 Exchange 中，此时这个 Exchange 被称为**死信交换机**，而与死信交换机绑定的队列被称为**死信队列**（Dead Letter Queue，DLQ），

- **死信产生条件**：

  - 消息被拒绝 `basic.rejec` 或者 `basic.nack`，且不重回队列时 `requeue = false`。
  - 消息 TTL 过期时。
  - 队列达到最大长度时。

- **原理**：

  1. DLX 本质上也是一个正常的 Exchange，和一般的 Exchange 没有区别，可以在任何队列上被指定，实际上只是设置了某个队列的属性而已。
  2. 当某个队列中有死信时，RabbitMQ 会自动将死信消息重新投递到设置的 DLX 上，进而被路由到另一个队列（**死信队列**）中。
  3. 通过监听这个死信队列中的消息，进行相应的处理。
     - 死信只能从队头被转发到 DLX，也就是说即是 TTL 消息已过期，如果还没出现在队头，那么该消息还会继续存留在队列中，直到出现在队头才会被转发到 DLX 中。

- **实现方法**：

  ```java
  // 生产者 - QueueArguments声明绑定的死信交换机、对应的RoutingKey、以及TTL队列参数
  HashMap<String, Object> queueArguments = new HashMap<>();
  queueArguments.put("x-dead-letter-exchange", DLX_EXCHANGE_NAME);
  queueArguments.put("x-dead-letter-routing-key", "123");// 任意RoutingKey
  queueArguments.put("x-message-ttl", 6000);// 6s过期时间
  // 生产者 - 声明普通交换机
  channel.exchangeDeclare(EXCHANGE_NAME, EXCHANGE_TYPE, true, false, false, null);
  // 生产者 - 声明普通队列（由于有TTL参数，所以为TTL队列）
  channel.queueDeclare(QUEUE_NAME, false, false, false, queueArguments);
  // 生产者 - 普通Queue绑定普通Exchange: 表示交换机EXCHANGE_NAME上ROUTING_KEY的消息会路由到QUEUE_NAME中
  String routingKey = ROUTING_KEY;
  channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, routingKey);
  // 直连式，发送到默认的交换机，由于为TTL队列且没有任何消费者消费，所以消息过期后被重新投递到DLX中
  channel.basicPublish("", QUEUE_NAME, properties, msg.getBytes());
  
  // 消费者 - 声明死信队列交换机(一般手动创建，不在Java代码中创建)
  channel.exchangeDeclare(DLX_EXCHANGE_NAME, DLX_EXCHANGE_TYPE, true, false, false,null);
  // 消费者 - 声明死信队列(一般手动创建，不在Java代码中创建)
  channel.queueDeclare(DLX_QUEUE_NAME, false, false, false, null);
  // 消费者 - 死信队列绑定死信交换机: 表示交换机DLX_EXCHANGE_NAME上ROUTING_KEY的消息会路由到DLX_QUEUE_NAME中
  channel.queueBind(DLX_QUEUE_NAME, DLX_EXCHANGE_NAME, DLX_ROUTING_KEY);
  // 消费者 - 开始消费死信队列DLX_QUEUE_NAME的消息
  channel.basicConsume(DLX_QUEUE_NAME, false, queueingConsumer);
  QueueingConsumer.Delivery delivery = queueingConsumer.nextDelivery();
  channel.basicAck(delivery.getEnvelope().getDeliveryTag(), false);
  ```

##### 延迟消息

延迟消息，就是消息投递到 Broker 后，Broker 会经过根据设定的延迟时间后，才把消息真正的投递到目标的业务 Exchange 中。

- **方案一，TTL 消息 + 单个等待队列 + 死信队列**：设置消息 TTL ，然后把等待队列中已过期的死信，投递到目标的业务 Exchange 中，此时该 Exchange 扮演者 DLX 的角色。

  - **缺点**：由于死信仅会从队头被移除，如果队列头部有一条 TTL 为 10 分钟的消息，后面又有一条 TTL 为 1 分钟的消息，那么第二条消息将等待 10 分钟，导致延迟时间容易失准。

  ```java
  // 方案一，TTL 消息 + 死信队列
  // public static final String X_DELAY = "x-delay";
  // 底层调用 => this.headers.put(X_DELAY, delay);
  org.springframework.amqp.core.messageProperties.setDelay(message.getDelayMills());
  ```

- **方案二，TTL 消息 + 多个等待队列 + 死信队列**：类似于方案一，不过不同的是，需要创建多个等待队列，并在队列本身上设置 TTL，比如 1、5 和 15 分钟等，然后根据 TTL 投递消息到不同的等待队列中。

  - **缺点**：设置延迟时间的灵活性有限，如果出现一个不在等待队列已有的 TTL，那么就需要新增一个等待队列，或者允许延迟时间失准，把消息投放到 TTL 相差较小的等待队列中。

- **方案三，NServiceBus**：

  - **原理**：
    1. 基于方案二的思路，使用级联 Topic，通过死信配置和 Topic 路由链接在一起。
    2. 创建多个延迟级别，其中每个级别负责自己的固定 2 的幂次的延迟时间（28 个级别的延迟时间），比如级别 1 为 1 分钟，级别 2 为 2 分钟，级别 3 为 4 分钟，级别 4 为 8 分钟等。
    3. 然后使用二进制样式的路由规则 与 RoutingKey ，在延迟队列之间移动消息（最多 27 次的路由交换），从而实现消息延迟投递。
  - **优点**：可以以 1 分钟的分辨率实现**任何**延迟时间。

  ![1633596886819](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633596886819.png)

#### 高可用架构

##### 主备模式

![1633428088126](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633428088126.png)

warren，兔子窝，Master - Slave 主备方案，与 ActiveMQ 利用 Zookeeper 实现 Master -Slave 方案类似。

- **原理**：如果 Master 挂了，可以利用 HaProxy 自动切换为 Slave，继续提供服务，从而实现热备份。

- **缺点**：服务器利用率低，只能从 Master 进行消费。

- **主备 HaProxy 配置**：

  ```shell
  # 集群名称
  listen rabbitmq_cluster
  # HaProxy IP+端口
  bind 0.0.0.0:5672
  # 配置TCP模式
  mode tcp
  # 简单的轮询，一开始默认为主节点，主节点挂了后轮询到下一个节点作为主节点
  balance roundrobin
  # 主节点
  server bhz76 192.168.11.76：5672 check inter 5000 rise 2 fall 2
  # 备用节点
  server bhz76 192.168.11.76：5672 backup check inter 5000 rise 2 fall 2
  ```


##### 远程模式

![1633441813577](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633441813577.png)

**远距离**通信和复制，可以实现**异地双活**的一种模式，简称  Shovel 模式，用于早期版本的多活存储和异地容灾。

- **原理**：通过把消息进行不同数据中心的复制，跨地域地让**两个 MQ 集群互联**。

- **缺点**：由于配置麻烦，无法动态配置，比如加一个 Exchange 必须重启服务，**实际用得并不多**，而且可用性也有待提高。

- **集群配置步骤**：

  ```shell
  # Step1：启动rabbitmq_shovel插件
  rabbitmq-plugins enable amqp_client
  rabbitmq-plugins enable rabbitmq_shovel
  # Step2：创建rabbitmq.config文件
  touch /etc/rabbitmq/rabbitmq.config
  # Step3：添加rabbitmq.config配置（省略，非常复杂，包括需要同步的每一个交换机、队列等信息）
  # Step4：保证源服务器与目的服务器都使用相同的rabbitmq.config配置
  ```

##### 镜像模式

![1633442866041](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633442866041.png)

镜像模式，Mirror，是非常经典的一种集群架构，可以保证 100% 数据不丢失，在实际工作中用的最多，并且实现起来**非常简单**，一般大厂都会使用这种架构模式来构建集群。

- **缺点**：只起到高可用的效果，**无限横向扩展没有意义**，因为当消息堆积过多时，无论如何横向扩展机器，每台机器堆积的消息量还是保持不变，机器增多只会增加 RabbitMQ 集群的通信负担，因此，RabbitMQ 集群一般选用 3 个节点保证高可用。

- **原理**：当 Broker 收到投递的消息后，镜像模式会把消息同步到集群中所有的节点，由于使用 Erlang 语言实现，天然地以交换机的方式进行数据同步，保持与原生 Socket 一样的延迟，性能非常好。

  1. Queue 分为 Master 和 Slave 节点，其中 Master 和 Slave 是针对一个 Queue 而言的，即一个 Queue **第一次创建**的 RabbitMQ 节点为 Master，其它 RabbitMQ 节点作为 Slave；而不是某个 RabbitMQ 节点作为所有 Queue 的 Master，其它 RabbitMQ 节点作为 Slave。
  2. 对某个 Queue来说，只有 Master 对外提供服务，而其他 Slave 只提供备份服务，以提供消息冗余，在Master 不可用时，RabbitMQ 会选出一个 Slave 作为新 Master 继续对外提供服务。
  3. 无论 Client 请求打到 Master 还是 Slave，最终数据都是**从 Master 获取**：
  4. 当 Client 请求打到 Master 时，Master 会直接将消息返回给 Client，同时 Master 会通过 GM 协议，将 Queue 的最新状态广播到其他 Slave。
     - GM 协议：Guaranteed Multicast，保证了**广播消息的原子性**，即要么都更新要么都不更新。
  5. 当 Client 请求打到 Slave 时，Slave 需要将 Client 请求先重定向到 Master，Master 再将消息返回给Client，同时 Master 会通过 GM 协议，将 Queue 的最新状态广播到 Slave节点。
  6. 因此，多个 Client 连接不同的镜像队列，不会产生同一 Message 被多次接受的情况。

  ![1633599572589](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633599572589.png)

- **新增节点**：

  - 如果有新节点加入，RabbitMQ 不会同步之前的历史数据，只会复制该节点加入到集群之后新增的消息。
  - 另外，对于 RabbitMQ 节点的重启，也是按照新节点来处理的。

- **Master 选举**：

  - RabbitMQ 集群内部会维护节点的状态**是否已经同步**，使用 rabbitmqctl 的 `synchronised_slave_pids` 参数，就可以查看状态。
  - 如果 `slave_pids` 和 `synchronised_slave_pids` 里面的节点是一致的，那说明全都同步了。
  - 如果不一致，则很容易比较出来哪些还没有同步，集群会在**最老**的 Slave 之间，选一个出来作为新的Master 。
  - 所以，RabbitMQ 选举过程，是不会选择新增节点作为新 Master 的。

- **故障恢复**：假设两个节点 A 和 B，组成一个镜像队列，其中 B 为 Master，A 为 Slave。

  1. **场景1**：A 先停，B 后停。
     - **恢复方案**：先启动 B，再启动A；或者先启动 A，在 30 秒内启动 B，即可恢复镜像队列。
  2. **场景2**：A、B 同时停。
     - **解决方案**：该场景可能是由掉电等原因造成，只需在 30 秒内，连续启动 A、B，即可恢复镜像队列。
  3. **场景3**：A 先停，B 后停，且 A 无法恢复。
     - **解决方案**：该场景是场景 1 的加强版，只需在 B 起来后，调用 `rabbitmqctl forget_cluster_node A`，解除与 A 的 Cluster 关系，再将新 Slave 加入 B，即可重新恢复镜像队列。
  4. **场景4**：A 先停，B 后停，且 B 无法恢复。
     - **解决方案**：
       1. 该场景是场景 3 的加强版，比较难处理，早在 3.1.x 时代之前没什么好的解决方法，由于 B 是 Master，所以直接启动 A 是不行的，而 A 无法启动，也就无法在 A 节点上调用 `rabbitmqctl forget_cluster_node B` 了。
       2. 而在 3.4.2 版本中，`forget_cluster_node` 支持 `–offline` 参数，允许 `rabbitmqctl` 在离线节点上执行 `forget_cluster_node` 命令，迫使 RabbitMQ 在未启动的 Slave 中选择一个作为Master。
       3. 此时，可以在 A 节点执行 `rabbitmqctl forget_cluster_node –offline B` 时，将 B 剔出Cluster，然后 A 就能正常启动了，最后将新 Slave 加入 A，即可重新恢复镜像队列。
  5. **场景5**：A 先停，B 后停，且 A和B 都无法恢复，但是能得到 A 或者 B 的磁盘文件。
     - **解决方案**：
       1. 该场景是场景4的加强版，更加难处理。
       2. 将 A 或 B 的数据库文件（默认在 `$RABBIT_HOME/var/lib`目录中），拷贝至新节点 C 的对应目录下，再把 C的 `hostname` 改成 A 或 B 的 `hostname` 。
       3. 如果拷过来的是 A 节点磁盘文件，则按场景 4 进行处理；如果拷过来的是 B 节点磁盘文件，则按场景 3 进行处理。
       4. 最后将新 Slave 节点加入 C，即可重新恢复镜像队列。
  6. **场景6**：A 先停，B 后停，且 A 和 B 都无法恢复，还无法得到 A 和 B 的磁盘文件。
     - **解决方案**：无法恢复 A 和 B 中的内容。

##### 多活模式

![1633445011959](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633445011959.png)

多活模式，Federation，也是实现**异地数据复制**的主流模式，由于 Shovel 模式配置比较复杂，所以一般来说，实现**异地集群**都是使用双活或者多活模型来实现的。

- **特点**：RabbitMQ 部署架构采用双中心或者多中心模式，各数据中心都部署一套 RabbitMQ 集群，除了需要为业务提供正常的消息服务外，中心与中心之间还需要实现部分关键队列的消息共享。

- **原理**：需要依赖 RabbitMQ#federation 插件。

  - **Federation 插件**：基于镜像队列集群，不需要重新构建集群，使用 **AMQP 协议**通讯，可以在集群之间高效传输消息，同时接受不连续的消息传输，接的双方可以使用不同的 users、virtual hosts、RabbitMQ、甚至 Erlang 环境。

  - **Federation Exchanges**：

    1. 可以看成下游主从上游拉取消息，但并不是拉取所有消息，而是只拉取下游绑定了上游 Queue 的 Exchange 的 消息。
    2. 更新时，通过使用 AMQP 协议实施代理间通信，下游在后台绑定或者解绑，然后把绑定和解除绑定命令，发送到上游交换机，进行动态更新配置。
    3. 因此，Federation Exchange 只接收被订阅了的消息，即只拉取自己有绑定关系的消息。

    ![1633445945055](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633445945055.png)

#### 常见问题解决

##### 如何保证数据不丢失？

在使用 MQ 过程中，应做到消息不能多消费，也不能少消费，如果无法做到可靠性传输，可能会给公司带来千万级别的财产损失。

- **数据丢失场景**：生产端丢数据、MQ 丢数据、消费端丢数据。

###### 生产端丢数据

需要生产端保证可靠性投递，即要保证生产者投递的消息 100% 投递成功，不存在投递失败，比如核心业务，订单下单支付成功后，通知物流时需要 100% 通知成功，一单都不能丢。

- **解决方案**：RabbitMQ 提供 transaction 或者 confirm 机制，来确保生产者不丢消息。

  - **transaction 机制**：

    - **执行流程**：
      1. 发送消息前，开启事务 `channel.txSelect()`。
      2. 然后发送消息。
      3. 如果发送过程中出现什么异常，事务就会回滚 `channel.txRollback()`。
      4. 如果发送成功则提交事务 `channel.txCommit()`。
    - **缺点**：吞吐量会下降很多。

  - **confirm 机制**： 消息状态打标 + 消息落库 + Broker ACK 回传 + 定时重发 + 人工介入/失败补偿 。

    ![1633339851782](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633339851782.png)

    - **实现关键点**：
      1. 保障消息能够成功发出。
      2. 保障 MQ Broker 节点能够成功接收。
      3. 发送端收到 MQ Broker 节点能够确认应答。
      4. 有完善的消息补偿机制。
    - **执行流程**：
      1. **STEP 1**：业务落库 BIZ_DB。
      2. **STEP 2**：消息落库 MSG_DB，需要保证 MSG_DB 与 BIZ_DB 属于同源数据库，即一个 Connection 可以同时操作该地址下面的多个数据库，这样可以使业务和消息落库处于同一个事务内，**保证原子性**。
         - 如果不想单独建立消息库或者消息表，可以在业务表上新增一个字段，用于记录消息的状态。
      3. **STEP 3**：发送消息到 MQ 上。
      4. **STEP 4**：MQ Broker 接收消息成功后返回 ACK 应答。
      5. **STEP 5**：接收到 Broker 的 ACK 应答后，更新 MSG_DB 中对应的消息状态。
      6. **STEP 6**：
         1. 为了防止 MSG_DB 对应的消息状态一直未改变，且已发生超时，或者一次消息投递失败，需要启动一个定时任务，去 MSG_DB 扫出投递超时的、失败可再投递的消息，进行 STEP 7 重新投递。
         2. 同时，如果重新投递的次数超过了最大限制，则说明消息投递最终失败，需要进行**人工介入** 或者 **失败补偿**。
      7. **STEP 7**：定时任务重新投递消息。
      8. **STEP 8**：定时任务定期执行，扫出投递超时的、失败可再投递的消息。

###### MQ 丢数据

处理 MQ 丢数据，需要（**开启持久化**配置 + RabbitMQ confirm 机制）配合使用，在消息持久化后，才给生产者发送一个 ACK 信号，如果消息持久化之前，RabbitMQ 就阵亡了，此时生产者收不到 ACK 信号，生产者就会自动重发，从容防止 MQ 丢数据。

- **开启持久化**：
  1. 将 Queue 的持久化标识 `durable` 设置为 true，代表是一个持久的 Queue。
  2. 在生产者发送消息时，将 `deliveryMode` 设置为2，代表该消息需要被持久化。
  3. 这样设置以后，RabbitMQ就算挂了，重启后也能恢复数据。
- **为什么不对所有消息都开启持久化**？
  1. 是否要对消息进行持久化，需要综合考虑性能的差距，因为写磁盘要比写 RAM 慢得多，开启持久化必然会导致 RabbitMQ 性能的下降，之间的消息吞吐量可能会有 3 倍以上的差距。
  2. 如果想达到单 RabbitMQ 10w/s 以上的消息吞吐量，一种处理方法是，使用非常快速的存储系统，来支持写磁盘持久化消息（比如使用 SSD）。
  3. 另外一种处理方法是，根据业务重要程度，仅对**关键消息**作持久化处理，这时仅仅保证关键消息的持久化不会导致性能瓶颈即可。

###### 消费端丢数据

消费端丢数据，一般是因为采用了**消费者自动 ACK** 的模式，在这种模式下，消费者收到消息后会自动确认，然后 RabbitMQ 则立即将消息删除，如果此时消费者出现异常，确认后没能处理该消息，则会丢失该消息。

- **解决方案**：采用**消费者手工 ACK** 即可。

##### 如何防止重复消费？

这个问题换一种问法就是，如何保证消息队列的**幂等性消费**，即消费者多次消费的结果只会被消费一次。

- **原因**：
  1. 正常情况下，消费者在消费完毕后，会发送一个**确认信息**给消息队列，消息队列就知道该消息被消费了，然后会将该消息从消息队列中删除。
  2. 无论是哪种消息队列，造成重复消费原因其实都是类似的，只是不同的消息队列发送的**确认消息**的形式不同，比如 RabbitMQ 是发送一个 `ACK` 确认消息，RocketMQ是返回一个 `CONSUME_SUCCESS` 成功标志，kafka实际上有个 `offset` 的概念。
  3. 如果出现网络传输等故障，确认信息没有传送到消息队列，导致消息队列不知道该消息已经被消费过了，下次会把该消息，再次分发给其他的消费者进行消费，从而导致重复消费。
- **解决方案**：
  1. **数据库主键去重**：消费这个消息做数据库的 `insert` 操作时，可以给这个消息做一个**主键**，那么就算出现重复消费的情况，就会导致主键冲突，避免数据库出现脏数据。
     - **局限**：并发时会导致大量的插入失败，浪费性能。
  2. **更新操作天然幂等**：消费这个消息做 Redis 的 `set` 操作时，无需做任何处理，因为无论 `set` 几次结果都是一样的。
     - **优点**：`set ` 操作天然幂等，无需做任何处理。
  3. **全局唯一 ID**：如果以上两种情况都不适合，那么可以准备一个第三方介质，用来做消费记录，以 Redis 为例，给消息分配一个**全局唯一 ID**，只要消费过该消息，可以吧 `ID-Message` 以 K-V 形式写入 Redis，当消费者开始消费前，需要先去 Redis 中查询有没消费记录，如果没有则消费，否则放弃消费。
     - **局限**：仅仅使用这种方案还是有可能重复消费的，因为当线程并发查询消费记录时，可能会导致并发通过消费记录的校验，从而重复创建订单。
  4. **全局唯一ID + 数据库主键去重**：由于高并发解决方案是不能加锁的，而仅仅使用全局唯一 ID，或者数据库主键去重，都是有幂等性缺陷的，因此，最终的解决方案是使用 **全局唯一ID + 数据库主键去重** ，利用消费记录校验来挡住大部分请求，利用数据库主键兜底剩余通过校验的并发请求，从而保证幂等性消费。
     - **优点**：简单有效。

##### 一致性与可用性保障？

###### 集群可调整参数

事实证明，在所有的故障模式下，分布式系统不可能同时保证无数据丢失的**最终一致性**以及时刻都接受读取和写入的**高可用性**，因此需要做的是，选择要针对其中的一些进行优化，让一致性和可用性处于一个**范围的两端**。对此，RabbitMQ 提供了调整参数，以获得更高一致性或者更高可用性。

1. **持久化 Queue / Exchange**：见 Queue / Exchange 的 `Durability` 属性。

   - 开启持久化的 Queue / Exchange 都会被持久化到 Mnesia 数据库，其可以在系统崩溃或服务器故障后，重新启动时，这些 Queue / Exchange 基础设施会重新上线。
   - 而关闭持久队的 Queue / Exchange，在重新启动时，则会被删除。

2. **持久化消息**：Queue / Exchange 的持久化，并不意味着其消息的持久化，只有被生产者设置为 `Delivery mode=2` 的消息，重新启动后才会恢复。

   - **缺点**：持久消息会给代理带来更多负载，所以并不需要对所有消息都做持久化处理，而是根据业务重要程度，仅对**关键消息**作持久化处理，这时仅仅保证关键消息的持久化不会导致性能瓶颈即可。

3. **消息 Confirm 机制**：见《高级特性 - 生产端 Confirm 机制》与《高级特性 - 消费端 ACK/NACK 与消息重回队列 》。

4. **镜像同步策略**：`ha-sync-mode`：

   - **automatic**：自动同步，节点重新上线后，集群会为新节点上的每个队列创建一个镜像，并自动将新 镜像与 Master 进行同步，包括 Master 原始的消息。
     - **为什么不默认使用自动同步**？同步是一个阻塞操作，同步期间 Master 无法执行任何读取或者写入操作，如果 Queue 为一个大队列，原始消息的同步将会耗费大量的时间，保证了一致性，但牺牲了可用性。
   - **manual**：手动同步，默认，节点重新上线后，集群会为新节点上的每个队列创建一个镜像，但该镜像只是保持为空的镜像，不会复制 Master 原始的消息，而仅仅是同步 Master 新写入的消息。

5. **Master 选举策略**：`ha-promote-on-failure` ：

   - **always**：默认值，允许故障转移到**数据未完全同步**的镜像中。
     - **特点**：可能会导致 Master 的消息丢失，但可以保持 Queue 的高可用性。
   - **when-synced**：仅在**数据已完全同步**的镜像中进行故障转移，否则让 Queue 不可用；只当 Master 再次上线时，Queue 才恢复可用，数据未完全同步的镜像继续同步 Master 数据。
     - **特点**：牺牲了可用性，某个方面增加了数据安全；但如果重新上线的 Master 丢失了所有数据，则会导致 Queue 的所有数据**全部丢失**，即使有大部分追上同步进度的镜像也会被丢弃，去同步数据为空的 Master，所以该策略是非常危险的。

6. **脑裂处理策略**：一个集群由于网络链接切断，被一分为二的地方，在分区的每一侧，都有镜像都被提升为 Master，这意味着最终每个队列有不止一个的 Master，RabbitMQ 提供 `cluster_partition_handling` 作为脑裂的处理策略：

   - `Ignore`：忽略模式，默认，此模式选择了**可用性**，当分区发生时，发生了裂脑，而在分区解决后，由管理员来决定分区的哪一边获胜，让失败侧重新启动，并且仅存在于该分区的任何数据都将会丢失。
   - `Autoheal`：自动修复模式，与忽略模式相同，不过是交由集群**自动决定**分区的失败侧，失败的一方会重新加入集群，从而丢失所有未被消耗的消息。
   - `Pause Minority`：暂停少数派模式，**拒绝**对分区的少数方进行读写，这是禁止出现裂脑的**唯一选择**。
     1. Broker 会自动暂停位于分区的少数方，意味着它会关闭所有现有连接，并拒绝任何新连接。
     2. 同时，Broker 会每秒检查一次网络状态，以判断分区是否已自行解决。
     3. 一旦分区已自行解决，Broker 将会自行取消暂停，并让那些少数方重新加入集群。

   ![1633606547241](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633606547241.png)

7. **客户端轮训策略**：轮训集群中的所有节点，并执行连接，重试直到成功。

###### 集群缺陷

- **一致性**：重新加入集群的节点，会被迫丢弃它们自己的数据。
- **可用性**：镜像同步 Master 阻塞时，会导致队列暂时不可用。

###### 追求高可用性

1. **持久化 Queue / Exchange**：开启。
2. **持久化消息**：只持久化关键消息。
3. **生产端 Confirm**：允许投递迅速消息，无需进行消息 Confirm。
4. **镜像同步策略**：`ha-sync-mode=manual`，手动同步镜像。
5. **Master 选举策略**：`ha-promote-on-failure=always`，允许选举数据未完全同步的镜像作为 Master。
6. **脑裂处理策略**： `cluster_partition_handling=Ignore`，忽略模式或者 `cluster_partition_handling=Autoheal`，自动修复模式。
7. **客户端轮训策略**：轮训集群中的所有节点，并执行连接，重试直到成功。

###### 追求高一致性

1. **持久化 Queue / Exchange**：开启。
2. **持久化消息**：只持久化关键/所有消息。
3. **生产端 Confirm**：生产端可靠性投递（消息状态打标 + 消息落库 + Broker ACK 回传 + 定时重发 + 人工介入/失败补偿） + 消费端幂等性消费 + 消费端手工 ACK。
4. **镜像同步策略**：`ha-sync-mode=automatic`，自动同步镜像，包括 Master 的原始消息。
   - 但对于大队列，由于镜像同步慢，还需要考虑**可用性导致的消息丢失**：生产端可靠性投递（消息状态打标 + 消息落库 + Broker ACK 回传 + 定时重发 + 人工介入/失败补偿） + 死信队列（超过最大投递次数） + 人工介入。
5. **Master 选举策略**：`ha-promote-on-failure=when-synced`，仅在数据已完全同步的镜像中进行故障转移，否则让 Queue 不可用；只当 Master 再次上线时，Queue 才恢复可用，数据未完全同步的镜像继续同步 Master 数据。
6. **脑裂处理策略**： `cluster_partition_handling=Pause Minority`，暂停少数派模式，**拒绝**对分区的少数方进行读写，只有当分区修复完成后，才会重新加入集群，开放读写。
7. **客户端轮训策略**：轮训集群中的所有节点，并执行连接，重试直到成功。

##### 如何重新平衡队列？

故障恢复后，现有的 Master 可能都落在同一个节点上，这是不理想的情况，需要 Master 重新平衡，以让其在节点之间均匀分布，而不幸的是，RabbitMQ Master 重新平衡没有很好的选择，而应该关注**如何重新平衡队列**： 

- 在 3.8.1 版本以前，可以使用 HA 策略来移动 Master，其工作原理是：

  1. 通过优先级高于现有 HA 策略的临时策略，来删除所有镜像。
  2. 将临时 HA 策略更改为使用 `nodes` 模式，指定要将 Master 迁移到的节点，然后强制**迁移同步队列**。
  3. 迁移完成后，删除临时策略，优先使用原始 HA 策略并创建所需数量的镜像。

  => **缺点**：如果有大队列，或者严格的冗余要求不能删除镜像时，则可能该方案不可行。

- 从 3.8.1 版本开始，可以使用 `rabbitmq-queues rebalance all` 命令，重新平衡镜像队列。

  ```shell
  rabbitmq-queues rebalance "all" --vhost-pattern "a-vhost" --queue-pattern ".*"
  ```

##### 如何保证顺序消费？

![1633703487247](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633703487247.png)

- **问题场景**：

  - 业务上产生三条消息，分别是对数据的 add、update 和 delete，如果没有保证顺序消费，结果可能是delete ->  update -> add，本来数据最终是要 delete 掉的，结果却变成 add。
  - 再如电商平台，先付钱，然后生成订单，最后通知物流，如果顺序改变了，则可能出现不用先付钱了，却通知物流送货。

- **解决思路**：必须要使用**单消费者消费单个队列**，目的是防止消费者争抢消息导致乱序消费的情况发生。

- **解决方案**：

  - **多队列、多消费者**：可以使用一致性哈希交换机 `x-consistent-hash Exchange`，来保证同一个 RoutingKey 多次投递，只会顺序进入同一个队列，然后被同一个 Consumer 顺序消费。

    - **局限**：消息不是全局保证顺序的，而只是相关消息才保证顺序；如果确实要保证顺序消费，则需要并发同步，比如搞分布式锁。

    ![1633703914760](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633703914760.png)

  - **单队列、多消费者**：多个 Consumer 消费。

    - **局限**：需要保证并发同步，引入了同步机制，可能会降低消费速度。

  - **单消费者、多线程**：一个 Consumer + 一个内存队列 + 多线程消费，即 Master - Worker 模式。

    - **局限**：与单队列、多消费者模式类似，只不过在同一个 Consumer 进程中，处理并发同步的成本可能要比不同进程的更低一些。

  - **单队列、单消费者**：始终保证使用 一个 Queue + 一个 Consumer 消费。

    - **局限**：Consumer 不能水平扩展，消费能力有限。

##### 如何优化消息积压？

###### 影响后果

消息积压在 Queue 中，可能会导致消息被丢弃、MQ 内存打满、MQ性能下降甚至停止服务，影响系统运行。

###### 原因分析

如果 Consumer 消费速度跟不上 Producer 生产速度，就会造成消息积压。

- Consumer 消费速度跟不上 Producer 生产速度，一般是业务逻辑没设计好，导致 Consumer 和 Producer 之间的效率不平衡。
  - **解决思路**：增加 Consumer 等，以提高消费速度。
- Consumer 出现异常，导致一直无法接收新的消息。
  - **解决思路**：优化消费程序，解决异常。

###### 优化思路

一定要保证 Consumer 的消费性能要高于 Producer 的发送性能，这样系统才能健康、持续地运行。

###### 优化方案

这里的优化方案，针对的是**消费速度低于生产速度**，而不是 Consumer 异常。

1. **提高 `prefetch_count`**：首先要考虑的是，Consumer 有没有已经发挥出其本身真正的后端处理能力，如果没有，可以**从等待消息阻塞的角度**入手。

   ![1633794700060](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633794700060.png)

   1. 消息消费速度，主要受到 `发送消息时间`、`消息被处理时间`、`消息 ACK 时间` 的影响。
   2. 如果一个消息走完这个流程后，才发送另一个消息的话，整体效率将会非常的低。
   3. 此时，可以让消息在这几个时间内恰当的分配，让消息总是连续不断地被 Consumer 接收处理，确保 Consumer 能够保持饱和的工作状态，从而发挥出其本身真正的后端处理能力，整体上提升 Consumer 的消费速度。
   4. 只有设置 `prefetch_count` 到一个合理的值，才可以最大限度地提升消息的消费速度，这个值的设定可以参考《消费端 Qos 限流》。
   5. **小结**：需要消费端 ACK、需要设置的合理的 `prefetch_count`。

2. **Consumer 批量 ACK**：其次还要考虑的是，Consumer 有没有已经发挥出其本身真正的后端处理能力，如果没有，可以从**减少 I/O 的角度**入手。

   ![1633795791575](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633795791575.png)

   1. 在每条消息分别被 ACK 的情况下，Consumer 需要多次发起 ACK 传输给 Broker，多次的 I/O 浪费了服务器性能与增加了带宽的占用。
   2. 通过批量 ACK 的方式，减少多次发起 ACK，以及结合 `prefetch_count` 批量一次性从 Broker 拉取消息，可以减少很多 I/O 浪费和带宽占用。
   3. 不过，如果 Consumer 在处理某条消息时失败了，而业务上又要求不能丢失任何消息，此时就不能对所有的消息进行批量 ACK，否则 RabbitMQ 就不会再次投递该消息了。
      - **解决方案**：可以跟踪所有消息的处理结果，如果全部成功，则使用批量 ACK；如果部分成功，则有两个选择：1）如果不需要顺序消费，则可以退化为每个消息分多次发送 ACK/NACK；2）如果需要顺序消费，则本次接收到的所有  `prefetch_count` 消息全部 NACK，否则这批消息重新投递时顺序就不一致了，但是需要做好幂等性消费。
   4. **小结**：Consumer 批量 ACK 的前提是，设置了 `prefetch_count` 批量一次性从 Broker 拉取消息，否则批量 ACK 将会失去意义。

   ```java
   // void basicAck(long deliveryTag, boolean multiple) throws IOException;
   // multiple：true表示采用批量ACK，凡是deliveryTag比e.DeliveryTag的消息都会被ACK。
   channel.BasicAck(e.DeliveryTag, true);
   ```

3. **多线程并发消费**：接着还要考虑的是，Consumer 有没有已经发挥出其本身真正的后端处理能力，如果没有，可以**从多线程并发消费**入手。

   ![1633794760275](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633794760275.png)

   1. 多线程并发消费，不需要建立多个 RabbitMQ 连接，在收到消息后，可以将其放入不同的线程中进行消费，这样进程中就会同时消费多个消息，增加了消费的吞吐量，从而提升消费速度。
   2. **小结**：与增加 Consumer 类似，同样存在并发冲突和顺序消费的问题，只不过在多线程并发消费是在同一个 Consumer 进程中，处理并发同步的成本可能要比不同进程的更低一些。

4. **增加 Consumer**：这个道理比较容易理解，多个人搬砖的速度肯定比一个人要快很多，不过实际情况还需要面对一些技术挑战，比如后端处理能力瓶颈、并发消费冲突，以及保持顺序消费三个问题。

   ![1633794730033](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633794730033.png)

   - **后端处理能力瓶颈**：
     1. 比如多个 Consumer 都要操作数据库，那么数据库连接的并发数和读写吞吐量就是后端处理能力。
     2. 如果达到了**数据库的最大处理能力**，出现了瓶颈，增加再多的 Consumer 也没有用，甚至会因为加剧了数据库拥塞，从而导致整体消费速度的进一步下降。
   - **并发消费冲突**：
     1. 比如两个 Consumer 都要去修改用户的积分，如果同时取出了相同的数据，并发处理的话就会出现并发安全问题。
     2. 此时需要保证**并发同步**，比如可以搞一个分布式锁，对于具体的某个用户，确保同时只能有一个消费者来处理其积分。
   - **保持顺序消费**：
     1. 由于增加了多个 Consumer，不再是单个 Consumer 消费单个 Queue，可能会出现乱序消费的情况。
     2. 如果仍需要保证顺序消费，那么可以参考一致性哈希的做法，搞成多队列、多消费者模式，不过只能保证相关消息顺序消费；如果确实要保证顺序消费，则需要并发同步，比如搞分布式锁。
   - **小结**：
     1. 解决并发消费冲突、保持顺序消费两个问题，常常需要引入多个 Consumer 之间的**并发同步**机制，如果这些机制设计得不好，还会给消费速度带来很大的影响。
     2. 因此，多人搬砖速度快的前提，是多个人搬砖时不需要大家频繁的坐下来协调谁搬哪块砖，否则，就会浪费很多时间在相互协调上，反而不能提升搬砖的速度。
     3. 所以，想要通过增加 Consumer，来提升消费速度，需要确保 Consumer **并发处理能力**要留有余地，Consumer 依赖的**后端服务处理能力**也要留有余地。

###### 方案总结

- **优化思路**：通过分析上边的这些方法，在进行消费优化时，可以遵循这样一个路径，以保证最大消费速度。
  1. 启用 `prefetch_count`。
  2. 先单个 Consumer 消费，`prefetch_count` 设置为 1，**1 次只接收 1 条消息**，消息消费完毕后再消费下一条，避免并发冲突和顺序消费的问题，减少同步机制的消耗。
  3. 如果消费速度不满足要求，则提高 `prefetch_count`，**1 次接收多条消息**，甚至批量 ACK，单线程按顺序消费，避免并发冲突和顺序消费的问题，减少同步机制的消耗。
  4. 如果消费速度还是不满足要求，则 **1 次接收多条 + 多线程消费**，甚至批量 ACK，但要注意并发冲突和顺序消费的问题。
  5. 如果消费速度还是不满足要求，则**多个消费者并发消费**，甚至批量 ACK，但要注意并发冲突和顺序消费的问题。
  6. 如果消费速度还是不满足要求，则考虑**改需求**，或者**换别的中间件**。
- **优化注意点**：
  1. **程序性能优化优先**：需要始终优先优化 Consumer 处理能力，以及其依赖的后端程序处理能力，比如要去优化 SQL 语句、使用缓存、使用负载均衡等，来加快消费速度，因为消息积压常常都是程序处理太耗时导致的。
  2. **幂等性消费**：由于不只 Producer 可能会重复发送消息，Consumer 也可能会触发消息的重复投递，所以，Consumer 要保证幂等性消费。
  3. **并发同步**：如果使用了多线程消费，或者多 Consumer 消费，则会存在并发冲突以及顺序消费的问题，此时需要保证并发同步，比如使用分布式锁。
  4. **顺序消费**：最好能做到无需顺序消费，否则需要在多线程消费，或者多 Consumer 消费时保证并发同步，以及批量 ACK 遇到消费失败时进行全部 NACK。

###### 【线上】如何紧急处理消息积压？

如果日常系统正常运转，没有积压或者只有少量积压很快就能消费掉，但是某一个时刻，突然就开始积压消息，并且积压持续上涨，这种情况下需要在短时间内排查消息积压的原因，迅速解决问题才不至于影响业务。

-  **排查思路**：能导致积压突然增加，最粗粒度的原因，只有两种，要么是**发送变快了**，要么就是**消费变慢了**。
- **发送变快了**：通过监控数据发现到是，单位时间内发送的消息增多了，即发送变快了，比如说是赶上大促或者抢购。
  - **解决方案**：保证消费速度 大于 提高后的发送速度。
    1. 这种情况，短时间内不太可能优化 Consumer 代码来提升消费性能，唯一的方法是通过扩容 Consumer 实例数来提升总体的消费能力。
    2. 如果没有足够服务器资源进行 Consumer  扩容，没办法的办法，可以考虑将系统降级，通过关闭一些不重要的业务，减少 Producer 发送的数据量，最低限度地让系统还能正常运转，服务一些重要业务。
    3. 当 MQ 快慢了时，如果也降级不了，可以临时写一个专门丢弃的 Consumer，接入不重要的业务进行消费，消费一个记录一个，然后丢弃掉，快速消费掉积压的消息，最后再在空闲时，根据记录到的消息重新补回数据。
- **消费变慢了**：通过监控数据发现到是消费编变慢了。
  - **解决方案**：保证消费速度回到以前的消费速度。
    1. 需要检查 Consumer 实例，分析一下是什么原因导致消费变慢，优先检查一下日志是否有大量的消费错误。
    2. 如果日志没有错误的话，可以通过 Dump 出堆栈信息，看一下消费线程是不是卡在什么地方不动了，比如触发了死锁或者卡在等待某些资源上了。
- **其他原因**：还有一种不太常见的情况，就是通过监控发现，无论是发送速度，还是消费速度，都和原来的没什么变化。
  - **解决方案**：需要检查一下 Consumer，是不是存在一条消息消费失败然后导致反复重新投递 + 消费这种情况，因为这种情况也是会拖慢整个系统的消费速度的。

### 1.6. 什么是磁盘衡量指标？

![1634119681801](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1634119681801.png)

1. **影响磁盘性能的关键因素**：磁盘服务时间，即磁盘完成一个 I/O 请求所花费的时间，由寻道时间、旋转延迟和数据传输时间三部分构成。
   - **寻道时间**：Tseek，指将读写磁头移动至正确的磁道上所需要的时间，寻道时间越短，I/O 操作越快，目前磁盘的平均寻道时间一般在3-15ms。
   - **旋转延迟**：Trotation，指盘片旋转将请求数据所在的扇区，移动到读写磁盘下方所需要的时间，旋转延迟取决于磁盘转速，通常用磁盘旋转一周所需时间的1/2表示。
     - 比如，7200rpm 的磁盘平均旋转延迟大约为 60*1000/7200/2 = 4.17ms，而转速为15000rpm 的磁盘其平均旋转延迟为 2ms。
   - **数据传输时间**：Ttransfer，指完成传输所请求的数据所需要的时间，取决于数据传输率，其值等于数据大小除以数据传输率。
     - 目前 IDE/ATA 能达到 133MB/s，SATA II 可达到 300MB/s 的接口数据传输率，数据传输时间通常远小于前两部分消耗时间，简单计算时**可忽略**。
2. **衡量磁盘的指标**：IOPS 和吞吐量。
   - **IOPS**：Input/Output Per Second，每秒输入输出量，也叫每秒读写次数，即指每秒内系统能处理的 I/O 请求数量，对于**随机读写频繁**的应用，比如小文件存储等，需要关注随机读写性能，此时 IOPS 则是关键衡量指标。
     - **公式**：IOPS = 1000ms / （Tseek + Trotation + Transfer）。
     - 如果忽略数据传输时间，理论上可以计算出随机读写最大的IOPS，常见磁盘的随机读写最大IOPS为：
       - 7200rpm 的磁盘 IOPS = 76 IOPS。
       - 10000rpm 的磁盘 IOPS = 111 IOPS。
       - 15000rpm 的磁盘 IOPS = 166 IOPS。
   - **Throughput**：吞吐量，指单位时间内成功传输的数据数量，对于**顺序读写频繁**的应用，如视频点播，关注连续读写性能、数据吞吐量是关键衡量指标，主要取决于磁盘阵列架构、数据通道大小以及磁盘的个数。
     - **磁盘阵列架构**：不同的磁盘阵列存在不同的架构，它们都有自己的内部带宽，一般情况下，内部带宽都设计足够充足，不会存在瓶颈。
     - **数据通道大小**：磁盘阵列与服务器之间的数据通道，对吞吐量影响很大，比如一个 2Gbps 的光纤通道，其所能支撑的最大流量仅为 250MB/s。
     - **磁盘个数**：磁盘越多，吞吐量也越大。

### 1.7. 什么是 Linux 磁盘读请求模型？

![1634136736890](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1634136736890.png)

- **背景**：虽然 15000rpm 的磁盘计算出的理论最大 IOPS 仅为 166，但在实际运行环境中，磁盘的 IOPS 往往能够突破 200，甚至更高，这其实就是在系统调用过程中，**操作系统**进行了一系列的优化。
- **概念**：虚拟文件系统层 -> 具体的文件系统层 -> Cache 层 -> 通用块层 -> I/O 调度层 -> 块设备驱动层 -> 物理块设备层。

#### 虚拟文件系统层

VFS Layer，允许 Linux 中共存众多不同的文件系统，并且对文件的操作可以跨文件系统执行。

- **VFS**：Virtual File System，虚拟文件系统，是一种软件机制，扮演着文件系统管理者的角色，与它相关的数据结构只存在于物理内存当中，其作用是，屏蔽下层具体文件系统操作的差异，为上层的操作提供一个统一的接口。

#### 具体的文件系统层

VFS的下一层即是具体的文件系统，一个文件系统一般使用块设备上一个独立的逻辑分区。

- 对于 Linux Ext2 文件系统来说，硬盘分区首先被划分为一个个的 Block，系统上的每个 Block 都是一样大小的，但是，不同 Ext2 文件系统，Block 大小可能不同，这是在创建 Ext2 系统决定的，一般为1k 或者 4k 。

#### Cache 层

- **目的**：为了提高 Linux 操作系统对磁盘访问的性能。
  - **提高读性能**：在内存中缓存了磁盘中的部分数据，当数据的请求到达时，如果在 Cache 中存在该数据且是最新的，则直接将数据传递给用户程序，避免了对底层磁盘的操作，提高了读性能，是磁盘 IOPS 能突破 200 的重要原因之一。
  - **提高写性能**：通过暂时将数据存在 Cache 里，然后统一**异步**写到磁盘中，通过这种异步的数据 I/O 模式，解决了程序中计算速度和数据存储速度不匹配的鸿沟，减少了访问底层存储介质的次数，使写性能大大提高。

- **两大功能**：预读和回写。

  - **预读**：根据应用程序是否需要等待预读完成，可分为**同步预读和异步预读**，如果所请求的页面处于预读的页面之中，则进行异步预读；如果所请求的页面处于预读页面之外，则进行同步预读。

    1. **同步预读**：如果所读页面不在 Cache 中，此时操作系统读入所请求的页面，同时，利用局部性原理，继续读入紧随其后的少数几个页面（通常是三个页面），其中第一个读请求必定是同步预读。
    2. **异步预读**：如果所读页面在 Cache 中，则表明前次预读命中，此时操作系统会把预读页的大小扩大一倍，但该预读过程是异步的，应用程序无需等待预读完成即可返回，只需后台慢慢读页面即可。

  - **回写**：

    - Linux 2.6.32 内核之前，采用 `pdflush` 机制将脏页真正地写到磁盘中，其刷脏时机为：

      1. **脏页太多**：在空闲内存大小少于一个特定的阈值时，内核必须将脏页写回磁盘，以便释放内存。
      2. **脏页太久**：当脏页驻留内存时间超过一定的阈值时，内核必须将超时的脏页写回磁盘，以确保脏页不会无限期地驻留在内存中。
      3. 主动调用 `fsync`。

      而回写开始后，`pdflush` 会持续写数据，直到满足以下两个条件：

      1. 已经有指定的最小数目的页被写回到磁盘。
      2. 空闲内存页已经回升，超过了阈值。空闲内存页已经回升，超过了阈值。

    - Linux 2.6.32 内核之后，放弃了原有的 `pdflush` 机制，改成了 `bdi_writeback` 机制，解决了在多磁盘的系统中，由于 `pdflush` 管理了所有磁盘的 Cache 导致了一定程度的 I/O 瓶颈。

      - `bdi_writeback` 机制为每个磁盘都创建了一个线程，专门负责这个磁盘的 Page Cache 的刷脏工作，从而实现了每个磁盘的数据刷新在线程级别上分离，从而提高了 I/O 性能。

    - 回写机制存在的问题：回写不及时，会引发数据丢失，且回写期间读I/O 性能很差。

- **Linux 实现**：一是 Page Cache，另一个 Buffer Cache，每一个Page Cache 包含若干 Buffer Cache。
  - **Page Cache**：主要作为文件系统上的**文件数据缓存**来使用，尤其是针对当进程对文件有 `read/write` 操作的。
  - **Buffer Cache**：主要用来在系统对块设备进行读写时，作为**块进行数据缓存**来使用。

#### 通用块层

通用块层的主要工作是，接收上层发出的磁盘请求，并最终发出I/O请求，该层隐藏了底层硬件块设备的特性，为块设备提供了一个通用的抽象视图。

#### I/O 调度层

I/O 调度层的功能是，管理块设备的请求队列，即接收通用块层发出的 I/O 请求，缓存请求并试图合并相邻的请求，并根据设置好的调度算法，回调驱动层提供的请求处理函数，以处理具体的I/O请求。

- **目的**：
  1. 如果简单地以内核产生请求的次序直接将请求发给块设备的话，那么块设备性能肯定让人难以接受，因为磁盘寻址是整个计算机中最慢的操作之一。
  2. 为了优化寻址操作，内核不会一旦接收到 I/O 请求后，就按照请求的次序发起块 I/O 请求，Linux 提供了几种 I/O 调度算法进行优化，其思想是通过合并和排序 I/O 请求队列中的请求，以此大大降低所需的磁盘寻道时间，从而提高整体I/O性能。
- **常见的 I/O 调度算法**：
  - **Noop 算法**：No Operation，最简单的 I/O 调度算法，该算法仅适当合并用户请求，是为不需要寻道的块设备而设计的（如 SSD，SSD 没有所谓的寻道时间且 I/O 响应时间非常短），并不排序请求。
    1. 新的请求通常被插在调度队列的开头或末尾，下一个要处理的请求总是队列中的第一个请求。
  - **CFQ 算法**：完全公正排队 I/O 调度算法，其主要目标是在触发 I/O 请求的所有进程中，确保磁盘 I/O 带宽的公平分配。
    1. 使用许多个排序队列，存放了不同进程发出的请求。
    2. 通过散列将同一个进程发出的请求插入同一个队列中。
    3. 采用轮询方式扫描队列，从第一个非空队列开始，依次调度不同队列中特定个数（公平）的请求，然后将这些请求移动到调度队列的末尾。
  - **Deadline 算法**：截止时间调度算法，避免了电梯调度策略（为了减少寻道时间，会优先处理与上一个请求相近的请求）带来的对某个请求忽略很长一段时间的可能。
    1. 引入了两个排队队列分别包含读请求和写请求，两个最后期限队列包含相同的读和写请求。
    2. 本质是一个超时定时器，当请求被传给电梯算法时开始计时，一旦最后期限队列中的超时时间已到，就想请求移至调度队列末尾。
  - **AS 算法**：AS预测调度算法，本质上依据局部性原理，预测进程发出的读请求与刚被调度的请求在磁盘上可能是“近邻”。
    1. 算法统计每个进程I/O操作信息，当刚刚调度了由某个进程的一个读请求之后，算法马上检查排序队列中的下一个请求是否来自同一个进程。
    2. 如果是，立即调度下一个请求。
    3. 否则，查看关于该进程的统计信息，如果确定进程p可能很快发出另一个读请求，那么就延迟一小段时间。

#### 块设备驱动层

驱动层中的驱动程序，对应具体的物理块设备，从上层中取出 I/O 请求，并根据该请求中指定的信息，通过向具体块设备的设备控制器发送命令的方式，来操纵设备传输数据。

#### 物理块设备层

具体的磁盘设备。

### 1.8. 什么是零拷贝技术？

#### 标准 I/O

传统 Linux 系统中，标准的 I/O 接口（比如 `File#read`、`File#write`）都是基于**数据拷贝**操作的，即是 I/O 操作会导致数据在内核地址空间的缓冲区，和用户地址空间的缓冲区之间进行拷贝，所以标准 I/O 也被称作**缓存 I/O**。

- **过程**：传统 I/O 中，读取一个文件并通过 socket 发送给用户的过程，数据经历了 2 次 DMA 数据拷贝，2 次用户空间与内核空间的 CPU 拷贝操作，以及 4 次上下文切换：

  ![1634173642699](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1634173642699.png)

  - **2 次 DMA 数据拷贝**：
    1. 通过 DMA 把文件数据拷贝到内核缓存。
       - **DMA**：Direct Memory Access，直接存储器访问，是一种无需 CPU 参与，让外设和系统内存之间进行双向数据传输的硬件机制，使用DMA可以使系统 CPU 从实际的 I/O 数据传输过程中摆脱出来，从而大大提高系统的吞吐量。
    2. 通过 DMA 把内核缓存拷贝并发送到网络。
  - **2 次用户空间与内核空间的 CPU 拷贝操作**：
    1. 从内核空间的内核缓存，读取文件数据，然后拷贝到用户空间的用户缓存。
    2. 用户程序调用 `Socket#write` ，从用户空间的用户缓存，读取文件数据，然后拷贝到内核空间的内核缓存。
  - **4 次上下文切换**：
    1. 用户程序调用 `File#read` 读取文件，需要从用户态切换到内核态。
    2. 文件读取完成后，用户程序又需要从内核态切换回用户态。
    3. 用户程序调用 `Socket#write` 发送数据，需要从用户态切换到内核态。
    4. 文件发送完成后，用户程序又需要从内核态切换回用户态。

- **优点**：如果所请求的数据已经存放在内核的高速缓冲存储器中，就可以减少实际的 I/O 操作。

- **缺点**：数据拷贝的过程会导致 CPU 的开销。

#### 零拷贝技术

- **概念**：零拷贝技术，并不是不需要拷贝，而是减少不必要的拷贝次数，避免多余地将数据从一块存储拷贝到另外一块存储，从而节省数据拷贝带来的CPU开销。

- **分类**：目前零拷贝技术主要有三种类型：

##### 直接 I/O

![1634175610607](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1634175610607.png)

数据直接跨过内核，在用户地址空间与 I/O 设备之间传递，内核只是进行必要的虚拟存储配置等辅助工作。

- **实现方法**：采用直接 I/O，需要在调用 `open` 时，传入 `O_DIRECT` 标识符，让操作系统知道接下来对文件的读写操作是使用 直接 I/O 的方式。

  ```c
  // Linux Open函数
  int open(const char *pathname, int oflag, … /*, mode_t mode * / );
  ```

- **使用场景**：

  - 这种类型的零拷贝多用于**数据库系统**中，方便他们自己实现一套缓存机制，以更好的提供服务。
  - 再如 Java 的 **Direct Buffer**。

##### 避免内核和用户空间的数据拷贝

当应用程序不需要对数据进行访问时，则可以通过避免将数据从内核空间拷贝到用户空间，实现零拷贝。

###### mmap

`mmap`，Memory Mapped Files，内存映射机制，并不是提供用户进程直接操作内核地址空间的能力，而是把内核中的部分内存空间映射到用户空间的内存，使得用户空间和内核空间共享一块相同的物理内存，从而提供用户进程对内存的直接访问能力。

- **实现方法**：

  ```c
  void *mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset);
  ```

- **实现原理**：调用 `mmap` 之后，并不会立即读取文件内容并加载到物理内存中，而是会在虚拟内存中分配地址空间，等到实际要访问数据时，才会因为内存地址对应的物理内存中没有数据，产生缺页异常，然后触发数据的加载。

- **使用场景**：

  1. 有了 `mmap` 的支持，从文件中读取数据到内核空间的文件数据缓存后，就不会再拷贝到用户空间的用户缓存了。
  2. 当调用 `Socket#send` 时，数据会直接从内核缓存，直接拷贝到 Socket 缓冲区中，避免了在用户空间中多中转一次。
  3. 所以，在 I/O 过程中，使用 `mmap` 可以减少 2 次用户空间与内核空间的 CPU 拷贝操作，替代为一次内核空间的直接拷贝。

  ![1634177254260](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1634177254260.png)

- **局限**：虽然 `mmap` 能减少一次数据拷贝，但是 I/O 过程还是需要 4 次上下文切换：

  1. 用户程序调用 `mmap` 读取文件，需要从用户态切换到内核态。
  2. 文件读取完成后，用户程序又需要从内核态切换回用户态。
  3. 用户程序调用 `Socket#send` 发送数据，需要从用户态切换到内核态。
  4. 文件发送完成后，用户程序又需要从内核态切换回用户态。

###### sendfile

`sendfile` 内核调用，是在 Linux 2.1 版本开始引入的，主要功能是在内核态中，可以在两个文件描述符之间传递数据，避免了用户空间和内核空间之间的数据拷贝操作。

- **实现方法**：

  ```c
  /**
   * in_fd：数据源的文件描述符，必须是一个可以 mmap 的文件描述符，必须指向真实的文件，不能是socket
   * out_fd：待输出的文件描述符，必须是一个socket
   */
  ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
  ```

- **实现原理**：使用 `sendfile` 时，数据中转与 `mmap` 类似，不经过用户空间， `sendfile` 全程在内核态执行，一次 I/O 只需要 2 次上下文切换：

  1. 用户程序调用 `sendfile` 通过 socket 发送文件，需要从用户态切换到内核态。
  2. 文件发送完成后，用户程序又需要从内核态切换回用户态。

- **sendfile 优化**：

  1. 在 Linux 2.4 版本中，对 `sendfile` 进一步做了优化，无需 CPU 从文件数据缓存拷贝到 socket缓存，而是让 socket 缓存只存储在文件数据缓存中的位置和偏移量。

  2. 在进行实际发送时，只需根据位置和偏移量，直接将文件数据缓存中的数据通过 DMA 拷贝到网卡设备中，又省掉了一次 CPU 拷贝操作。

     ![1634179262531](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1634179262531.png)

###### splice

```c
// 功能与sendfile类似，但fd_in和fd_out中，必须至少有一个是管道文件描述符（pipe）
ssize_t splice(int fd_in, loff_t *off_in, int fd_out, loff_t *off_out, size_t len, unsigned int flags);
```

###### tee

```c
// 功能与sendfile类似，但fd_in和fd_out都必须是管道文件描述符（pipe）
ssize_t tee(int fd_in, int fd_out, size_t len, unsigned int flags);
```

###### sockmap

上面的几种方式，都不支持从 socket 到 socket 的转发，而 Linux 4.14 带来的 `sockmap`，可以支持在内核态中实现从 socket 到 socket 的数据转发。

##### Copy on Writes

- **概念**：写时复制技术，也算是一种零拷贝技术，其核心思想是，数据不需要提前拷贝，而是当需要修改时才进行部分拷贝。
- **原理**：
  1. 当有多个调用者都需要请求相同资源时，一开始资源只会有一份，多个调用者共同读取这一份资源。
  2. 当某个调用者需要修改数据的时候，才会分配一块内存，把数据拷贝过去，供这个调用者使用，而其他调用者依然还是读取最原始的那份数据。
  3. 每次有调用者需要修改数据时，就会重复一次拷贝流程，供调用者修改使用。
- **作用**：使用 `copy-on-write` 可以避免或者减少数据的拷贝操作，极大地提高性能。
- **实现**：其应用十分广泛，比如Linux 的 `fork` 调用、Linux 的文件管理系统、一些数据库服务、Java中的 CopyOnWriteArrayList、C++98/C++03中的std::string等等。

### 1.9. 详细介绍 Kafka？

#### 概念

![1634043435602](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1634043435602.png)

Kafka 是一个分布式的、基于发布订阅模式的消息系统，可用于实现高性能数据管道、流分析、数据集成和关键任务等相关的应用程序，在大数据领域的实时计算、日志采集等场景表现出色。

- **通用用途**：异步处理、系统解耦、削峰填谷、蓄流压测。

- **特色用途**：日志收集、数据同步、数据采集。

  - **日志收集**：KafKa 做日志堆积，减轻 Logstash 压力。

    ![1633839541805](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633839541805.png)

  - **数据同步**：MySQL 分库分表后，统一经过 Cannal + Kafka 同步到 ES 中，方便搜索查询。

    ![1633839719421](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633839719421.png)

  - **数据采集**：实时计算分析平台，埋点采集数据，上报到 Kafka，Flink 周期性从 Kafka 获取数据进行分析。 

    - **用户活动跟踪**：Kafka 常常被用于记录 Web 用户或者 App 用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到 Kafka Topic 中，然后 Consumer 通过订阅这些 Topic来做运营数据的实时监控分析。

    ![1633839898897](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1633839898897.png)

| 专业术语       | 释义                                                         |
| -------------- | ------------------------------------------------------------ |
| Broker         | 服务器实体，用于连接 Producer 和 Consumer，单个 Kafka Broker 可以轻松处理数千个 Partition 以及百万级/s的消息量 |
| Message        | 消息，Kafka 中的数据单元，可以看成是数据库里的一个数据行或一条记录 |
| Topic          | 主题，每条发布到 Kafka 集群的消息都有一个类别，这个类别被称为 Topic，Topic 是逻辑的概念，物理上不同 Topic 的消息会分开存储，逻辑上一个 Topic 的消息虽然保存于一个或多个Broker上，但用户只需指定消息的 Topic，即可生产或者消费数据，而无需关心数据存放在物理上的何处 |
| Partition      | 分区，Parition 是物理上的概念，每个 Topic 包含一个或多个 Partition |
| Replica        | 副本，每个 Partition 有多个 Replica，每个 Replica 都会分布在不同的 Broker 中，都有一个 Leader Replica 进行复制同步 |
| Producer       | 生产者，负责发布消息到 Broker，默认情况下 Broker 会把消息均衡地分布到 Topic 下的所有 Partition 上，另外，也可以通过直接指定、根据 Key 散列取模、轮询方式来得到消息要存放的 Partition |
| Consumer       | 消费者，负责向 Broker Partition 读取消息，Consumer 通过 Consume Offset 来区分已经读过的消息，消费未读过的消息，然后会把每个 Partition 最后读取的 Consume Offset 保存在 Zookeeper 或 Kafka上，使得即使 Consumer 被关闭或重启，当前的读取状态也不会丢失 |
| Consumer Group | 消费者组，每个 Consumer 属于一个特定的消费者组，消费者组可为每个 Consumer 指定 group name，若不指定 group name，则该 Consumer 属于默认的消费者组；消费者组可以保证每个 Partition 只能被一个 Consumer 消费，如果组内一个 Consumer 失效，那么组内的其他 Consumer 会重新再平衡，接管已失效的 Consumer 的工作 |
| Consume Offset | 消费偏移量，不同消费者组的 Consumer，可以对同一个 Partition 存储不同的 Offset，它们之间互不影响，用于记录读取状态，区分已经读过的消息，消费未读过的消息；指向最后消费的消息，通过在客户端库维护这个偏移量，并且根据 Kafka 版本，存储在 ZooKeeper 或者 Kafka 中 |
| Log Offset     | 日志偏移量，消息写入时，每一个 Partition 都有一个 Offset，它是每个 Partition 中最新、最大的 Offset |
| Log Segment    | 一个 Partition 由多个 LogSegment 组成，一个 LogSegment 由 `.log`、`.index`、 `.timeindex` 组成，`.log` 是顺序追加写入的，其文件名是以文件中第一条 Message 的Offset 来命名的，`.Index` 可以在日志删除和数据查找时进行快速定位，`.timeStamp` 则可以根据时间戳查找对应的偏移量 |

#### 原理

##### 日志分区原理

![1634092563478](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1634092563478.png)

###### Kafka 本质

Kafka 本质上是一个**分布式的、异步复制的提交日志**，本身并没有队列的概念：

- **分布式**：Kafka 集群可以实现容错和方便扩展。
- **异步复制**：Follow Replica 副本定时异步拉取 Leader Partition Offset，以实现消息跨多个节点复制。
- **提交日志**：消息存储在 Partition 中，附加到 Topic 的日志上。

###### 日志分区模型

1. **消息存储方面**：Kafka 不像 RabbitMQ 那样存储在 Queue 中，只是把消息追加到日志中，无论消息被消费一次还是一千次，消息都会保持原状，最后根据数据的保留策略，来决定是否被删除。
   - RabbitMQ 通过把消息放入 FIFO 的队列 Queue 上，并跟踪 Queue 中该消息的状态，当 Consumer 消费该消息后，无论该消息有没有被持久化，Queue 都会把该消息删除掉。
2. **消息消费方面**：Kafka 不像 RabbitMQ 那样通过 **Broker Push 模型**消费，而是让每个 Consumer 跟踪它自己在日志中的消费偏移量，通过 **Consumer Pull 模型**，每次都从消费偏移量位置开始消费消息。
   - **消费偏移量**：
     - Consume Offset，不同消费者组的 Consumer，可以对同一个 Partition 存储不同的 Offset，它们之间互不影响，用于记录读取状态，区分已经读过的消息，消费未读过的消息。
     - 指向最后消费的消息，通过在客户端库维护这个偏移量，并且根据 Kafka 版本，存储在 ZooKeeper 或者 Kafka 中。
   - **Broker Push VS Consumer Pull 模型**：
     - RabbitMQ 使用 Broker Push 模型，结合 Consumer 配置的预取限制，来防止单个 Consumer 负担过重，对于低延迟消息传递非常有用，适用于 RabbitMQ 基于队列的架构。
     - Kafka 使用 Consumer Pull 模型，Consumer 根据消费偏移量来批量拉取消息，由于 Kafka Partition 能够保证消息顺序，所以可以通过消息批量拉取，来实现更高效的消息传递，从而提供更高的吞吐量。
   - **消息日志优势**：
     1. **消息持久化时间长**：消息投递后，Kafka 会长时间持久化消息，直到删除策略触发，而 RabbitMQ 则是在消息被消费后就删除。 
     2. **允许消费先前消息**：对于 Kafka 而言，可以允许 Consumer 倒带并消费先前偏移量的消息，只需将 Consumer 的偏移量往先前方向移动 N 小时即可，而使用 RabbitMQ 需要以某种方式重新发布先前的消息。
3. **消息分区方面**：
   1. 生产者负责发布消息到 Broker，默认情况下 Broker 会把消息均衡地分布到 **Topic** 下的所有 Partition 上，另外，也可以通过直接指定、根据 Key 散列取模、轮询方式来得到消息要存放的 Partition。
   2. 每个**分区**（Partition ）都是一个单独的数据结构，可以保证消息顺序，但仅在单个 Partition 内得到保证，一个 Partition 不能支持竞争消费者，因此同一个消费者组内只能有一个 Consumer 去消费某个 Partition。
   3. 每个**消费者**（Consumer）属于一个特定的消费者组，消费者组可为每个 Consumer 指定 group name，若不指定 group name，则属于默认的消费者组。
   4. **消费者组**（Consumer Group）可以保证每个 Partition 只能被一个 Consumer 消费，如果组内一个 Consumer 失效，那么组内的其他 Consumer 会重新再平衡，接管已失效的 Consumer 的工作。
      - 消费者组中的每个  Consumer  将处理 Topic 下所有消息的一个 Partition 子集，即从同一Topic 的不同分区消费。
      - 而 RabbitMQ 的竞争消费者，是从同一个 Queue 中消费，在这一点上，RabbitMQ 看起来更加灵活，因为它保证了队列中的消息顺序，并且能够应对不断变化的竞争消费者数量。

###### 分区分配策略

- **Broker 分区存储**：
  1. 先把所有 Broker 和 Partition 排好序。
  2. 然后把第 i 个 Partition 分配到第 **i mod n** 个 Broker 上。
- **Producer 消息投递**：
  1. 当 key 为空时，Producer 生产的消息，将随机发送到各个 Partition，不同的 Kafka 版本会有不同方式，比如轮训、随机、固定等。
  2. 当 key 不为空时，采取 **key#hash mod Partion#size** 的方式，来决定把消息发送到哪个 Partition 上。
- **Consumer 消息消费**：一个 Partition 只能被同一个 Consumer Group 内的一个 Consumer 消费。
  - **RangeAssignor**：默认，简单相除。
    - **原理**：
      1. 用 Partition#size / Consumer#size 来决定每个 Consumer 消费几个Partition。
      2. 当除不尽时，则前面的 Consumer 会比后面的 Consumer 多消费 Partition。
      3. 当 Consumer#size > Partition#size 时，会有空消费的 Consumer。
    - **缺点**：分配不均匀，可能会出现部分 Consumer 过载的情况。
  - **RoundRobinAssignor**：排序 + 轮训。
    - **原理**：
      1. 把 Consumer Group 内所有 Consumer，以及其所订阅的所有 Topic#Partition 按照字典顺序排好序。
      2. 然后通过轮询的方式，逐个把 Partition 分配给每个 Consumer。
      3. 如果同一个 Consumer Group 内所有 Consumer 订阅的信息都是相同的，那么该策略的分区分配会是均匀的。
      4. 如果同一个 Consumer Group 内所有 Consumer 订阅的信息都不相同，那么在执行分区分配时就不是完全的轮询分配了，可能会导致分区分配的不均匀。
         - 比如某个 Consumer 没有订阅某个Topic，但该 Topic 会被组内其他 Consumer 订阅，则会在分配分区时，该 Consumer 将分配不到该 Topic 的任何 Partition。
    - **缺点**：还是会出现分配均匀的情况。
  - **StickyAssignor**：粘性分配，0.11.x 版本开始引入。
    - **原理**：
      1. 分区的分配要尽可能地均匀。
      2. 分区的分配尽可能地与上次分配的保持相同。
         - 如果发生分区重分配，那么对于同一个  Partition 而言，有可能之前的 Consumer 新指派的 Consumer 不是同一个。
         - 此时，如果使用非粘性策略，则对于之前 Consumer 进行到一半的处理，还要在新指派的消费者中再次复现一遍，很浪费系统资源。
         - 但 StickyAssignor 策略具备一定的粘性，可以尽可能地让前后两次分配相同，进而减少系统资源的损耗，以及其它异常情况的发生。
      3. 当前面两点发生冲突时，第一个点优先于第二个点。
    - **缺点**：实现比较复杂。

##### 日志文件存储原理

![1634213772578](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1634213772578.png)

1. 在 Kafka 日志文件存储中，同一个 Topic 下会有多个不同的 Partition，每个 Partiton 为一个目录，所以，Partition 是实际物理上的概念，而 Topic 则是逻辑上的概念。
   - **Partition 目录命名规则**：Topic + 有序序号，第一个序号从 0 开始，最大的序号为 Partition 数量减 1。
   - **为什么不能以 Partition 作为存储单位**？
     1. 虽然 Kafka Consumer 能够根据 Consume Offset 查找到具体的某个消息，但是查找过程是顺序查找，如果数据量很大的话，查找效率依然很低，所以，Kafka 采用了**分段和索引**的方式，通过**二分查找**来解决查找效率问题。
     2. 而且，每个 Partition 被平均分配到多个 Segment 文件，也方便 Old Segment 的删除，即方便已消费消息的清理，提高磁盘的利用率。
   - **Segment 分段策略**：
     - **按大小分片**：当日志分段文件大小，超过了 Broker 参数 `log.segment.bytes` 配置的值时，则需要继续分段，默认为 `1073741824（1GB）`。
     - **按时间分片**：当日志分段文件中，消息的最大时间戳与当前系统时间戳的差值，大于 `log.roll.ms` 或者大于 `log.roll.hours` 配置的值时，则需要继续分段，默认为 `168（7天）`。
     - **按索引分片**：当 `.index` 或者 `.timeindex` 文件大小达到 Broker  `log.index.size.max.bytes` 配置的值时，则需要继续分段，默认为 `10MB`。
     - **按偏移量分片**：当新追加消息的偏移量 `offset`，与日志分段文件偏移量的差值 `baseOffset`，大于 `Integer.MAX_VALUE` 时，则需要继续分段。
2. 然后，Partition 还可以细分为多个**小文件段** Segment，一个 Partition 物理上由多个 Segment 组成，Segment 不是一个目录，而是由 3 部分组成，分别为 `.index` 文件、`.timeindex` 文件 和 `.log` 文件，分别表示偏移量索引文件、时间戳索引文件和日志数据文件。
   - **Segment 文件命名规则**：
     1. Partition 中第一个 Segment 从 `0` 开始，数值为 `20` 位数字字符长度，不够的用 `0` 填充。
     2. 后续每个 Segment 文件名，为上一个 Segment 文件**最后一条消息**的 `offset` 值。
   - **Segment 索引文件的作用**：为了进一步提高查找效率，Kafka 还为每个分段后的数据建立了**索引文件**，然后通过索引文件的**稀疏存储**，来降低 Partition 元数据的占用大小。
   - **如何查找偏移量为 118 的消息**？根据时间戳查找的方式同理。
     1. 首先，Kafka 会用一个 `ConcurrentSkipListMap` 跳跃表，来记录每个日志分段，通过它可以根据偏移量 `118` 定位到 Segment 在 00000000000000000000.index 中。
     2. 然后，通过**二分查找**在该 `.index` 文件中，找到**不大于 `offset:118`  的最大索引项**，即 `offset:116` 那栏，得到 `position:9679`。
     3. 接着，从 `.log` 文件中，物理位置为 `position:9679` 的位置，开始顺序查找 `offset:118` 的消息。

##### 生产者消息投递原理

![1634213881470](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1634213881470.png)

1. Producer 客户端分为两个线程，在创建时会创建一个 `Sender` 守护线程。
2. 主线程负责把生产的消息 -> 拦截器 -> 序列化器 -> 分区器 -> 缓存到消息累加器 -> 追加到每个分区 ProducerBatch 队列的队尾。
3. 其中，消息存放在 ProducerBatch 对象中，相当于 ProducerBatch 队列对消息进行了组装，在满足批次发送的条件时，则会通过 `Sender` 线程批次发送消息，减少网络资源消耗。
   - **批次发送的条件**：缓冲区数据大小达到 `batch.size` ，或者 `linger.ms` 达到上限时。
4. `Sender` 线程会从 ProducerBatch 队列的队头获取消息 -> 创建请求 -> 提交给 Selector 发送到 Broker 指定的分区，同时还会把请求缓存在 Node 结点，代表已发送但为收到 Broker ACK 确认的请求，如果 Producer 收到消息后，则会对其清理。
5. Broker 收到消息后，会根据 Producer 的 `acks` 配置参数，落盘消息到 Partition 中。
6. 如果 Producer 还配置了大于 0 的 `retrires` 参数，当未收到 Broker ACK 确认时，Producer 会对该消息进行重试。

##### 消费者消息消费原理

![1634213916345](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1634213916345.png)

1. Consumer 客户端负责向 Broker Partition 读取消息，通过 Consume Offset 来区分已经读过的消息，消费未读过的消息。
2. 然后会把每个 Partition 最后读取的 Consume Offset 保存在 Zookeeper 或 Kafka上，使得即使 Consumer 被关闭或重启，当前的读取状态也不会丢失。
3. 而对于消费者组，每个 Consumer 属于一个特定的消费者组，消费者组可为每个 Consumer 指定 group name，若不指定 group name，则该 Consumer 属于**默认的消费者组**。
4. 消费者组可以保证每个 Partition 只能被一个 Consumer 消费，如果组内一个 Consumer 失效，那么组内的其他 Consumer 会 Rebalance，接管已失效 Consumer 的工作。

##### 高性能原理 | 为什么 Kaka 这么快？

Kafka 对性能做了大量的优化，使得单个 Broker 就可以轻松处理数千个 Partition 以及百万级/s 的消息量，其高性能原理为：

###### 1、利用 Partition 实现并行处理

并行处理可以提升速度，因为多个人搬砖肯定比一个人搬得快。

- **集群优势**：每个 Topic 都包含一个或多个 Partition，不同 Partition 可以位于不同 Broker 节点，因此，可以充分利用集群优势，实现机器间的并行处理。
- **多磁盘优势**：另外，由于 Partition 在物理上对应一个文件夹，即使多个 Partition 位于同一个 Broker 节点，也可通过配置，让不同 Partition 落于不同的磁盘上，从而实现磁盘间的并行处理，充分发挥多磁盘的优势。

###### 2、顺序写磁盘

在许多的开源框架，比如 Kafka、HBase，都通过追加写，即顺序写磁盘的方式，来尽可能的将随机 I/O 转换为顺序 I/O，以此来降低寻址时间和旋转延时，从而最大限度的提高 IOPS。

- **消息写入时**：每个 Partition 是一个有序的、不可变的消息序列，新消息只需不断追加到 Partition 的末尾，实现顺序写磁盘。
  - **顺序写性能高的原因**：机械硬盘的连续读写性能很好，但随机读写性能很差，主要是因为磁头移动到正确的磁道上需要时间，随机读写时，磁头需要**不停的移动**，时间都浪费在了磁头寻址上，所以性能不高。
  - **顺序写的缺点**：当从文件中读一些数据时，需要倒序扫描，直到找到所需要的内容，这将会花费更多的时间。
    - **Kafka 解决方案**：日志分段 + 日志索引，见《日志索引原理》。
- **消息清除时**：
  1. 由于磁盘有限，不可能保存所有数据，所以 Kafka 还需要删除旧的数据。
  2. 又由于顺序写入的原因，Kafka 采用各种删除策略进行数据删除时，并非通过使用 `读 - 写` 模式去修改文件，而是将 Partition 分为多个 Segment，每个 Segment 对应一个物理文件，通过删除整个 Segment 的方式去删除 Partition 内的数据，从而避免了对文件随机写的操作。

###### 3、充分利用 Page Cache

![1634213586951](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1634213586951.png)

- **对于读操作**：可直接在 Page Cache 内进行，避免了对底层磁盘的操作，提高了读性能。
  - 如果消费和生产速度相当，甚至不需要通过物理磁盘，而是直接通过 Page Cache 进行交换数据。
- **对于写操作**：
  1. Broker 收到数据后，写磁盘只是暂时把数据存在 Page Cache 中，然后交由操作系统来统一**异步**写到磁盘中，减少了 Broker 访问磁盘的次数，提高了写性能。
  2. 另外，根据 I/O 调度算法， I/O Scheduler 会把连续的小块写，组装成大块的物理写，同时会尝试将一些写操作重新按顺序排好，减少磁盘头的移动时间，进一步提高了写性能。
  3. **局限**：写磁盘只是把数据写入 Page Cache，并不保证数据一定完全写入磁盘，虽然 Kafka 进程重启时， Page Cache 仍然可用，但如果在机器宕机时，则可能会由于 Page Cache 内的数据未写入磁盘，从而导致数据的丢失。
     - **解决方案**：
       - **副本机制**：这种丢失只发生在机器断电等，造成操作系统不工作的场景里，可以由 Kafka Replication 机制去解决。
       - **强制刷脏**：Kafka 提供了 `flush.messages` 和 `flush.ms` 两个参数，可以把 Page Cache 中的数据强制 Flush 到磁盘中，但是 Kafka 并**不建议使用**，这是因为如果为了保证这写情况下的数据不丢失，而强制将 Page Cache 中的数据 Flush 到磁盘，反而会降低性能。

###### 3、使用零拷贝技术

![1634213686949](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1634213686949.png)

- **背景**：Kafka 存在大量的**网络数据持久化到磁盘**（Producer 到 Broker）和**磁盘文件通过网络发送**（Broker 到 Consumer）的过程，这些过程的性能直接影响 Kafka 整体的吞吐量。

- **Producer 到 Broker**：网络数据持久化到磁盘。

  1. **传统 I/O 模式**：数据从网络传输到文件，需要 4 次数据拷贝、4 次上下文切换和 2 次系统调用。

     ![1634181215053](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1634181215053.png)

     - **4 次数据拷贝**：

       1. 首先，通过 DMA copy 将网络数据，拷贝到内核态 Socket Buffer。
       2. 然后，应用程序将内核态 Socket Buffer 数据，通过 CPU copy 读入用户态 Buffer。
       3. 接着，用户程序将用户态 Buffer 数据，再通过 CPU copy 拷贝到内核态 Buffer。
       4. 最后，通过 DMA copy 将数据拷贝到磁盘文件。
          - 这里的数据落盘，对于 Kafka的是**非实时**的，Kafka 充分利用了 Page Cache 来提高 I/O 效率。

     - **4 次上下文切换**：

       1. 用户程序调用 `Socket#read()` 读取网络数据，需要从用户态切换到内核态。
       2. 网络数据读取完成后，用户程序又需要从内核态切换回用户态。
       3. 用户程序调用 `File#write` 写入文件数据到磁盘，需要从用户态切换到内核态。
       4. 文件写入磁盘完成后，用户程序又需要从内核态切换回用户态。

     - **2 次系统调用**：socket#read、file#write。

       ```java
       data = socket.read()；// 读取网络数据 
       File file = new File()；
       file.write(data)；// 持久化到磁盘 
       file.flush()；
       ```

  2. **mmap 零拷贝优化 I/O**：

     ![1634181981080](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1634181981080.png)

     - **优化思路**：
       1. Broker 读取到 Socket Buffer 的网络数据，其实可以直接在内核空间完成落盘，没有必要将其再拷贝到用户空间中。
       2. 所以，Kafka 采用了 `mmap` ，将内核中读缓冲区 read buffer 的地址与用户空间的缓冲区 user buffer 进行映射，实现内核缓冲区与应用程序内存的共享，省去了用户空间到内核空间复制的开销，减少了 2 次用户空间与内核空间的 CPU 拷贝操作，替代为一次内核空间的直接拷贝，提高了 I/O 性能。
     - **优化实现**：
       1. Kafka Java NIO，提供了一个 `MappedByteBuffer` 类可以用来实现内存映射。
       2. MappedByteBuffer 只能通过调用 `FileChannel#map（）` 抽象方法取得，具体实现是在 `FileChannelImpl.c` ，该方法底层正是调用了 Linux 内核 `mmap` 的API。
       3. 使用 `MappedByteBuffer` 类要注意的是，`mmap` 在 Full GC 时才会被进行释放，手动 Close 时，需要反射调用 `sun.misc.Cleaner` 方法来手动清除内存映射文件。

- **Broker 到 Consumer**：磁盘文件通过网络发送。

  1. **传统 I/O 模式**：数据从文件到网络传输，也需要 4 次数据拷贝、4 次上下文切换和 2 次系统调用。

     ![1634173642699](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1634173642699.png)

     - **4 次数据拷贝**：
       1. 首先，调用 `File#read` ，通过 DMA 拷贝将文件数据读入到内核态 Buffer。
       2. 然后，应用程序通过 CPU 拷贝将内存态 Buffer 数据读入到用户态 Buffer。
       3. 接着，调用 `Socket#send `时，将用户态 Buffer 数据，通过 CPU 拷贝到内核态 Buffer。
       4. 最后，通过 DMA 拷贝将数据拷贝到 NIC Buffer。
     - **4 次上下文切换**：
       1. 用户程序调用  `File#read` 读取文件，需要从用户态切换到内核态。
       2. 文件读取完成后，用户程序又需要从内核态切换回用户态。
       3. 用户程序调用  `Socket#send` 发送数据，需要从用户态切换到内核态。
       4. 文件发送完成后，用户程序又需要从内核态切换回用户态。
     - **2 次系统调用**：file#read、socket#send。

     ```java
     buffer = File.read()；// 读取文件
     Socket.send(buffer)；// 发送文件
     ```

  2. **sendfile 零拷贝优化 I/O**：

     ![1634190319687](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1634190319687.png)

     - **优化思路**：

       1. Linux 2.4+ 内核通过 `sendfile` 系统调用，提供零拷贝，使用 `sendfile` 时，数据中转与 `mmap` 类似，不经过用户空间， `sendfile` 全程在内核态执行，一次 I/O 只需要 2 次上下文切换，数据通过 DMA 拷贝到内核态 Buffer 后，直接通过 DMA 拷贝到 NIC Buffer，无需 CPU 拷贝，这也是零拷贝这一说法的来源，大大提高了性能。
       2. 在这里，Kafka Customer 从 Broker 读取数据，采用 `sendfile`，将磁盘文件读到内核缓冲区后，转到 NIO buffer 进行网络发送，无需 CPU 拷贝，减少了 CPU 消耗，提高 I/O 吞吐量。

     - **优化实现**：

       1. Java NIO 对 `sendfile` 的支持是 `FileChannel.transferTo()/transferFrom()`，把磁盘文件读取内核缓冲区 fileChannel 后，直接转给 socketChannel 进行发送。

          ```java
          // java.nio.channels.FileChannel
          public abstract long transferTo(long position, long count,
                           WritableByteChannel target) throws IOException;
          public abstract long transferFrom(ReadableByteChannel src,
                           long position, long count) throws IOException;
          ```

       2. Kafka 数据传输通过 `TransportLayer` 来完成，其子类 `PlaintextTransportLayer` 正是通过Java NIO 的 `FileChannel.transferTo()/transferFrom()` 方法实现零拷贝。

       3. 注意，`transferTo()/transferFrom()` 并不保证一定能使用零拷贝，实际上是否能使用零拷贝与操作系统相关，如果操作系统提供 `sendfile` 这样的零拷贝系统调用，则这两个方法会通过这样的系统调用充分利用零拷贝的优势，否则并不能通过这两个方法本身实现零拷贝。

###### 4、消息批处理

在很多情况下，系统瓶颈并不是 CPU 或磁盘，而是网络 I/O，所以，Kafka 的客户端和 Broker 还会在通过网络发送数据之前，在一个 Batch 中累积多条记录（包括读和写）再批次发送，分摊数据包网络往返的开销，使用更大的数据包提高带宽的利用率。

###### 5、数据压缩后传输

数据压缩，一般都和批处理作为优化手段配套使用，Producer 可将数据压缩后再传输给 Broker，可以减少网络传输代价，目前支持的压缩算法有：Snappy、Gzip、LZ4，可通过 `compression.type` 配置。

#### API

##### Server.properties 重要配置

| 属性                               | 释义                                                         |
| ---------------------------------- | ------------------------------------------------------------ |
| zookeeper.connect                  | 指明 Broker 要连接的 ZK 集群，多个节点用逗号分隔开，ZK 用于管理 Kafka集群的元数据，比如 Topic、Partition、Leader Partition、副本 Replicas 等 |
| listeners                          | 与客户端进行交互的端口，比如消息投递、消息创建，可结合 `listener.security.protocol.map` 指定具体传输的协议类型，比如有 PLAINTEXT 明文传输、SSL 加密传输等 |
| log.dirs                           | 日志存储路径，建议配置多个路径，因为多个不同磁盘的路径，Kafka 会在含有分区目录最少的文件夹下创建新的分区目录，一来可提高吞吐量，二来可提高磁盘的容错性 |
| log.retention.{hours\|minutes\|ms} | Broker 级别的日志留存寿命，默认为 hours=168                  |
| log.retenion.bytes                 | Broker 级别的日志留存大小，默认为 -1，表示没有限制           |
| message.max.bytes                  | Broker 级别的最大消息大小，默认为 976 KB                     |
| retention.ms                       | Topic 级别的日志留存寿命                                     |
| retention.bytes                    | Topic 级别的日志留存大小                                     |
| max.message.bytes                  | 消息级别的最大消息大小                                       |
| auto.create.topics.enable          | 是否允许自动创建 Topic，建议为 false                         |
| unclean.leader.election.enable     | 是否允许选举未完全同步的副本作为 Leader                      |
| auto.leader.rebalance.enable       | 是否允许一段时间后进行 Leader 重选举，重新更换 Leader，建议为 false |

##### Broker 增删改查

```bash
# 启动Kafka
./kafka-server-start.sh -daemon ../config/server.properties
# 关闭Kafka
./kafka-server-stop.sh ../config/server.properties
# 增
kafka-topics.sh --zookeeper localhost:2181/myKafka --create --topic topic_x 
								--partitions 1 --replication-factor 1
# 删
kafka-topics.sh --zookeeper localhost:2181/myKafka --delete --topic topic_x
# 改
kafka-topics.sh --zookeeper localhost:2181/myKafka --alter --topic topic_x
								--config max.message.bytes=1048576
# 查
kafka-topics.sh --zookeeper localhost:2181/myKafka --describe --topic topic_x
```

##### 原生 POM 依赖

```xml
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka_2.12</artifactId>
</dependency>
```

##### Producer | HelloWorld

Producer 是线程安全的，允许多线程使用同一个 Producer 进行消息投递。

###### Producer 必要配置

| 属性              | 释义                                                         |
| ----------------- | ------------------------------------------------------------ |
| bootstrap.servers | Kafka Broker 地址，存在多个时使用逗号分隔，建议使用多个地址，提高容错性 |
| key.serializer    | 消息 Key 序列化器                                            |
| value.serializer  | 消息 Value 序列化器                                          |
| client.id         | 标记 kafka 客户端的 ID                                       |

```java
public static void main(String[] args) {
    // 1. 配置生产者启动的关键属性参数
    Properties properties = new Properties();
    /*1.1. 连接kafka集群的服务列表，如果有多个，使用逗号进行分隔*/
    properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "192.168.1.111:9092");
    /*1.2. 标记kafkaClient的ID*/
    properties.put(ProducerConfig.CLIENT_ID_CONFIG, "quickstart-producer");
    /*1.3. Key序列化器: kafka用于做消息投递计算具体投递到对应的主题的哪一个partition而需要的*/
    properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
    /*1.4. Value序列化器: 实际发送消息的内容序列化*/
    properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());

    // 2. 传递properties属性参数集合, 构造kafka生产者对象
    KafkaProducer<String, String> producer = new KafkaProducer<>(properties);
    for(int i = 0; i < 10; i++){
        // 3. 构造消息内容: topic, 实际消息体
        User user = new User("00" + i, "张三");
        ProducerRecord<String, String> record = new ProducerRecord<>(Const.TOPIC_QUICKSTART, JSON.toJSONString(user));

        // 4. 发送消息, 返回的是一个future对象
        producer.send(record);
        System.err.println("quickstart producer send....");
    }

    // 5. 关闭生产者
    producer.close();
}
```

##### Consumer | HelloWorld

Consumer 是非线程安全的，不允许多线程使用同一个 Consumer 进行消息投递。

- 因为在 `org.apache.kafka.clients.consumer.KafkaConsumer#acquire` 中，会校验同一个 Consumer 是否存在其他线程，如果是则会抛出 `ConcurrentModificationException`，Kafka Consumer 在执行任何都做都会先执行 `acquire` 方法来检测线程是否安全。

###### Consumer 必要配置

| 属性                    | 释义                                                         |
| ----------------------- | ------------------------------------------------------------ |
| bootstrap.servers       | Kafka Broker 地址，存在多个时使用逗号分隔，建议使用多个地址，提高容错性 |
| key.deserializer        | 消息 Key 反序列化器                                          |
| value.deserializer      | 消息 Value 反序列化器                                        |
| group.id                | 消费者所属的消费组，不配置时会使用默认的 `""` 消费者组       |
| subscribe               | 订阅消费的 Topic，支持集合方式（订阅多个 Topic）或者正则表达式（正则匹配 Topic） |
| assign                  | 指定消费 Topic 下的某个 Partition                            |
| enable.auto.commit      | 是否开启自动提交，默认为 true，实际工作中建议设置为 false，即手工提交，分为 `commitSync` 同步提交，以及 `commitAsync` 异步提交两种方式 |
| auto.commit.interval.ms | 自动提交周期，默认值为 5 s                                   |

```java
public static void main(String[] args) {
    // 1. 配置属性参数
    Properties properties = new Properties();
    /*1.1. 连接kafka集群的服务列表，如果有多个，使用逗号进行分隔*/
    properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "192.168.1.111:9092");
    /*1.2. 设置订阅组ID, 与消费者订阅组有关系*/
    properties.put(ConsumerConfig.GROUP_ID_CONFIG, "quickstart-group");
    /*1.3. Key反序列化器*/
    properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
    /*1.4. Value反序列化器*/
    properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
    /*1.5. 设置常规属性: 会话连接超时时间*/
    properties.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, 10000);
    /*1.6. 设置常规属性: 自动提交与自动提交周期, 默认不用设置*/
    properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true);
    properties.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, 5000);

    // 2. 创建消费者对象
    KafkaConsumer<String, String> consumer = new KafkaConsumer<>(properties);

    // 3. 订阅感兴趣的主题
    consumer.subscribe(Collections.singletonList(Const.TOPIC_QUICKSTART));
    System.err.println("quickstart consumer started...");

    /*监听消息*/
    try {
        while (true){
            // 4. 采用PULL的方式消费数据
            ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
            for (ConsumerRecord<String, String> record : records) {
                System.out.println(String.format("topic=%s, partition=%s, offset=%s, key=%s, value=%s",
                                                 record.topic(), record.partition(), record.offset(), record.key(), record.value()));
            }
        }
    } catch (Exception e){
        throw e;
    } finally {
        consumer.close();
    }
}
```

##### 客户端重要配置

###### Producer 重要配置

- 在请求完成之前，Producer 要求 Broker 返回 ACK 确认的策略， 同时也控制着所发送消息的持久性：
  - **acks=0**：如果设置为 0，那么 Producer 不会 Leader Broker 的任何 ACK 确认，该消息记录会被将立即添加到 socket 缓冲区并视为已发送。
    - 在这种情况下，不能保证 Broker 已经收到记录，并且 Producer 配置的重试机制也会生效，为每个记录返回的偏移量将始终设置为 -1。
  - **acks=1**：默认为 1，意味着 Leader Broker 会把记录写入其本地日志，然后做出 ACK 响应给 Producer，并不会等待所有 Follower 确认完。
    - 在这种情况下，如果 Leader Broker 在返回 ACK 确认后发生失败，Follower 被选举为新 Leader 时，由于该记录 Follower 还没完成同步，所以将导致丢失。
  - **acks=all**：相当于 `ack=-1`，意味着 Leader Broker 将等待 ISR 中所有的 Broker 确认后才返回 ACK 响应给 Producer。
    - 这是最高的可用保证，只要至少有一个同步副本保持活动状态，该记录就不会丢失。 

| 属性                    | 释义                                                         |
| ----------------------- | ------------------------------------------------------------ |
| acks                    | Broker ACK 确认策略，默认为 1                                |
| max.request.size        | 用于限制 Producer 发送消息的最大值                           |
| retries                 | 重试次数，默认为 0                                           |
| retry.backoff.msretries | 重试间隔，默认为 100                                         |
| compression.type        | 消息的压缩方式，默认为 none，支持 gzip、snappy、lz4 压缩格式 |
| connections.max.idle.ms | 用于指定在多久之后关闭限制的连接，默认为 540000 ms（9 分钟） |
| linger.ms               | 用于指定 Producer 批发送之前，等待消息加入 ProducerBatch Deque 时间，默认为 0 |
| batch.size              | 指定累加多少条消息，才进行一次批发送                         |
| buffer.memeory          | Producer 缓冲待批发送消息的大小，默认为 32 MB                |
| receive.buffer.bytes    | 用于设置 Socket 接收消息缓冲区 SO_RECBUF 的大小，默认为 32 KB |
| send.buffer.bytes       | 用于设置 Socket 发送消息缓冲区 SO_SNDBUF 的大小，默认为 128 KB |
| request.timeout.ms      | 用于设置 Producer 等待请求响应的最长时间，默认为 3000 ms     |

###### Consumer 重要配置

- 当 Kafka 中没有初始偏移量时，比如偏移量数据已被删除，可以根据以下策略进行设置：
  1. **earliest**：自动将偏移量重置为最早的偏移量。
  2. **latest**：自动将偏移量重置为最新的偏移量。
  3. **none**：如果没有找到之前的偏移量，则向 Consumer 抛出异常。

| 属性                      | 释义                                             |
| ------------------------- | ------------------------------------------------ |
| fetch.min.bytes           | 一次拉取的最小数据量，默认为 1 B                 |
| fetch.max.bytes           | 一次拉取的最大数据量，默认为 50 MB               |
| max.partition.fetch.bytes | 一次拉取一个 Partition 的最大数据量，默认为 1 MB |
| fetch.max.wait.ms         | 拉取请求的最大延迟等待时间，默认为 500 ms        |
| max.poll.records          | 每次拉取的消息最大条数                           |
| auto.offset.reset         | 偏移量丢失处理策略，默认为 latest                |

##### 客户端拦截器

###### Producer 拦截器

```java
// Kafka自定义Producer拦截器
public class CustomProducerInterceptor implements ProducerInterceptor<String, String> {
    // 消息发送前置拦截器
    @Override
    public ProducerRecord<String, String> onSend(ProducerRecord<String, String> record) {...}
    
    // 消息发送后置拦截器
	@Override
    public void onAcknowledgement(RecordMetadata recordMetadata, Exception e) {...}
    
    // 生产者关闭拦截器
    @Override
    public void close() {...}
    
    // 生产者初始化拦截器
    @Override
    public void configure(Map<String, ?> map) {...}
}

// Kafka Producer拦截器测试类
public class InterceptorProducer {
    public static void main(String[] args) {
        Properties properties = new Properties();
        ...
        // 添加生产者拦截器属性: 可以配置多个拦截器
        properties.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, CustomProducerInterceptor.class.getName());
        KafkaProducer<String, String> producer = new KafkaProducer<>(properties);
        ... 
    } 
}
```

###### Consumer 拦截器

```java
// Kafka自定义消费者拦截器
public class CustomConsumerInterceptor implements ConsumerInterceptor<String, String> {
    // 消费消息前拦截器
    @Override
    public ConsumerRecords<String, String> onConsume(ConsumerRecords<String, String> consumerRecords) {...}
    
    // 消息消费完毕提交前拦截器: 默认配置了每5s轮训一次是否提交, 所有也会每5s执行该拦截器
    @Override
    public void onCommit(Map<TopicPartition, OffsetAndMetadata> map) {...}
    
    // 消费者关闭拦截器: 从控制台关闭的不会执行, 只有代码自动关闭的才会
    @Override
    public void close() {...}
    
    // 消费者初始化拦截器
    @Override
    public void configure(Map<String, ?> map) {...} 
}

// Kafka Consumer拦截器测试类
public class InterceptorConsumer {
    public static void main(String[] args) {
        Properties properties = new Properties();
        ...
        // 添加消费端拦截器属性: 可以配置多个拦截器
        properties.put(ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, 	CustomConsumerInterceptor.class.getName());
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(properties);
        ...
    }
}
```

##### 客户端序列化/反序列化器

###### Producer 序列化器

序列化，Producer 需要用序列化器  Serializer 把对象转换成字节数组，Kafka Broker 才能接受。 

```java
// Kafka自定义User序列化器
public class UserSerializer implements Serializer<User> {
    // User序列化器初始化方法
    @Override
    public void configure(Map<String, ?> map, boolean b) {...}
    
    // User序列化器序列化方法
	@Override
    public byte[] serialize(String s, User user) {
        ...
        // 分配需要传输的字节数组: 各属性字节数组长度 + 各属性实际字节数组
        ByteBuffer byteBuffer = ByteBuffer.allocate(4 + idBytes.length + 4 + nameBytes.length);
        
        // 设置ID属性: 字节数组长度 + 实际字节数组
        byteBuffer.putInt(idBytes.length);
        byteBuffer.put(idBytes);
        
        // 设置NAME属性: 字节数组长度 + 实际字节数组
        byteBuffer.putInt(nameBytes.length);
        byteBuffer.put(nameBytes);
        
        // 返回组装好的字节数组
        return byteBuffer.array();
        ...
    }
    
    // User序列化器关闭方法
    @Override
    public void close() {...}
}

// Kafka 自定义Producer序列化器测试类
public class SerializerProducer {
    public static void main(String[] args) {
        Properties properties = new Properties();
        ...
        // Key使用默认的序列化器
        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        // Value使用自定义的序列化器
        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, UserSerializer.class.getName());
        KafkaProducer<String, User> producer = new KafkaProducer<>(properties);
        ...
    }
}
```

###### Consumer 反序列化器

反序列化，Consumer 需要把从 Broker 拉取出来的字节数组，使用反序列化器 Deserializer 转换成相应的对象。

```java
// Kafka自定义User反序列化器
public class UserDeserializer implements Deserializer<User> {
    // 自定义User反序列化器初始化方法
    @Override
    public void configure(Map<String, ?> map, boolean b) {...}
    
    // 自定义User反序列化器反序列化方法
 	@Override
    public User deserialize(String topic, byte[] data) {
        ...
        // 包装字节数组成ByteBuffer对象
        ByteBuffer byteBuffer = ByteBuffer.wrap(data);
        
        // 获取ID属性
        int idLen = byteBuffer.getInt();
        byte[] idBytes = new byte[idLen];
        byteBuffer.get(idBytes);

        // 获取NAME属性
        int nameLen = byteBuffer.getInt();
        byte[] nameBytes = new byte[nameLen];
        byteBuffer.get(nameBytes);
        ...
    }
    
    // 自定义User反序序列化器关闭方法: 从控制台关闭的不会执行, 只有代码自动关闭的才会
    @Override
    public void close() {...}
}

// Kafka 自定义Consumer反序列化器测试类
public class DeserializerConsumer {
    public static void main(String[] args) {
        Properties properties = new Properties();
        ...
        // Key使用默认的序列化器
        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        // Value使用自定义的序列化器
        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, UserDeserializer.class.getName());
        KafkaConsumer<String, User> consumer = new KafkaConsumer<>(properties);
        ...
    }
}
```

##### Producer 客户端分区器

- **默认消息分区规则**：org.apache.kafka.clients.producer.internals.DefaultPartitioner
  1. key 为空时，默认为随机值取模。
  2. key 不为空时，默认为使用 `Utils,murmur2` 计算 Hash 值后取模。

- **使用场景**：根据业务 ID Hash 到指定的 Partition，然后根据不同的业务，使用不同的 Consumer 消费。

  ![1634463959870](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1634463959870.png)

```java
// 自定义Producer Partition
public class CustomPartitioner implements Partitioner {
    
    // 自定义Producer Partition计算Partition方法
    @Override
    public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {...}
    
    // 自定义Producer Partition关闭方法
    @Override
    public void close() {...}
    
    // 自定义Producer Partition自定义方法
    @Override
    public void configure(Map<String, ?> map) {...}
}

// Kafka Producer测试类: 自定义Producer Partition
public class PartitionProducer {
    public static void main(String[] args) {
        Properties properties = new Properties();
        ...
        // 添加自定义分区器
        properties.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, CustomPartitioner.class.getName());
        KafkaProducer<String, User> producer = new KafkaProducer<>(properties);
        ...
    }
}
```

##### Consumer Group 消费者组

![1634464669407](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1634464669407.png)

- **Consumer Group 与 Consumer 的概念**：
  1. **Consumer Group 与 Consumer 是一对多的关系**：一个 Consumer Group 包含多个 Consumer，一个 Consumer 只能同时出现在一个 Consumer Group 中。
  2. **Partition 与 Consumer Group 是一对多的关系**：一个 Partition 可以被多个 Consumer Group 消费。
  3. **在同一个 Consumer Group 中，Partition 与 Consumer 是一对多的关系**：但同一个 Consumer Group 中，只允许一个 Partition 被 一个 Consumer 去消费，而一个 Consumer 又可以去消费多个 Partition。
     - 在不指定 `group.id` 时，Consumer 会使用默认的 `""` Consumer Group，所以，才会出现一个 Partition 只能被一个 Consumer 消费的规定。

- **应用场景**：得益于 Consumer 与 Consumer Group 的设计，Kafka 同时支持消息中间两种模型：

###### Piont to Piont | 点对点模式

- **概念**：点对点模式，是基于队列的，Producer 发送消息到队列，Consumer 从队列中消费消息，其中 Consumer 只能有一个。

- **Kafka 实现方式**：Consumer 都属于同一个 Consumer Group 时，此时一个 Partition 只能被一个 Consumer 消费，相当于点对点模式。

  ```java
  // Kafka Consumer1: 消费者+消费者组实现点对点模式
  public class ModuleConsumer1 {
      public static void main(String[] args) {
  		Properties properties = new Properties();
          // 同一个消费者组
          properties.put(ConsumerConfig.GROUP_ID_CONFIG, "module-group-id-1");
          ...
      } 
  }
  
  // Kafka Consumer2: 消费者+消费者组实现点对点模式
  public class ModuleConsumer2 {
      public static void main(String[] args) {
  		Properties properties = new Properties();
          // 同一个消费者组
          properties.put(ConsumerConfig.GROUP_ID_CONFIG, "module-group-id-1");
          ...
      } 
  }
  ```

###### Pub/Sub | 发布订阅模式

- **概念**：发布订阅模式，定义了如何向一个内容节点即主题 Topic，进行发布和订阅消息，Publisher 把消息发布到某个 Topic 上，Subscriber 从 Topic 中订阅消息，其中 Subscriber 可以有多个，类似于广播的模式。

- **Kafka 实现方式**：存在多个 Consumer Group 去消费 Partition，此时一个 Partition 可以被多个 Consumer Group 中的 Consumer 消费，相当于发布订阅模式。

  ```java
  // Kafka Consumer1: 消费者+消费者组实现发布订阅模式
  public class ModuleConsumer1 {
      public static void main(String[] args) {
  		Properties properties = new Properties();
          // 不同消费者组
          properties.put(ConsumerConfig.GROUP_ID_CONFIG, "module-group-id-1");
          ...
      } 
  }
  
  // Kafka Consumer2: 消费者+消费者组实现发布订阅模式
  public class ModuleConsumer2 {
      public static void main(String[] args) {
  		Properties properties = new Properties();
          // 不同消费者组
          properties.put(ConsumerConfig.GROUP_ID_CONFIG, "module-group-id-2");
          ...
      } 
  }
  ```

##### Consumer subscribe/assign

```java
// Kafka Consumer: 测试消费者订阅参数
public class CoreConsumer {
    public static void main(String[] args) {
        Properties properties = new Properties();
        ...
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(properties);
        
        /*1) 对于Consume消息的订阅subscribe方法: 可以订阅一个或者多个topic*/
//        consumer.subscribe(Collections.singletonList(Const.TOPIC_CORE));
        /*2) 对于Consume消息的订阅subscribe方法: 也可以支持正则表达式方式的订阅, 初次如果先执行Consumer会导致一开始找不到Topic的问题, 所以第一次Producer产生的消息会错过*/
//        consumer.subscribe(Pattern.compile("topic-.*"));
        /*3) 对于assign方法: 可以指定订阅某个主题下的某一个或者多个partition*/
//        consumer.assign(Arrays.asList(new TopicPartition(Const.TOPIC_CORE, 0), new TopicPartition(Const.TOPIC_CORE, 2)));
        /*4) 对于assign方法: 还拉取拉取主题下的所有partition*/
        List<TopicPartition> topicPartitionList = new ArrayList<>();
        List<PartitionInfo> partitionInfoList = consumer.partitionsFor(Const.TOPIC_CORE);
        for (PartitionInfo partitionInfo : partitionInfoList) {
            topicPartitionList.add(new TopicPartition(partitionInfo.topic(), partitionInfo.partition()));
        }
        consumer.assign(topicPartitionList);
    }
}
```

##### Consumer 手工提交

###### 开启手工提交

```java
// Kafka Consumer: 测试消费者手工提交方式
public class CommitConsumer {
    public static void main(String[] args) {
        Properties properties = new Properties();
        ...
        /* 自动提交*/
//        properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true);
//        properties.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, 5000);
        /* 手工提交*/
        properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(properties);
        ...
    }
}
```

###### 整体同步提交

```java
// ... 所有消息都消费完后
/*1) 整体提交: 同步提交(线程阻塞)*/
consumer.commitSync();
```

###### 整体异步提交

```java
// ... 所有消息都消费完后
/*2) 整体提交: 异步提交(线程非阻塞), 可回调可不回调, 这里会轮训回调函数*/
consumer.commitAsync(new OffsetCommitCallback() {
    @Override
    public void onComplete(Map<TopicPartition, OffsetAndMetadata> map, Exception e) {
        if(e != null) {
            System.err.println("error处理");
        }
        System.err.println("整体异步提交成功: " + map);
    }
});
```

###### 按消息同步提交

```java
// ... 遍历、消费每条消息完成后
/*3) 按消息做提交动作: 同步提交, 可靠*/
consumer.commitSync(Collections.singletonMap(consumerRecord.partition(), new OffsetAndMetadata(consumerRecord.offset() + 1)));
```

###### 按消息异步提交

```java
// ... 遍历、消费每条消息完成后
/*4) 在Partition内一条消息做一次提交动作: 异步提交, 可靠且高性能*/
consumer.commitAsync(Collections.singletonMap(consumerRecord.partition(), new OffsetAndMetadata(consumerRecord.offset() + 1))), new OffsetCommitCallback() {
    @Override
    public void onComplete(Map<TopicPartition, OffsetAndMetadata> map, Exception e) {
        if(e != null) {
            System.err.println("error处理");
        }
        System.err.println("在Partition内一条消息做一次异步提交成功: " + map);
    }
});
```

##### Consumer Reblance

```java
// Kafka Consumer: 测试同组消费者再均衡
public class RebalanceConsumer1 {
    public static void main(String[] args) {
        ...
        /*测试同组消费者再均衡*/
        consumer.subscribe(Collections.singletonList(Const.TOPIC_REBALANCE), new ConsumerRebalanceListener() {
            // 撤销/回收已分配的Partition
            @Override
            public void onPartitionsRevoked(Collection<TopicPartition> partitions) {
                System.err.println("Revoked Partitions:" + partitions);
            }

            // 重新分配Partition
            @Override
            public void onPartitionsAssigned(Collection<TopicPartition> partitions) {
                System.err.println("Assigned Partitions:" + partitions);
            }
        });
        ...
    }
}
```

##### Consumer 多线程消费

###### Consumer Group 多线程模型

![1634469825754](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1634469825754.png)

```java
// 一个Consumer一个线程，同属一个消费者组，消费不同的分区
public class ConsumerTest {
    public static void main(String[] args) {
        Properties properties = new Properties();
        ...
        // 通过消费者再均衡机制，保证5个分区被5个Consumer消费
        int coreSize = 5;
        ExecutorService executorService = Executors.newFixedThreadPool(coreSize);
        for(int i = 0; i < coreSize; i++){
            executorService.execute(new KafkaConsumerMt1(properties, Const.TOPIC_MT1));
        }
    }
}

// 一个Consumer一个线程，同属一个消费者组，消费不同的分区
public class KafkaConsumerMt1 implements Runnable{
    public KafkaConsumerMt1(Properties properties, String topic) {
        // 构建消费者
        this.consumerName = "KafkaConsumerMt1-" + counter.getAndIncrement();
        this.consumer = new KafkaConsumer<>(properties);

        /*查看同组消费者会再均衡情况*/
        this.consumer.subscribe(Arrays.asList(Const.TOPIC_MT1), new ConsumerRebalanceListener() {
            ...
        });
    }
    
    @Override
    public void run() {
        // 消费当前分区的消息
    }
}
```

###### Master-Worker 多线程模型

![1634470430311](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1634470430311.png)

```java
private final KafkaConsumer<String, String> consumer;
private ExecutorService executors;
...

private int workerNum = ...;
executors = new ThreadPoolExecutor(
            workerNum, workerNum, 0L, TimeUnit.MILLISECONDS,
            new ArrayBlockingQueue<>(1000),
            new ThreadPoolExecutor.CallerRunsPolicy());
...

while (true) {
  // 单个Consumer拉取消息
    ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(1));
    for (final ConsumerRecord record : records) {
        // 多线程分发消息处理，可以包含调用Consumer进行手工提交
        executors.submit(new Worker(record));
    }
}
```

##### SpringBoot POM 依赖

```xml
<dependency>
    <groupId>org.springframework.kafka</groupId>
    <artifactId>spring-kafka</artifactId>
</dependency>
```

##### SpringBoot 生产者配置

```properties
# Spring整合Kafka
spring.kafka.bootstrap-servers=192.168.1.111:9092
# Kafka Producer发送消息失败时的重试次数
spring.kafka.producer.retries=0
# 批量发送数据的配置
spring.kafka.producer.batch-size=16384
# 设置Kafka生产者内存缓冲区大小(32M)
spring.kafka.producer.buffer-memory=33554432
# Kafka消息序列化配置
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
# Kafka可靠性投递配置: 是kafka生产端最重要的选项
# Acks = 0: 生产者在成功写入消息之前不会等待任何来自服务器的响应
# Acks = 1: 只要集群的首领节点收到消息，生产者就会收到一个来自服务器成功响应(推荐, 可以发挥Kafka真正的威力)
# Acks = -1: 表示分区leader必须等待消息被成功写入到所有的ISR副本(同步副本)中才认为producer请求成功. 这种方案提供最高的消息持久性保证, 但是理论上吞吐率也是最差的
spring.kafka.producer.acks=1
```

##### SpringBoot 生产者 | HelloWorld

```java
// SpringBoot生产者
@Component
@Slf4j
public class KafkaProducerServiceImpl implements KafkaProducerService {
    @Autowired
    private KafkaTemplate<String, Object> kafkaTemplate;
    
    @Override
    public void sendMessage(String topic, Object data) {
        ListenableFuture<SendResult<String, Object>> future = kafkaTemplate.send(topic, data);
        future.addCallback(new ListenableFutureCallback<SendResult<String, Object>>() {
            /**
             * 失败回调
             * @param throwable
             */
            @Override
            public void onFailure(Throwable throwable) {
                log.error("发送消息失败: " + throwable.getMessage());

            }

            /**
             * 成功回调
             * @param result
             */
            @Override
            public void onSuccess(SendResult<String, Object> result) {
                log.info("发送消息成功: " + result.toString());
            }
        });
    }
}
```

##### SpringBoot 消费者配置

```properties
# Spring整合Kafka
spring.kafka.bootstrap-servers=192.168.1.111:9092
# Consumer消息签收机制: 手工签收
spring.kafka.consumer.enable-auto-commit=false
spring.kafka.listener.ack-mode=manual
# 该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下该作何处理:
# none: 抛出异常
# latest: (默认值)在偏移量无效的情况下, 消费者将从最新的记录开始读取数据(在消费者启动之后生成的记录)
# earliest: 在偏移量无效的情况下, 消费者将从起始位置读取分区的记录
spring.kafka.consumer.auto-offset-reset=earliest
# 序列化配置
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
# 消费并行数
spring.kafka.listener.concurrency=5
```

##### SpringBoot 消费者 | HelloWorld

```java
// SpringBoot消费者
@Component
@Slf4j
public class KafkaConsumer {
    @KafkaListener(groupId = "group02", topics = "topic02")
    public void onMessage(ConsumerRecord<String, Object> record, Acknowledgment acknowledgment, Consumer<?, ?> consumer){
        log.info("消费端接收消息: {}", record.value());
        acknowledgment.acknowledge();// 手工签收
    }
}
```

#### 高可用架构

1. Kafka 为分区引入了**多副本机制**，通过增加副本数量可以提升容灾能力，以及实现故障的自动转移，当集群中某个 Broker 失效时仍然能保证服务可用，副本处于不同的 Broker 中，当 Leader 副本出现故障时，Kafka 会从 Follower 副本中重新选举新的 Leader副本对外提供服务。
2. 同一分区的不同副本中保存的是相同的消息（不过同一时刻，副本之间可能并非完全一样），副本之间是**一主多从**的关系，Leader 副本负责处理读写请求，Follower 副本只负责与 Leader 副本的消息同步，很多时候Follower 副本中的消息相对 Leader 副本而言会有一定的滞后。
3. Kafka Consumer 也具备一定的容灾能力，Consumer 使用 Pull 模式从 Broker 拉取消息，并且保存消费的具体位置，当消费者宕机后恢复上线时，可以根据之前保存的消费位置，重新拉取需要的消息进行消费，不会造成消息丢失。

##### 分区多副本机制

1. Kafka 的复制单位是 Partition，每个 Topic 都有一个或多个 Partition，每个 Partition 都有一个 Leader 和零个或多个 Follower，这些 Leader 和 Follower 都可以称为 Replica 副本。
2. 在创建 Topic 时，可以指定分区数和复制因子，复制因子通常为 3，相当于一个 Leader 和两个 Follower。
3. 分区上的所有读取和写入都会转到 Leader Replica，Follower 会定期向 Leader 发送获取请求以获取最新消息，而 Consumer 不会从 Follower 那里消费，Follower 只是为了**冗余和故障转移**而存在。
4. 当 Broker 死亡时，对于那些失去 Leader 的 Partition，剩余节点上的 Follower 可以被提升为 Leader，但要看是否存在与 Leader 完成同步的 Follower，如果没有，还要看具体的配置策略，以决定是否允许故障转移到未完成同步的 Replica。

![1634973615452](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1634973615452.png)

##### Leader 选举机制

- **概念**：每个 Kafka 节点集群都与 Zookeeper 集群一起部署，Zookeeper 是一种分布式共识服务，允许分布式系统围绕某个给定状态达成共识，由于本身是分布式的，并且选择了一致性而不是可用性（CP），所以需要过半的 Zookeeper 节点同意后，才能接受读取和写入。

- **Zookeeper 的作用**：Zookeeper 负责存储 Kafka 集群相关状态，总是会更新 Kafka 集群状态的任何变化，以便在故障转移的情况下，Follower 可以顺利过渡到 Leader。

  1. 记录Topic 列表、Partition、配置、Leader Replica、首选 Leader Replica。
  2. **记录集群成员**：每个 Broker 都会向 Zookeeper 集群发送心跳，当 Zookeeper 在一段时间后未能收到心跳时，Zookeeper 会认为 Broker 已失败或不可用。
  3. **记录控制器节点**：包括控制器死机时的故障转移节点。

- **控制器节点**：Electing the controller node，Controller 节点是 Kafka Broker 之一，负责在节点加入或离开集群时选举 Leader，在 Zookeeper 向 Controller 发送有关集群成员和主题更改的通知后， Controller 会对这些更改采取相关的行动，其关系如下：

  1. **创建 Partition**：比如，当创建具有 10 个 Partition 以及复制因子为 3 的新 Topic 时，Controller 会为每个 Partition 选出一个 Leader，尝试在 Broker 之间以最佳方式分配 Leader。
  2. **ISR 变化**：然后，Leader 负责维护 ISR 集合，使用 `replica.lag.time.max.ms` 来确定ISR 成员的资格，当 ISR 发生变化时，Leader 会更新其变化到 ZK，然后由 ZK 通知 Controller 节点。
  3. **Leader 宕机**：而当 Leader 宕机时，ZK 会向 Controller 发送选举新 Leader 的通知，Controller 会向所有 Broker 发送一个命令，通知 Leader 即将发生变化。
  4. 总之，Zookeeper 总是会更新状态的任何变化，以便在故障转移的情况下，**让新 Leader 可以顺利过渡**。

  ![1634979553992](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1634979553992.png)

##### Leader Rebalance 方法

1. 虽然 Kafka 有首选副本 Leader（preferred replica leaders）的概念，当 Kafka 创建 Topic 的 Partition 时，会尝试将每个 Partition 的Leader Replica 均匀分布在节点上，并将这些第一个 Leader 标记为首选副本 Leader，但随着时间的推移，由于服务器重启、服务器故障和网络分区等原因，Leader Replica 可能最终都会落在同一个节点上。

2. 为了解决这个问题，Kafka 提供了两个选项：

   - 主题配置  `auto.leader.rebalance.enable=true` ：允许 Controller 节点将领导权重新分配回首选副本 Leader，从而恢复均匀分布。
   - 另外，管理员也可以使用 `kafka-preferred-replica-election.sh` 脚本手动恢复均匀分布。

   ![1634974526584](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1634974526584.png)

##### ISR、OSR、AR 概念

- **目的**：Partition Replica 机制确实可以实现一定程度的可用性，但实际情况更加复杂，为了**平衡数据一致性与可用性**，Kafka 引入了 ISR 同步副本机制，允许在大多数副本失败时，仍然能够提供可用性，最大程度减少**死副本与慢副本**在延迟方面的影响。

  - 对比 RabbitMQ 镜像模式中，慢速镜像会引入更长的延迟，死镜像则会占用要检测的心跳时间。

- **概念**：

  - **ISR**：In Sync Replicas，分区中所有与 Leader 保持**一定程度同步**的副本集合，在 Kafka 中，消息会先发送到 Leader，然后 Follower 会以 `replica.fetch.wait.max.ms` 间隔发出 fetch 请求，默认为 500ms，从 Leader 中拉取消息进行同步，同步期间内 Follower 相对于 Leader 会存在**一定程度**的滞后，这种 “**一定程度**” 是指 Kafka **可以忍受**的滞后范围，可通过`replica.lag.time.max.ms` 进行配置。
    - **replica.lag.time.max.ms**：指 Follower 每次同步的最大延迟，默认为 10s，在使用 `acks=all` 时则代表每次客户端请求的最大延迟，如果 Follower 在该时间段内的某个时间点能够与 Leader 实现完全同步，则可以认为 Follower 是**一定程度同步**的，Leader 将允许该副本在 ISR 中。
      - 对比 RabbitMQ 的复制不是由镜像发起，而是由 Master 发起，再由 Master 把更改推送到其他镜像。
      - 如果发现死副本或者慢副本，则 Leader 会把它从 ISR 中剔除，而死副本与慢副本的判断与该值有关：
        - **死副本**：如果 Follower 在 `replica.lag.time.max.ms` 时间段内都没有发出 fetch 请求，则 Leader 会认为该 Follower 已经死了。
        - **慢副本**：如果 Follower fetch 时间超过了 `replica.lag.time.max.ms`，则 Leader 会认为该 Follower 是一个慢 Follower。
  - **OSR**：Out Sync Replicas，相对于 ISR 相反，与 Leader 同步滞后过度的副本组成 OSR 集合。
  - **AR**：Assigned Replicas，分区中所有的副本集合，即 AR = ISR + OSR，在正常情况下，所有 Follower 应该都与 Leader 保持**一定程度的同步**，即 AR = ISR，此时 OSR 为空。

  ![1635092377216](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635092377216.png)

- **ISR与 OSR 的转换规则**：

  1. Leader 负责维护与跟踪 ISR 中所有 Follower 的滞后状态，当 Follower 落后太多或者失效时，Leader 会把它从 ISR 中剔除；而如果 OSR 中 Follower 追上了 Leader，则 Leader 会将它加入 ISR  集合。
  2. 只有在 ISR 中的副本才有资格被选举为 Leader，而在 OSR 中的副本则没有选举为 Leader 的资格，而这种规则可通过 `unclean.leader.election.enable` 进行配置。

- **局限**：当 ISR 缩小仅为 Leader Replica 时，如果 Leader 宕机，且没有任何同步副本时，则无法保证高可用。

  - **解决方案**：使用 `min.insync.replicas` 主题配置进行控制。

##### LEO、LW、HW、LSO 概念

- **概念**：ISR 与 HW、LEO 有紧密的关系，HW 可以辅助 Kafka 完成副本复制，而如果 LEO#Remote_LEO < Leader#LEO 的时间，不超过 `replica.lag.time.max.ms`，则可以认为该 Remote_LEO 对应的 Follower 处于**一定程度**上的同步，也就处于 ISR 集合中。

  - **LEO**：Log End Offset，日志结束偏移量，表示当前日志文件中下一条待写入消息的 offset。

    - 比如，LEO=15，表示当前日志记录的最后一条消息 offset=14，下一条写入的消息 offset=15。

  - **HW**：High Watermark，高水位标记，表示下一条能消费的 offset，Consumer 只能拉取到这个 offset 之前的消息。

    - 比如，HW=8，表示 Consumer 只能消费 offset < 8 的消息，而 offset=8 的消息对 Consumer 而言是不可见的。
    - 对于 ISR 而言，HW = MIN（LEO），即最小的 LEO 等于分区的 HW，小于等于 HW 的消息都可以被认为**已备份**。
    - **HW 作用**：用于标识哪些消息可以被消费，以及帮助 Kafka 完成副本同步。

    ![1634981204253](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1634981204253.png)

  - LW：Low Watermark，低水位标记，标识 AR 集合中最小的 logStartOffset。

    - 副本的拉取请求 FetchRequest，有可能触发新建日志分段以及旧分段的清理，进而导致logStartoffset 的增加，从而促使 LW 的增长。

  - LSO：Last Stable Offset，具体与 Kafka 事务有关，对于未完成的事务而言，LSO 等于事务中的第一条消息所在的位置；对于已经完成的事务而言，LSO 与 HW 相等，因此，LSO <= HW <= LEO。

- **复制流程**：

  1. **初始状态**：初始时，Leader#HW、Leader#LEO、Leader#Remote LEO、Follower#HW、Follower#LEO 都等于 0。

     ![1635137338175](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635137338175.png)

  2. **Leader 消息持久化**：当 Leader 收到消息时，首先会在本地持久化，注意，这里的持久化，Kafka 只是将消息写入内存，而不是磁盘，此后，Leader#LEO=1。

     - 由于追求性能，Kafka 决定在消息进入内存后，就进行发送 ACK 确认给 Producer，然后每隔一段时间 `fsyncs` 内存中的消息到磁盘，通过**副本冗余**的机制，进行弥补消息未落盘的风险。
     - RabbitMQ 也会定期写入磁盘，但不同的是 RabbitMQ 只会在 Master 和所有镜像都将消息写入磁盘后，才发送 ACK 确认给 Producer。

     ![1635137428129](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635137428129.png)

  3. **Follower 第一次 Fetch**：Follower 定期发出一个 fetch 请求，发送 Follower#LEO=0 给 Leader，Leader 接收到 Follower#LEO=0 后，则响应 offset=1 消息，以及 Leader#HW=0 给 Follower，此后，Follower#LEO=1，此时，Follower 和 Leader 的 LEO 都等于 1，但各自的 HW 还是为 0，需要在下一轮的拉取中才被更新。

     ![1635137604461](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635137604461.png)

  4. **Follower 第二次 Fetch**：Follower 定期发出一个 fetch 请求，发送 Follower#LEO=1 给 Leader，Leader 接收到 Follower#LEO=1 后，更新 Leader#Remote LEO=1，然后如果判断到所有 Follower 都以该 offset 持久化完毕后，则会推进 Leader#HW，把 Leader#HW 更新为 1，响应下一条 offset 消息，以及 Leader#HW=1 给 Follower，以让它们更新 Follower#HW=1。

     ![1635137982950](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635137982950.png)

  | 更新对象          | 更新时机                                                     |
  | ----------------- | ------------------------------------------------------------ |
  | Leader#LEO        | Leader "持久化" Producer 消息后，会更新其本地的 LEO 值       |
  | Follower#LEO      | Follower 从 Leader 拉取消息并“持久化”后，会更新其本地的 LEO 值 |
  | Leader#Remote LEO | Follower 把自己本地的 Follower#LEO 传给 Leader，告知从哪个位移开始拉取，Leader 会使用这个值更新 Leader#Remote LEO |
  | Follower#HW       | Follower 更新完 Follower#LEO 后，会比较该值与收到的 Leader#HW，取两者中的最小值更新 Follower#HW |
  | Leader#HW         | Leader 更新完 Leader#LEO 或者 Leader#Remote LEO 后，会取 Leader#LEO 与 所有的 Leader#Remote LEO 中的最小值更新 Leader#HW |

- **数据丢失场景**：前提为 `min.insync.replicas=1`，即 ISR 集合只有一个副本也可以提供服务。

  1. **初始状态**：初始时，Leader#HW、Leader#LEO、Leader#Remote LEO、Follower#HW、Follower#LEO 都等于 1。
  2. **Leader 消息持久化**：某个时刻，有一个使用了默认 `acks=1` 设置的 Producer 向 A 发送了一条消息，A 持久化完毕后，会通知 Producer 消息已投递成功。
  3. **Follower 第一次 Fetch**：Follower 定期发出一个 fetch 请求，发送 Follower#LEO=1 给 Leader，Leader 接收到 Follower#LEO=1 后，则响应 offset=2 消息，以及 Leader#HW=1 给 Follower，此后，Follower#LEO=2。
    - 此时，Follower 和 Leader 的 LEO 都等于 2，但各自的 HW 还是为 1，需要在下一轮的拉取中才被更新。
  4. **Follower 第二次 Fetch**：Follower 定期发出一个 fetch 请求，发送 Follower#LEO=2 给 Leader，Leader 接收到 Follower#LEO=2 后，更新 Leader#Remote LEO=2，然后如果判断到所有 Follower 都以该 offset 持久化完毕后，则会推进 Leader#HW，把 Leader#HW 更新为 2，响应下一条 offset 消息，以及 Leader#HW=2 给 Follower，以让它们更新 Follower#HW=2。
  5. **Follower B 重启并截断**：然而，由于 Follower#HW 的更新时间与 Leader#HW 的更新时间有错配，在 Leader#HW 更新后，响应时 Follower#HW 才会更新，如果响应时 Follower B 发生重启，那么在 B 重启完成后，B 会根据当前的 HW=0 进行日志截断，把 LEO 调整为 0，导致之前更新 LEO=2 的消息在 B 上发生了丢失。
    - **日志阶段的原因**：确保副本之间没有分歧，在选举时截断到新 Leader 的 HW，保证每个 Follower 日志的一致性。
  6. **Leader A 重启并截断**：B 截断后 LEO=2 后，发生定期同步 Leader A 的日志，但此时 Leader A 也发生了重启，需要进行故障转移，如果 B 还在 A 的 ISR 集合中，则 A 会把故障转移到 B，B 就是成为了新的 Leader。当 A 重启恢复后，成为了 B 的 Follower，由于 Follower#HW 不能高于 Leader#HW，所以 A 会对 LEO=2 进行日志截断，导致 offset=2 的消息永久丢失。

  ![1635044350580](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635044350580.png)

- **数据不一致场景**：

  1. 假设有 A、B 两个Broker，初始时 A 是 Leader，B 是 Follower，首先 A 接收到消息 m2，但 B 还没来得及复制，B 就宕机了，然后 A#HW+1=1，A#LEO=1，B#LEO=0，B#HW=0。
  2. 过了一会，A 宕机了，B 却恢复了，B 成为了 Leader，然后又接收了消息 m3，由于 A 仍未恢复，所以 B#HW+1=1，B#LEO=1。
  3. 再过了一会，A 也恢复了，由于 A#HW=B#HW，所以，A 并不会进行日志截断，导致此时出现了消息不一致的情况。

- **Leader Epoch 机制**：出现上面数据丢失、数据不一致场景的根本原因是，Follower#HW 需要在第二轮 Fetch 响应时才被更新，如果在这期间出现 Follower 重启，会导致之前的 LEO 处的日志被截断，对此 Kafka 0.11 引入了 Leader Epoch 机制来取代 HW 的辅助复制工作， 修复这种在 Leader 连续变更场景下的数据丢失和数据不一致问题。

  - **概念**：大致可以认为是 Leader 的版本号，由两部分数据组成。

    - **Epoch**：一个单调增加的版本号，每当 Leader 发生变更时，都会增加该版本号，小版本号的 Leader 会被认为是过期的 Leader，不再行使 Leader 的权力。
    - **Start Offset**：起始位移，Leader 在该 Epoch 上，首条写入消息的 offset。

  - **原理**：

    1. 使用该机制后，每个消息都会包含一个 4 字节的 Epoch 数字，每个 log 目录会创建 **Leader Epoch Sequence File** 来存储 Epoch 和 Start Offset。
    2. 当一个副本**成为 Leader** 后，它会首先在 Leader Epoch Sequence File 末尾添加一个新的记录，并把该记录刷到磁盘中，以后在该 Leader 下每条新的记录就被 Epoch 标记。
    3. 而当一个副本**成为 Follower** 时（比如发生了重启），则会执行以下工作：
       1. 从 Leader Epoch Sequence File 中恢复所有的 Epoch，因为可能宕机太久，这期间换了好几次leader，所以才要把这些 Epoch 消息都恢复过来。
       2. 向 Leader 发送一个 LeaderEpochRequest，其中该请求包含了 Follower#Leader Epoch Sequence File 中最新的 Epoch。
    4. Leader 接收到 LeaderEpochRequest 后，会向 Follower 响应对应 LeaderEpoch 的 LastOffset，这个 **LastOffset** 有两种可能：
       1. 一种是，比 LeaderEpochRequest#Epoch 大 1 的 offset。
       2. 另一种是，如果 LeaderEpochRequest#Epoch 与 Leader#Epoch 相等，则返回 Leader#LEO。
    5. 如果有任何 Follower#StartOffset **大于**返回的 Leader#LastOffset，那么 Follower 会重置自己的 Leader Epoch Sequence File 来和 Leader 保持一致，然后**截断本地日志**到 Leader#LastOffset 位置，再开始从 Leader 获取数据。
    6. 而 Follower 在从 Leader 获取数据时，如果 Follower 发现 Leader#Epoch 比 Follower#Epoch **还大**，那么它会添加这个 Epoch 和 StartOffset 到 Leader Epoch Sequence File 文件，并刷写到磁盘，然后继续获取数据。

  - **解决数据丢失问题**：

    1. 假设有 A、B 两个Broker，初始时 B 为 Leader，首先，Follower A 从 Leader B 中取到消息 m2，此时 A#，但是 A#HW 只在下一轮 RPC 才会更新，所以此时 A#HW 没变。
    2. 这时候 A 重启并恢复，采用 Leader Epoch 机制，A 并不会根据 HW 进行截取自己的日志到 HW，而是向 Leader B 发送 **LeaderEpochRequest**，由于 B#Epoch 等于 LeaderEpochRequest#Epoch，所以 B 返回给 A 的是 B#LEO=2，同时也并不存在 A#StartOffset 大于返回的 B#LEO=2，所以 A 不会对日志进行截取。
    3. 接着，当 B 宕机时，A 成了新的 Leader，则A 会在 Leader Epoch Sequence File 文件中添加新的Epoch 和 StartOffset。
    4. 当 B 恢复时，B 也会采取 Leader Epoch 机制，向 Leader A 发送 **LeaderEpochRequest**，由于 A#Epoch 等于 LeaderEpochRequest#Epoch，所以 A 返回给 B 的是 A#LEO=2，同时也并不存在 B#StartOffset 大于返回的 A#LEO=2，所以 B 也不会对日志进行截取，因此，不会发生消息 m2 永久丢失的情况，即使用 Leader Epoch 机制解决了数据丢失的问题。

    ![1635155757877](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635155757877.png)

  - **解决数据不一致问题**：（忽略 m1）

    1. 假设有 A、B 两个Broker，初始时 A 是 Leader，B 是 Follower，首先 A 接收到消息 m2，但 B 还没来得及复制，B 就宕机了，然后 A#HW+1=1，A#LEO=1，B#LEO=0，B#HW=0。
    2. 过了一会，A 宕机了，B 却恢复了，B 成为了 Leader，然后又接收了消息 m3，由于 A 仍未恢复，所以 B#HW+1=1，B#LEO=1，同时又因为采用了 Leader Epoch 机制，B#Epoch+1=1。
    3. 再过了一会，A 也恢复了，由于采用了 Leader Epoch 机制，A 并不根据 A#HW=B#HW 进行日志截断，而是向 B 发送 A#Epoch=0 < B#Epoch，B 返回 B#LEO=1 以及 m3。
    4. A 接收到 m3 后，发现自己的 A#LEO=2 > B#LEO=1 了，同时发现 B#Epoch=1 > A#Epoch，则重置自己的 Leader Epoch Sequence File 来和 B 保持一致，即Epoch=1 和 StartOffset=1，并刷写到磁盘，然后截断本地日志到 B#LEO=1 的位置，从而保证了消息的数据一致性，因此使用 Leader Epoch 机制解决了数据不一致的问题。

    ![1635156753134](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635156753134.png)

##### 节点新加入集群场景

|                  | Kafka                                      | RabbitMQ                        |
| ---------------- | ------------------------------------------ | ------------------------------- |
| 是否会拒绝写入   | 新节点异步获取日志消息，Master 无阻塞      | Master 拒绝任何客户端的读写操作 |
| 是否会读入旧消息 | 是，异步获取，直到追赶上 Leader 被加入 ISR | 会扔掉旧数据                    |
| 适用场景         | 大队列                                     | 小队列                          |

##### 网络分区场景

与 RabbitMQ 相比，Kafka 具有更多的组件，所以当 Kafka 集群出现网络分区时，会出现更复杂的场景，每一种场景都会 Kafka 都会表现出不同的行为：

###### 1. Follower 看不到 Leader，但能看到 ZK

- **分区现象**：

  1. 网络分区把 Broker 3 与 Broker 1、2 分开，但没有和 ZK 分开。
  2. 此时，Broker 3 不再能够发送 fetch 请求，且在由于 `replica.lag.time.max.ms` 被剔出 ISR 集合，将不能参与消息提交。
  3. 而 ZK 在整个过程中，可以继续接收 Leader 的心跳，始终被认为处于活动的状态且运行良好。

- **分区解决后**：一旦分区被解决，Broker 3 将恢复 fetch 请求，在赶上 Leader 一定程度后，会重新加入 ISR 集合。

- **优点**：对比 RabbitMQ，Kafka 分区并不会出现脑裂或暂停节点的现象。

- **缺点**：减少了某个副本的冗余。 

  ![1635238242566](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635238242566.png)

###### 2. Leader 看不到 Followers，但能看到 ZK

- **分区现象**：

  1. 网络分区把 Leader 与其他所有 Follower 分开，但仍然可以看到 ZK。
  2. 类似于场景 1 一样，由于 Leader 维护着 ISR 集合，看不到任何 Follower 会导致 ISR 缩小到只有 Leader。
  3. 而 ZK 在整个过程中，可以继续接收 Leader 的心跳，始终被认为处于活动状态且运行良好。

- **分区解决后**：一旦分区被解决，所有 Follower 将恢复与 Leader 的同步，它们在赶上 Leader 一定程度后，会重新加入 ISR 集合。

- **优点**：同样没有裂脑或者暂停节点的发生。

  **缺点**：在分区解决之前，新写入的消息将会丢失所有的副本冗余，当 Leader 宕机，Kafka 将会**停止服务**。

  ![1635238385327](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635238385327.png)

###### 3. Follower 能看到 Leader，但看不到 ZK

- **分区现象**：
  1. Follower 与 ZK 分开，但没有和 Leader 分开。
  2. 此时，Follower 可以继续发出 fetch 请求，并维持 ISR 成员的状态。
  3. 而 ZK 将不再接收 Follower 的心跳，会认为它已经死了，但由于它只是一个 Follower，因此不会受到任何影响。
- **分区解决后**：一旦分区被解决，该 Follower 将重新向 ZK 发送心跳包，ZK 则认为它重新上线。
- **优点**：同样没有裂脑或者暂停节点的发生，且没有副本冗余的丢失。

![1635238848539](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635238848539.png)

###### 4. Leader 能看到 Followers，但看不到 Zk

- **分区现象**：
  1. Leader 与 ZK 分开，但没有与 Followers 分开。
  2. 一段时间后，ZK 会该 Leader 标记为已死，并且通知 Controller 选举一个 Follower 成为新 Leader。
  3. 然而，旧 Leader 还继续认为它是 Leader，能够在短时间内继续接受 `ack=1` 的写入，而 Follower 则不再向该旧 Leader 发送 fetch 请求，旧 Leader 会认为 Followers 都发生死亡，并尝试把 ISR 缩小到旧 Leader 自身，但它又无法这么做，因为 ISR 的任何变化都会先给 ZK，而它没有与 ZK 相连，将停止接受写入。
  4. 而 `acks=all` 的消息 Producer 将不会得到 ACK 确认，因为旧 ISR 包括所有副本，Followers 不会返回消息确认给旧 Leader，旧 Leader 也就不会返回 ACK 给 Producer，而且 旧 Leader 在尝试把 Followers 都从 ISR 中删除时，由于没有与 ZK 相连，将导致它删除失败，因此会一直拒绝写入。
  5. 而客户端每 60 秒会更新一次最新的元数据，然后他们将被告知 Leader 发生变更，并开始发送消息给新 Leader。
- **分区解决后**：一旦网络分区得到解决，旧 Leader 将知道它自己不再是 Leader，然后会把自己的日志截断到新 Leader#HW 处，然后开始向新 Leader 发送 fetch 请求。
- **优点**：同样没有裂脑或者暂停节点的发生，且没有副本冗余的丢失。
- **缺点**：自网络分区开始以来，对旧 Leader 所做的 `ack=1` 确认写入都将丢失。
  1. 集群会在短时间内处于裂脑的状态，但前提是 `acks=1` 并且 `min.insync.replicas=1` ，当网络分区被解决时，裂脑会自动结束，旧 Leader 将会意识到它不再是 Leader，或者所有客户端都意识到 Leader 已经改变，并开始发送消息给新 Leader，但无论哪种方式，都会在短时间内发生一些消息丢失，但仅限于 `acks=1` 。
  2. 这种情况还有一个变体，就在网络分区之前，Follower 落后，Leader 将 ISR 缩小到自己，然后发生网络分区隔离了 Leader，同时还选举了一个新 Leader，但旧 Leader 仍继续接受写入，甚至 `acks=all` 也因为 ISR 只有旧 Leader 自己，当网络分区解决后，这些写入将发生丢失。
  3. 因此，为了避免这些情况，唯一的解决方案是使用 `min.insync.replicas = 半数节点`。

![1635239217734](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635239217734.png)

###### 5. Follower 完全分区

- **分区现象**： 
  1. Follower 与 Leader 和 ZK 完全隔离。
  2. Follower 只会被简单地从 ISR 中剔除。
- **分区解决后**：一旦网络分区得到解决，它将恢复 fetch 请求，在赶上 Leader 一定程度后，会重新加入 ISR 集合。
- **优点**：同样没有裂脑或者暂停节点的发生。
- **缺点**：减少了某个副本的冗余。

![1635240105377](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635240105377.png)

###### 6. Leader 完全分区

- **分区现象**：

  1. Leader 与 Followers、Controller 和 ZK 完全隔离。
  2. Leader 能够在短时间内继续接受 `ack=1` 的写入，但将不能接受 Followers#fetch 请求，此时会认为 Followers 都发生死亡，并尝试把 ISR 缩小到自身，但它又无法这么做，因为 ISR 的任何变化都会先给 ZK，而它没有与 ZK 相连，将停止接受写入。
  3. 而 `acks=all` 的消息 Producer 将不会得到 ACK 确认，因为旧 ISR 包括所有副本，Followers 不会返回消息确认给旧 Leader，旧 Leader 也就不会返回 ACK 给 Producer，而且 旧 Leader 在尝试把 Followers 都从 ISR 中删除时，由于没有与 ZK 相连，将导致它删除失败，因此会一直拒绝写入。
  4. 同时，一段时间后，ZK 会该 Leader 标记为已死，并且通知 Controller 选举一个 Follower 成为新 Leader。
  5. 而客户端每 60 秒会更新一次最新的元数据，然后他们将被告知 Leader 发生变更，并开始发送消息给新 Leader。

- **分区解决后**：一旦网络分区被解决，旧 Leader 将通过 ZK 发现它不再是 Leader，然后它将把日志截断到新 Leader#HW，并作为 Follower 开始发送 fetch 请求。
- **缺点**：自网络分区开始以来，对旧 Leader 所做的 `ack=1` 确认写入都将丢失。
  1. 集群会在短时间内处于裂脑的状态，但前提是 `acks=1` 并且 `min.insync.replicas=1` ，当网络分区被解决时，裂脑会自动结束，旧 Leader 将会意识到它不再是 Leader，或者所有客户端都意识到 Leader 已经改变，并开始发送消息给新 Leader，但无论哪种方式，都会在短时间内发生一些消息丢失，但仅限于 `acks=1` 。
  2. 这种情况还有一个变体，就在网络分区之前，Follower 落后，Leader 将 ISR 缩小到自己，然后发生网络分区隔离了 Leader，同时还选举了一个新 Leader，但旧 Leader 仍继续接受写入，甚至 `acks=all` 也因为 ISR 只有旧 Leader 自己，当网络分区解决后，这些写入将发生丢失。
  3. 因此，为了避免这些情况，唯一的解决方案是使用 `min.insync.replicas = 半数节点`。

![1635240417911](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635240417911.png)

###### 7. Controller 看不到 Broker

- **分区现象**：
  1. Controller 与 Broker 节点隔离，会导致 Controller 将无法向该 Broker 节点传达任何领导权的变更。
  2. 在最坏的情况下，可能会发生像场景 6 那样短期的裂脑，即 Controller 无法告诉旧 Leader 它已经不是 Leader了，从而使得它与新 Leader 共存，且在短时间内，旧 Leader 仍能接受 `ack=1` 的消息写入。
- **分区解决后**：一旦分区被解决，Controller 将恢复与该 Broker 的通信，该 Broker 也会意识到它不再是 Leader，然后它将把日志截断到新 Leader#HW，并作为 Follower 开始发送 fetch 请求。

###### 8. Controller 看不到 ZK

- **分区现象**：
  1. Controller 与 ZK 隔离。
  2. 由于 Controller 缺少心跳，ZK 会将 broker 标记为已死，并且选举一个新的 Broker 节点作为 Controller。
  3. 旧 Controller 可能会继续认为它自己是 Controller，但由于它无法接收来自 ZK 的任何通知，因此并不会执行任何操作。
- **分区解决后**：一旦分区被解决，它将会意识到它不再是 Controller，而只是一个普通的 Kafka 节点，因此不会造成什么影响。

###### 网络分区场景总结

1. 可以看到，Follower 发生网络分区，不会导致消息丢失，只是减少了 Follow 的副本冗余。
2. Leader 与 Follower 隔离，会丢失所有的副本冗余，当 Leader 还发生宕机，有较小概率发生不可用。
3. Leader 与 ZK 隔离，会发生短暂的脑裂，导致自网络分区开始以来，对旧 Leader 写入 `ack=1` 的消息都会丢失，解决方案为 `ack=all` ，而使用 `min.insync.replicas=半数节点` 可以提供额外的一致性保证，保证脑裂期间不会发生消息丢失，但会失去部分的可用性。
4. Controller 与 Broker 隔离，会导致 Broker 感知不到 Leader 的变化，当隔离的 Broker 刚好为 Leader 时，还可能会发生同 3 一样的短暂脑裂现象，而 Controller 与 ZK 隔离，则不会造成任何影响。

#### 常见问题解决

##### 如何保证数据不丢失？

参考《RabbitMQ - 如何保证数据不丢失》，数据丢失的场景有：生产端丢数据、MQ 丢数据、消费端丢数据：

- **解决方案**：
  1. **生产端丢数据**：生产消息可以通过 Comfirm 机制解决，消息状态打标 + 消息落库 + `ack=all` + 定时重发（`retries` + `retry.backoff.msretries`） + 人工介入/失败补偿 。
  2. **MQ 丢数据**：Broker 使用 ISR 副本机制保证高可用。
  3. **消费端丢数据**：可以关闭自动提交offset功能 `enable.auto.commit=false`，在消费完成后才提交offset。

##### 如何防止重复消费？

参考《RabbitMQ - 如何防止重复消费》。

##### 一致性与可用性保障？

详情见《Kafka - 高可用架构》

`acks=all` 是最安全的选项，但会引入额外的延迟。事实证明，在所有的故障模式下，分布式系统不可能同时保证无数据丢失的**最终一致性**以及时刻都接受读取和写入的**高可用性**，因此需要做的是，选择要针对其中的一些进行优化，让一致性和可用性处于一个**范围的两端**。对此，Kafka 提供了调整参数，以让一致性和可用性适合需要的场景。

###### 集群可调整参数

1. **消息 Confirm 机制**：Producer `ack=all`，以及 Consumer `enable.auto.commit=false`。
2. **分区多副本机制**：在创建 Topic 时，可以指定分区数和复制因子，复制因子通常为 3，相当于一个 Leader 和两个 Follower。
   - 其中，复制因子为 5 且 `min.insync.replicas` 为 3 发生消息丢失，是一件非常罕见的事件。
3. **故障转移策略**：`unclean.leader.election.enable`：
   - **true**：允许故障转移到未完成同步的 Follower。
   - **false**：默认为false，禁止故障转移到未完成同步的 Follower，只允许转移到已经完成同步的 Follower，如果没有任何同步副本，则故障转移时会拒绝客户端所有的读取和写入操作。
4. **ISR 同步策略**：主题配置 `min.insync.replicas`，ISR 集合中最少的副本数量，如果客户端写入消息时，ISR 中的副本数量低于该数量，那么 Broker 以 NotEnoughReplicas 的错误响应给客户端，以拒绝写入。
5. **脑裂处理策略**：`ack=all` + `min.insync.replicas=半数节点` 可以提供最高级别的一致性。
6. **确保客户端连接**：在 Producer 和 Consumer `bootstrap.servers` 配置中，指定多个可以连接的 Broker，这样，即使其中一个 Broker 节点出现故障，客户端也有它知道的多个 Broker 节点，用于打开与 Broker 的连接，此时客户端可以询问出哪个节点，才是托管其想要读/写的分区 Leader。

###### 对比 RabbitMQ

|                      | Kafka                                                        | RabbitMQ                                                     |
| -------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 实现持久性与高可用性 | 主从复制                                                     | 主从复制                                                     |
| 持久性与可用性问题   | 异步非阻塞复制，不存在持久性与可用性问题，但复制需要占用大量的网络带宽 | 大队列持久性问题，要么减少冗余增加数据丢失风险，要么接受长时间不可用，可保持小队列，舍弃部分可用性，通过连接重试来处理，以保证冗余，减少数据丢失风险 |
| 集群同步发生故障     | 消息不落盘，效率高，但可能会消息丢失，通过副本冗余机制来减少风险 | 消息落盘，能够发挥出持久化优势，但增加了额外的延迟           |
| 总结                 | 处理大量消息、易扩展、一致性可调                             | 无大队列（大队列可拆成多个小队列）、用法灵活                 |

##### 如何实现 Consumer Rebalance？

Consumer Rebalance，本质上是一种协议，规定了一个 Consumer Group 下，所有 Consumer 如何达成一致，来分配订阅 Topic 下的每个 Partition。

######  触发条件

Consumer Rebalance 的触发条件有3个：

- **组成员数量发生变化**：比如有新的 Consumer 加入或者离开 Consumer Group。
- **订阅的 Topic 数量发生变化**。
- **订阅 Topic 下的 Partition 数量发生变化**。

###### 对集群的影响

Consumer Rebalance 发生时，Consumer Group 下所有的 Consumer 都会协调在一起，共同参与，以让 Kafka 能够尽量达到最公平的分配，但是，这些 Consumer 都会停止工作，来等待 Consumer Rebalance 过程的完成，如果集群内节点较多，比如上百个，那该过程可能会非常耗时，导致数分钟到数小时，使得 kafka 基本处于不可用的状态，对 TPS 影响极大。

- 经过几轮本地测试，可以发现，每次 Consumer Rebalance 所消耗的时间，大概在 **80ms~100ms** 内，平均耗时在 **87ms** 左右。

###### Group Coordinator

1. Consumer Group Coordinator 是一个服务，每个 Broker 启动时都会启动一个该服务，用于存储 Consumer Group 相关的 Meta 信息，并将对应的 Partition#Offset 信息，记录到内置的 `Topic(__consumer_offsets) ` 中。

2. 而在 0.9 版本之前，Partition#Offset 是基于 ZK# `(consumers/{group}/offsets/{topic}/{partition})` 进行存储的，但由于 ZK 并不适合频繁的写操作，所以，在 0.9 版本之后，通过内置 Topic 的方式，来记录对应 Partition#Offset。

3. 每个 Consumer Group 都会选择一个 Coordinator，来完成组内各 Partition#Offset 信息，选择方式如下：

   1. 计算 Consumer Group 对应在 `Topic(__consumer_offsets)#Partition`。

      ```java
      // Topic(__consumer_offsets)#Partitio计算规则：hash(消费者组的ID) mod 50
      // groupMetadataTopicPartitionCount对应offsets.topic.num.partitions参数值，默认值是50个分区
      partition-Id(__consumer_offsets) = Math.abs(groupId.hashCode() % groupMetadataTopicPartitionCount)
      ```

   2. 根据该 Partition 寻找对应的 Leader Replica#Group Coordinator，作为该 Consumer Group#Coordinator。

###### 执行过程

Rebalance 过程分为两步：**Join 和 Sync**。

1. **Join**：

   1. 加入组，所有 Consumer 会向 Coordinator 发送 `JoinGroup` 请求，请求加入 Consumer Group。
   2. 一旦所有成员都发送了 `JoinGroup` 请求，Coordinator 会从中选择一个 Consumer 来担任 Leader 的角色，并把组成员信息以及订阅信息发给 Leader，由 Leader 来负责制定消费分配方案。
      - 注意，这里的 Leader 和 Coordinator 不是同一个概念，Leader 属于 Consumer，Coordinator 属于 Broker 中的一个服务。

   ![1635483089251](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635483089251.png)

2. **Sync**：

   1. Leader 开始分配消费方案，即哪个 Consumer 负责消费哪些 Topic#Partition。
   2. 一旦完成分配，Leader 会将这个方案封装进 `SyncGroup` 请求中，然后发给 Coordinator。
      - Consumer 的分区分配策略，见《日志分区原理 - 分区分配原理》。
   3. 而非 Leader 也会发 `SyncGroup` 请求，只是内容为空。
   4. Coordinator 接收到分配方案后，会把方案塞进 `SyncGroup` 的 response 中，响应给各个Consumer。
   5. 这样，Consumer Group 内的所有 Consumer 成员，就都知道自己应该消费哪些 Partition 了。

   ![1635483199299](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635483199299.png)

###### 场景分析

- **新成员加入组**：

  1. 成员 M1 原本就处于 Consumer Group#generation2，成员 M2 新加入 Consumer Group 中。
  2. Coordinator 收到 M2 的 `JoinGroup` 请求后，然后会告诉 M1 重新加入 Consumer Group#generation2 处于 Rebalance 中，需要重新加入。
  3. M1 向 Coordinator 发送 `JoinGroup` 请求。
  4. Coordinator 收到完 Consumer M1 和 M2 的 `JoinGroup` 请求后，则选择 M2 来担任 Leader 角色，并把组成员信息以及订阅信息发给 M2，由 M2 来负责制定消费分配方案，同时其他响应 `JoinGroup` 请求，告诉他们当前处于 Consumer Group#generation3 中的 Follower 角色。
  5. M2 制定方案完毕后，则会将这个方案封装进 `SyncGroup` 请求中，然后发给 Coordinator。
  6. Coordinator 接收到分配方案后，会把方案塞进 `SyncGroup` 的 response 中，响应给 M1 和 M2，完成一次 Consumer Rebalance 操作。

  ![1635483703977](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635483703977.png)

- **组成员主动离开**：

  1. 成员 M1 和 M2 原本就处于 Consumer Group#generation2，成员 M1 请求主动离开。
  2. Coordinator 收到 M1 的 `LeaveGroup` 请求后，则会通过 `HeaderBeat` 响应 M1 同意结果，然后要求 M2 重新加入 Consumer Group。
  3. M2 向 Coordinator 发送 `JoinGroup` 请求。
  4. Coordinator 收到完 Consumer M2 的 `JoinGroup` 请求后，则选择 M2 来担任 Leader 角色，并把组成员信息以及订阅信息发给 M2，由 M2 来负责制定消费分配方案，同时其他响应 `JoinGroup` 请求，告诉他们当前处于 Consumer Group#generation3 中的 Follower 角色。
  5. M2 制定方案完毕后，则会将这个方案封装进 `SyncGroup` 请求中，然后发给 Coordinator。
  6. Coordinator 接收到分配方案后，会把方案塞进 `SyncGroup` 的 response 中，响应给 M2，完成一次 Consumer Rebalance 操作。

  ![1635484277208](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635484277208.png)

- **组成员崩溃**：

  1. 与组成员主动离开类似，但不同的是，在崩溃时，组成员并不会主动地告知Coordinator 离开组这件事。
  2. 而是 Coordinator 在一个完整的 `session.timeout.ms` 心跳周期后，才检测出组成员的这种崩溃，势必会造成消费滞后，因为期间 M1 停止了消费，而且 Rebalance 所有 Consumer 都会停止消费。
  3. 因此，可以说，主动离开组是主动发起 Consumer Rebalance，而组员崩溃则是被动发起 Consumer Rebalance。

  ![1635484185247](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635484185247.png)

###### 如何避免不必要的 Consumer Rebalance？

- **思路**：
  1. 要避免 Consumer Rebalance，还是要从触发条件入手：
     - **组成员数量发生变化**：比如有新的 Consumer 加入或者离开 Consumer Group。
     - **订阅的 Topic 数量发生变化**。
     - **订阅 Topic 下的 Partition 数量发生变化**。
  2. 对于后两个触发条件，可以人为地去避免，比如控制好 Topic 和 Partition 的数量。
  3. 所以，最常见的原因还是 Consumer Group 中的组成员发生了变化。
     1. Consumer 正常的添加、删除导致的 Rebalance，是无法避免的。
     2. 但是，在某些情况下是可以尽力避免的，比如，Consumer 可能会被 Coordinator 错误地认为已死亡，从而被错误地踢出 Group，导致多余的 Rebalance 发生。

- **调整参数**：

  - `session.timeout.ms`：Coordinator 判定 Consumer 已死亡的超时时间，可在 Consumer 端进行参数配置，默认为 10s。
    1. 当 Consumer Group 完成 Rebalance 之后，每个 Consumer 实例都会以 `heartbeat.interval.ms` 周期地向 Coordinator 发送心跳请求，表明它还存活着。
    2. 但如果某个 Consumer 实例在超过该超时时间，仍未能及时地发送这些心跳请求，那么 Coordinator 就会认为该 Consumer 已经死亡，从而将其从 Group 中移除，然后开启新一轮的 Rebalance。
  - `heartbeat.interval.ms`：Consumer 心跳报发送周期，用于控制发送心跳请求频率，可在 Consumer 端进行参数配置。
    1. 这个值设置得越小，Consumer 发送心跳请求的频率就越高。
    2. 频率高的好处就是，能够更加 Consumer 快速地知晓当前是否开启 Rebalance，因为目前 Coordinator 通知各个 Consumer 开启 Rebalance 的方法，就是将 `REBALANCE_NEEDED` 标志封装进Heartbeat 心跳包的响应体中。
    3. 而坏处就是，会额外消耗带宽资源。
  - `max.poll.interval.ms`：Consumer#pull 方法之间的最大时间间隔，可在 Consumer 端进行参数配置，默认值为 5min，用于控制 Consumer 的实际消费能力。
    1. 如果 Consumer 在该时间间隔内，无法消费完上一次 pull 方法返回的消息，那么该 Consumer 会主动发起离开组的请求，然后 Coordinator 就会开启新一轮的 Rebalance。

- **优化结论**：

  1. **增大超时时间，减少心跳周期**：以减少 Consumer 未能及时发送，Coordinator 未能及时收到心跳，从而 导致 Consumer 被踢出 Group 的非必要 Rebalance 情况发生。

     ```shell
     # 加大超时时间，设置成6s主要是为了，让Coordinator能够更快地定位已经挂掉的Consumer，早日把它们踢出Group
     session.timout.ms=6s
     # 减少心跳周期，加大心跳频率，在Consumer实例在被判定为死亡之前，保证能够发送至少3轮的心跳请求，即session.timeout.ms >= 3*heartbeat.interval.ms
     heartbeat.interval.ms=2s
     ```

  2. 增大可消费的时间：以减少某些 Consumer 消费时间过长，超出两次 pull 消息间隔，从而导致 Coordinator 进行非必要的 Rebalance 的情况发生。

     ```shell
     # 增长可消费的时间，总之，要为业务处理逻辑留下充足的时间，这样Consumer就不会因为处理这些消息的时间太长而引发非必要的Rebalance
     max.poll.interval.ms=某个合理值
     ```

##### 如何保证顺序消费？

- **问题场景**：

  - 业务上产生三条消息，分别是对数据的 add、update 和 delete，如果没有保证顺序消费，结果可能是delete ->  update -> add，本来数据最终是要 delete 掉的，结果却变成 add。
  - 再如电商平台，先付钱，然后生成订单，最后通知物流，如果顺序改变了，则可能出现不用先付钱了，却通知物流送货。
- **解决思路**：必须要使用**单消费者消费单个分区**，目的是防止消费者争抢消息导致乱序消费的情况发生。

  1. 与 RabbitMQ 保证单消费者消费单个队列不同，由于 Kafka#Consumer Group 机制，天然就能保证单个消费者消费，然后 Kafka 针对的不是队列，而是一个 Partition。
- **解决方案**：参考《RabbitMQ - 如何保证顺序消费》。

  - **多分区、多消费者**：类似于 RabbitMQ#一致性哈希交换机 `x-consistent-hash Exchange`，通过业务ID进行 hash，保证同一个业务 ID 的消息只落在同一个 Partition 中，然后被同一个 Consumer 顺序消费。

    - **局限**：消息不是全局保证顺序的，而只是相关消息才保证顺序；如果确实要保证顺序消费，则需要并发同步，比如搞分布式锁。
    - **实现方法**：自定义 Producer 分区器与分区规则。

  - **单分区、多消费者**：多个 Consumer 消费。

    - **局限**：需要保证并发同步，引入了同步机制，可能会降低消费速度。

  - **单消费者、多线程**：一个 Consumer + 一个内存队列 + 多线程消费，即 Master - Worker 模式。

    - **局限**：与单分区、多消费者模式类似，只不过在同一个 Consumer 进程中，处理并发同步的成本可能要比不同进程的更低一些。

  - **单分区、单消费者**：始终保证使用 一个 Partition + 一个 Consumer 消费。

    - **局限**：Consumer 不能水平扩展，消费能力有限。

    - **实现方法**：

      ```shell
      # topic_x 只创建一个分区，一个副本
      kafka-topics.sh --zookeeper localhost:2181/myKafka --create --topic topic_x 
                                      --partitions 1 --replication-factor 1
      ```

##### 如何优化消息积压？

###### 影响后果

1. 消息积压越多，Consumer 寻址的性能就越慢，最差的情况则会导致整个 kafka 对外提供服务的性能很差，从而影响其他服务的访问速度，最后造成雪崩效应。
2. 而且还可能发生磁盘被堆满，导致 Producer 消息无法写入磁盘，一直报错，然后引发连锁反应，同样会造成雪崩效应。

###### 原因分析

如果 Consumer 消费速度跟不上 Producer 生产速度，就会造成消息积压。

- Consumer 消费速度跟不上 Producer 生产速度，一般是业务逻辑没设计好，导致 Consumer 和 Producer 之间的效率不平衡。
  - **解决思路**：增加 Partition、增加 Consumer 等，以提高消费速度。
- Consumer 出现异常，导致一直无法接收新的消息。
  - **解决思路**：优化消费程序，解决异常。

###### 优化思路

一定要保证 Consumer 的消费性能要高于 Producer 的发送性能，这样系统才能健康、持续地运行。

###### 优化方案

这里的优化方案，针对的是**消费速度低于生产速度**，而不是 Consumer 异常。

1. **Consumer 批量拉取消息与批量提交 offset**：这里要考虑的是，Consumer 有没有已经发挥出其本身真正的后端处理能力，如果没有，可以从**减少 I/O 的角度**入手。

   1. 对于每条消息分别拉取的情况，Consumer 需要多次从 Broker 上拉取消息，而对于每条消息分别被提交的情况，Consumer 需要多次发起提交 offset 传输给 Broker，这样的多次 I/O，浪费了服务器性能与增加了带宽的占用。
   2. 通过批量拉取消息+批量提交 offset 的方式，减少多次发起 Broker 请求，可以减少很多 I/O 浪费和带宽占用。
   3. 不过，如果 Consumer 在处理某条消息时失败了，而业务上又要求不能丢失任何消息，此时就不能对所有的消息进行批量提交，因为消费失败的需要重新消费。
      - **解决方案**：可以跟踪所有消息的处理结果，如果全部成功，则使用批量提交 offset；如果部分成功，则有两个选择：1）如果不需要顺序消费，则可以退化为每个消息分多次发送提交 offset；2）如果需要顺序消费，则本次消费失败之后的消息全部重新消费，保证重新消费时消息顺序保持一致。
   4. **小结**：Consumer 批量提交的前提是，设置了批量拉取消息，否则将会失去意义。

   ```java
   // Consumer参数：批量拉取消息
   // 一次拉取的最小数据量，默认为 1 B
   // fetch.min.bytes
   // 一次拉取的最大数据量，默认为 50 MB
   // fetch.max.bytes
   // 一次拉取一个 Partition 的最大数据量，默认为 1 MB
   // max.partition.fetch.bytes
   // 每次拉取的消息最大条数
   // max.poll.records
   
   /* Consumer开启手工提交*/
   properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);
   /*1) 整体提交: 同步提交(线程阻塞)*/
   // consumer.commitSync();
   /*2) 或者整体提交: 异步提交(线程非阻塞), 可回调可不回调, 这里会轮训回调函数*/
   consumer.commitAsync(new OffsetCommitCallback() {...}
   ```

2. **多线程并发消费**：接着还要考虑的是，Consumer 有没有已经发挥出其本身真正的后端处理能力，如果没有，可以**从多线程并发消费**入手，代码实现见《Kafka API - Consumer 多线程消费》。

   1. 多线程并发消费，不需要建立多个 Kafka 客户端连接，在收到消息后，可以将其放入不同的线程中进行消费，这样进程中就会同时消费多个消息，增加了消费的吞吐量，从而提升消费速度。
   2. **小结**：与增加 Consumer 类似，存在并发冲突和顺序消费的问题，只不过在多线程并发消费是在同一个 Consumer 进程中，处理并发同步的成本可能要比不同进程的更低一些。

3. **增加 Consumer**：这个道理比较容易理解，多个人搬砖的速度肯定比一个人要快很多，不过实际情况还需要面对一些技术挑战，比如后端处理能力瓶颈、并发消费冲突，以及保持顺序消费三个问题。
   - **后端处理能力瓶颈**：
     1. 比如多个 Consumer 都要操作数据库，那么数据库连接的并发数和读写吞吐量就是后端处理能力。
     2. 如果达到了**数据库的最大处理能力**，出现了瓶颈，增加再多的 Consumer 也没有用，甚至会因为加剧了数据库拥塞，从而导致整体消费速度的进一步下降。
   - **并发消费冲突**：
     1. 比如两个 Consumer 都要去修改用户的积分，如果同时取出了相同的数据，并发处理的话就会出现并发安全问题。
     2. 此时需要保证**并发同步**，比如可以搞一个分布式锁，对于具体的某个用户，确保同时只能有一个消费者来处理其积分。
   - **保持顺序消费**：
     1. 由于增加了多个 Consumer，不再是单个 Consumer 消费单个分区，可能会出现乱序消费的情况。
     2. 如果仍需要保证顺序消费，那么可以参考一致性哈希的做法，搞成多队列、多消费者模式，不过只能保证相关消息顺序消费；如果确实要保证顺序消费，则需要并发同步，比如搞分布式锁。
   - **小结**：
     1. 解决并发消费冲突、保持顺序消费两个问题，常常需要引入多个 Consumer 之间的**并发同步**机制，如果这些机制设计得不好，还会给消费速度带来很大的影响。
     2. 因此，多人搬砖速度快的前提，是多个人搬砖时不需要大家频繁的坐下来协调谁搬哪块砖，否则，就会浪费很多时间在相互协调上，反而不能提升搬砖的速度。
     3. 所以，想要通过增加 Consumer，来提升消费速度，需要确保 Consumer **并发处理能力**要留有余地，Consumer 依赖的**后端服务处理能力**也要留有余地。

###### 方案总结

- **优化思路**：通过分析上边的这些方法，在进行消费优化时，可以遵循这样一个路径，以保证最大消费速度。
  1. 先单个 Consumer 消费，**1 次只接收 1 条消息**，消息消费完毕后再消费下一条，避免并发冲突和顺序消费的问题，减少同步机制的消耗。
  2. 如果消费速度不满足要求，则提高 `max.poll.records`，**1 次接收多条消息**，甚至批量提交 offset，单线程按顺序消费，避免并发冲突和顺序消费的问题，减少同步机制的消耗。
  3. 如果消费速度还是不满足要求，则 **1 次接收多条 + 多线程消费**，甚至批量提交 offset，但要注意并发冲突和顺序消费的问题。
  4. 如果消费速度还是不满足要求，则**多个消费者并发消费**，甚至批量提交 offset，但要注意并发冲突和顺序消费的问题。
  5. 如果消费速度还是不满足要求，则考虑**改需求**，或者**换别的中间件**。
- **优化注意点**：
  1. **程序性能优化优先**：需要始终优先优化 Consumer 处理能力，以及其依赖的后端程序处理能力，比如要去优化 SQL 语句、使用缓存、使用负载均衡等，来加快消费速度，因为消息积压常常都是程序处理太耗时导致的。
  2. **幂等性消费**：由于不只 Producer 可能会重复发送消息，Consumer 也可能会触发消息的重复投递，所以，Consumer 要保证幂等性消费。
  3. **并发同步**：如果使用了多线程消费，或者多 Consumer 消费，则会存在并发冲突以及顺序消费的问题，此时需要保证并发同步，比如使用分布式锁。
  4. **顺序消费**：最好能做到无需顺序消费，否则需要在多线程消费，或者多 Consumer 消费时保证并发同步，以及批量 ACK 遇到消费失败时进行全部 NACK。

###### 【线上】如何紧急处理消息积压？

参考《RabbitMQ - 【线上】如何紧急处理消息积压》。

对于 Kafka 而言，由于 Partition 机制与 RabbitMQ Queue 并不相同，所以除了参考，还有以下的处理方式：

1. **提高 Partition 分区数**：分区数足够，可以提高消费的并行度，并且也决定者同组 Consumer 的最大数量。
2. **临时 Topic + Consumer 进行消息转储消费**：
   1. 如果消息积压的 Topic 在建立时，就没有建立足够多的 Partition，导致同一个 Consumer Group 中的 Consumer#size 已经增加到 Partition#size，却仍有大量的消息积压。
   2. 此时，可以建立一个临时的 Topic、设置足够多的 Partition 以及上线一批足够多的 Consumer ，然后把所有原 Topic 的 Consumer 拉取到的消息统统转储到临时 Topic 上，供新上线那批 Consumer 去消费。
   3. 这样，消费速度上去了，消息积压的问题才有机会解决掉，解决掉后再视实际情况，来决定是否恢复原状。

