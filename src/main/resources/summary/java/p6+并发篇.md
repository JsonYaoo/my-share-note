# 三、并发篇 

### 1.1. CPU、内存、外存、操作系统、应用程序、进程、线程、协程、管程、超线程？

在计算机中：

- **CPU**：是核心的硬件资源，承担所有的计算任务。
- **内存**：承担运行时数据的保存任务。
- **外存**：承担数据外部永久存储的任务，如硬盘等。
- **操作系统**：统领计算任务的调度、资源的分配。
- **应用程序**：是存放在硬盘中的可执行文件，主要包括代码指令和数据，以进程的形式运行于操作系统之上，享受操作系统提供的服务。
- **进程**：是程序的一次启动执行，是**操作系统资源分配的最小单位**。操作系统将程序装入内存，给程序分配必要的系统资源， 并且开始运行程序的指令。
- **线程**：指进程代码段的一次顺序执行流程，是**CPU任务调度和执行的最小单位**，一个进程可以有多个线程。
- **协程**：是一种比线程更加轻量级的存在，一个线程可以拥有多个协程，协程没有增加线程数量，只是在线程的
  基础之上通过**分用复用**的方式运行多个协程。
- **管程**：是管理共享变量以及对共享变量的操作过程，以让它们支持并发，是一种**进程同步互斥工具**。
- **超线程**：指**在单核CPU上可以并发AB两个线程**，如果AB资源不冲突，则AB两个线程就可以并发执行；而如果AB都在访问同一个资源，那么只能等前一个线程执行完，后一个线程才能执行。

### 1.2. 详细介绍进程的结构？

进程是程序的一次启动执行，是**操作系统资源分配的最小单位**。操作系统将程序装入内存，给程序分配必要的系统资源， 并且开始运行程序的指令。

一般来说，一个进程由**程序段**、**数据段**和**进程控制块**三部分组成。在进程内部，代码段和数据段有自己的独立地址空间，不同进程的地址空间是相互隔离的。

- **程序段**：一般称为代码段，是进程的程序指令在内存中的位置，包含需要执行的指令集合。
- **数据段**：是进程操作数据在内存中的位置，包含需要操作的数据集合。
- **程序控制块**：Program Control Block，PCB，包含进程的描述信息和控制信息，是进程存在的唯一标志。
  - **进程的描述信息**：主要包括：
    - **进程ID**：是唯一的，代表进程的身份。
    - **进程状态**：比如运行、就绪、阻塞；
    - **进程优先级**：是进程调度的重要依据。
  - **进程的调度信息**：主要包括：
    - **程序起始地址**：即程序第一行指令的内存地址，是从这里开始程序的执行。
    - **通信信息**：进程间通信时的消息队列。
  - **进程的资源信息**：主要包括：
    - **内存信息**：内存占用情况和内存管理所用的数据结构。
    - **I/O设备信息**：所用的I/O设备编号及相应的数据结构。
    - **文件句柄**：所打开文件的信息。
  - **进程上下文**：
    - 即**进程的环境**，主要包括**执行时各种CPU寄存器的值**、**当前程序计数器（PC）的值**以及**各种栈的值**等。
    - 在操作系统切换进程时，当前进程被迫让出CPU，当前进程的上下文就保存在PCB结构中，供下次恢复运行时使用。

![1629615113218](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629615113218.png)

### 1.3. 详细介绍线程的结构？

为了充分发挥CPU的计算性能，提升CPU硬件资源的利用率，同时弥补进程调度过于笨重产生的问题，进程内部演进出了并发调度的诉求，于是就发明了线程。

线程指进程代码段的一次顺序执行流程，是**CPU任务调度和执行的最小单位**，一个进程可以有多个线程。

一个标准的线程主要由**线程描述信息**、**程序计数器（ProgramCounter，PC）**和**栈内存**三部分组成。

- **线程描述信息**：也即线程的基本信息，主要包括：
  - **线程ID**：Thread ID，线程标识符，是线程的唯一标识，同一个进程内不同线程的ID不会重复。
  - **线程名称**：主要是方便用户识别，用户可以指定线程的名字，如果没有指定，系统就会自动分配一个名称。
  - **线程优先级**：表示线程调度的优先级，优先级越高，获得CPU的执行机会就越大。
  - **线程状态**：表示当前线程的执行状态，为新建、就绪、运行、阻塞、结束等状态中的一种。
  - **其他信息**：比如是否为守护线程等。
- **程序计数器**：记录着线程下一条指令的代码段内存地址。
- **栈内存**：
  - 是代码段中局部变量的存储空间，为线程所独立拥有，在线程之间不共享。
  - 在JDK 1.8中，每个线程在创建时默认被分配**1MB**大小的栈内存，其中栈内存和堆内存不同，栈内存不受垃圾回收器管理。
    - 在Java中，执行程序流程的重要单位是“**方法**”，而栈内存的分配单位是“**栈帧**”（或者叫“方法帧”）。
    - 方法的每一次执行都需要为其分配一个栈帧（方法帧），栈帧主要保存该方法中的局部变量、方法的返回地址以及其他方法的相关信息。
    - 当线程的执行流程进入方法时，JVM就会为方法分配一个对应的栈帧压入栈内存；当线程的执行流程跳出方法时，JVM就从栈内存弹出该方法的栈帧，此时方法帧的局部变量的内存空间就会被回收。
    - 由于栈帧（方法帧）的操作是后进先出的模式，这也是标准的栈操作模式，因此**存放方法帧的内存也被叫作栈内存**。

![1629615141818](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629615141818.png)

### 1.4. 线程上下文切换？

- **线程上下文**：是指某一时刻**CPU寄存器**和**程序计数器**的内容，CPU通过时间片分配算法来循环执行线程，由于CPU时间片非常短，因此CPU需要通过不停地切换上下文以执行不同的线程。
- **上下文切换**：在当前任务执行完CPU时间片切换到另一个任务之前，操作系统会先保存该任务的状态（包括程序计数器、虚拟机栈中每个栈帧的信息），以便下次再切换回这个任务时，可以再加载这个任务的状态。这种**任务从保存到再加载的过程就是一次上下文切换**。
- **线程上下文切换的开销**：
  - **直接消耗**：指CPU寄存器需要保存和加载、系统调度器的代码需要执行、TLB实例需要重新加载等。
  - **间接消耗**：指多核的CPU高速缓存之间需要共享数据，间接对程序造成影响。
- **线程上线文切换的场景**：
  - **抢占式**：一般跟锁竞争有关，可以减少锁争用，来减少线程上下文切换。
    - **线程的CPU时间片已用完**：当前执行线程（任务）的**CPU时间片用完**之后，CPU会调度下一个线程。
  - **时间片轮转**：一般跟时间片有关，可以减少线程数，来减少线程上下文切换。
    - **线程被挂起**：比如调用了Thread#sleep、Thread#yield、Object#wait、LockSupport#park、synchronized、lock、阻塞式I/O等方法后。
- **如何减少线程上下文切换**：
  - **合理使用线程**：合理设置线程数目，避免创建不必要的线程，既可以最大化利用CPU，又可以减少线程切换的开销。
    - **高并发，低耗时的情况，**建议减少线程数。
    - **低并发，高耗时的情况**，建议增加线程数。
    - **高并发高耗时，**需要分析任务类型、增加排队、加大线程数等。
  - **减少锁争用**：通过设计算法来减少争抢锁的概率，比如JDK 7 ConcurrentHashMap中的**分段锁**，将ConcurrentHashMap分为多个段，每个段有自己的哈希表，线程只需要获取某段的分段锁，就可以操作该段的哈希表。这样保证线程安全的同时，还可以减少锁的争用，从而减少线程的上下文切换。
  - **无锁并发编程**：如CAS算法，通过自旋+CAS，不需要加锁也可以实现线程安全，其实现有Atomic包下的原子类、JDK 8 ConcurrentHashMap等。
  - 使用协程：通过线程的分用复用，在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换。

### 1.5. 进程与线程的区别？

|              | 线程                                                         | 进程                                                         |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 目的不同     | 为了充分发挥CPU的计算性能，提升CPU硬件资源的利用率，同时弥补进程调度过于笨重产生的问题，满足进程内部并发调度的诉求 | 用于将程序装入内存，运行程序的指令                           |
| 概念不同     | 线程是“进程代码段”的一次顺序执行流程，是CPU调度的最小单位    | 进程是程序的一次启动执行，是操作系统分配资源的最小单位，一个进程由一个或多个线程组成，一个进程至少有一个线程 |
| 共享空间不同 | 各线程之间共享进程的方法区内存、堆内存、系统资源（文件句柄、系统信号等） | 进程之间是相互独立的，但进程内部的各个线程之间并不完全独立   |
| 切换速度不同 | 线程上下文切换快                                             | 进程上下文切换慢                                             |

### 1.6. 进程的状态？

- **背景**：进程是程序的一次执行，在这个执行过程中，有时进程正在被CPU处理，有时又需要等待CPU服务，其状态会有各种变化。**为了方便对各个进程的管理**，操作系统需要将进程合理地划分为几种状态。
- **进程的几种状态**：
  - **创建态**：New，进程正在被创建，操作系统为进程分配资源、初始化PCB。
  - **就绪态**：Ready，进程已经具备运行条件，但由于没有空闲CPU，而暂时不能运行。
    - 进程处于就绪态，代表已经拥有了除处理机之外所有需要的资源，一旦获得处理机，即可立即进入运行态开始运行，即**万事俱备，只欠CPU**。
  - **运行态**：Running，进程占有CPU，并在CPU上运行。
    - 注意，单机处理机环境下，每时刻最多只有一个进程处于运行态；而双核环境下，可以同时有两个进程处于运行态。
  - **阻塞态**：Waiting/Blocked，又称等待态，进程因申请某一资源没有被分配，或者等待某一事件而暂时不能运行。
    - 比如等待操作系统分配打印机、等待读磁盘操作的结果。CPU是计算机中最昂贵的部件，为了提高CPU的利用率，需要先将其他进程需要的资源分配到位，才能得到CPU的服务。
  - **终止态**：Terminated，进程运行结束，或者由于bug导致进程无法继续执行下去（比如数组越界错误，此时需要撤销进程），此时进程需要从系统中撤销，操作系统会回收进程拥有的资源、撤销PCB。

### 1.7. 进程状态的转换？

- **创建态 -> 就绪态**：系统完成创建进程相关的工作。
- **就绪态 -> 运行态**：进程被CPU调度。
- **运行态 -> 就绪态**：进程被分配的CPU时间片到了，或者CPU被其他高优先级进程抢占了。
- **运行态 -> 阻塞态**：等待系统资源分配，或者等待某事件的发生，属于进程的**主动行为**。
- **阻塞态 -> 就绪态**：系统资源已分配到位，或者等待的事件已发生，属于进程的**被动行为**。
  - 注意，不能由阻塞态直接转换为运行态，因为需要等待CPU的调度。
  - 也不能由就绪态直接转换为阻塞态，因为进入阻塞态是进程的主动请求，必然需要进程在运行时才能发出这种请求。
- **运行态 -> 终止态**：进程运行结束，或者运行过程中遇到不可修复的错误。

![1629536288202](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629536288202.png)

### 1.8. Java线程状态与状态切换？

Java线程的生命周期，即Java线程状态，线程在某时刻只能处于一种状态。注意！这些状态只为JVM虚拟机状态，不代表任何操作系统的线程状态。

- **NEW**：新建状态**（对应进程创建态）**，处于该状态的线程尚未启动。
  - new Thread（...）创建了线程，但未调用start（）启动线程时。
- **RUNNABLE**：可执行状态，该状态包含操作系统进程的就绪、运行两种状态。
  - **Ready**：就绪状态**（对应进程就绪态）**，仅仅代表当前线程具备运行的资格，如果线程没有被操作系统的调度程序挑选中，则会永远处于就绪状态。
    - Thread#start（）、线程的CPU时间片用完、Thread#sleep（long）、线程抢到对象锁（Object Monitor）、Thread#yield（）。
  - **Running**：运行状态**（对应进程运行态）**，调用Thread#run（）方法不一定会马上被并发执行，在线程获取了CPU时间片之后，才真正启动并发执行，此时线程进入运行状态。
- **BLOCKED**：阻塞状态**（对应进程阻塞态）**，处于该状态的线程不会占用CPU资源，不会被分配CPU时间片，且在线程抢到锁或者等待事件发生后，会回到就绪状态。
  - 线程阻塞等待锁、阻塞式I/O操作。
- **WAITING**：等待状态**（对应进程阻塞态）**，处于该状态的线程不会占用CPU资源，不会被分配CPU时间片，且需要被其他线程**显式地唤醒**，才会回到就绪状态。
  - 调用无时限的Object#wait（）、Thread#join（）、LockSupport#park（）时。
- **TIMED_WAITING**：限时等待状态**（对应进程阻塞态）**，处于该状态的线程不会占用CPU资源，不会被分配CPU时间片，且如果指定时间内没有被唤醒，限时等待的线程会被**系统自动唤醒**，会回到就绪状态。
  - Thread#sleep（long）、Object#wait（long）、LockSupport#park Nanos（long）、LockSupport#parkUntil（long）、Thread#join（long）。
  - 因此，对应进程状态可得出结论：进入BLOCKED状态、WAITING状态、TIMED_WAITING状态的线程，都会让出CPU的使用权，在处于等待或者阻塞状态的线程被唤醒后，才会回到就绪状态，然后需要重新获取CPU时间片才能接着运行。
- **TERMINATED**：
  - 终止状态**（对应进程终止态）**，也叫死亡状态，处于RUNNABLE状态的线程，在**Thread#run（）方法执行完成之后**，就会变成该状态。
  - 当然，如果在Thread#run（）方法执行过程中，发生了**运行时异常**而没有被捕获，Thread#run（）方法将被异常终止，线程也会变成该状态。

![1629615205233](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629615205233.png)

### 1.9. 线程切换相关方法？

| 方法                            | 作用                                                         | 状态转换（JVM线程）                                          | 状态转换（进程）                    |
| ------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ----------------------------------- |
| Thread#start（）                | 被synchronized修饰，开始执行Thread线程，然后JVM会调用run方法 | NEW -> RUNNABLE                                              | NEW -> READY                        |
| Thread#run（）                  | 如果运行的目标任务不为null，则调用Runnable#run方法           | RUNNABLE -> TERMINATED                                       | READY -> RUNNING -> TERMINATED      |
| Thread#yiled（）                | 让步CPU，向调度程序提示，当前线程愿意放弃其当前对处理器的使用 | RUNNABLE                                                     | RUNNING -> READY                    |
| Thread#sleep（long）            | 使当前正在执行的线程休眠（暂时停止执行），且当前线程不会失去任何监视器的所有权 | RUNNABLE -> TIMED_WAITING -> RUNNABLE                        | RUNNING -> Waiting/Blocked -> READY |
| Object#wait（）                 | 需要被synchronized修饰，当前线程阻塞等待该对象调用notify、notifyAll、中断 | RUNNABLE -> WAITING ->  BLOCKED(唤醒后等待锁) -> RUNNABLE    | RUNNING -> Waiting/Blocked -> READY |
| Object#wait（long）             | 需要被synchronized修饰，当前线程阻塞等待该对象调用notify、notifyAll、中断或者指定时间过去(为0时需要一直等待) | RUNNABLE -> TIMED_WAITING -> BLOCKED(唤醒后等待锁) ->RUNNABLE | RUNNING -> Waiting/Blocked -> READY |
| Thread#join（）                 | 被synchronized修饰，无限等待调用线程的死亡，实质上是调用当前Thread实例的wait方法进行阻塞等待 | RUNNABLE -> WAITING ->  BLOCKED(唤醒后等待锁) -> RUNNABLE    | RUNNING -> Waiting/Blocked -> READY |
| Thread#join（long）             | 被synchronized修饰，等待调用线程的死亡，实质上是调用当前Thread实例的wait方法进行阻塞等待 | RUNNABLE -> TIMED_WAITING -> BLOCKED(唤醒后等待锁) ->RUNNABLE | RUNNING -> Waiting/Blocked -> READY |
| LockSupport#park                | 基于Linux#mutex和condition实现，无限阻塞当前线程，直到当前线程unpark被调用、被中断 | *RUNNABLE -> WAITING(唤醒后不需要等待对象锁) ->RUNNABLE*     | RUNNING -> Waiting/Blocked -> READY |
| LockSupport#parkNanos           | 基于Linux#mutex和condition实现，在指定的等待时间内阻塞当前线程，直到当前线程unpark被调用、被中断、time时间过去 | *RUNNABLE -> TIMED_WAITING(唤醒后不需要等待对象锁) ->RUNNABLE* | RUNNING -> Waiting/Blocked -> READY |
| LockSupport#parkUntil           | 基于Linux#mutex和condition实现，在绝对时间前阻塞当前线程, 直到当前线程unpark被调用、被中断、time时间过去 | *RUNNABLE -> TIMED_WAITING(唤醒后不需要等待对象锁) -> RUNNABLE* | RUNNING -> Waiting/Blocked -> READY |
| Condiction#await                | 基于LockSupport#park实现，当前线程阻塞等待, 直到收到唤醒或者中断信号, 在可以返回当前线程之前, 必须重新获取与此Condition关联的锁 | RUNNABLE -> WAITING ->  BLOCKED(唤醒后等待Lock锁) -> RUNNABLE | RUNNING -> Waiting/Blocked -> READY |
| Condiction#awaitUninterruptibly | 基于LockSupport#park实现，当前线程阻塞等待, 直到收到唤醒或者中断信号, 在可以返回当前线程之前, 必须重新获取与此Condition关联的锁 | RUNNABLE -> WAITING ->  BLOCKED(唤醒后等待Lock锁) -> RUNNABLE | RUNNING -> Waiting/Blocked -> READY |
| Condiction#await(long)          | 基于LockSupport#park实现，当前线程阻塞等待, 直到时间过去、收到唤醒或者中断信号, 在可以返回当前线程之前, 必须重新获取与此Condition关联的锁 | RUNNABLE -> TIMED_WAITING -> BLOCKED(唤醒后等待锁) ->RUNNABLE | RUNNING -> Waiting/Blocked -> READY |
| Condiction#awaitNanos           | 基于LockSupport#park实现，当前线程阻塞等待, 直到时间过去、收到唤醒或者中断信号, 在可以返回当前线程之前, 必须重新获取与此Condition关联的锁 | RUNNABLE -> TIMED_WAITING -> BLOCKED(唤醒后等待锁) ->RUNNABLE | RUNNING -> Waiting/Blocked -> READY |
| Condiction#awaitUntil           | 基于LockSupport#park实现，当前线程阻塞等待, 直到时间过去、收到唤醒或者中断信号, 在可以返回当前线程之前, 必须重新获取与此Condition关联的锁 | RUNNABLE -> TIMED_WAITING -> BLOCKED(唤醒后等待锁) ->RUNNABLE | RUNNING -> Waiting/Blocked -> READY |

### 2.0. 线程通信？

线程的通信，指当多个线程共同操作共享的资源时，线程间通过某种方式互相告知自己的状态，以避免无效的资源争夺。

- 等待 - 通知：Java中使用普遍的线程间通信方式，指的是一个线程A调用了同步对象的wait()方法进入等待状态，而另一线程B调用了同步对象的notify()或者notifyAll()方法通知等待线程，当线程A收到通知后，重新进入就绪状态，准备开始执行。
  - 线程间的通信需要借助同步对象（Object）的监视器来完成，Object对象的wait()、notify()方法就如开关信号，用于完成等待方和通知方之间的通信。
- 共享内存：见进程通信。
- 管道流：见进程通信。

### 2.1. Object#wait核心原理？

在调用同步对象的wait()和notify()系列方法时，“当前线程”必须拥有该对象的**同步锁**。

1. 当线程调用了locko（某个同步锁对象）的wait()方法后，JVM会将当前线程加入locko监视器的WaitSet（等待集），等待被其他线程唤醒。
2. 当前线程会释放locko对象监视器的Owner权利，让其他线程可以抢夺locko对象的监视器。
3. 让当前线程等待，其状态变成WAITING。

![1629677601364](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629677601364.png)

### 2.2. Object#notify核心原理？

在调用同步对象的wait()和notify()系列方法时，“当前线程”必须拥有该对象的**同步锁**。

1. 当线程调用了locko（某个同步锁对象）的notify()方法后，JVM会唤醒locko监视器WaitSet中的第一条等待线程。
2. 当线程调用了locko的notifyAll()方法后，JVM会唤醒locko监视器WaitSet中的所有等待线程。
3. 等待线程被唤醒后，会从监视器的WaitSet移动到EntryList，线程具备了排队抢夺监视器Owner权利的资格，其状态从WAITING变成BLOCKED。
4. EntryList中的线程抢夺到监视器的Owner权利之后，线程的状态从BLOCKED变成Runnable，具备重新执行的资格。

![1629677703149](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629677703149.png)

### 2.3. Object#wait与Thread#sleep的区别？

1. wait 方法必须在 synchronized 保护的代码中使用，而 sleep 方法并没有这个要求。
2. wait 方法会主动释放 monitor 锁，在同步代码中执行 sleep 方法时，并不会释放 monitor 锁。
3. wait 方法意味着永久等待（因为没带时间参数），直到被中断或被唤醒才能恢复，不会主动恢复，sleep 方法中会定义一个时间，时间到期后会主动恢复。
4. wait/notify 是 Object 类的方法，而 sleep 是 Thread 类的方法。

### 2.4. LockSupport#park与Thread#sleep的区别？

LockSupport.park()和Thread.sleep()方法，进入阻塞的线程**不会释放持有的锁**，因此在持有锁的时候调用该方法需要谨慎。

1. Thread.sleep()没法从外部唤醒，只能自己醒过来；而被LockSupport.park()方法阻塞的线程可以通过调用LockSupport.unpark()方法去唤醒。
2. Thread.sleep()方法声明了InterruptedException中断异常，这是一个受检异常，调用者需要捕获这个异常或者再抛出；而调用LockSupport.park()方法时不需要捕获中断异常。
3. 被LockSupport.park()方法、Thread.sleep()方法所阻塞的线程有一个特点，当被阻塞线程的Thread.interrupt()方法被调用时，被阻塞线程的中断标志将被设置，该线程将被唤醒。不同的是，二者对中断信号的响应方式不同：LockSupport.park()方法不会抛出InterruptedException异常，仅仅设置了线程的中断标志；而Thread.sleep()方法会抛出InterruptedException异常。
4. 与Thread.sleep()相比，调用LockSupport.park()能更精准、更加灵活地阻塞、唤醒指定线程。
5. Thread.sleep()本身就是一个Native方法；LockSupport.park()并不是一个Native方法，只是调用了一个Unsafe类的Native方法（名字也叫park）去实现。
6. LockSupport.park()方法还允许设置一个Blocker对象，主要用来供监视工具或诊断工具确定线程受阻塞的原因。

### 2.5. LockSupport#park与Object#wait的区别？

1. Object.wait()方法需要在synchronized块中执行；而LockSupport.park()可以在任意地方执行。
2. Object.wait()方法，进入阻塞的线程**会释放持有的锁**；而LockSupport.park()可以在任意地方执行，进入阻塞的线程**不会释放持有的锁**。
3. 当被阻塞线程被中断时，Object.wait()方法抛出了中断异常，调用者需要捕获或者再抛出；当被阻塞线程被中断时，LockSupport.park()不会抛出异常，调用时不需要处理中断异常。
4. 如果线程在没有被Object.wait()阻塞之前被Object.notify()唤醒，也就是说在Object.wait()执行之前去执行Object.notify()，就会抛出IllegalMonitorStateException异常，是不被允许的；而线程在没有被LockSupport.park()阻塞之前被LockSupport.unPark()唤醒，也就是说在LockSupport.park()执行之前去执行LockSupport.unPark()，不会抛出任何异常，是被允许的。

### 2.6. 线程中断的相关方法？

| 方法                                | 作用                                                         |
| ----------------------------------- | ------------------------------------------------------------ |
| public void interrupt()             | 1. 中断实例线程（注意！实例线程不⼀定是当前线程，⽽是指调⽤该⽅法的Thread实例所代表的线程）。2. 如果实例线程处于阻塞状态（比如调用了wait方法或者io等待时），则会立马退出阻塞，并抛出InterruptedException异常，线程就可以通过捕获InterruptedException来做一定的处理，然后线程退出。3. 如果实例线程正在运行中，实际上只是给线程设置⼀个中断标志，线程仍会继续运⾏，线程自己要在适当的位置通过调用isInterrupted方法来查看自己是否被中断，并做退出操作。4. 如果线程的interrupt方法先被调用，然后线程调用阻塞方法进入阻塞状态，InterruptedException异常依旧会抛出。5. 如果线程捕获InterruptedException异常后，继续调用阻塞方法，将不再触发InterruptedException异常。 |
| public static boolean interrupted() | 测试当前线程是否已经被中断（检查中断标志），返回⼀个boolean并清除中断状态，第⼆次再调⽤时中断状态已经被清除，将返回⼀个false。 |
| public boolean isInterrupted()      | 只测试实例线程是否被中断 ，不清除中断状态。                  |

### 2.7. 创建线程的方式？

#### 实现Runnable接口

- 优先使用。

```java
public class RunnableThread implements Runnable {
    @Override
    public void run() {System.out.println('用实现Runnable接口实现线程');}
}
```

#### 实现Callable接口

- 有返回值、可抛出异常。

```java
class CallableTask implements Callable<Integer> {
    @Override
    public Integer call() throws Exception { return new Random().nextInt();}
}
```

#### 继承Thread类

- 在不修改Thread方法情况下，不建议使用。

```java
public class ExtendsThread extends Thread {
    @Override
    public void run() {System.out.println('用Thread类实现线程');}
}
```

#### 使用线程池

- 底层都是实现Runable#run方法。

```java
static class DefaultThreadFactory implements ThreadFactory {
    
    DefaultThreadFactory() {
        SecurityManager s = System.getSecurityManager();
        group = (s != null) ? 
            s.getThreadGroup() : Thread.currentThread().getThreadGroup();
        namePrefix = "pool-" + poolNumber.getAndIncrement() +"-thread-";
    }
    
    public Thread newThread(Runnable r) {
        Thread t = 
            new Thread(group, r,namePrefix + threadNumber.getAndIncrement(),0);
        
        if (t.isDaemon()) t.setDaemon(false);// 是否守护线程
        if (t.getPriority() != Thread.NORM_PRIORITY) 
            t.setPriority(Thread.NORM_PRIORITY);// 线程优先级
        
        return t;
    }
}
```

### 2.8. 详细介绍线程池？

#### 特点

- ThreadPoolExecutor，继承AbstractExecutorService，实现ExecutorService、Executor接口，**使用多个线程之一来执行每个提交的任务**，通常使用 {@link Executors} 工厂方法进行配置。
- **线程池优点**：
  - **降低资源消耗**：线程是稀缺资源，如果无限制地创建，不仅会消耗系统资源，还会降低系统的稳定性，通过**重复利用已创建的线程**可以降低线程创建和销毁造成的消耗。
  - **提高响应速度**：当任务到达时，可以不需要等待线程创建就能**立即执行**。
  - **提高线程的可管理性**：线程池提供了一种限制、管理资源的策略，维护一些基本的线程统计信息，如已完成任务的数量等。通过线程池可以**对线程资源进行统一的分配、监控和调优**。

#### 原理记忆图

![1629770131750](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629770131750.png)

#### 构造方法 

```java
// 线程池构造函数7大参数
public ThreadPoolExecutor(
    int corePoolSize,
    int maximumPoolSize,
    long keepAliveTime,
    TimeUnit unit,
    BlockingQueue<Runnable> workQueue,
    ThreadFactory threadFactory,
    RejectedExecutionHandler handler) {
    
}
```

| 参数                     | 作用                                                         |
| ------------------------ | ------------------------------------------------------------ |
| corePoolSize             | 核心线程数                                                   |
| maximumPoolSize          | 最大线程数                                                   |
| keepAliveTime            | 空闲线程的保活时间，线程池中超过corePoolSize数目的空闲线程最大存活时间（等待任务的最长时间） |
| TimeUnit                 | 空闲线程的保活时间单位                                       |
| workQueue                | 任务阻塞队列                                                 |
| threadFactory            | 线程工厂                                                     |
| RejectedExecutionHandler | 拒绝策略处理程序，当提交任务数超过maxmumPoolSize+workQueue容量之和，或者线程池已关闭时，任务会交给拒绝策略处理程序来处理 |

#### 线程池状态

![1629772009056](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629772009056.png)

- **线程池控制状态ctl**，是一个原子整数，封装了两个概念字段：
  - workerCount，低29位，表示**有效线程数**（当前活动的线程数）。
  - runState，高3位，表示**线程池状态**，比如是否正在运行、正在关闭等。
- runState提供主要的**生命周期控制**，提供5种取值，这些值之间的数字顺序很重要，以允许进行有序比较。
  - runState取值有：
    - **RUNNING**：接受新任务并处理排队任务。
    - **SHUTDOWN**：不接受新任务，但处理排队任务。
    - **STOP**：不接受新任务，不处理排队任务，并中断正在进行的任务。
    - **TIDYING**：所有任务都已终止，workerCount 为0，转换到状态 TIDYING 的线程将运行 terminate()
      钩子方法。
    - **TERMINATED**： terminate() 钩子方法已完成.
  - runState 会随时间单调增加，但**可以不用命中每个状态**。转换有：
    - **RUNNING -> SHUTDOWN**：在调用 **shutdown()** 时，可能隐含在 finalize() 中。
    - **（RUNNING 或 SHUTDOWN）-> STOP**：在调用 **shutdownNow()** 时。
    - **SHUTDOWN -> TIDYING**：当**队列为空**且**工作线程数量为0**时。
    - **STOP -> TIDYING**：当**工作线程数量为0**时。
    - **TIDYING -> TERMINATED**：当 terminate() 钩子方法完成时。

```java
public class ThreadPoolExecutor extends AbstractExecutorService {
	// 线程池控制状态ctl, 高3位表示线程池状态, 低29位表示有效线程数, 初始为(1110, 0000, 0000, 0000, 0000, 0000, 0000, 0000) < 0
    private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));
    
    // 29位
    private static final int COUNT_BITS = Integer.SIZE - 3;
    
    // 低29位存储有效线程数(约5亿个线程)
    private static final int CAPACITY   = (1 << COUNT_BITS) - 1;
    
    private static final int RUNNING    = -1 << COUNT_BITS;// 运行状态, 高3位: 111
    private static final int SHUTDOWN   =  0 << COUNT_BITS;// 关闭状态, 高3位: 000
    private static final int STOP       =  1 << COUNT_BITS;// 停止状态, 高3位: 001
    private static final int TIDYING    =  2 << COUNT_BITS;// 整理状态, 高3位: 010
    private static final int TERMINATED =  3 << COUNT_BITS;// 终止状态, 高3位: 011
}
```

#### 线程复用原理

![1629788027448](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629788027448.png)

- 线程工人类Worker：
  - 继承AQS抽象类，以实现轻量级的独占锁，用于**记录独占线程**。
  - **持有Thread引用**，在构造Worker时设置，在ThreadPoolExecutor#runWorker（）或者ThreadPoolExecutor#addWorker（）中，用于执行任务或者启动线程。
  - 持有firstTask引用，**作为第一个任务执行**，之后便从任务队列中获取任务执行了。
  - 实现Runnable接口，在线程工厂工造线程时，把**本身当做一个执行任务传入**，使其能够在ThreadPoolExecutor#addWorker（）方法中，执行完firstTask后Thread#start（）启动Worker引用的线程实例时，从而调用Worker#run（）方法。
  - Worker#run（）方法调用ThreadPoolExecutor#runWorker（）方法，在ThreadPoolExecutor#runWorker（）方法中，清空firstTask（此时firstTask肯定被执行过了），接着ThreadPoolExecutor#getTask（）**轮训获取**任务队列中的任务，然后调用Runnable#run（）方法执行任务，其中如果检测到STOP状态时，还会中断Worker持有的实例线程。
- **空闲线程淘汰原理**：
  - 在ThreadPoolExecutor#getTask（）方法中，如果线程池为SHUTDOWN状态且队列为空，或者线程池为STOP状态，或者空闲线程获取任务超时，则都会扣减ctl低29位的有效工人数量，并返回null的任务。否则阻塞获取任务。
  - 返回null的任务，在ThreadPoolExecutor#runWorker（）方法轮训被检测到，则会调用ThreadPoolExecutor#processWorkerExit（）来真实移除工人集合中的指定Worker，接着调用tryTerminate（）尝试清空线程，最后根据ctl获取当前工作线程数、核心线程数、最大核心线程数、是否允许核心线程空闲，来决定是否需要增加Worker，如果不需要则直接返回即可，如果需要则重新调用ThreadPoolExecutor#addWorker（）增加非核心线程（允许核心线程空闲也代表只有非核心线程）。
  - tryTerminate（）在线程池为STOP状态 或者 为SHUTDOWN状态且队列为空时, 如果工作线程数不为0, 则需要中断一个工作线程，如果为TIDYING状态还会调用钩子terminated()方法 ，代表线程池已终止。

```java
// 线程工人, 实现Runnable本身可以作为一个任务, 实现AQS以简化获取和释放围绕每个任务执行的锁, 实现了一个简单的不可重入互斥锁
private final class Worker extends AbstractQueuedSynchronizer implements Runnable {
    
    final Thread thread;// 该工人正在运行的线程, 如果工厂生产线程失败, 则为null
    Runnable firstTask;// 该工人要运行的初始任务, 可能会为空
    volatile long completedTasks;// worker已完成任务计数器
    
    // 注意! 在构造Worker时, 使用了当前worker实例作为Thread#Runnable实例变量, 如果运行的目标任务不为null, 则调用Runnable#run方法
    Worker(Runnable firstTask) {
        setState(-1); // inhibit interrupts until runWorker 禁止中断直到 runWorker
        this.firstTask = firstTask;
    
     // 注意! 在构造Worker时, 使用了当前worker实例作为Thread#Runnable实例变量, 如果运行的目标任务不为null, 则调用Runnable#run方法
        this.thread = getThreadFactory().newThread(this);
    }

    // 指定当前工人来运行任务: 先获取firstTask -> 从任务队列中获取任务 -> beforeExecute -> 运行获取到的任务 -> afterExecute -> 线程运行后的清理工作processWorkerExit
    public void run() {
        runWorker(this);
    }
}
```

#### 任务处理过程

![1629786785786](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629786785786.png)

1. 首先获取线程池控制位ctl，根据ctl获取当前工作线程数，如果工作线程数小于核心线程数，则使用当前任务作为firstTask创建核心线程。
2. 如果核心线程创建成功，则直接返回即可；如果核心线程创建失败，说明线程池可能是状态发生了变化，或者线程数超过了核心线程数，则需要重新获取线程池控制位ctl，再次判断。
3. 如果判断到线程池状态仍然为RUNNING，说明只是发生了线程数超过了核心线程数，这时只需要把任务追加到任务队列即可。
   1. 如果任务追加成功，为了稳重起见，重新获取线程池控制位ctl，再次判断线程池状态是否为RUNNING。
   2. 如果线程池确认仍然为RUNNING，为了保证能够有线程拉取任务，则再判断当前工作线程数是否为0，如果没有则不适用firstTask创建非核心线程；如果有则直接返回即可，代表线程池任务投递成功。
   3. 如果线程池确实不为RUNNING，则移除刚才追加的任务，然后履行拒绝策略，代表无论是SHUTDOWN还是STOP或者其他，都不接收任何新的任务了。
4. 如果判断到线程池状态不为RUNNING，或者任务追加失败，则尝试使用当前任务作为firstTask创建非核心线程。
5. 如果非核心线程创建失败，则履行拒绝策略；如果非核心线程创建成功，则直接返回即可（因为非核心线程执启动后会执行firstTask）。

```java
// 在未来的某个时间执行给定的任务, 任务可以在新线程或现有池线程中执行, 如果任务无法提交执行, 则该任务由当前 {@code RejectedExecutionHandler} 来处理
public void execute(Runnable command) {
    if (command == null)
        throw new NullPointerException();

    // 获取ctl控制位c
    int c = ctl.get();

    // 根据控制位ctl获取工作线程数, 如果工作线程数小于核心线程数
    if (workerCountOf(c) < corePoolSize) {
        // 则检查是否可以根据当前池状态和给定界限（核心或最大值）添加新的工作线程, 如果添加工作线程成功, 则返回true; 如果添加工作线程失败, 则回滚工作线程并返回false
        if (addWorker(command, true))
            // 如果启动工作线程成功, 则直接返回
            return;

        // 如果启动工作线程失败, 则重新获取ctl控制位c
        c = ctl.get();
    }

    // 如果线程池仍为运行状态, 则往任务队列填充任务command
    if (isRunning(c) && workQueue.offer(command)) {
        // 如果任务填充成功, 则再获取ctl控制位recheck
        int recheck = ctl.get();

        // 如果此时线程池不为运行状态, 则从执行程序的内部队列中删除该任务，从而导致它在尚未启动时无法运行
        if (!isRunning(recheck) && remove(command))
            // 如果删除成功, 则履行任务command和当前任务执行者executor的拒绝策略
            reject(command);
        // 如果此时线程池仍为运行状态, 但工作线程数为0, 则检查是否可以根据当前池状态和给定界限（核心或最大值）添加新的工作线程, 如果添加工作线程成功, 则返回true; 如果添加工作线程失败, 则回滚工作线程并返回false
        else if (workerCountOf(recheck) == 0)
            addWorker(null, false);
    }
    // 如果线程池不为运行状态, 或者往任务队列填充任务command失败, 则检查是否可以根据当前池状态和给定界限（核心或最大值）添加新的工作线程, 如果添加工作线程成功, 则返回true; 如果添加工作线程失败, 则回滚工作线程并返回false
    else if (!addWorker(command, false))
        // 如果worker添加失败, 则履行任务command和当前任务执行者executor的拒绝策略
        reject(command);
}
```

#### 线程工厂

| 实现类                  | 特性                                                         |
| ----------------------- | ------------------------------------------------------------ |
| DefaultThreadFactory    | 默认线程工厂，创建的线程都在同一个 {@link ThreadGroup} 中，并且具有相同的 {@code NORM_PRIORITY} 优先级和非守护进程状态。 |
| PrivilegedThreadFactory | 能够继承权限的线程工厂, 创建的线程具有相同的线程上下文和类加载器 |

#### 阻塞队列

![1629788141222](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629788141222.png)

##### BlockingQueue接口实现

| 实现类                | 界限性 | 特性                                                         |
| --------------------- | ------ | ------------------------------------------------------------ |
| ArrayBlockingQueue    | 有界   | 1. 基于数组实现的有界阻塞队列。2. 初始化时必须指定容量大小。3. 一旦指定容量大小，其容量就不能修改了。 |
| LinkedBlockingQueue   | 有界   | 1. 基于单向链表实现的有界阻塞队列。2. 容量可选，空参构造时为Integer.MAX_VALUE，相当于无界队列。 |
| LinkedBlockingDeque   | 有界   | 1. 基于双向链表实现的阻塞双端队列。2. 容量可选，空参构造时为Integer.MAX_VALUE，相当于无界队列。 |
| PriorityBlockingQueue | 无界   | 1. 基于优先级堆实现的无界优先级阻塞队列。2. 允许插入NULL元素。3. 元素必须实现 Comparable接口，用于队列排队。 |
| DelayQueue            | 无界   | 1. Delayed元素（标记了给定延迟后应作用的对象）、底层依赖PriorityQueue的无界优先级队列。2. 元素必须实现Delayed接口，同时需要实现Comparable接口，一般情况下按照过期时间的优先级排序。3. 使用场景：定时关闭连接、缓存对象、超时处理 |
| DelayedWorkQueue      | 无界   | 1. ScheduledThreadPoolExecutor延迟线程池的内部类、基于小顶堆的数据结构的、专门的延迟任务队列。2. 元素需要实现RunnableScheduledFuture接口。 |
| SynchronousQueue      | 有界   | 1. 同步队列，其容量为0，不存储任何元素，每个插入操作都会阻塞等待另一个线程进行相应的删除操作才会恢复，take和put需要配对使用。2. 利用自旋 + LockSupport方式阻塞 + CAS乐观锁方式实现栈或者队列结构实现，全程没有使用悲观锁也能保证同步，吞吐量高。 |

| LinkedTransferQueue   | 无界   | 1. 基于链表实现的无界消息阻塞队列，一个新增的元素会与一个删除的空元
素配对。2. 比其他队列多了 transfer（）和tryTransfer（）方法，用于数据交换。3. 既有同步队列中生产者与消费者的特性，也包含了对高并发无锁CAS + 自旋的优化，通过跳过2个结点才更新head指针，减少一半线程的CPU自旋损耗，又利用LockSupport.park（）来实现BlockingQueue的阻塞特性，还提供了异步非阻塞、延迟消费消息的方法。 |

##### BlockingQueue接口方法

|          | 抛出异常                   | 特殊值（null或者false，取决于具体实现） | 阻塞                 | 超时                                                        |
| -------- | -------------------------- | --------------------------------------- | -------------------- | ----------------------------------------------------------- |
| **插入** | {@link #add add(e)}        | {@link #offer offer(e)}                 | {@link #put put(e)}  | {@link #offer(Object, long, TimeUnit) offer(e, time, unit)} |
| **删除** | {@link #remove remove()}   | {@link #poll poll()}                    | {@link #take take()} | {@link #poll(long, TimeUnit) poll(time, unit)}              |
| **检索** | {@link #element element()} | {@link #peek peek()}                    | 不适用               | 不适用                                                      |

#### 拒绝策略

![1629794826356](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629794826356.png)

- 当Executor已经关闭，或者当Executor对最大线程数和工作队列容量已经饱和时，在
  {@link #execute(Runnable)} 方法中提交的新任务将被 {@link RejectedExecutionHandler} 的{@link
  RejectedExecutionHandler#rejectedExecution(Runnable, ThreadPoolExecutor)} 拒绝。

| 实现类              | 特性                                                         | 优点                                                         |
| ------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| AbortPolicy         | 默认的拒绝策略，在任务被拒绝时，会抛出运行时异常             | 可以阻止系统正常运行                                         |
| CallerRunsPolicy    | 调用execute时，线程自己会去运行任务，提供了一个简单的反馈控制机制，可以减慢提交新任务的速度 | 不会造成任务丢失，同时减缓提交任务的速度，给执行任务缓冲时间 |
| DiscardPolicy       | 被拒绝时，任务会被简单地丢弃                                 | 如果允许任务丢失，这是最好的一种方案                         |
| DiscardOldestPolicy | 如果Executor没有关闭，工作队列头部的任务被丢弃，然后重试执行，但可能会再次失败，导致重复执行 | 丢弃最老的一个请求，也就是**即将被执行的任务**，并尝试再次提交当前任务 |

#### 线程池调优

##### 线程池大小设置

需要先确认任务的类型，分为CPU密集型任务、IO密集型任务、混合型任务，以下都是经验公式以及通过工具粗略计算得出的范围，而最优的参数还是需要在实际环境中**不断压测、调优**才能得到的。

- **CPU密集型任务**：
  - **概念**：CPU密集型指定的是，任务需要大量的运算，没有阻塞，CPU一直全速运行。
  - **目的**：需要尽可能少的线程数量，以减少线程上下文切换的次数，提高CPU的利用率，一般此时合理的目标线程数为（CPU核数 + 1）。
  - **经验公式**：N + 1。
- **IO密集型任务**：
  - **概念**：IO密集型指的是，任务大量时间都花在IO的阻塞上，希望的是CPU尽可能去调度其他任务，而不是等待线程，浪费CPU资源，因此需要更多的线程数以供CPU调度。
  - **目的**：需要尽可能多的线程数，但过多的线程数会带来过多的上下文切换，因此要适度，一般此时合理的目标线程数为（CPU核数 * 2）。
  - **经验公式**：2 * N。
- **混合型任务**：
  - **概念**：既有CPU密集型任务，又有IO密集型任务。
  - **经验公式**：N * U * （1 + WT / ST） = CPU核心数 * 目标CPU利用率 * （1 + 平均线程等待时间 / 平均线程运行时间），当**目标CPU利用率为100%**时，等于 CPU核心数 * （1 + 平均线程等待时间 / 平均线程运行时间）。
    - N为CPU核心数，U为目标CPU利用率，WT为线程等待时间，ST为线程运行时间。
    - 观察公式可得，**如果平均线程等待时间越长，则需要的线程数越多；如果平均线程运行时间越短，则需要的线程越少**。
  - **相关工具**：使用VisualVM观察、继承PoolSizeCalculator工具类粗略计算。

##### 阻塞队列设置

- **内存上**：估算单个任务占用内存，以及线程池计划占用内存。
- **排队策略上**：
  - **直接交接**：
    - **实现举例**：工作队列的一个很好的默认选择是 {@link SynchronousQueue}，它将任务交给线程而不用其他方式保留它们。在这里，如果没有线程可立即运行，则将任务排队的尝试将失败，因此将构
      建一个新线程。 
    - **适用场景**：在处理可能具有内部依赖性的请求集时，**可避免锁定**。
    - **缺点**：直接切换通常需要无限的maximumPoolSizes以避免拒绝新提交的任务，所以局限在于，当命
      令平均持续到达速度快于它们可以处理的速度时，可能会出现**无限线程的增长问题**。
  - **无界队列**：
    - **实现举例**：使用无界队列（如没有预定义容量的 {@link LinkedBlockingQueue}）将导致新任务在所有 corePoolSize 线程都忙时在队列中等待。因此，不会创建超过 corePoolSize 的线程，也即设置的maximumPoolSize没有任何作用。
    - **适用场景**：当**每个任务完全独立于其他任务**时，这可能是合适的，因此任务不会影响彼此的执行。例如，在网页服务器中。
    - **缺点**：虽然这种排队方式在平滑请求的**瞬时爆发**方面很有用，但局限在于，当命令的平均到达速度超
      过它们的处理速度时，**工作队列可能会无限增长**。
  - **有界队列**：
    - **实现举例**：有界队列（如{@link ArrayBlockingQueue}）在与有限的 maximumPoolSizes 一起使用。
    - **优点**：有助于**防止资源耗尽**。
    - **缺点**：可能更难以调整和控制。

##### 池大小与队列调优总结

- 当使用**有界队列**来构造**有界线程池**时，队列大小和最大池大小会**相互制衡**：
  - **使用大队列和小池**：
    - 可以最大限度地减少CPU使用率、操作系统资源和上下文切换开销。
    - 但可能会导致人为地降低吞吐量，如果任务频繁阻塞，则任务可能需要更长的响应时间。
  - **使用小队列和大池**：
    - 这会使CPU更忙，可能会遇到不可接受的线程调度开销，降低吞吐量。

#### Executors便捷构造方法

![1629794884916](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629794884916.png)

- **Executors便捷构造方法的弊端**：
  - FixedThreadPool、SingleThreadExecutor、ScheduledThreadPool、ScheduledThreadPoolExecutor：使用无界的任务队列，有可能会导致OOM。
  - CachedThreadPool、ScheduledThreadPool、ScheduledThreadPoolExecutor：使用无界的线程池，允许创建最大线程数为 Integer.MAX_VALUE，可能会导致OOM。
- **解决方案**：不建议使用Executors便捷构造，建议专门指定参数来创建线程池，底层使用有界队列以及创建有界线程池，以防止OOM。

| 方法                           | 参数                                                         | 线程池实例类型              | 特性                                                         |
| ------------------------------ | ------------------------------------------------------------ | --------------------------- | ------------------------------------------------------------ |
| newCachedThreadPool            | 0核心线程数，MAX最大线程数，60秒保活时间，同步队列，默认线程工厂，拒绝时抛出异常 | ThreadPoolExecutor          | 1. 缓存型、无界线程池，会先查看线程能否复用，有就复用，没有就新建，且具有自动回收线程的功能。2. 适用于生存周期很短的异步任务，或者负载较轻的服务。 |
| newFixedThreadPool             | n核心线程数，n最大线程数，0秒保活时间，无界阻塞队列，默认线程工厂，拒绝时抛出异常 | ThreadPoolExecutor          | 1. 固定大小线程池，任意时间最多只有固定数目的活动线程存在。2. 适用于线程数比较稳定的并发场景，或者执行长期的任务。 |
| newSingleThreadExecutor        | 1核心线程数，1最大线程数，0秒保活时间，无界阻塞队列，默认线程工厂，拒绝时抛出异常 | ThreadPoolExecutor          | 1. 单后台线程池，任意时间最多只有一个活动线程存在，可以保证任务按照提交顺序执行。2. 适用于需要严格控制执行顺序的场景。 |
| newScheduledThreadPool         | n核心线程数，MAX最大线程数，0秒保活时间，延迟任务无界队列，默认线程工厂，拒绝时抛出异常 | ScheduledThreadPoolExecutor | 1. 调度型线程池，拥有调度能力。2. 适用于执行定时任务或者延期任务。 |
| newScheduledThreadPoolExecutor | 1核心线程数，MAX最大线程数，0秒保活时间，延迟任务无界队列，默认线程工厂，拒绝时抛出异常 | ScheduledThreadPoolExecutor | 1. 调度型线程池，拥有调度能力，但核心线程只有一个。2. 适用于执行定时任务或者延期任务。 |
| newWorkStealingPool            | 等于CPU核数的并发线程数， 默认ForkJoin线程工厂，异步模式     | ForkJoinPool                | 1. 创建一个ForkJoinPool。2. 适用于分而治之、递归计算的CPU密集型场景 |

#### 如何优雅地关闭线程池？

优雅地关闭线程池，指的是允许任务丢弃，**保证所有线程被中断退出**。

1. 使用Runtime.getRuntime（）.**addShutdownHook**（ ShutdownHookThread, Callable）注册线程池关闭任务，其任务逻辑为：
2. 先调用**shutdown（）**拒绝接收新任务，结合**awaitTermination（long）**指定估计等待时间（比如60s），同步等待线程处理任务完毕。
3. 如果awaitTermination（）返回true，可以提前返回；如果awaitTermination（）返回false，说明预估的时间到了仍然没处理完，或者抛出中断异常，说明当前等待线程中用户中断了，则调用**shutdownNow（）**拒绝处理任务队列任务。
4. 最后模仿Dubbo框架中线程池关闭源码的部分代码，补充检测线程池状态是否为TERMINATED，如果仍然没关闭，则1000次**循环awaitTermination（）等待 +  shutdownNow（）**中断所有线程，如果抛出异常则记录日志。

#### 线程池其中一个线程异常会发生什么？

##### execute调用

1. 当异常线程是被execute调用时，在ThreadPoolExecutor#runWorker（）中，如果没有实现钩子方法处理异常，则**会抛出Throwable异常**。

2. 接着调用finally块的processWorkerExit(w, completedAbruptly)方法，由于completedAbruptly为true，所以会先从工作线程集合中移除这个异常的Worker，然后重新生成一个新的Worker加入到工人集合中，**以实现替换**。

   ```java
   // 使用指定工人运行任务: 先获取firstTask -> 从任务队列中获取任务 -> beforeExecute -> 运行获取到的任务 -> afterExecute -> 线程运行后的清理工作processWorkerExit
   final void runWorker(Worker w) {
       // 获取当前线程wt， 用于运行beforeExecute方法
       Thread wt = Thread.currentThread();
   
       // 获取要运行的初始任务task
       Runnable task = w.firstTask;
       w.firstTask = null;
   
       // 先释放锁, 同步器状态从-1更改为0, 允许中断当前线程
       w.unlock(); // allow interrupts 允许中断
   
       // worker需要突然死亡
       boolean completedAbruptly = true;
       try {
           // 执行阻塞或定时等待任务, 如果需要淘汰线程, 则使用存活时间定时获取任务, 在获取不到时则标记超时等待下一轮清空多余线程; 如果不需要淘汰线程, 则阻塞获取任务
           // 通过worker里的线程启动后, 自旋获取任务队列中的任务, 实现线程复用!!! 通过存活时间、核心线程与任务队列, 控制资源消耗
           while (task != null || (task = getTask()) != null) {
               // 如果任务不为空, 则获取worker锁, 设置当前线程为独占线程
               w.lock();
   
               // 如果运行状态为停止(不接受新任务)、整理(任务终止)、终止状态(已完成), 且线程已被中断, 但当前线程中断标记位不为true, 则中断当前线程
               if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() && runStateAtLeast(ctl.get(), STOP))) && !wt.isInterrupted())
                   // 中断当前线程, 如果从Thread其他实例方法调用该方法, 则会清除中断状态, 然后会收到一个{@link InterruptedException}
                   wt.interrupt();
               try {
                   // 执行任务前的钩子方法, 用于给子类实现回调
                   beforeExecute(wt, task);
   
                   // 运行任务
                   Throwable thrown = null;
                   try {
                       task.run();
                   } catch (RuntimeException x) {
                       thrown = x; throw x;
                   } catch (Error x) {
                       thrown = x; throw x;
                   } catch (Throwable x) {
                       thrown = x; throw new Error(x);
                   } finally {
                       // 执行任务后的钩子方法, 用于给子类实现回调, 该方法由执行任务的线程调用, t为执行任务期间抛出的Throwable
                       afterExecute(task, thrown);
                   }
               } finally {
                   task = null;
   
                   // 更新worker的任务完成数
                   w.completedTasks++;
   
                   // 释放锁, 同步器状态从1更改为0
                   w.unlock();
               }
           }
   
           // worker需要不突然死亡
           completedAbruptly = false;
       } finally {
           // 从工作线程集中删除线程, 并且可能会终止池或替换工作线程, 当指定的completedAbruptly为true时, 会先减少ctl工作线程数并替换工作线程
           processWorkerExit(w, completedAbruptly);
       }
   }
   ```

3. 此后由于没有任何程序捕获这个异常，将在ThreadGroup#uncaughtException（）中捕获到，并**打印出异常堆栈信息**。

   ```java
   // 当此线程组中的线程由于未捕获的异常而停止，并且该线程没有安装特定的 {@link Thread.UncaughtExceptionHandler} 时，由Java虚拟机调用
   public void uncaughtException(Thread t, Throwable e) {
       if (parent != null) {
           parent.uncaughtException(t, e);
       } else {
           Thread.UncaughtExceptionHandler ueh = Thread.getDefaultUncaughtExceptionHandler();
           if (ueh != null) {
               ueh.uncaughtException(t, e);
           } else if (!(e instanceof ThreadDeath)) {
               System.err.print("Exception in thread \"" + t.getName() + "\" ");
               e.printStackTrace(System.err);
           }
       }
   }
   ```

##### submit调用

1. 当异常线程是被submit调用时，同样在ThreadPoolExecutor#runWorker（）中，如果没有实现钩子方法处理异常，则**会抛出Throwable异常**。

2. 接着调用finally块的processWorkerExit(w, completedAbruptly)方法，由于completedAbruptly为true，所以会先从工作线程集合中移除这个异常的Worker，然后重新生成一个新的Worker加入到工人集合中，以**实现替换**。

3. 但在FutureTask#run（）程序中捕获到Throwable异常，并设置到Future#outcome结果中，因此并**不会打印异常堆栈信息**。

   ```java
   try {
       result = c.call();
       ran = true;
   } catch (Throwable ex) {
       result = null;
       ran = false;
   
       // 如果计算时发生异常, 则为异步计算结果设置异常结果, 并更新任务状态为已发生异常状态, 最后遍历等待线程堆栈结点, 并清空唤醒每个结点的线程, 并在完成前调用done方法触发子类的回调, 以及清空运行的任务
       setException(ex);
   }
   ```

### 2.9. Java对象结构？

Java对象（Object实例）结构包括三部分：**对象头、对象体和对齐字节**：

- **对象头**：
  - **Mark Word**：
    - 标记字，用于存储自身运行时的数据，例如GC标志位、哈希码、锁状态等信息。
    - 主要用于表示对象的线程锁状态，另外还可以用来配合GC存放该对象的HashCode。
  - **Class Pointer**：类对象指针，用于存放方法区Class对象的地址，虚拟机通过这个指针来确定这个对象是哪个类的实例。
  - **Array Length**：数组长度，是一个可选的字段。
    - 如果对象是一个Java数组，那么此字段必须有，用于记录数组的长度。
    - 如果对象不是一个Java数组，那么此字段不存在。
- **对象体**：
  - 包含对象的成员变量，包括父类的成员变量，其内存按4字节对齐。
- **对齐字节**：
  - 也叫填充对齐，用来保证Java对象所占内存字节数为8的倍数。
  - HotSpot VM的内存管理要求对象起始地址必须是8字节的整数倍，由于对象头本身是8的倍数，当对象的成员变量数据不是8的倍数时，则需要填充数据来保证8字节的对齐。

![1629615319144](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629615319144.png)

#### Oop对象指针压缩

- Mark Word、Class Pointer、Array Length与JVM位数有关。
  - 在32位JVM中，它们为32位。
  - 在64位JVM中，它们为64位。
- 如果JVM中对象数量过多，想要节约内存，可以使用**+UseCompressedOops**开启指针压缩，开启后，以下类型的指针将会从64位压缩到32位：
  - Class对象的属性指针，即静态变量。
  - Object对象的属性指针，成员变量。
  - 普通对象数组的元素指针。
- 当然，也不是所有的指针都会压缩，一些特殊类型的指针不会被压缩，比如：
  - 指向PermGen（永久代）的Class对象指针，而在JDK 8的是指向元空间的Class对象指针。
  - 本地变量、堆栈元素、入参、返回值和NULL指针等。
- 在堆内存小于32GB的情况下，64位JVM的UseCompressedOops选项是默认开启的，表示会将原来64位的Oop对象指针压缩为32位。

### 3.0. Java内置锁原理？

#### 内置锁的Mark Word结构信息

- Java内置锁（synchronized）涉及很多重要信息，它们都存放在对象结构的对象头Mark Word字段中，其中Mark Word的位结构不会受到Oop对象指针压缩选项的影响。
- Java内置锁状态一共有4种，级别由低到高分别为：**无锁、偏向锁、轻量级锁和重量级锁**。
  - 在JDK 1.6之前，Java内置锁是一个重量级锁，效率比较低下。
  - 在JDK 1.6之后，为了提高锁的获取和释放的效率，JVM对synchronized的实现进行了优化，引入了偏向锁和轻量级锁。
  - 从此Java内置锁就有了以上4种状态，并且它们会随着竞争的情况逐渐升级，但不可降级。
  - Java内置锁的状态与Mark Word字段的结构强相关，为了让Mark Word字段存储更多的信息，JVM将Mark Word**最低两位**设置为Java内置锁状态位，不同锁状态对应着不同的Mark Word结构。
- 64位Mark Word与32位Mark Word类似，它们的结构信息如下图：
  - **lock**：锁状态标记位，2bit，占2个字节。由于Mark Word希望用尽可能少的二进制位表示尽可能多的信息，因此设置了lock标记，该值的不同则整个Mark Word表示的含义也不同。
  - **biased_lock**：对象是否启用偏向锁标记，1bit，占1个字节。为1时表示对象启用偏向锁，为0时表示对象没有偏向锁。
    - 因此，【lock，biased_lock】两个标记位组合在一起，共同表示Object实例处于什么样的锁状态。
    - 【01，0】表示无锁，【01，1】表示偏向锁，【00，无】表示轻量级锁，【10，无】表示重量级锁，【11，无】表示GC标记。
  - **age**：Java对象的分代年龄，4bit，占4个字节。在GC中，对象在Survivor区复制一次，年龄就增加1，当对象达到设定的阈值时，将会晋升到老年代。
    - 由于age只有4位，因此-XX：MaxTenuringThreshold选项最大值为15，即对象的年龄阈值为15。
  - **identity_hashcode**：hashcode，对象标识，哈希码，31bit，占31位字节。
    - 采用延迟加载技术，当调用Object.hashCode（）方法，或者System.identityHashCode（）方法，来计算对象的HashCode后，其结果将被写到该对象头中。
    - 当对象被锁定时，该值会移动到 Monitor监视器中。
  - **thread**：线程ID值，54bit，占54个字节，持有偏向锁的线程ID。
  - **epoch**：偏向时间戳，2bit，占2个字节。
  - **ptr_to_lock_record**：在轻量级锁的状态下，指向栈帧中锁记录的指针，62bit，占62个字节。
  - **ptr_to_heavyweight_monitor**：在重量级锁的状态下，指向对象监视器的指针，62bit，占62个字节。

![1629596133871](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629596133871.png)

![1629596155693](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629596155693.png)

#### 无锁

![1629597556442](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629597556442.png)

无锁，即无线程竞争，在Java对象刚创建时，还没有任何线程来竞争，此时对象处于无锁状态。

- 此时，偏向标志位为0，锁状态标志位为01。

#### 偏向锁

![1629597869808](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629597869808.png)

偏向锁，是指一段同步代码一直被同一个线程所访问，偏向锁状态下的Mark Word会记录内置锁偏爱的线程ID，从而让内置锁认定该线程为“熟人”，从而该线程可以自动获取锁，降低获取锁的代价。

- **主要解决无竞争下的锁性能问题**：由于线程ID被记录在锁对象的Mark Word中（CAS设置），以后该线程获取锁时，只需要判断一下线程ID和标志位，就可以直接进入同步块，连CAS操作都不需要，省去了大量有关锁申请的操作，消除了无锁竞争情况下的同步原语，从而提高了程序的性能。
- **偏向锁的膨胀**：
  1. 如果偏向锁已经被A占据，一旦有第二个线程B来争抢这个对象，由于偏向锁不会主动释放，所以B看到的内置锁是偏向状态，表明已经存在了竞争，则JVM会去检查原来持有该对象锁的占有线程A是否依然存活。
  2. 如果发现A已经挂了，则将锁对象变为无锁状态，然后重新偏向B线程。
  3. 如果发现A依然存活，则会进一步检查A的调用堆栈是否有锁记录持有该偏向锁。如果存在锁记录，表明原来的线程A仍然在使用该偏向锁，即A和B此时发生了锁竞争，JVM则会撤销原来的偏向锁，将偏向锁膨胀（INFLATING）为轻量级锁。
- **偏向锁的撤销**：如果锁对象经常被多个线程竞争，那么偏向锁就是多余的，并且撤销偏向锁的过程也会带来一些性能开销。
  1. 在一个安全点停止拥有锁的线程。
  2. 遍历线程的栈帧，检查是否存在锁记录，如果存在，则需要清空**锁记录**（锁记录是线程私有的，每个线程都有自己的一份锁记录），使其变为无锁状态，并修复锁记录指向的Mark Word，清除其线程ID。
  3. 将当前锁升级为轻量级锁。
  4. 唤醒当前线程。
- **偏向锁的撤销条件**：
  - 多个线程竞争偏向锁。
  - 调用偏向锁对象的hashCode（）方法，或者System.identityHashCode（）方法，计算对象的**HashCode**之后，将哈希码放置到了Mark Word中，内置锁变成无锁状态，偏向锁会被撤销。
- 经验表明，大部分情况下一个同步代码块的线程都是同一个线程，总体来说，**使用偏向锁带来的好处大于偏向锁撤销和膨胀带来的代价**。
  - 如果某些临界区存在两个或者两个以上的线程竞争，偏向锁反而会降低性能，此时可以在启动JVM时把偏向锁的默认功能关闭。

#### 轻量级锁

![1629598045639](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629598045639.png)

轻量级锁，也被成为**非阻塞同步锁、乐观锁**，是指当锁处于偏向锁，又被另一个线程企图抢占时，偏向锁就会升级为轻量级锁，而企图抢占的线程会通过**自旋**的形式尝试获取锁，不会阻塞抢锁线程，以提高性能。其中，哪个线程先占有锁对象，锁对象的Mark Word就指向哪个线程栈帧中的锁记录。

- **引入轻量级锁的主要目的**：在多线程竞争不激烈的情况下，通过CAS机制竞争锁，减少重量级锁产生的性能损耗，尽可能不动用操作系统层面的互斥锁，在应用层面上通过**自旋**来解决线程同步的问题。

  - **自旋原理**：如果持有锁的线程能在很短时间内释放锁资源，竞争等待锁的线程则不需要进行内核态和用户态的切换进入阻塞状态，它们只需要CPU自旋，等待持有锁的线程释放锁后即可立即获取锁，避免了用户线程和内核切换的消耗，从而提高性能。
  - **自旋锁分类**：
    - **普通自旋锁**：指当有线程来竞争锁时，抢锁线程会在原地循环等待，而不是被阻塞，直到那个占有锁的线程释放锁之后，这个抢锁线程才可以获得锁。
      - JDK 1.6的轻量级锁使用的是普通自旋锁，需要使用-XX：+UseSpinning选项手工开启。
      - 默认情况下，自旋次数为10次，可以通过-XX：PreBlockSpin选项来进行更改。
      - 然而，线程自旋需要消耗CPU，如果一直获取不到锁，那么线程也不能一直占用CPU自旋做无用功，因此需要设定一个自旋等待的最大时间。
    - **适应性自旋锁**：
      - JVM对于自旋周期的选择，JDK 1.7引入了**适应性自旋锁**（自动开启），指的是自旋的时间不是固定式的，而是由前一次在同一个锁上的自旋时间，以及锁的拥有者状态来决定的，解决的是**锁竞争时间不确定**的问题，使得**竞争程度趋于稳定**。
      - **自旋成功了则下次自旋的次数就会更多**：如果抢锁线程在同一个锁对象上之前成功获得过锁（竞争力强，适合竞争），JVM则会认为这次自旋很可能再次成功，此次允许自旋等待持续相对更长的时间。
      - **自旋失败了则下次自旋的次数就会减少**：如果对于某个锁，抢锁线程很少成功获得过（竞争力弱，不适合竞争），JVM则减少自旋时间甚至省略自旋过程，以避免浪费CPU资源。

- **轻量级锁的抢占过程**：

  1. 在抢锁线程进入临界区之前，如果该内置锁没有被锁定，JVM首先将在抢锁线程的栈帧中建立一个锁记录（Lock Record），用于存储对象目前Mark Word的拷贝。

     ![1629601733727](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629601733727.png)

  2. 然后，抢锁线程将使用CAS自旋操作，尝试将内置锁对象头的Mark Word的ptr_to_lock_record（锁记录指针）更新为抢锁线程栈帧中锁记录的地址，如果这个更新执行成功了，**这个线程就拥有了对象锁**。

  3. 接着，JVM将Mark Word中的lock标记位改为00，代表该内置锁对象处于轻量级锁状态。

  4. 抢锁成功之后，JVM会将Mark Word中原来的锁对象信息（如哈希码等），保存在抢锁线程锁记录中的Displaced Mark Word字段中，再将抢锁线程中锁记录的owner指针指向锁对象。

     ![1629602512221](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629602512221.png)

- **轻量级锁的膨胀**：

  - 如果临界区代码执行耗时较长，在其执行期间，其他线程都在原地CAS+自旋等待替换ptr_to_lock_record，导致一直空耗CPU，带来很大的性能损耗。
  - 而轻量级锁的本意，是**为了减少多线程进入操作系统层面互斥锁的概率**，并不是要替代其互斥锁。
  - 因此，在内置锁争用激烈的场景下，轻量级锁会膨胀为基于操作系统内核互斥锁实现的重量级锁。

#### 重量级锁

![1629599089661](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629599089661.png)

重量级锁，也称为**同步锁**，是指当锁处于轻量级锁，如果持有锁的线程执行的时间，超过了自旋等待的最大时间仍然没有释放锁，自旋等待锁的线程不会一直自旋下去，而是会进入阻塞状态，该锁也膨胀为重量级锁。

- 锁对象的Mark Word会再次发生变化，指向一个**监视器对象**，该监视器对象会用集合的形式来登记和管理排队的线程。

  - **监视器**：是一个同步工具，相当于一个许可证，拿到许可证的线程即可进入临界区进行操作，没有拿到的则需要阻塞等待。在JVM中，每个对象都关联一个监视器，监视器和对象一起创建、销毁，保证同一时间只有一个线程可以访问被保护的临界区代码。监视器也可以说是同步机制，主要特点是：
    - **同步**：监视器所保护的临界区代码互斥地执行，一个监视器是一个运行许可，任一线程进入临界区代码都需要获得这个许可，离开时会把许可归还。
    - **协作**：监视器提供Signal机制，允许正持有许可的线程暂时放弃许可进入阻塞等待状态，等待其他线程发送Signal去唤醒；其他拥有许可的线程可以发送Signal，唤醒正在阻塞等待的线程，让它可以重新获得许可并启动执行。

- 重量级锁通过**监视器**的方式，保障了任何时间只允许一个线程通过受到监视器保护的临界区代码。在Hotspot虚拟机中，监视器由C++类**ObjectMonitor**实现：

  - **_recursions**：线程的重入次数。
  - **_owner**：标识拥有该Monitor的线程，即获得锁的线程。
  - **cxq**：竞争队列，所有请求锁的线程首先会被放入这个竞争队列中。cxq由Node及其next指针逻辑构成（单向链表），并不存在一个队列的数据结构，只是一个虚拟队列：
    1. 在线程进入cxq前，抢锁线程会先尝试通过CAS自旋获取锁，如果获取不到，则会进入cxq队列，显然抢锁操作这对于那些已经进入了cxq队列的线程是不公平的，因此**synchronized同步块所使用的重量级锁是非公平锁**。
    2. 每次新加入的Node会在cxq的队头进行，通过CAS改变第一个结点的指针为新增结点，同时设置新增结点的next指向后续结点。
    3. 从cxq取出元素时，会从队尾获取。由于只有owner线程才能从队尾取出元素，即线程出列操作无争用，因此cxq是**无锁结构**。
  - **_EntryList**： 候选竞争队列，由于cxq会被线程并发访问，为了降低对cxq队尾的争用，在owner线程释放锁时，JVM会从cxq中迁移线程到EntryList中，并会指定EntryList中某个线程（一般为Head）为OnDeck Thread（Ready Thread），因此EntryList中的线程是作为候选竞争线程而存在的。
    - **OnDeck Thread**：
      - JVM不直接把锁传递给Owner Thread，而是把锁竞争的权利交给OnDeck Thread，On Deck需要重新竞争锁，这种行为称为**竞争切换**，虽然牺牲了一些公平性，但极大提升了系统的吞吐量。
      - OnDeck Thread获取到锁资源后将会变为Owner Thread，无法获得锁的OnDeck Thread则会依然留在EntryList中。
      - 另外，在OnDeck Thread成为Owner的过程中，还要一个**不公平**的事情：后来的新抢锁线程可能会直接通过CAS自旋成为Owner而获得锁。
  - **_WaitSet**：等待队列，某个拥有ObjectMonitor的线程（owner线程）在调用Object.wait（）方法之后将被阻塞，然后该线程将被放置在 _WaitSet链表中，直到某个时刻通过Object.notify（）或者Object.notifyAll（）唤醒后，该线程才会重新进入EntryList中继续候选竞争锁。

  ![1629615372409](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629615372409.png)

- **重量级锁性能低**：重量级锁使用了操作系统底层的互斥锁（Mutex Lock），会导致线程在**用户态和内核态之间的频繁切换**，从而带来较大的性能损耗。

  - 处于cxq、EntryList、WaitSet中的线程都处于阻塞状态，线程的阻塞或者唤醒，都需要操作系统来帮忙。比如在Linux内核中采用pthread_mutex_lock系统调用（互斥锁）来实现，因此，进程需要从用户态切换到内核态。
  - 进程从用户态切换到内核态，这种切换需要消耗很多时间，有可能比用户执行代码的时间还要长，而由于轻量级锁使用CAS进行自旋抢锁，都处于用户态下，进程不存在用户态和内核态之间的切换，因此轻量级锁开销较小，而重量级锁使用了Linux内核态下的互斥锁，会造成较大的性能开销。

#### 执行过程总结

![1629617805708](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629617805708.png)

![1629617826584](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629617826584.png)

synchronized执行过程总结：

1. **确认是否为可偏向状态**：线程抢锁时，JVM首先检测内置锁对象Mark Word中的biased_lock（偏向锁标识）是否设置成1，lock（锁标志位）是否为01，如果都满足，则确认内置锁对象为**可偏向状态**。
2. **确认线程ID**：在内置锁对象确认为可偏向状态后， JVM会检查Mark Word中的线程ID是否为抢锁线程的ID。
3. **同线程ID，直接执行临界区代码**：如果是，则表示抢锁线程处于偏向锁状态，然后抢锁线程快速获得锁，开始执行临界区代码。
4. **竞争锁成功，升级为偏向锁**：如果Mark Word中的线程ID并未指向抢锁线程，则通过CAS操作去竞争锁。
   - 如果竞争成功，则将Mark Word中的线程ID设置为抢锁线程，偏向标志位设置为1，锁标志位设置为01，然后执行临界区代码，此时内置锁对象为**偏向锁状态**。
5. **发生锁竞争失败，偏向锁升级为轻量级锁**：如果CAS操作竞争失败，说明发生了竞争，此时JVM会去检查原先持有锁的线程是否存活。
   - 如果原线程已经死亡，则设置Mark Word为抢到锁的线程ID，内置锁继续保持为偏向锁。
   - 如果原线程没有死亡，且其堆栈中存在该内置锁的所记录，则需要撤销偏向锁，进而升级为**轻量级锁**（继续由原持有锁线程持有）。
6. **其他线程继续CAS竞争，以获取轻量级锁**：如果仍然有其他线程继续竞争轻量级锁，则在JVM将在替换锁对象Mark Word中的ptr_to_lock_record过程中，使用**CAS替换**为抢锁线程的锁记录指针。
   - 如果替换成功，则代表抢锁线程获得了**轻量级锁**。
   - 如果替换失败，表明存在其他线程竞争锁，则JVM接着尝试使用**CAS+自旋**方式来替换。
     - 如果替换成功，代表抢锁成功，则锁对象继续保持为**轻量级锁状态**。
7. **CAS+自旋失败，轻量级锁升级为重量级锁**：如果CAS+自旋替换失败，即超过了最大自旋次数，轻量级锁则升级为重量级锁，此后等待锁的线程都需要进入**阻塞状态**。

**总的来说**：

1. **偏向锁**是在没有发生锁争用的情况下使用的。
2. 一旦有了第二个线程争用锁，偏向锁则会升级为**轻量级锁**。
3. 如果锁争用很激烈，轻量级锁的CAS+自旋到达自旋阈值后，轻量级锁则会升级为**重量级锁**。

#### 适用场景总结

| 锁       | 优点                                                         | 缺点                                           | 适用场景                                 |
| -------- | ------------------------------------------------------------ | ---------------------------------------------- | ---------------------------------------- |
| 偏向锁   | 加锁和解锁不需要额外的消耗，和执行非同步方法比，仅存在纳秒级的差距 | 如果线程间存在锁竞争，则会带来额外的锁撤销消耗 | 适用于只有一个线程访问临界区的场景       |
| 轻量级锁 | 竞争的线程不会阻塞，提高程序的响应速度                       | 抢不到锁竞争的线程会使用CAS自旋等待，会消耗CPU | 锁占用时间很短，追求响应时间，但吞吐量低 |
| 重量级锁 | 线程竞争不使用自旋，不会消耗CPU                              | 使用系统互斥锁，线程阻塞，响应时间缓慢         | 吞吐量高，追求吞吐量，但锁占用时间较长   |

#### 锁消除

锁消除是指，JVM在 JIT编译时，通过对运行上下文的扫描，经过逃逸分析，去除不可能存在共享资源竞争的锁（没发生逃逸的变量作为内置锁对象时），消除没有必要的锁，节省毫无意义的锁请求时间，以提高性能。

#### 锁粗化

锁粗化是指，将多个连续的加锁和解锁操作连接在一起，扩展成一个范围更大的锁，避免频繁的加锁和解锁操作。

- **JVM默认是开启锁消除和锁粗化的**。
- 也可以通过-server -XX：-DoEscapeAnalysis -XX：-EliminateLocks来关闭锁消除和锁粗化。

### 3.1. Java锁分类？

- 显示锁分类有：可重入锁和不可重入锁、悲观锁和乐观锁、公平锁和非公平锁、可中断锁和不可中断锁、共享锁和独占锁。
- 其他锁概念有：互斥锁和读写锁、分段锁、自旋锁、偏向锁、轻量级锁、重量级锁。

#### 可重入锁和不可重入锁

- 从**同一个线程是否可以重复占有同一个锁对象**的角度来分：
  - **可重入锁**：也叫作递归锁，指的是一个线程可以**多次**抢占同一个锁。
  - **不可重入锁**：与可重入锁相反，指的是一个线程只能抢占**一次**同一个锁。
- JUC的ReentrantLock类是可重入锁的一个标准实现类，而synchronized内置锁逻辑上也是可重入的。

#### 悲观锁和乐观锁

- 从**线程进入临界区前是否锁住同步资源**的角度来分：
  - **悲观锁**：先锁再用。
    - 就是悲观思想，每次进入临界区操作数据的时候都认为别的线程会修改，所以线程每次在读写数据时都会**上锁**，锁住同步资源，这样其他线程需要读写这个数据时就会**阻塞**，一直等到拿到锁。
    - 总体来说，悲观锁适用于**写多读少**的场景，遇到**高并发写时性能高**。
  - **乐观锁**：用时检查。
    - 是一种乐观思想，每次去拿数据的时候都认为别的线程不会修改，所以**不会上锁**，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，采取在写时先读出当前版本号，然后加锁操作（比较跟上一次的版本号，如果一样就更新），如果失败就要重复**读-比较-写**的操作。
    - 总体来说，乐观锁适用于**读多写少**的场景，遇到**高并发写时性能低**。
    - Java中的乐观锁基本都是通过CAS自旋操作实现的，但在争用激烈的场景下，CAS自旋会出现大量的**空自旋**，会导致乐观锁性能大大降低。
- Java的synchronized轻量级锁是一种乐观锁，synchronized重量级锁、ReentrantLock是一种悲观锁。

#### 公平锁和非公平锁

- 从**线程抢占锁的机会是否公平、平等**的角度来分：
  - **公平锁**：指不同的线程抢占锁的机会是公平的、平等的，从抢占时间上来说，先对锁进行抢占的线程**一定**被先满足，抢锁成功的**次序体现为FIFO（先进先出）顺序**。简单来说，公平锁就是保障各个线程获取锁都是按照顺序来的，**先到的线程先获取锁**。
    - **优点**：所有的线程都能得到资源，不会饿死在队列中，适合大任务使用。
    - **缺点**：**吞吐量会下降**，队列里面除了第一个线程，其他的线程都会阻塞，CPU唤醒阻塞线程的开销大。
  - **非公平锁**：指不同的线程抢占锁的机会是非公平的、不平等的，从抢占时间上来说，先对锁进行抢占的线程**不一定**被先满足，抢锁成功的**次序不会体现为FIFO（先进先出）顺序**。
    - **优点**：非公平锁由于线程有机会提前插队抢锁，减少了线程挂起的概率，从而减少了一些唤醒线程的开销，**因此整体的吞吐量会比公平锁的高点**。
    - **缺点**：可能会导致线程饥饿问题，即队列中的线程可能会一直获取不到锁，或者长时间获取不到锁。
- 默认情况下，ReentrantLock实例是非公平锁，但是，如果在实例构造时传入了参数true，所得到的锁就是公平锁。另外，ReentrantLock的tryLock（）是一个特例，无论ReentrantLock实例是公平的还是非公平的，都会进行非公平的方式抢锁，即一旦有线程释放了锁，正在tryLock的线程就能优先取到锁，即使已经有其他线程在等待队列中。
- **公平锁效率低原因**：
  - 公平锁要维护一个队列，后来的线程要加锁，即使锁空闲，也要先检查有没有其他线程在等待，如果有，则要将当前线程挂起，并加到队列，由于没有抢锁机制，导致每次都要唤醒队列最前面线程，这样相比较非公平锁就多了一次**线程的挂起和唤醒**。
  - 而对于非公平锁，后来的线程**有一定几率**逃离被挂起的开销，因此减少了线程挂起和唤醒的几率，整体上提高了吞吐量。**因此整体的吞吐量会比公平锁的高点**。

#### 可中断锁和不可中断锁

- 从**线程抢锁等待时是否会响应中断**的角度来分：
  - **可中断锁**：如果某一线程A正占有锁在执行临界区代码，另一线程B正在阻塞式抢占锁，可能由于**等待时间过长**，线程B不想等待了，想先处理其他事情，我们可以让它**中断自己的阻塞等待**，这种就是可中断锁。
  - **不可中断锁**：一旦这个锁被其他线程占有，如果自己还想抢占，**只能选择等待或者阻塞**，直到别的线程释放这个锁，如果别的线程永远不释放锁，那么自己只能**永远等下去**，并且没有办法终止等待或阻塞。
- Java的synchronized内置锁就是一个不可中断锁，而JUC的显式锁（如ReentrantLock）是一个可中断锁。

#### 共享锁和独占锁

- 从**每次是否只有一个线程能持有锁**的角度来分：
  - **独占锁**：
    - 指每次**只有一个线程**能持有的锁。独占锁是一种悲观保守的加锁策略，它不必要地限制了读读竞争，如果某个只读线程获取锁，那么其他的读线程都只能等待，这种情况下就**限制了读操作的并发性**，因为读操作并不会影响数据的一致性。
    - JUC的ReentrantLock类是一个标准的独占锁实现类。
  - **共享锁**：
    - 允许**多个线程**同时获取锁，容许线程并发进入临界区。与独占锁不同，共享锁是一种乐观锁，它放宽了加锁策略，并**不限制读读竞争**，允许多个执行读操作的线程**同时访问共享资源**。
    - JUC的ReentrantReadWriteLock（读写锁）类是一个共享锁实现类。使用该读写锁时，读操作可以有很多线程一起读，但是写操作只能有一个线程去写，而且在写入的时候，别的线程也不能进行读的操作。

#### 互斥锁和读写锁

- 上面讲的共享锁和独享锁是一种广义的说法，而互斥锁与读写锁是指**具体的实现**。
- **JDK中的互斥锁和读写锁实现有**：
  - **互斥锁**：ReentrantLock。
  - **读写锁**：ReadWriteLock。
- 用ReentrantLock锁替代ReentrantReadWriteLock锁虽然可以保证线程安全，但是也会浪费一部分资源，因为多个读操作并没有线程安全问题，所以在读的地方使用ReentrantReadWriteLock读锁，在写的地方使用ReentrantReadWriteLock写锁，可以**提高程序执行效率**。

#### 偏向锁、轻量级锁和重量级锁

- 这些锁指的是**Synchronized的锁状态**，在Java 5通过引入锁升级的机制来实现高效Synchronized，详细介绍如上。
- **偏向锁**：
  - 是指一段同步代码**一直被一个线程所访问**，那么该线程会自动获取锁，从而降低获取锁的代价。
- **轻量级锁**：
  - 是指当锁是偏向锁的时候，**被另一个线程所访问**，偏向锁就会升级为轻量级锁，其他线程会通过自旋的形式尝试获取锁，不会阻塞，提高性能。
- **重量级锁**：
  - 是指当锁为轻量级锁的时候，另一个线程虽然是自旋，但自旋不会一直持续下去，当**自旋一定次数**的时候，还没有获取到锁，就会进入阻塞，该锁膨胀为重量级锁。
  - 重量级锁会让其他申请的线程进入阻塞，性能降低。

#### 分段锁

- 分段锁，是一种**锁的设计思想**，并不是具体的一种锁，目的是**细化锁的粒度**，对于ConcurrentHashMap而言，当操作不需要更新整个数组的时候，就仅仅针对数组中的一项进行加锁操作，通过分段锁的形式来实现高效的并发操作。
- ConcurrentHashMap中的分段锁称为Segment， 当需要put元素的时候，并不是对整个hashmap进行加锁，而是先通过hashcode来知道他要放在那一个分段中，然后对这个分段进行加锁，所以当多线程put的时候，只要不是放在一个分段中，就实现了真正的**并行插入**。

#### 自旋锁

在Java中，自旋锁是指尝试获取锁的线程不会立即阻塞，而是**采用循环的方式去尝试获取锁**。

- **优点**：减少线程上下文切换的消耗。
- **缺点**：循环会消耗CPU，如果出现大量的空自旋，可能还会导致总线风暴。

##### CLH自旋锁

- CLH锁，是一种基于队列排队（单向链表）的自旋锁，由于是Craig、Landin和Hagersten三人一起发明的，因此被命名为CLH锁，也叫CLH队列锁，能够确保无饥饿性、提供先来先服务的公平性。

  - 申请线程只在本地变量上**普通自旋**，不断**轮询前驱的状态**，如果发现前驱释放了锁就结束自旋，成功获取锁。
  - 因此，在节点加入队列之后，抢锁线程不需要进行CAS自旋，只需普通自旋即可，在争用激烈的场景下，**可以大大减少CAS操作的数量，以避免CPU的总线风暴**。

- **CLH自旋锁的大致流程**：

  1. 初始状态队列尾部属性tail，指向一个EMPTY节点。
  2. 线程在抢锁时，会创建一个新的Node加入等待队列尾部，此时tail指向该Node，同时该Node的preNode属性指向tail之前指向的节点（第一个时为EMPTY节点），并且该操作是通过CAS自旋完成的，以确保操作成功。
  3. 线程加入抢锁队列之后，如果不为头结点，则会在前驱节点上自旋，循环判断前驱节点的locked属性是否为false。如果为false就表示前驱节点释放了锁，当前线程抢占到锁，当线程抢到锁之后，**其locked属性一直为true，代表正在使用锁**。
  4. 等到该线程临界区代码执行完，然后调用unlock（）方法释放锁，设置node的前驱引用为null，更新locked属性才为false，代表成功释放了锁。

  ![1630134819633](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630134819633.png)

  ![1630134919788](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630134919788.png)

- **CLH自旋锁的优缺点**：

  - **优点**：空间复杂度低，如果有N个线程、L个锁，每个线程每次只获取一个锁，那么需要的存储空间是O(L+N)，N个线程有N个Node，L个锁有L个Tail。
  - **缺点**：在NUMA架构的CPU平台上性能很差，如果在NUMA架构上使用CLH自旋锁，每个CPU内核有自己的内存，如果前驱节点在不同的CPU内核上，且其内存位置比较远，在CLH自旋判断前驱节点的locked属性时，性能将大打折扣。而CLH锁在SMP架构的CPU平台上则不存在这个问题，性能还是挺高的。
  - **解决方案**：使用MCS队列锁来提升NUMA架构下自旋锁的性能。

##### MCS自旋锁

与CLH自旋锁最大的不同是**线程自旋的规则不同**，CLH是在前驱结点的locked域上自旋等待，而MCS是在**自己的结点的locked域上自旋等待**，从而解决了CLH在NUMA系统架构中，获取locked域状态内存过远的问题。

- **MCS自旋锁的大致流程**：

  1. 队列初始化时没有结点，tail=null。
  2. 线程A想要获取锁，于是将自己置于队尾，由于它是第一个结点，**其locked域为false，表示获得了锁**。
  3. 线程B和C相继加入队列，此时a -> next=b, b -> next=c，尾指针指向线程C对应的结点。由于B和C现在没有获取锁，处于等待状态，所以它们的locked域为true。
  4. 线程A释放锁后，顺着它的next指针找到了线程B，并把B的locked域设置为false，触发线程B获取锁。

  ![1630136366948](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630136366948.png)

### 3.2. Java锁优化？

- **系统层面上**：偏向锁、轻量级锁、重量级锁、适应性自旋锁、CLH自旋锁、MCS自旋锁、锁消除、锁粗化。
- **使用层面上**：
  - **减少锁的时间**：不需要同步执行的代码，能不放在同步快里面执行就不要放在同步快内，可以让锁尽快释放。
  - **减少锁的粒度**：将物理上的一个锁，拆成逻辑上的多个锁，增加并行度，从而降低锁竞争。它的思想也是用空间来换时间，java中很多数据结构都是采用这种方法提高并发操作的效率，比如分段锁。
  - **锁粗化**：大部分情况下我们是要让锁的粒度最小化，锁的粗化则是要增大锁的粒度，比如有一个循环，循环内的操作需要加锁，我们应该把锁放到循环外面，否则每次进出循环，都进出一次临界区，效率是非常差的。
  - **锁分离**：使用读写锁，ReentrantReadWriteLock 是一个读写锁，读操作加读锁，可并发读，写操作使用写锁，只能单线程写。
  - **无锁**：使用CAS，如果需要同步的操作执行速度非常快，并且线程竞争并不激烈，这时候使用CAS效率会更高，因为加锁会导致线程的上下文切换，如果上下文切换的耗时比同步操作本身更耗时，且线程对资源的竞争不激烈，使用volatiled+CAS操作会是非常高效的选择。

### 3.3. 并发编程三大问题？

要想并发程序正确地执行，必须要保证**原子性、可见性以及有序性**。只要有一个没有得到保证，就有可能会导致程序运行不正确。而由于需要尽可能释放CPU的能力，因此在CPU上不断增加内核和缓存，而随着**CPU内核和缓存的增加**，导致了并发编程的**可见性**和**有序性**问题。

#### 原子性问题

所谓原子操作，就是**不可中断的一个或一系列操作**，是指不会被线程调度机制打断的操作。这种操作一旦开始，就一直运行到结束，中间不会有任何线程的切换。

- **问题发生原因**：线程并发操作时，发生了意想不到的调度或者中断。
- **解决方案**：互斥锁、乐观锁（CAS）。
- **场景举例**：比如i++操作。

#### 可见性问题

一个线程对共享变量的修改，另一个线程能够立刻可见，我们称该共享变量具备**内存可见性**。

- **问题发生原因**：线程并发执行，**某些线程读到了没及时刷到主内存的值**，导致最后并发操作后刷回主存的值发生错误。

  - JMM（Java Memory Model，**Java内存模型**）规定，将所有的变量都存放在公共主存中，当线程使用变量时会把主存中的变量复制到自己的工作空间（或者叫私有内存）中，线程对变量的读写操作，是自己工作内存中的变量副本。

- **解决方案**：

  - 所有线程都将共享变量**刷新到主存**，比如使用Java提供的关键字volatile修饰共享变量。
  - 硬件层面可由**MESI协议**来解决。

- **场景举例**：

  1. 主存中有变量sum，初始值为0。
  2. 线程A计划将sum加1，先将sum=0复制到自己的私有内存中，然后更新sum的值。线程A操作完成之后其私有内存中sum的值为1，然而线程A将更新后的sum值回刷到主存的时间是**不固定**的。
  3. 在线程A没有回刷sum到主存前，刚好线程B同样从主存中读取sum，此时值为0，和线程A进行同样的操作，最后期盼的sum=2目标没有达成，最终sum=1。
  4. 最终导致，虽然发生了两次+1操作，但结果只增加了1，导致结果不对，原因为线程A的修改还在其工作内存中，还没有刷入主存，**对线程B不可见**，此时线程B读取到了主内存中的值并发执行了+1操作。

  ![1629965063896](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629965063896.png)

#### 有序性问题

所谓程序的有序性，是指程序**按照代码的先后顺序执行**。如果程序执行的顺序与代码的先后顺序不同，并导致了错误的结果，即发生了有序性问题。

- **问题发生原因**：发生了**指令级重排序**，虽然不会影响单个线程的执行，但是会影响多个线程并发执行的正确性。

- **解决方案**：Java中使用**volatile**关键字可以解决指令重排序的问题。

- **场景举例**：x、y赋值操作发生在a、b赋值操作之前，导致出现（0，0）的结果。

  ![1629966035812](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629966035812.png)

### 3.4. 字节序列大小端问题？

- **背景**：
  - 第一大阵营是PowerPC系列CPU，采用**大端模式**存放数据。
  - 第二大阵营是X86系列CPU，采用**小端模式**存放数据。
- **大端模式**：
  - 是指数据的**高字节保存在内存的低地址**中，而数据的**低字节保存在内存的高地址**中。
  - 大端存放模式有点类似于把数据当作字符串顺序处理：**地址由小向大增加，而数据从高位往低位放**。
  - **适用场景**：由于所有网络协议都是采用大端模式来传输数据的，因此有时也会把大端模式称为**“网络字节序”**。当两台采用不同字节存放模式的主机通信时，在发送数据之前，都必须经过字节次序转换，转成“网络字节序”（大端模式）后再进行传输。
- **小端模式**：
  - 是指数据的**高字节保存在内存的高地址**中，而数据的**低字节保存在内存的低地址**中。
  - 这种存储模式将地址的高低和数据位权有效地结合起来，**高地址部分权值高，低地址部分权值低**，此模式和日常的数字计算在方向上是一致的。
  - **适用场景**：在处理器（即CPU）的计算过程中，因为使用小端模式在数据类型转换的时候（尤其是指针转换）不用考虑地址问题，所以小端模式是**处理器的主流字节存放模式**。**JVM所采用的字节存放模式是小端模式**。

![1629896683986](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629896683986.png)

![1629896701845](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629896701845.png)

### 3.5. CPU三大架构？

- **多处理器结构**：Symmetric Multi-Processor，SMP，对称多处理器，服务器中多个CPU对称工作，每个CPU访问内存地址所需时间相同，主要特征是**共享**，包含对CPU，内存，I/O等进行共享。所有的CPU会共享一条总线，靠此总线连接主存，每个核都有自己的高速缓存，各核相对于BUS对称分布，因此，这种结构称为对称多处理器。

  - **优点**：常见的PC、手机、老式服务器都是SMP架构，其**架构简单**，但**拓展性能非常差**。
  - **缺点**：SMP能够保证内存一致性，但这些**共享的资源很可能成为性能瓶颈**，随着CPU数量的增加，每个CPU都要访问相同的内存资源，可能导致内存访问冲突，可能会导致CPU资源的浪费。

  ![1630129986362](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630129986362.png)

- **非一致存储访问结构**：Non-Uniform Memory Access，NUMA，非一致存储访问，将CPU分为CPU模块，每个CPU模块由多个CPU组成，并且具有独立的本地内存、I/O槽口等，模块之间可以通过互联模块相互访问，访问本地内存的速度将远远高于访问远地内存（系统内其它节点的内存）的速度，这也是非一致存储访问的由来。

  - **优点**：NUMA较好地解决SMP的扩展问题。
  - **缺点**：当CPU数量增加时，因为访问远地内存的延时远远超过本地内存，系统性能无法线性增加。

  ![1630201447035](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630201447035.png)

- **海量并行处理结构**：Massive Parallel Processing，MPP，大规模并行处理，由多个 SMP 服务器通过一定的节点互联网络进行连接，协同工作，完成相同的任务，从用户的角度来看是一个服务器系统。

  - **优点**：每个节点只访问自己的本地资源（如内存、存储等），是一种**完全无共享**结构（节点之间数据不共享，只有通过网络连接实现的协同），因而**扩展能力最好**，理论上其扩展无限制，目前的技术可实现512个节点互联，数千个 CPU。
  - **缺点**：数据按某种规则散布到了各个节点上，**很难做高可用**；每个客户端同时连接所有节点通信，**很影响网络**。

  ![1630201176460](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630201176460.png)

### 3.6. CPU物理缓存架构？

- **背景**：由于CPU的运算速度比主存（物理内存）的存取速度快很多，**为了提高处理速度**，现代CPU不直接和主存进行通信，而是在CPU和主存之间设计了多层的Cache（**高速缓存**），越靠近CPU的高速缓存越快，容量也越小。
- **CPU高速缓存结构**：
  - L1高速缓存和L2高速缓存只能被一个单独的CPU内核使用，容量最小，速度最快。
    - L1高速缓存最接近CPU，容量最小（如32KB、64KB等）、存取速度最快，每个核上都有一个L1高速缓存。
    - ·L2高速缓存容量更大（如256KB）、速度低些，在一般情况下，每个内核上都有一个独立的L2高速缓存。
  - L3高速缓存被同一个CPU芯片上的所有CPU内核共享。
    - L3高速缓存最接近主存，容量最大（如12MB）、速度最低，由在同一个CPU芯片板上的不同CPU内核共享。
  - 主存由系统中的所有CPU共享，容量最大，速度最慢。

![1629962629036](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629962629036.png)

- **缓存命中过程**：
  1. CPU内核读取数据时，先从L1高速缓存中读取。
  2. 如果没有命中，再到L2、L3高速缓存中读取。
  3. 假如这些高速缓存都没有命中，则会到主存中读取所需要的数据。
- **CPU处理过程**：
  1. 先将计算需要用到的数据缓存在CPU的高速缓存中。
  2. 在CPU进行计算时，直接从高速缓存中读取数据。
  3. 在计算完成之后写回高速缓存中。
  4. 在整个运算过程完成后，再把高速缓存中的数据同步到主存。
- **高速缓存优点**：
  - 写缓冲区可以保证指令流水线持续运行，可以避免由于CPU停顿下来等待向内存**写入数据而产生的延迟**。
  - 通过以批处理的方式刷新写缓冲区，以及合并写缓冲区中对同一内存地址的多次写，**减少对内存总线的占用**。

### 3.7. MESI缓存一致性协议？

硬件层的MESI协议是一种用于解决内存可见性问题的手段，其中，CPU主要提供了两种解决办法：**总线锁和缓存锁**。

#### 总线锁

- **背景**：操作系统提供了总线锁机制。线程总线（也叫CPU总线）是所有CPU与芯片组连接的主干道，负责CPU与外界所有部件的通信，包括高速缓存、内存、北桥，其控制总线向各个部件发送控制信号，通过地址总线发送地址信号指定其要访问的部件，通过数据总线实现双向传输。

- **概念**：总线锁的意思是，在线程总线中加入一把锁，当不同的CPU内核访问同一个缓存行时，只允许一个CPU内核进行读取，该CPU内核将**独享共享内存**。

  1. 在CPU内核1要对a执行访问操作的时候，将在总线上发出一个LOCK#信号，使得其他CPU无法通过总线来访问共享主存中的数据，把CPU和主存之间的通信锁住，从而锁住变量a所在的缓存行。
  2. 这样其他CPU内核就不能操作缓存，从而阻塞其他CPU内核，使CPU内核1可以独享此共享内存，即总线被锁住，得等CPU内核1访问完，CPU内核2才能访问b。

  ![1629969079553](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629969079553.png)

- **缺点**：某一个CPU**访问主存**时，总线锁把CPU和主存的通信给锁住了，其他CPU不能操作其他主存地址的数据，使得**效率低下，开销较大**。

#### 缓存锁

- **背景**：总线锁的粒度太大了，最好的方法就是**控制锁的保护粒度**，只需要保证被多个CPU缓存的同一份数据一致即可。所以引入了缓存锁（如缓存一致性机制），后来的CPU都提供了缓存一致性机制，Intel 486之后的处理器就提供了这种优化。
- **概念**：
  - 相比总线锁，**缓存锁降低了锁的粒度**，实现对共享内存的独占性访问，缓存锁（缓存一致性协议）会阻止两个CPU同时修改共享内存的数据。
  - 为了达到数据访问的一致，需要各个CPU在访问高速缓存时遵循一些协议，在存取数据时根据协议来操作，常见的协议有**MSI、MESI、MOSI**等，最常见的就是MESI协议。

#### 缓存一致性机制

缓存一致性机制就是，当某CPU对高速缓存中的数据进行操作之后，通知其他CPU将放弃存储在它们内部的缓存数据，或者从主存中重新读取。

#### 缓存一致性协议

- **背景**：在多CPU的系统中，为了保证各个CPU的高速缓存中数据的一致性，会实现缓存一致性协议，每个CPU通过嗅探在总线上传播的数据来检查自己的高速缓存中的值是否过期，当CPU发现自己缓存行对应的主存地址被修改时，就会将当前CPU的缓存行设置成**无效状态**，当CPU对这个数据执行修改操作时，会重新从系统主存中把数据读到CPU的高速缓存中。
- **高速缓存副本一致性写入模式**：
  - **直写模式**：Write-Through，在数据更新时，同时写入低一级的高速缓存和主存。
    - **优点**：操作简单，因为所有的数据都会更新到主存，所以其他CPU读取主存时都是最新值。
    - **缺点**：数据写入速度较慢，因为数据修改之后需要同时写入低一级的高速缓存和主存。
  - **回写模式**：Write-Back，数据的更新并不会立即反映到主存，而是只写入高速缓存，只在数据被**替换出高速缓存或者变成共享（S）状态**时，如果发现数据有变动，才会将最新的数据更新到主存。
    - **优点**：数据写入速度快，因为发生数据变动时不需要写入主存，所以这种模式占用总线少，**大多数CPU的高速缓存采用这种模式**。
    - **缺点**：实现一致性协议比较复杂，因为最新值可能存放在私有高速缓存中，而不是存放在共享的高速缓存或者主存中。
- **主要实现**：MSI协议、MESI协议等。

##### MSI协议

MSI协议，也叫作**写入失效协议**，采用的是**缓存回写模式**，是缓存一致性协议的基础版本。

1. c1和c2先后读取主存中的同一变量m值0。
2. c1更新m值为1后，并不会更新主存，而是通知c2使其高速缓存中的变量m失效。
3. 在c2第二次读取m时，c1会将m的最新值返回给c2，并且更新主存中m值，此时c1和c2的m值会变成共享状态，且等于主存中的m值。

![1629978291010](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629978291010.png)

##### MESI协议

MESI协议，是MSI协议的扩展，要求在每个**缓存行**（64字节，高速缓存操作的基本单位）维护两个状态位（2bit），使得每个数据位可能处于**M、E、S和I**这4种状态之一，是一种基于过期机制的高速缓存一致性保障协议。

- **MESI状态**：

  - **M：Modified**，被修改，处于Modified状态的缓存行数据只在本CPU中有缓存，且其数据与主存中的数据不一致，数据被修改过。
    1. 该缓存行的数据只在本CPU的私有高速缓存中进行了缓存，而**其他CPU中没有**，是被修改过的（Dirty），即**与主存中的数据不一致**，且没有更新到内存中。
    2. 该缓存行中的内存需要在未来的某个时间点（允许其他CPU读取主存中相应的数据之前）写回（Write Back）主存，当被写回主存之后，该缓存行的状态会变成**独享状态**。
  - **E：Exclusive**，独享的，处于Exclusive状态的缓存行数据只在本CPU中有缓存，且其数据与内存中一致，没有被修改过。
    1. 该缓存行的数据只在本CPU的私有高速缓存中进行了缓存，而**其他CPU中没有**，缓存行的数据是未被修改过的（Clean），并且**与主存中的数据一致**。
    2. 该状态下的缓存行在任何时刻被其他CPU读取之后，其状态将变成**共享状态**。
    3. 在本CPU修改了缓存行中的数据后，该缓存行的状态可以变成**Modified状态**。
  - **S：Shared**，共享的，处于Shared状态的缓存行的数据在多个CPU中都有缓存，且与主存一致。
    1. 该缓存行的数据可能在**本CPU以及其他CPU**的私有高速缓存中进行了缓存，并且各CPU私有高速缓存中的数据**与主存数据一致**（Clean）。
    2. 当有一个CPU修改该缓存行时，其他CPU中该缓存行将被作废，变成**无效状态**。
  - **I：Invalid**，无效的，该缓存行是**无效的**，可能有其他CPU修改了该缓存行。

- **MESI状态转换过程**：

  1. **初始阶段**：开始时，缓存行没有加载任何数据，所以它处于“**I状态**”。
  2. **本地写阶段**：Local Write，如果CPU内核写数据到处于“I状态”的缓存行，缓存行的状态就变成“**M状态**”。
     - 注意：处于“M状态”的缓存行，再由本地CPU写入或者读出，状态是不会改变的。
  3. **本地读阶段**：Local Read，如果本地CPU读取处于“I状态”的缓存行，很明显此缓存没有数据给它。此时分两种情况：
     - ①其他CPU的高速缓存中也没有此行数据，那么从内存加载数据到此缓存行后，将它设成“**E状态**”，表示只有本CPU有此行数据，其他CPU都没有；
     - ②其他CPU的高速缓存有此行数据，就将此缓存行的状态设为“**S状态**”。
  4. **远程读阶段**：Remote Read，假设我们有两个CPU c1和c2，如果c2需要读c1的缓存行内容，c1需要把它的缓存行内容通过主存控制器（MemoryController）发送给c2，c2接收到后将相应的缓存行状态设为“**S状态**”。在设置之前，**主存要从总线上得到这份数据并保存**。
  5. **远程写阶段**：Remote Write，其实确切地说不是远程写，而是c2得到c1的数据后，不是为了读，而是为了写，**也算是本地写**。此时由于本来数据就是从c1那里拷贝过来的，此时c2需要发出一个**RFO（Request For Owner）请求**，说明它需要拥有这行数据的权限，此后其他CPU的相应缓存行设为“**I状态**”，从而保证了数据的安全，但处理RFO请求以及设置“I状态”的过程将给写操作带来**很大的性能消耗**。

  ![1629980427644](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629980427644.png)

#### 总线风暴

CPU会通过MESI协议保障变量的缓存一致性，为了保障“缓存一致性”，不同的内核需要通过总线**来回通信**，因而所产生的流量一般被称为“**缓存一致性流量**”，而由于总线被设计为固定的通信能力，如果缓存一致性流量过大，总线将成为瓶颈，这就是所谓的**总线风暴**。

- 总线风暴与CPU的架构和设计有关，并不是所有的CPU都会产生总线风暴，由于使用lock前缀指令的Java操作（包括CAS、volatile）恰恰会产生缓存一致性流量，**当有很多线程同时执行lock前缀指令操作时，在SMP架构的CPU平台上必然会导致总线风暴**。

### 3.8. 详细介绍Unsafe？

- Unsafe是位于sun.misc包下的一个类，主要提供一些用于**执行低级别、不安全的底层操作**，如**直接访问系统内存资源、自主管理内存资源**等。

- Unsafe类的全限定名为sun.misc.Unsafe，从名字中可以看出这个类对普通程序员来说是“危险”的，一般的应用开发都不会涉及此类，**Java官方也不建议直接在应用程序中使用这些类**。

  - 由于使用Unsafe类可以像C语言一样使用**指针操作内存空间**，这无疑增加了指针相关问题、内存泄漏问题出现的概率。

  - 总之，在程序中过度使用Unsafe类会使得程序出错的概率变大，使得安全的语言Java变得**不再安全**，因此对Unsafe的使用一定要慎重。

  - **Unsafe实例获取方式**：

    ```java
    // 不受信任的代码, 只能通过反射获取Unsafe类并通过其分配直接内存
    Field unsafeField = Unsafe.class.getDeclaredFields()[0];
    unsafeField.setAccessible(true);
    Unsafe unsafe = (Unsafe) unsafeField.get(null);
    ```

- Unsafe大量的方法都是native方法，基于C++语言实现，这些方法在**提升Java运行效率、增强Java语言底层资源操作能力**方面起到了很大的作用。

#### 操作Java变量方法

- **getObject(Object o, long offset)**：根据对象和64位地址偏移量, 获取某个Object类型Java变量的值。
- **putObject(Object o, long offset, Object x)**：根据对象和64位地址偏移量, 将x值存储到Java变量中, 其字段类型必须为Object类型。
- **staticFieldOffset(Field f)**：返回给定静态字段的位置, 任何给定的字段将始终具有相同的偏移量, 并且同一类的两个不同字段永远不会具有相同的偏移量和基数。
- **objectFieldOffset(Field f)**：返回给定字段在其类的存储分配中的位置, 任何给定的字段将始终具有相同的偏移量, 并且同一类的两个不同字段永远不会具有相同的偏移量和基数。
- **staticFieldBase(Field f)**：获取给定静态字段的基本“对象”(可能引用一个对象, 它是一个“cookie”, 不能保证是一个真正的对象, 它不应该以任何方式使用, 除了作为此类中get和put例程的参数), 如果有的话,可以通过 {@link #getInt(Object, long)} 之类的方法访问给定类的静态字段。
- **arrayBaseOffset(Class<?> arrayClass)**：返回给定数组Class的存储分配中第一个元素的偏移量。

#### 操作类和对象方法

- **defineClass(String name, byte[] b, int off, int len, ClassLoader loader, ProtectionDomain protectionDomain)**：告诉VM定义一个类。
- **allocateInstance(Class<?> cls)**：分配一个实例，但不运行任何构造函数。如果尚未初始化类，则初始化该类。

#### 操作C堆方法

- **getByte(long address)**：从给定的内存地址获取一个值。
- **putByte(long address, byte x)**：将值存储到给定的内存地址中。
- **getAddress(long address)**：从给定的内存地址获取本地指针。
- **putAddress(long address, long x)**：将本地指针存储到给定的内存地址。

#### 分配内存方法

- **allocateMemory(long bytes)**：分配一个新的本地内存块，以字节为单位给定大小。
- **reallocateMemory(long address, long bytes)**：重新分配一个新的本地内存块，以字节为单位给定大小。
- **setMemory(Object o, long offset, long bytes, byte value)**：将给定内存块中的所有字节设置为固定值(通常为零), 当对象引用为空时, 偏移量提供一个绝对基地址。
- **copyMemory(Object srcBase, long srcOffset, Object destBase, long destOffset, long bytes)**：将给定内存块中的所有字节设置为另一个块的副本, 当对象引用为空时, 偏移量提供一个绝对基地址。
- **freeMemory(long address)**：释放本地内存块。

#### 系统属性方法

- **addressSize()**：获取本地指针的字节大小。
- **pageSize()**：获取本地内存页面的字节大小。

#### 多线程同步方法

- **compareAndSwapObject(Object o, long offset, Object expected, Object x)**：如果当前保持预期状态，则将Java变量原子更新为x。
- **getAndAddInt(Object o, long offset, int delta)**：以原子方式将给定值添加到给定对象o中给定偏移量处的字段或数组元素的当前值。
- **getAndSetInt(Object o, long offset, int newValue)**：在给定的偏移量处以原子方式将给定值与给定对象 o 内的字段或数组元素的当前值进行交换。
- **getObjectVolatile(Object o, long offset)**：从给定具有 volatile 加载语义的Java变量中获取引用值, 否则等同于 {@link #getObject(Object, long)}。
- **putObjectVolatile(Object o, long offset, Object x)**：使用volatile存储语义将引用值存储到给定的Java变量中。否则等同于{@link #putObject(Object, long, Object)}, 该方法会保证存储对其他线程的立即可见性。
- **putOrderedObject(Object o, long offset, Object x)**：使用volatile存储语义将引用值存储到给定的Java变量中，否则等同于{@link #putObject(Object, long, Object)}, 该方法不保证存储对其他线程的立即可见性。

#### 挂起与恢复方法

- **unpark(Object thread)**：唤醒在park上阻塞的指定线程, 如果该线程并未阻塞, 则它在后续调用park时不会被阻塞。
- **park(boolean isAbsolute, long time)**：阻塞当前线程, 直到当前线程unpark被调用、被中断、time时间过去(非绝对时为纳秒, 绝对时为毫秒, 为0时代表无限阻塞)。

#### 内存屏障方法

- **loadFence()**：确保在栅栏之前不会对loads重排序, 在栅栏后不会对loads或stores重排序。
- **storeFence()**：确保栅栏前不会对stores重排序, 在栅栏后不会对loads或stores重排序。
- **fullFence()**：确保在栅栏之前不会对loads或stores进行重新排序，而在栅栏之后则不会对stores或loads进行重新排序。

### 3.9. 详细介绍CAS？

#### 概念

- CAS，**CompareAndSwap**，比较并交换，是乐观锁的一种实现方式，是一种**无锁算法**，该算法关键依赖两个值——期望值（旧值）和新值，底层CPU利用原子操作判断内存原值与期望值是否相等，如果相等就给内存地址赋新值，否则不做任何操作。
- 操作系统层面的CAS是一条**CPU的原子指令（cmpxchg指令）**，正是由于该指令具备**原子性**，因此使用CAS操作数据时不会造成数据不一致的问题，Unsafe提供的CAS方法直接通过native方式（封装C++代码）调用了底层的CPU指令cmpxchg。
- 当CAS将内存地址的值与预期值进行比较时，如果相等，就证明内存地址的值没有被修改，可以替换成新值，然后继续往下运行；如果不相等，就说明内存地址的值已经被修改，放弃替换操作，然后重新自旋。
- **使用CAS进行无锁编程的步骤大致如下**：
  1. 获得字段的期望值（oldValue）。
  2. 计算出需要替换的新值（newValue）。
  3. 通过CAS将新值（newValue）放在字段的内存地址上，如果CAS失败就重复第（1）步到第（2）步，一直到CAS成功，这种重复俗称**CAS自旋**。
- **CAS对象字段偏移量参数概念**：可见，**对象字段偏移量**，指的是从对象结构的对象头中开始算起，落在对象体中的字节数。

![1629897755808](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629897755808.png)

#### 优点

CAS是处于用户态下的CPU指令级的原子操作，在用于线程同步时，可以不使用锁机制实现，线程也无需进入阻塞状态，没有用户态与内核态之间的切换，**性能开销较小**。

- **JDK应用场景**：
  - java.util.concurrent.atomic包中的原子类、Java AQS、显式锁以及CurrentHashMap等。
  - synchronized重量级锁涉及操作系统内核态下互斥锁的使用，其线程阻塞和唤醒需要进程在用户态到内核态的频繁切换，导致**重量级锁开销大、性能低**；synchronized轻量级锁使用CAS进行自旋抢锁，**CAS是CPU指令级的原子操作**，并处于用户态下，所以**轻量级锁的开销较小**。

#### 缺点

##### 只能保证单个变量原子性

当对一个共享变量执行操作时，可以使用循环CAS的方式来保证原子操作，但是对多个共享变量操作时，CAS就无法保证操作的原子性。

- **解决方案**：把多个共享变量**合并**成一个共享变量来操作。

##### 空耗CPU资源

在高并发、线程争用激烈的场景下，大量的CAS**空自旋**会浪费大量的CPU资源，大大降低了程序的性能。

- **优化思路**：当并发修改的线程少，冲突出现的机会少时，自旋的次数也会很少，CAS的性能会很高；当并发修改的线程多，冲突出现的机会多时，自旋的次数也会很多，CAS的性能会大大降低。所以，**提升CAS无锁编程效率的关键在于减少冲突的机会**，其有效方式之一是**以空间换时间**。

- **解决方案**：

  - **分散操作热点**：LongAdder，其核心思想是**热点分离**，将value值分离成一个数组，当多线程访问时，通过Hash算法将线程映射到数组的一个元素进行操作，在获取最终的value结果时，则将数组的元素求和即可。可见，LongAdder通过**以空间换时间**的方式，将原始的一个value值拆分为分布式的value值，减少了CAS时线程之间的冲突，以提升性能。

    ![1629957184893](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629957184893.png)

  - **使用队列削峰**：将发生CAS争用的线程加入一个队列中排队，降低CAS争用的激烈程度，比如JUC中非常重要的基础类抽象队列同步器AQS。

##### ABA问题

指线程A进行CAS操作，但是线程A发现M位置的数据仍然是V1，然后线程A操作成功。尽管线程A的CAS操作成功，但是不代表这个过程是没有问题的，线程A操作的数据V1可能已经不是之前的V1，而是**被线程B替换过的V1**，这就是ABA问题。

- **场景举例**：如果使用得不合理，CAS原子操作就会存在ABA问题：

  1. 现有一个LIFO（后进先出）堆栈，该堆栈使用单向链表实现，元素的插入和删除都发生在单向链表的头部。

     ![1629950310341](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629950310341.png)

  2. 假设线程A和线程B是两个在堆栈上进行并发操作的线程，其中线程A计划从Head位置通过CAS进行元素E2的弹出操作。

  3. 在线程A刚好启动CAS的执行，但是没有开始之前，线程B抢在前面从Head位置中弹出元素E2、E1，并压入了一个新元素E3，再压入了E2，线程B完成操作之后，栈帧的Head位置的数据仍然是E2，此时head -> E2 -> E3。

  4. 但线程A认为的是head -> E2 -> E1，此时CAS将E2出栈并设置E1为头结点后，会出现head -> E1，而E3却成为了一个游离的NULL -> E3 -> NULL结点。因此，也就是在线程A执行完毕后，线程B之前压入的E3元素处于游离状态，不再存在于堆栈中，平白无故被丢掉了，这就是ABA问题引发的不正常状态。

- **解决方案**：

  - 可以使用**版本号（Version）**方式来解决。每次在执行数据的修改操作时都会带上一个版本号，版本号和数据的版本号一致就可以执行修改操作并对版本号执行加1操作，否则执行失败。
  - 由于每次操作的版本号都会随之增加，因此不会出现ABA问题，因为版本号只会增加，不会减少。

  ```java
  // 1、使用AtomicStampedReference解决ABA问题
  public class AtomicStampedReference<V> {
      // 使用原始数量、初始版本号构造
      public AtomicStampedReference(V initialRef, int initialStamp) {
          pair = Pair.of(initialRef, initialStamp);
      }
      
      // CAS+版本号更新为newReference和newStamp
      public boolean compareAndSet(V   expectedReference,
                                   V   newReference,
                                   int expectedStamp,
                                   int newStamp) {
          Pair<V> current = pair;
          return
              expectedReference == current.reference &&
              expectedStamp == current.stamp &&
              ((newReference == current.reference &&
                newStamp == current.stamp) ||
               casPair(current, Pair.of(newReference, newStamp)));
      }
  }
  
  // 2、或者使用AtomicMarkableReference解决ABA问题
  public class AtomicMarkableReference<V> {
      // 使用原始数量、初始标记构造
      public AtomicMarkableReference(V initialRef, boolean initialMark) {
          pair = Pair.of(initialRef, initialMark);
      }
  
      // CAS+标记更新为newReference和newMark
      public boolean compareAndSet(V       expectedReference,
                                   V       newReference,
                                   boolean expectedMark,
                                   boolean newMark) {
          Pair<V> current = pair;
          return
              expectedReference == current.reference &&
              expectedMark == current.mark &&
              ((newReference == current.reference &&
                newMark == current.mark) ||
               casPair(current, Pair.of(newReference, newMark)));
      }
  }
  ```

##### 总线风暴

在**SMP架构**的CPU平台上，大量的CAS操作可能会导致**总线风暴**。

- sun.misc.Unsafe#compareAndSwapInt（），会根据当前CPU的类型**是否为多核CPU**，来决定是否为cmpxchg指令**添加lock前缀**。
  - 如果程序在多核CPU上运行，就为cmpxchg指令加上lock前缀（lockcmpxchg）。反之，如果程序在单核CPU上运行，就省略lock前缀，因为单核CPU不需要lock前缀提供的内存屏障效果。
  - 在Intel X86平台下，CAS的汇编指令lock cmpxchg是一个l**ock前缀指令**，因此CAS操作和volatile一样，也需要CPU保障变量的**缓存一致性**。
- CPU会通过MESI协议保障变量的缓存一致性，为了保障“缓存一致性”，不同的内核需要通过总线**来回通信**，因而所产生的流量一般被称为“**缓存一致性流量**”，而由于总线被设计为固定的通信能力，如果缓存一致性流量过大，总线将成为瓶颈，这就是所谓的**总线风暴**。
  - 总线风暴与CPU的架构和设计有关，并不是所有的CPU都会产生总线风暴，由于使用lock前缀指令的Java操作（包括CAS、volatile）恰恰会产生缓存一致性流量，**当有很多线程同时执行lock前缀指令操作时，在SMP架构的CPU平台上必然会导致总线风暴**。
- **解决方法**：分散热点、使用队列削峰（比如AQS的对抢锁线程进行排队），从而最大程度上减少了CAS操作数量。

### 4.0. 什么是重排序？

- 重排序是单核时代非常优秀的优化手段，有足够多的措施保证其在单核下的正确性，而在多核时代，如果工作线程之间不共享数据或仅共享不可变数据，重排序也是性能优化的利器。
- 但如果工作线程之间共享了可变数据，由于两种重排序的结果都不是固定的，因此会导致工作线程似乎表现出了**随机行为**。
- 编译器和CPU常常会对指令进行重排序，因此重排序主要分为两类：**编译器重排序**和**CPU重排序**。

![1630031189798](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630031189798.png)

#### 编译器重排序

- 编译器重排序指的是，在代码编译阶段进行指令重排，不改变程序执行结果的情况下，为了提升效率，编译器对指令进行乱序的编译。
  - 其目的在于，与其等待阻塞指令（如等待缓存刷入）完成，不如先去执行其他指令。
  - 其优点在于，与CPU重排序相比，编译器重排序能够完成**更大范围、效果更好**的乱序优化。

#### CPU重排序

- CPU重排序指的是，为了CPU的执行效率，流水线都是并行处理的，在不影响语义的情况下，处理次序和程序次序是允许不一致的，只要满足**As-if-Serial规则**即可。
  - **处理次序**：Process Ordering，机器指令在CPU实际执行时的顺序。
  - **程序次序**：Program Ordering，程序代码的逻辑执行顺序。
- 一般来说，CPU为了提高程序运行效率，可能会对输入代码进行优化，它不保证程序中各个语句的执行顺序同代码中的先后顺序一致，但是它会保证程序**最终的执行结果**和代码顺序执行的结果是一致的。
- CPU重排序包括两类：**指令级重排序**和**内存系统重排序**。

##### 指令级重排序

- 指令级重排序指的是，在不影响程序执行结果的情况下，为了提升效率，CPU内核采用ILP（Instruction-Level Parallelism，指令级并行运算）技术来将多条指令重叠执行。
- 如果指令之间**不存在数据依赖性**，CPU就可以改变语句的对应机器指令的执行顺序。

##### 内存系统重排序

- 内存系统重排序指的是，对于现代的CPU来说，在CPU内核和主存之间都具备一个**高速缓存**，其主要作用是减少CPU内核和主存的交互。在CPU内核进行读操作时，如果缓存没有的话就从主存取，而对于写操作都是先写在缓存中，最后再一次性写入主存，从而提升性能，但可能会导致一个**数据不一致**的问题。
- 内存系统重排序和指令级重排序不同，内存系统重排序为**伪重排序**，也就是说只是**看起来**像在乱序执行而已。

### 4.1. As-if-Serial规则？

- As-if-Serial规则指的是，无论如何重排序，都必须保证代码在单线程下运行正确。
  - 为了遵守As-if-Serial规则，编译器和CPU不会对存在数据依赖关系的操作进行重排序，因为这种重排序会改变执行结果。
    - 为了保证As-if-Serial规则，Java异常处理机制也会为指令重排序做一些特殊处理：JIT在重排序时会在catch语句中插入错误补偿代码，补偿执行语句，将程序恢复到发生异常时应有的状态。
    - 这种做法会将异常捕捉的和处理的底层逻辑变得非常复杂，但是JIT的优化原则是，尽力保障正确的运行逻辑，哪怕以catch块逻辑变得复杂为代价。
  - 但是，如果指令之间不存在数据依赖关系，这些指令可能被编译器和CPU重排序。
- 虽然编译器和CPU遵守了As-if-Serial规则，保证在单CPU执行的情况下保证结果的正确性，在多核CPU并发执行的场景下，由于CPU的一个内核无法清晰分辨其他内核上指令序列中的数据依赖关系，因此可能出现乱序执行，从而导致程序运行结果错误。
- 因此，As-if-Serial规则只能保障**单内核**指令重排序之后的执行结果正确，不能保障多内核以及跨CPU指令重排序之后的执行结果正确。

### 4.2. 硬件层内存屏障？

- 内存屏障（Memory Barrier），又称内存栅栏（Memory Fences），是一系列的CPU指令，是一项让**CPU高速缓存内存可见**的技术，也是一项**保障跨CPU内核有序执行指令**的技术。
- **硬件层内存屏障**分为三种：读屏障、写屏障和全屏障。
  - **读屏障**：Load Barrier，在指令前插入读屏障，可以在指令执行时，**让高速缓存中的数据失效**，强制重新从主存加载数据，并且读屏障会告诉CPU和编译器，**先于这个屏障的指令必须先执行**。
    - 读屏障既使得指令执行时，当前CPU内核对共享变量的更改对所有CPU内核可见，又阻止了一些可能导致读取无效数据的指令重排。
    - 读屏障对应着X86处理器上的lfence指令，将强制**所有读操作都在lfence指令执行之后**被执行，并且强制本地高速缓冲区的值**全部失效**，以便从主存中重新读取共享变量的值。
  - **写屏障**：Store Barrier，在指令后插入写屏障，可以在指令执行时，**让高速缓存中的最新数据更新到主存**，让其他线程可见，并且写屏障会告诉CPU和编译器，**后于这个屏障的指令必须后执行**。
    - 写屏障对应X86处理器上的sfence指令，会保证**所有写操作都在sfence指令执行之前**被完成，并把高速缓冲区的数据都**刷新到主存**中，使得当前CPU对共享变量的更改对所有CPU可见。
  - **全屏障**：Full Barrier，又称为StoreLoad Barriers，是一种全能型的屏障，具备读屏障和写屏障的能力
    - X86处理器平台上mfence指令综合了sfence指令与lfence指令的作用，强制所有在mfence之前的store/load指令都在mfence执行之前被执行，所有在mfence之后的store/load指令都在该mfence执行之后被执行。简单来说，**X86处理器禁止对mfence指令前后的store/load指令进行重排序**。
    - X86处理器上的**lock前缀指令**也具有内存全屏障的功能。
- **硬件层内存屏障的作用**：
  - **强制让高速缓存的数据失效**：硬件层的内存屏障强制把高速缓存中的最新数据写回主存，让高速缓存中相应的脏数据失效，一旦完成写入，任何访问这个变量的线程将会得到最新的值。
  - **阻止屏障两侧的指令重排序**：编译器和CPU可能为了使性能得到优化而对指令重排序，但是插入一个硬件层的内存屏障，相当于告诉CPU和编译器先于这个屏障的指令必须先执行，后于这个屏障的指令必须后执行。
- **volatile与硬件层内存屏障**：
  - volatile在X86处理器上被JVM编译之后，它的汇编代码中会被插入一条**lock前缀指令**（lock ADD），从而实现**全屏障**目的。
  - 由于不同的物理CPU硬件所提供的内存屏障指令的差异非常大，因此**JMM**定义了自己的一套相对独立的内存屏障指令，用于屏蔽不同硬件的差异性。
  - 很多Java关键字（如volatile）在语义中包含JMM内存屏障指令，在不同的硬件平台上，这些**JMM内存屏障指令**会要求JVM为不同的平台生成相应的硬件层的内存屏障指令。

### 4.3. Java内存模型JMM？

![1630043244494](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630043244494.png)

#### JMM概念

- JMM，Java Memory Model，Java内存模型，并不像JVM内存结构一样是真实存在的运行实体，而是体现为一种规范和规则，该规范定义了一个线程对共享变量写入时，**如何确保对另一个线程是可见的**。
  - 为此，JMM提供了合理的禁用缓存以及禁止重排序的方法，用于解决**可见性和有序性**。
  - 同时，JMM还能**屏蔽各种硬件和操作系统的访问差异**，保证Java程序在各种平台下对内存的访问最终都是一致的。
- **Java内存模型规定**，所有的变量都存储在主存中（类似于物理内存），每个线程都有自己的工作内存（类似于CPU中的高速缓存）。工作内存保存了线程使用到的变量的拷贝副本，线程对变量的所有操作（读取、赋值等）必须在该线程的工作内存中进行，不同线程之间无法直接访问彼此工作内存中的变量，要想访问只能通过主存来传递。
- **JMM两个重要概念**：
  - **主存**：堆内存。
    - 主要存储的是**Java实例对象**，所有线程创建的实例对象都存放在主存中，包括成员变量对象、方法中的局部变量对象，以及共享的类信息、常量、静态变量等。
    - 由于是共享数据区域，因此多条线程对同一个变量进行访问可能**会发现线程安全问题**。
  - **工作内存**：线程私有数据区域。
    - 主要存储**当前方法的所有本地变量信息**，工作内存中存储着主存中的变量副本，每个线程只能访问自己的工作内存，即线程中的本地变量对其他线程是不可见的，即使两个线程执行的是同一段代码，它们也会各自在自己的工作内存中创建属于当前线程的本地变量，包括字节码行号指示器、相关Native方法的信息。
    - 注意，由于工作内存是每个线程的私有数据，线程间无法相互访问工作内存，因此存储在工作内存的数据**不存在线程安全问题**。
- **JMM场景举例**：以Java为例，一个i++方法编译成字节码后，在JVM中是分成以下三个步骤运行：
  1. 从主存中复制i的值到CPU的工作内存中。
  2. CPU取工作内存中的值，然后执行i++操作，完成后刷新到工作内存。
  3. 将工作内存中的值更新到主存。

#### JMM与JVM物理内存的区别

|        | JMM                                                          | JVM                                                          |
| ------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 模型上 | 属于概念和规范维度的模型，是一个参考性质的模型，是一组规则，并不实际存在 | 虽然也是一个概念和规范维度的模型，但是大家常常将JVM理解为实体的、实现维度的虚拟机，通常是指HotSpot VM |
| 规定上 | 规定所有的变量都存储在主存中（类似于系统内存，但有区别），还能包含部分共享缓存，而每个Java线程都有自己的工作内存（类似于CPU高速缓存，但也有区别）。 | 定义了一个指令集、一个虚拟计算机架构和一个执行模型，具体的JVM实现需要遵循JVM的模型 |
| 作用上 | 确保了在不同的编译器和不同的CPU平台上，为Java程序员提供了一致的内存可见性和指令并发执行的有序性 | 能够运行根据JVM模型指令集编写的代码，就像真机可以运行机器代码一样 |

#### JMM与硬件内存架构的关系

多线程的执行最终都会映射到CPU上执行，但是Java内存模型和硬件内存架构并不完全一致。总体上来说，JMM和计算机硬件内存架构是**相互交叉**的关系，是一种**抽象概念划分与真实物理硬件的交叉**：

- 对于硬件内存来说只有寄存器、缓存内存、主存的概念，并没有工作内存（线程私有数据区域）和主存（堆内存）之分，也就是说JMM对内存的划分对硬件内存并没有任何影响。
- 而JMM只是一种抽象的概念，是一组规则，并不实际存在，无论是工作内存的数据还是主存的数据，对于计算机硬件来说都会存储在计算机主存中，当然也有可能存储到CPU高速缓存或者寄存器中。

![1630044892085](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630044892085.png)

#### JMM 8个操作

JMM定义了一套自己的主存与工作内存之间的交互协议，即**一个变量如何从主存拷贝到工作内存，又如何从工作内存写入主存**，该协议包含8种操作，并且要求JVM具体实现必须保证其中每一种操作都是**原子的、不可再分的**。

- 如果要把一个变量从主存复制到工作内存，就要按**顺序执行Read和Load操作**；如果要把变量从工作内存同步回主存，就要**按顺序执行Store和Write操作**。
- JMM规定，执行上述8种基本操作时必须满足如下规则：
  1. 不允许read和load、store和write操作之一单独出现，以上两个操作**必须按顺序执行，但没有保证必须连续执行**，也就是说，read与load之间、store与write之间是可插入其他指令的。
  2. 不允许一个线程丢弃它最近的assign操作，也就是说当线程使用assign操作对私有内存的变量副本进行变更时，它必须使用write操作将其同步到主存中。
  3. 不允许一个线程无原因地（比如没有发生过任何assign操作）把数据从线程的工作内存同步回主存中。
  4. 一个新的变量只能从主存中“诞生”，不允许在工作内存中直接使用一个未被初始化（load或assign）的变量，也就是说对一个变量实施use和store操作之前，必须先执行assign和load操作。
  5. 一个变量在同一个时刻只允许一个线程对其执行lock操作，但lock操作可以被同一个线程重复执行多次，多次执行lock后，只有执行相同次数的unlock操作，变量才会被解锁。
  6. 如果对一个变量执行lock操作，将会清空工作内存中此变量的值，在执行引擎使用这个变量前，需要重新执行load或assign操作初始化变量的值。
  7. 如果一个变量实现没有被lock操作锁定，就不允许对它执行unlock操作，也不允许unlock一个被其他线程锁定的变量。
  8. 对一个变量执行unlock操作之前，必须先把此变量同步回主存（执行store和write操作）。

![1630045226983](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630045226983.png)

| 操作   | 作用对象 | 说明                                                         |
| ------ | -------- | ------------------------------------------------------------ |
| Read   | 主存     | 读取。把一个变量的值从主存传输到工作内存中，以便随后的Load操作使用。 |
| Load   | 工作内存 | 载入。把Read操作从主存中得到的变量值，载入到工作内存的变量副本中（可以简单理解为CPU的高速缓存）。 |
| Use    | 工作内存 | 使用。每当JVM遇到一个需要使用变量值的字节码指令时，都会执行Use操作，会把工作内存中的一个变量值传递给执行引擎。 |
| Assign | 工作内存 | 赋值。每当JVM遇到一个给变量赋值的字节码指令时，都会执行Assign操作，操作引擎通过Assign操作给工作内存的变量赋值。 |
| Store  | 工作内存 | 存储。把工作内存的一个变量值传递到主存中，以便随后的Write操作使用。 |
| Write  | 主存     | 写入。把Store操作从工作内存中得到的变量值，写入到主存的变量中。 |
| Lock   | 主存     | 锁定。把一个变量标识为某个线程独占的状态。                   |
| Unlock | 主存     | 解锁。把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定。 |

#### JMM内存屏障

由于不同CPU硬件实现内存屏障的方式不同，**JMM屏蔽了这种底层CPU硬件平台的差异，定义了不对应任何CPU的JMM逻辑层内存屏障，提供了自己的内存屏障指令**，要求JVM编译器实现这些指令，由JVM在不同的硬件平台生成对应的内存屏障机器码，禁止特定类型的编译器（不是所有的编译器重排序都要禁止）和CPU重排序，解决**有序性问题**。

- JMM内存屏障主要有**Load和Store**两类：

  - **Load Barrier**：读屏障，在读指令前插入读屏障，可以在指令执行时，让高速缓存中的数据失效，重新从主存加载数据。
  - **Store Barrier**：写屏障，在写指令后插入写屏障，可以在指令执行时，让写入缓存的最新数据写回主存。

- 在实际使用时，JMM会对Load Barrier和Store Barrier两类屏障进行组合，用于**禁止特定类型的CPU重排序**：

  - **LoadLoad**：LL屏障。

    - 在执行预加载或者支持乱序处理的指令序列中，需要显示地声明LoadLoad屏障。
    - 在Load2要读取的数据被访问前，使用LoadLoad屏障，能保证Load1要读取的数据被读取完毕。

    ```java
    // LoadLoad屏障伪代码
    Load1；LoadLoad；Load2
    ```

  - **StoreStore**：SS屏障。

    - 如果CPU刷新数据时，不能保证按顺序从高速缓冲向主存（或其他CPU），则需要使用StoreStore屏障。
    - 在Store2以及后续写入操作执行前，使用StoreStore屏障，能保证Store1的写入结果对其他CPU可见。

    ```java
    // StoreStore屏障伪代码
    Store1；StoreStore；Store2
    ```

  - **LoadStore**：LS屏障。

    - LoadStore屏障，用于在数据写入操作执行前，确保完成数据的读取。
    - 在Store2以及后续写入操作执行前，使用LoadStore屏障，能保证Load1要读取的数据被读取完毕。

    ```java
    // LoadStore屏障伪代码
    Load1；LoadStore；Store2
    ```

  - **StoreLoad**：SL屏障。

    - StoreLoad屏障，用于在数据读取操作执行前，确保完成数据的写入。该屏障是一个**全能型屏障**，其开销是4种屏障中最大的，因为兼具其他3个屏障的效果，现代的多核CPU大多支持该屏障。
    - 在Load2以及后续所有读取操作执行前，使用StoreLoad屏障，能保证Store1的写入对所有CPU可见。

    ```java
    // StoreLoad屏障伪代码
    Store1；StoreLoad；Load2
    ```

### 4.4. volatile原理？

#### 保证内存可见性

使用volatile修饰的变量，在变量值发生改变时，会立刻同步到主存，并使其他线程的变量副本失效，即一个线程修改了某个volatile变量的值，该值对其他线程立即可见。

- 在正常情况下，系统操作并不会校验共享变量的缓存一致性，只有当共享变量用**volatile**关键字修饰后，才可以保证共享变量的**内存可见性**，也就是将共享变量的改动值立即刷新回主存，该变量所在的缓存行才会被要求进行**缓存一致性的校验**。
- 分析volatile关键字对应的汇编指令，可知在操作volatile变量之前，多出了一个 lock前缀指令**lock addl（lock ADD）**。
- **lock前缀指令具有以下功能**：
  - **将当前CPU缓存行数据立即写回主存**：在执行指令期间，CPU可以**独占共享内存**（即主存）。
    - 对共享内存的独占，老的CPU（如Intel 486）通过**总线锁**方式实现。
    - 由于总线锁开销比较大，因此新版CPU（如IA-32、Intel 64）通过**缓存锁**实现对共享内存的独占性访问，缓存锁（缓存一致性协议）会阻止两个CPU同时修改共享内存的数据。
  - **失效其他CPU中相同地址的缓存行**：
    1. 写回操作时要**经过总线传播数据**，而每个CPU通过嗅探在总线上传播的数据来检查自己缓存的值是否过期。
    2. 当CPU发现自己缓存行对应的内存地址被修改时，就会将当前CPU的缓存行设置为**无效状态**。
    3. 当CPU要对这个值进行修改的时候，会强制重新从主存中把数据读到CPU缓存。
  - **禁止指令级重排序**：lock前缀指令还可以作为**内存屏障**，禁止指令重排序，避免多线程环境下程序出现乱序执行的现象。

#### 禁止指令重排序

##### 硬件层面上

用volatile修饰的变量，在硬件层面上，会通过在指令前后加入**内存屏障指令**（lock前缀指令）来实现，以保证执行的有序性。

- 为了实现volatile关键字语义的有序性，JVM编译器在生成字节码时，会在指令序列中插入**内存屏障**来禁止特定类型的处理器重排序。

- JMM建议JVM volatile采取**保守策略**严格禁止重排序：

  - 在每个volatile读操作的后面插入一个**LoadLoad屏障**，以及一个**LoadStore屏障**，禁止后面的普通读、普通写和前面的volatile读操作之间发生重排序。
    - LoadLoad屏障：在Load2要读取的数据被访问前，使用LoadLoad屏障，能保证Load1要读取的数据被读取完毕。
    - LoadStore屏障：在Store2以及后续写入操作执行前，使用LoadStore屏障，能保证Load1要读取的数据被读取完毕。

  ![1630053036307](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630053036307.png)

  - 在每个volatile写操作前插入一个StoreStore屏障，在写操作后面插入一个StoreLoad屏障，禁止前面的普通写和后面的volatile写操作之间发生重排序，同时禁止后面的普通读和前面的volatile写操作之间发生重排序。
    - StoreStore屏障：在Store2以及后续写入操作执行前，使用StoreStore屏障，能保证Store1的写入结果对其他CPU可见。
    - StoreLoad屏障：在Load2以及后续所有读取操作执行前，使用StoreLoad屏障，能保证Store1的写入对所有CPU可见。

  ![1630052862174](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630052862174.png)

- 由于上述JMM建议的volatile写和volatile读的内存屏障插入策略，是针对任意处理器平台的，所以非常保守。不同的处理器有不同“松紧度”的处理器内存模型，只要不改变volatile读写操作的内存语义，不同JVM编译器可以根据具体情况省略不必要的JMM屏障。

  - 以X86处理器为例，该平台的JVM实现仅仅在volatile写操作后面插入一个StoreLoad屏障，其他的JMM屏障都会被省略。
  - 由于StoreLoad屏障的开销大，因此在X86处理器中，volatile写操作比volatile读操作的开销会大很多。

##### JMM层面上

用volatile修饰的变量，在JMM层面上，是通过对volatile有着特殊约束来实现的，以保证执行的有序性。

- 使用volatile修饰的变量其**read、load、use都是连续出现的**，所以每次使用变量的时候都要从主存读取最新的变量值，替换私有内存的变量副本值（如果不同的话）。
- 其对同一变量的**assign、store、write操作都是连续出现的**，所以每次对变量的改变都会立马同步到主存中。

#### 复合操作不具备原子性

虽然volatile修饰的变量，其要求对变量的（read、load、use）以及（assign、store、write）必须是连续出现的，每次读取的变量可以是最新值，且可以强制刷新回主存，但是在不同CPU内核上并发执行的线程，还是有可能出现读取脏数据的时候，因此volatile变量的复合操作并不具备原子性。

- **场景举例**：

  1. 假设有两个线程A、B分别运行在Core1、Core2上，并假设此时的value为0，线程A、B也都读取了value值到自己的工作内存。
  2. 现在线程A将value变成1之后，完成了assign、store的操作，假设在执行write指令之前，线程A的CPU时间片用完，线程A被空闲，**但是线程A的write操作没有到达主存**。
  3. 由于线程A的store指令触发了写的信号，线程B缓存过期，**重新从主存读取到value值**，但是线程A的写入没有最终完成，线程B读到的value值还是0。
  4. 线程B执行完成所有的操作之后，将value变成1写入主存。
  5. 线程A的时间片重新拿到，重新执行store操作，将过期了的1写入主存。

  => 可见，虽然A、B两线程执行了两次+1操作，但是最终结果只是增加了1而不是2，因此，volatile变量的复合操作不具备原子性。

- **解决方法**：对于复合操作，volatile变量无法保障其原子性，如果要保证复合操作的原子性，就需要使用**锁**。

![1630118559462](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630118559462.png)

### 4.5. Happens-Before规则？

JMM定义了一套Happens-Before规则（先行发生规则），确保只要两个Java语句之间存在Happens-Before关系，JMM需要尽量确保这两个Java语句之间的**内存可见性和指令有序性**。

Happens-Before规则的主要内容包括以下几个方面：

1. **程序顺序执行规则**：即as-if-serial规则，在同一个线程中，有依赖关系的操作按照先后顺序，前一个操作必须**先行发生于**后一个操作，换句话说就是，单个线程中的代码顺序无论怎么重排序，对于结果来说是不变的。
   - As-if-Serial规则，仅仅用来保证程序在单线程执行结果的正确性，但是无法保证程序在多线程执行结果的正确性。
2. **volatile变量规则**：对volatile变量的写操作必须**先行发生于**对volatile变量的读操作。
   - 如果第二个操作为volatile写，无论第一个操作是什么都不能重排序。
   - 如果第一个操作为volatile读，无论第二个操作是什么都不能重排序。
3. **传递性规则**：如果A操作先于B操作，而B操作又先行发生于C操作，那么A操作**先行发生于**C操作。
4. **监视锁规则**：对一个监视锁的解锁操作**先行发生于**后续对这个监视锁的加锁操作。
   - 即无论在单线程还是多线程中，同一个锁如果处于被锁定状态，那么必须先对锁进行释放操作，后面才能继续执行lock操作，先获取锁的线程，对x赋值之后释放锁，另一个再获取锁时，一定能看到前一个加锁线程对x赋值的改动。
   - 由于监视器互斥执行的特性，监视锁规则不会对临界区内的代码进行约束，临界区内的代码可以重排序，其他线程根本无法“观察”到该线程在临界区内的重排序，这种重排序既提高了执行效率，又没有改变程序的执行结果。但JMM不允许临界区内的代码“逸出”到临界区之外，因为那样会破坏监视器的语义。
5. **start规则**：对线程的start操作**先行发生于**这个线程内部的其他任何操作，具体来说就是，如果线程A执行B.start()启动线程B，那么线程A的B.start()操作先行发生于线程B中的任意操作。
   - 即如果主线程A启动子线程B后，线程B能看到线程A在start（）操作前的任何操作。
6. **join规则**：如果线程A执行了B.join()操作并成功返回，那么线程B中的任意操作**先行发生于**线程A所执行的ThreadB.join()操作。
   - 即线程A等待子线程B完成后，线程B的所有赋值操作，线程A都能够看到。

### 4.6. 详细介绍AQS？

![1630209591346](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630209591346.png)

#### 背景

- **CAS自旋的两大性能问题**：
  - CAS恶性空自旋会浪费大量的CPU资源。
  - 在SMP架构的CPU上会导致“总线风暴”。
- 解决CAS恶性空自旋的有效方式之一是**以空间换时间**，较为常见的方案有两种：**分散操作热点和使用队列削峰**。
  - JUC并发包使用的是**队列削峰**的方案解决CAS的性能问题，并提供了一个基于双向队列的削峰基类——抽象基础类**AbstractQueuedSynchronizer**。

#### 特点

- AbstractQueuedSychronizer，**简称AQS**，抽象队列同步器，提供一个构建锁和同步器的框架，能够简单且⾼效地构造出应⽤⼴泛的⼤量的同步器，可以用于实现**依赖先进先出 (FIFO) 等待队列**的阻塞锁和相关同步器（信号量、事件等），旨在成为大多数依赖**单个原子 {@code int} 值来表示状态**的同步器的有用基础。
- AQS支持**独占模式（默认）和共享模式**，在不同模式下等待的线程**共享同一个 FIFO 队列**。
  - 当以独占模式获取时，其他线程尝试获取不会成功。
  - 当以共享模式获取时， 多个线程尝试获取可能会成功。当共享模式获取成功时，下一个等待线程（如果存在）也必须确定它自己是否也可以获取（前驱通过调用setHeadAndPropagate方法传播告知）。
- AQS定义了一个嵌套的 {@link ConditionObject} 类，{@link ConditionObject}可以被支持独占模式的子类用作 **{@link Condition} 实现**，而{@link ConditionObject} 的行为当然取决于其同步器实现的语义。
- AQS还为内部队列提供检查、检测和监视方法，同时也为ConditionObject提供类似方法。

#### 实现原理

##### AQS核⼼思想

1. 如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的⼯作线程，并且将共享资源设置为锁定状态。
2. 如果被请求的共享资源被占⽤，那么就需要⼀套线程阻塞等待以及被唤醒时锁分配的机制，而这个机制AQS是通过CLH队列（自旋锁）实现的，即将暂时获取不到锁的线程加⼊到队列中。
3. 每当线程通过AQS获取锁失败时，线程将被封装成一个Node节点，通过CAS原子操作插入队列尾部。
4. 当有线程释放锁时，AQS会尝试让队头的后继节点占用锁。

![1630209724753](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630209724753.png)

##### AQS主等待队列

AQS主等待队列，是CLH队列（自旋锁）的一个变种，AQS将它们改为用于阻塞同步器：

- 主等待队列的每个节点都充当一个**特定的通知式监视器**，持有一个**等待线程**（用于保存后继有关控制信息），一个**状态字段**（用于跟踪线程是否应该阻塞）。
- 在前驱结点Release时，当前结点将会收到信号，从而可能会尝试获取成为在队列中的head结点，但并不保证成功，只是给予当前结点参与竞争的权利。
- 而要加入CLH队列，需要原子地将其拼接为新的尾部，要出列则需要设置该结点成为head结点。
- “prev”前驱指针（原始CLH锁中没有前驱指针），用于处理取消结点：如果一个结点被取消，则它的后继会重新链接到一个未取消的前驱；以及判断是否为头结点，方便后继执行一次尝试抢占锁的操作。
- “next”后继指针，用于实现阻塞与通知机制：每个节点的线程id保存在它自己的节点中，因此前驱通过遍历下一个链接来确定它是哪个线程来通知下一个节点唤醒。
- 此外，AQS主等待队列还需要一个**虚拟头结点**来启动，但是AQS不会在构建时创建它们，因为如果从不存在争用，那将是浪费精力。因此，AQS只会在第一次争用时，才构造结点并设置头指针和尾指针。
- 即使AQS内部基于FIFO队列，但它也不会自动执行FIFO采集策略，因为在入队之前会先调用获取的检查，所以新的获取线程可能会抢在其他被阻塞和排队的线程之前。
  - 虽然该策略无法保证公平或者无饥饿，但允许较早的排队线程在较晚的排队线程之前竞争，从而能够保持**最高的吞吐量和可扩展性**。
  - 如果需要可以定义{@code tryAcquire} 或者 {@code tryAcquireShared} ，通过内部调用一种或多种检查方法来禁用插入，从而提供公平的 FIFO 获取顺序。比如{@link #**hasQueuedPredecessors**}（一种专门设计用于公平同步器使用的方法）返回 {@code true}时不允许插入，因此大多数公平同步器可以基于该方法，从而定义 {@code tryAcquire} 返回 {@code false}，代表不允许提前插入。

![1630238528789](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630238528789.png)

##### AQS条件队列

- Condition，是JUC用来替代传统Object的wait()/notify()线程间通信与协作机制的新组件，相比调用Object的wait()/notify()，调用Condition的await()/signal()这种方式实现**线程间协作更加高效**。
  - Condition与Object的wait()/notify()作用是相似的，都是使得一个线程等待某个条件，只有当该条件具备signal()或者signalAll()方法被调用时等待线程才会被唤醒，从而重新争夺锁。
  - 不同的是，Object的wait()/notify()由JVM底层实现，而Condition接口与实现类完全使用Java代码实现。当需要进行线程间的通信时，建议结合使用ReetrantLock与Condition，通过Condition的await()和signal()方法进行线程间的阻塞与唤醒。
- ConditionObject类是实现条件队列的关键，每个ConditionObject对象都维护一个单独的条件等待队列。每个ConditionObject对应一个条件队列，它记录该队列的头节点和尾节点。
  - 在一个显式锁上，我们可以创建多个等待任务队列，这点和内置锁不同，Java内置锁上只有唯一的一个等待队列。
  - Condition条件队列是单向的，而AQS同步队列是双向的，AQS节点会有前驱指针。一个AQS实例可以有多个**条件队列，是聚合关系**；但是一个AQS实例只有一个**同步队列，是逻辑上的组合关系**。
- AQS条件队列，也使用了相同的Node队列结点，但额外维护了一个nextWaiter指针，在调用Condition#await时，会把一个Condition结点插入到条件队列中，然后根据Condition#signal信号，该结点将会被转移到主队列中，去参与AQS竞争，竞争失败的会重新阻塞，阻塞后依赖于AQS的CLH机制实现唤醒。

![1630238750935](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630238750935.png)

##### 队列结点数据结构

![1630208598316](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630208598316.png)

- **waitStatus**：
  - **waitStatus为0**：表示当前节点处于初始状态。
  - **CANCELLED = 1**：表示线程因为中断或者等待超时，需要从等待队列中取消等待，其节点不会参与竞争，且会一直保持取消状态。
  - **SIGNAL = -1**：表示其后继的节点处于等待状态，当前节点对应的线程如果释放了同步状态或者被取消，就会通知后继节点，使后继节点的线程得以运行。
  - **CONDITION = -2**：表示该线程在条件队列中阻塞，当持有锁的线程调用了CONDITION的signal（）方法之后，节点会从条件队列转移到AQS主等待队列上，去竞争锁。
  - **PROPAGATE = -3**：表示下一个线程获取共享锁后，自己的共享状态会被无条件地传播下去，因为共享锁可能出现同时有N个锁可以用，这时直接让后面的N个节点都来工作，这样就不会让其他等待的线程等很久，这种向后传播的目的也是**通知其他等待的线程尽快获取锁**。
- **thread**：存放进入AQS队列中的线程引用。
- **nextWaiter**：如果当前结点为条件等待结点，则说明该结点处于某个Condition的等待队列上，指向该条件队列的后继等待结点；如果当前结点为普通结点，则只作为独占（null）/共享（new Node）模式的标记。
  - **SHARED**：new Node（），共享模式的标记，表示线程是因为获取共享资源时阻塞而被添加到队列中的。
  - **EXCLUSIVE**：null，独占模式的标记，表示线程是因为获取独占资源时阻塞而被添加到队列中的。
- **prev**：前驱结点，当前结点会在前驱结点上自旋，循环检查前驱结点的waitStatus状态。用于处理取消结点，如果一个结点被取消，则它的后继会重新链接到一个未取消的前驱；以及判断是否为头结点，方便后继执行一次尝试抢占锁的操作。
- **next**：后继结点，用于实现阻塞与通知机制，每个节点的线程id保存在它自己的节点中，因此前驱通过遍历下一个链接来确定它是哪个线程来通知下一个节点唤醒。

#### 同步状态

AQS中维持了一个单一的volatile修饰的状态信息state，使用int类型的state标示锁的状态，可以理解为锁的同步状态。

#### 钩子方法

AQS钩子方法，默认实现是抛出UnsupportedOperationException异常，而AQS其他方法都是final类型的方法，无法被子类重写。

- **tryAcquire(int)**：独占锁钩子，尝试获取资源，若成功则返回true，若失败则返回false。
- **tryRelease(int)**：独占锁钩子，尝试释放资源，若成功则返回true，若失败则返回false。
- **tryAcquireShared(int)**：共享锁钩子，尝试获取资源。
  - 负数，表示失败。
  - 正数，表示成功，且有剩余资源。
  - 0，也表示成功，但没有剩余可用资源。
- **tryReleaseShared(int)**：共享锁钩子，尝试释放资源，若成功则返回true，若失败则返回false。
- **isHeldExclusively()**：独占锁钩子，判断该线程是否正在独占资源，只有用到condition条件队列时才需要去实现它。

#### 独占模式

![1630209980487](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630209980487.png)

![1630210001133](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630210001133.png)

- 独占模式下，tryAcquire返回的是boolean值，其含义取决于实现类定义的语义。
  - 经过推算，要实现独占就要保证：如果想要获取同步器，则当已存在独占线程时，则要返回false，当没存在独占线程时，则要返回true。
- 每个结点尝试获取同步器，获取失败的生成Node结点，并加入AQS主队列参与竞争，竞争失败的则会将前驱结点设置为SINGAL状态，然后阻塞。
- 每个结点（此时作为后继的前驱）释放同步器时，由于当前为SINGAL状态，所以会唤醒后继结点，重新参与AQS竞争。

##### 获取资源

1. 尝试以独占模式获取同步器，如果获取成功，则直接返回即可。
2. 如果获取失败，则使用当前线程构建独占模式的Node结点，并CAS+自旋直至入队成功。
3. 入队成功后，则调用acquireQueued方法，自旋判断前驱是否为头结点，如果是则执行一次抢占锁的操作，抢占成功则更新头结点，并返回中断标记interrupted（该方法唯一返回入口）。
4. 如果检测到前驱不为头结点，或者抢占锁失败，则使用LockSupport.park（this）来阻塞当前线程。
5. 直到LockSupport.unpark（当前线程的实例）调用后，当前线程被唤醒，会先检查线程中断状态，然后才会重新自旋检查前驱状态、抢占锁或者继续阻塞。

##### 可抛中断异常原理

对比普通独占获取资源方法，可抛中断异常获取的方式，**主要不同的地方在于**：

1. 尝试以独占模式获取同步器前，会先检查线程是否被中断过，如果是会**提前抛出中断异常**，阻止继续往下执行。
2. 在抢占失败一次后，直接调用doAcquireInterruptibly方法来添加入队结点。
3. 接着自旋判断到前驱为头结点，并成功抢占锁后，不会返回任何值。
4. 被LockSupport.unpark（当前线程的实例）被唤醒后，会先检查线程中断状态，如果发现线程已被中断，则会**立马抛出中断异常**。

##### 定时获取原理

对比普通独占获取资源方法，定时获取的方式，**主要不同的地方在于**：

1. 尝试以独占模式获取同步器前，会先检查线程是否被中断过，如果是会提前抛出中断异常，阻止继续往下执行。
2. 在抢占失败一次后，直接调用doAcquireNanos方法来添加入队结点，并且根据当前系统纳秒时间，来**计算剩余过期时间**。
3. 接着自旋判断到前驱为头结点，并成功抢占锁后，会**返回true**，代表在规定时间内成功获取到了锁。
4. 如果前驱不为头结点，或者抢锁失败，则会继续根据当前系统纳秒时间，来**计算剩余过期时间**，并且如果发现超过了定时时间，则会**返回false**，代表未能在规定时间内获取到锁，抢锁失败。
5. 如果没超过定时时间，则调用的是LockSupport.parkNanos(this, nanosTimeout)来**定时阻塞当前线程**。
6. 被LockSupport.unpark（当前线程的实例）被唤醒后，会先检查线程中断状态，如果发现线程已被中断，则会立马抛出中断异常。

##### 释放资源

1. 尝试以独占模式释放同步器，如果释放失败，则直接返回false即可，代表释放失败。

2. 如果释放成功，则需要继续判断头结点，如果头结点为空，或者为初始的普通结点，则返回true，代表释放成功。

3. 如果头结点为业务结点，则还需要调用**unparkSuccessor（head）**方法来唤醒后续排队的结点，唤醒后则返回true，代表释放成功。

   - 调用释放方法并不会重新设置头结点，而仅仅是更新头结点的waitStatus以及唤醒后继结点。
   - 而重新设置头结点和清空头结点，将留到其后继获取同步状态成功后进行更新，这样可以保证后继线程自己来确保成为头结点，比放在释放方法里由非头线程设置头结点要严谨一些。

   4. unparkSuccessor（head）方法，会先更新头结点的waitStatus为0，然后获取后继结点s，使用LockSupport.unpark(s.thread)来唤醒s结点的排队线程，代表当前结点出队成功。

#### 共享模式

- 共享模式下，tryAcquireShared返回的是int值，其含义取决于实现类定义的语义。
- PROPAGATE相当于启动一个**占位的作用**：
  - 获取到的资源非最后一个资源的结点，状态为PROPAGATE。
  - 获取到的资源最后一个资源的结点，状态一开始为0，不过很快就会在下一个结点阻塞前，设置为SINGAL；
  - 当它们释放共享锁时，如果为PROPAGATE结点释放，则无任何唤醒后继结点的操作，而如果为SINGAL结点释放，则会唤醒后继结点，接着又会重复出现PROPAGATE和SINGAL结点，然后往复之前的操作。

##### 获取资源

对比普通独占获取资源方法，共享获取资源方式，**主要不同的地方在于**：

1. 在尝试获取共享资源时，如果获取到的数量大于等于0，说明获取成功，此时直接返回即可。
2. 但在获取到的数量小于0时，说明获取失败，此时需要调用doAcquireShared方法来添加入队结点，但此时设置的结点的nextWaiter不在是默认的null，而是SHARED，代表为共享模式的结点。
3. 接着自旋判断到前驱为头结点，会进行一次尝试获取共享资源，如果获取的数量大于等于0，说明获取成功，则**调用setHeadAndPropagate方法**更新当前结点为头结点，**调用doReleaseShared方法**设置头结点为PROPAGATE，表示下一个线程获取共享锁后，自己的共享状态会被无条件地传播下去，**通知其他等待的线程尽快获取锁**。传播完毕后，返回之前需要判断当前线程是否有被中断，如果是的话则调用一次中断方法，重新设置线程中断标志再返回。
4. 如果获取的数量小于0，说明获取失败，则在调用shouldParkAfterFailedAcquire方法时，由于此时该前驱为最后一个成功获取资源的结点，所以会更新前驱为SINGAL，代表在前驱释放锁成功后，需要唤醒当前线程，接着使用LockSupport.park（this）来阻塞当前线程。
5. 被LockSupport.unpark（当前线程的实例）被唤醒后，会先检查线程中断状态，如果发现线程已被中断，不会抛出中断异常，而是更新代码中断标志位为true。

##### 可中断式获取

对比普通共享获取资源方法，可抛中断异常获取的方式，**主要不同的地方在于**：

1. 尝试以独占模式获取同步器前，会先检查线程是否被中断过，如果是会**提前抛出中断异常**，阻止继续往下执行。
2. 在抢占失败一次后，直接调用doAcquireInterruptibly方法来添加入队结点。
3. 接着自旋判断到前驱为头结点，在尝试获取共享资源成功，并传播完毕后，返回之前不会判断当前线程是否有被中断，而是**直接返回**。
4. 被LockSupport.unpark（当前线程的实例）被唤醒后，会先检查线程中断状态，如果发现线程已被中断，则会**立马抛出中断异常**。

##### 定时式获取

对比普通共享获取资源方法，定时获取的方式，**主要不同的地方在于**：

1. 尝试以独占模式获取同步器前，会先检查线程是否被中断过，如果是会提前抛出中断异常，阻止继续往下执行。
2. 在抢占失败一次后，直接调用doAcquireSharedNanos方法来添加入队结点，并且根据当前系统纳秒时间，来**计算剩余过期时间**，如果定时时间小于等于0，则**返回false**，表未能在规定时间内获取到锁，抢锁失败。
3. 接着自旋判断到前驱为头结点，在尝试获取共享资源成功，并传播完毕后，返回之前不会判断当前线程是否有被中断，而是**返回true**，代表在规定时间内成功获取到了锁。
4. 如果前驱不为头结点，或者抢锁失败，则会继续根据当前系统纳秒时间，来**计算剩余过期时间**，并且如果发现超过了定时时间，则会**返回false**，代表未能在规定时间内获取到锁，抢锁失败。
5. 如果没超过定时时间，则调用的是LockSupport.parkNanos(this, nanosTimeout)来**定时阻塞当前线程**。
6. 被LockSupport.unpark（当前线程的实例）被唤醒后，会先检查线程中断状态，如果发现线程已被中断，则会立马抛出中断异常。

##### 释放资源

1. 尝试以共享模式释放同步器，如果释放失败，则直接返回false即可，代表释放失败。
2. 如果释放成功，则需要继续判断头结点，如果头结点为SIGNAL结点，则需要调用unparkSuccessor（head）方法来唤醒后续排队的结点，唤醒后则返回true，代表释放成功。
   - unparkSuccessor（head）方法，会先更新头结点的waitStatus为0，然后获取后继结点s，使用LockSupport.unpark(s.thread)来唤醒s结点的排队线程，代表当前结点出队成功。
3. 如果头结点是waitStatus为0的普通结点（在并发设置传播特性时走到这里），则需要CAS更新waitStatus为PROPAGATE，然后返回true，代表释放成功。
   - 调用释放方法并不会重新设置头结点，而仅仅是更新头结点的waitStatus以及唤醒后继结点。
   - 而重新设置头结点和清空头结点，将留到其后继获取同步状态成功后进行更新，这样可以保证后继线程自己来确保成为头结点，比放在释放方法里由非头线程设置头结点要严谨一些。

#### 条件同步

![1630210049637](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630210049637.png)

![1630210030025](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630210030025.png)

- **ConditionObject数据结构**：
  - **firstWaiter**：条件队列的第一个节点。
  - **lastWaiter**：条件队列的最后一个节点。
- **ConditionObject构造方法**：
  - 由ReentrantLock.Sync#newCondition()方法调用，从而创建一个ConditionObject实例对象。

##### 条件阻塞等待

1. 首先会创建一个**Condition结点**，并放入Condition条件队列的队尾。
2. 然后获取全部的同步状态来释放锁，其中会调用**unparkSuccessor方法**唤醒AQS主等待队列中头结点的后继线程。
   - 这里执行的一次AQS释放锁操作，类似于执行一次synchronized重量级锁的【指定EntryList头结点线程为OnDeck Thread】一样，用于从**候选竞争队列EntryList**中选取就绪线程。
   - 由于要触发一次AQS抢锁操作，需要一定的资源次数来获取，为了不影响其他结点的使用，先获取了全部的同步器状态，然后马上全部释放掉，保证同步器状态不变，但是如果发生了并发问题，释放失败的话则会抛出IllegalMonitorStateException监视器异常。
3. 接着判断当前结点是否已经在AQS主等待队列中，如果还没有进入主等待队列，则执行while循环，将当前线程阻塞，直到该结点被调用**doSignal方法**离开等待队列，重新回到同步队列成为同步节点后，线程才退出while循环。如果期间发生了虚假唤醒，则需要检查线程中断状态，如果已发生了中断，则需要退出while循环。
   - 条件队列，相当于synchronized重量级锁的**cxq队列**，所有请求等待线程都是被放入这个队列中，在调用到结点的doSignal方法时，会将结点waitStatus更新为0，放入AQS主等待队列中参与AQS主等待队列的排队，在成功抢占锁后会退出Object#await那里的while循环。
   - 而这里的AQS主等待队列，则相当于synchronized重量级锁的**EntryList队列**，用于管理等待队列竞争锁的问题，同理，这里把条件队列结点转移到AQS主等待队列结点中等待，是因为需要把竞争锁的工作，交由更加专业的AQS主等待队列去完成。
4. 退出while循环后，代表结点肯定在AQS主等待队列中了，则开始调用acquireQueued方法尝试抢锁，如果没抢到锁，则使用LockSupport.part（this）进入阻塞，**根据AQS机制来实现唤醒**。
5. 当从acquireQueued方法中返回，说明当前线程已经抢到锁了，则先检查返回值，如果为true，说明抢锁期间发生了中断，则更新代码的中断模式为THROW_IE或者REINTERRUPT，然后继续运行。
6. 最后在void返回前，清空Condition条件队列中被取消的节点，响应代码更新到的中断模式。
   - REINTERRUPT，代表在退出等待时重新标记线程为已中断状态。
   - THROW_IE，代表在退出等待时要抛出nterruptedException。

##### 定时式等待

对比普通条件阻塞等待方法，定时等待的方式，**主要不同的地方在于**：

1. 在会创建Condition结点前，会先**校验线程是否已中断**，如果是则会提前抛出中断异常，阻止继续往下执行。
2. 接着进入判断是否在主等待队列+while阻塞循环前，会先根据系统纳秒时间，计算出超时时间，然后调用LockSupport.parkNanos(this, nanosTimeout)方法，来**定时阻塞当前线程**。
3. 当在while循环中发生了虚假唤醒，则重新计算超时时间，继续判断+循环，如果发现**已经超时了**，则取消Condition结点，交由AQS主等待队列去管理和释放。
4. 最后，在清空Condition条件队列中被取消的节点，且响应代码更新到的中断模式完毕后，不再是void返回了，而是返回是否发生了超时，**如果为true代表没有发生超时，如果为false代表发生了超时**。

##### 条件唤醒

1. 先更新指定的条件队列结点的waitStatus为0，然后enq方法自旋入队，返回其前驱结点p。
2. 如果前驱结点p的状态是取消状态，或者设置p为Signal状态失败，则唤醒当前线程，让其退出Condition#await中的while循环，进行一次抢占锁的操作，抢占失败则需要AQS机制来唤醒，从而保证必定能被AQS管理到。
3. 如果结点顺利入队，则参与AQS主等待队列的排队，交由AQS机制来管理和释放。

#### 典型实现

![1630201659838](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630201659838.png)

##### CountDownLatch

- CountDownLatch，一种同步辅助，**它允许一个或多个线程等待，直到执行的一组操作完成才唤醒这些线程**。
- {@code CountDownLatch} 使用给定的计数进行初始化，{@link #await await} 方法阻塞，直到当前计数由于 {@link #countDown} 方法的调用而达到零，之后所有等待线程都被释放，并且 {@link #await await} 的任何后续调用立即返回。这是一种**一次性现象**（即计数无法重置），如果需要重置计数的版本，请考虑使用 {@link CyclicBarrier}。
- {@code CountDownLatch} 是一种**多功能同步工具**，可用于多种用途。 
  - 计数初始化为1的 {@code CountDownLatch} 用作简单的开关锁存器或门闩。所有调用 {@link #await await} 的线程在门处等待，直到它被调用 {@link # countDown}。
  - 计数初始化为 N 的 {@code CountDownLatch} 可用于使一个线程等待，直到 N 个线程完成某个操作，或者某个操作已完成 N 次。

###### Sync

- **tryAcquireShared**：实现钩子方法，尝试获取共享资源。
  1. 如果计数为0，则返回1，代表获取资源成功，此时在共享模式下，会唤醒一个又一个的线程，实现**释放全部等待线程**的功能。
  2. 如果计数不为0，则返回-1，代表获取资源失败，此时在共享模式下，会**阻塞所有排队线程**。
- **tryReleaseShared**：实现钩子方法，尝试释放共享资源。这里的共享释放，做的是**传播，而非许可的累加**。
  1. 开始自旋，如果计数为0，则返回false，代表门闩资源释放失败，因为计数已经为0，不能再减了。
  2. 如果计数不为0，则计数-1，然后使用CAS更新，最后返回更新后是否为0。
     - 如果为0，则返回true，代表当前线程为最后一个到达门闩的线程，需要**释放资源**。
     - 如果不为0，则返回false，代表当前线程不是最后一个到达的线程，需要继续**阻塞等待**。

###### await

1. 由“主线程”调用，传入的参数为1，代表需要获取1个资源。
2. 由于实现了tryAcquireShared钩子方法，1又不等于0，所以每次都会返回-1，代表获取资源失败，导致**主线程入队+阻塞**。

###### countDown

1. 由"子线程"调用，传入的参数为1，代表需要释放1个资源。
2. 由于实现了tryReleaseShared钩子方法，该方法每次都会减少1，如果没减到0，则返回false，代表当前线程还不是最后一个到达的线程，因此不会做任何唤醒操作。
3. 而如果减到了0，则返回true，代表当前线程为**最后一个到达的线程**，因此会调用doReleaseShared方法，唤醒头结点后继线程，由于资源为0，所以该后继线程能够获取资源成功，进而继续唤醒后继线程，实现唤醒等待线程的功能。

##### ReentrantLock

- ReentrantLock，**{@link Lock}接口的可重入、互斥锁实现**，与使用{@code synchronized}方法和语句访问的隐式监视器锁具有相同的基本行为和语义，但具有扩展功能，由上次成功lock但尚未unlock的线程拥有：
  - 当锁不属于另一个线程时，调用 {@code lock} 的线程将返回并成功获取锁。
  - 如果当前线程已经拥有锁，该方法将立即返回。 可以使用方法 {@link #isHeldByCurrentThread} 和 {@link #getHoldCount} 进行检查。
- ReentrantLock的构造函数接受一个可选的公平参数，当设置{@code true}时，在争用情况下，锁倾向于授予对**等待时间最长的线程**的访问权限，否则这个锁不能保证这个特定的访问顺序。与使用默认设置的程序相比，使用由多个线程访问的公平锁的程序可能会显示出较低的总体吞吐量（即更慢，通常慢得多），但是能避免线程饥饿问题。
  - 请注意，**锁的公平性并不能保证线程调度的公平性**。因此，使用公平锁的许多线程之一可能会连续多次获得它，而其他活动线程没有进行并且当前没有持有该锁。
  - 另请注意，**未计时的 {@link #tryLock()} 方法不符合公平性设置**。 即使其他线程正在等待，如果锁可用，它也会成功。
- ReentrantLock，建议的做法是，在lock成功获取锁后需要try-finally或try-catch的保护，以确保在必要时unlock。

###### Sync

- **nonfairTryAcquire**：非钩子方法，用于提供公共的**非公平、非阻塞**获取独占资源的方法。
  1. 先获取当前锁次数，如果锁次数为0，则CAS更新锁次数为需要获取的次数，更新成功后设置当前线程为独占线程，然后返回true，代表获取独占资源成功。
  2. 如果锁次数不为0，但当前独占线程为当前线程，则累加需要获取的次数为锁次数，代表**锁重入**，然后返回true，代表获取独占资源成功。
  3. 如果锁次数不为0，当前独占线程也不为当前线程，则返回false，代表获取锁失败，需要等待独占线程释放锁。
- **tryRelease**：实现钩子方法，**作为公共的锁释放方法**，尝试释放独占资源。
  1. 计算剩余锁次数 = 当前锁次数 - 需要释放的锁次数。
  2. 如果当前独占的线程不为当前线程，则抛出IllegalMonitorStateException监视器异常。
  3. 如果当前独占的线程确实为当前线程，如果剩余锁次数为0，则**清空当前独占的线程**、更新锁次数，并返回true，代表锁已经被释放了。
  4. 如果剩余锁次数不为0，则只更新锁次数，然后返回false，代表锁仍然被持有。

###### NonfairSync

- **lock**：实现Lock#lock方法。
  1. CAS更新锁次数为1，只有锁次数为0，才会更新成功，更新成功后设置当前线程为独占线程，实现**非公平提前抢锁成功**的功能。
  2. 如果锁次数不为0，则会更新失败，此时调用AQS#acquire方法非公平+入队+阻塞，实现**非公平、阻塞获取锁**的功能。
- **tryAcquire**：实现钩子方法，尝试获取独占资源。
  1. 调用Sync公共的非钩子方法，用于提供公共的**非公平、非阻塞**获取独占资源的方法。

###### FairSync

- **lock**：实现Lock#lock方法，阻塞获取锁。
  1. 直接调用AQS#acquire方法公平抢锁+入队+阻塞，实现**公平、阻塞获取锁**的功能。
- **tryAcquire**：实现钩子方法，尝试获取独占资源。
  1. 先获取当前锁次数，如果锁次数为0，且AQS主等待队列中不存在任何排队线程，才会使用CAS更新锁次数为需要获取的次数，更新成功后设置当前线程为独占线程，然后返回true，代表获取独占资源成功，实现**公平按排队顺序抢锁**的功能。
  2. 如果锁次数不为0，但当前独占线程为当前线程，则累加需要获取的次数为锁次数，代表**锁重入**，然后返回true，代表获取独占资源成功。
  3. 如果锁次数不为0，当前独占线程也不为当前线程，则返回false，代表获取锁失败，需要等待独占线程释放锁。

###### 阻塞式获取锁

```java
// 阻塞方式获取锁
public void lock() {
    sync.lock();
}
```

###### 可中断式获取锁

```java
// 以阻塞、独占可中断模式获取同步器
public void lockInterruptibly() throws InterruptedException {
    sync.acquireInterruptibly(1);
}
```

###### 快速失败式获取锁

```java
// 非公平方式尝试获取锁, 如果可用则获取锁并立即返回值{@code true}; 如果锁不可用, 则立即返回值{@code false}
public boolean tryLock() {
    return sync.nonfairTryAcquire(1);
}
```

###### 定时式获取锁

```java
// 定时可中断式尝试获取锁, 获得则返回true, 否则返回false
public boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException {
    return sync.tryAcquireNanos(1, unit.toNanos(timeout));
}
```

###### 释放锁

```java
// 释放锁
public void unlock() {
    sync.release(1);
}
```

##### ReentrantReadWriteLock

- ReentrantReadWriteLock，{@link ReadWriteLock} 的实现，支持与 {@link ReentrantLock} 类似的语义。
- ReentrantReadWriteLock具有一下属性：
  - **非公平模式（默认）**: 
    - 当构造为**非公平（参数为false）**时，读写锁的进入顺序是未指定的，受重入约束。
    - 持续竞争的非公平锁，可能会无限期推迟一个或多个读或写线程（线程饥饿），但通常比公平锁具有更高的吞吐量。
  - **公平模式**：
    - 当构造为**公平（参数为true）**时，线程使用近似到达顺序策略竞争进入。只有当前持有的锁被释放时，等待时间最长的写线程才会被分配写锁。或者如果有一组读线程等待的时间比所有等待写入器线程都长，则该组将被分配读取锁。
    - 如果写锁被持有，如果有一个试图获取**公平读锁（非可重入）**的线程将被阻塞，那么它将会直到当前等待的最老的写线程获得并释放写锁之后，该线程才会获得读锁。
    - 同理，如果一个等待的写线程放弃等待，留下一个或多个读线程作为队列中最长的等待者，并且写入锁空闲，那么这些读取器将被分配读取锁。
    - 除非读锁和写锁都是空闲的（这意味着没有等待线程），否则尝试获取公平写锁（非可重入）的线程将阻塞。
    - 请注意，非阻塞 {@link ReadLock#tryLock()} 和 {@link WriteLock#tryLock()} 方法不遵守此公平设置，如果可能将会立即获取锁，而不管等待线程。
  - **可重入性**：
    - ReentrantReadWriteLock，允许读和写以 {@link ReentrantLock} 接口的样式重新获取读或写锁。 
    - 但在写线程持有的所有写锁都被释放之前，不允许非可重入读锁。
    - 此外，写线程可以获取读锁，但反之则不行。基于这点，当调用或回调在读锁下，执行读取的方法期间持有写锁时，可重入可能很有用。
    - 而如果读线程试图获取写锁，它将永远不会成功。
  - **锁降级**：
    - 根据可重入性，ReentrantReadWriteLock允许从写锁降级到读锁，通过获取写锁，然后获取读锁，然后释放写锁，此时仍然持有读锁。
    - 但是，无法从读锁升级到写锁，即**只支持锁降级，不支持锁升级**。
  - **锁获取中断**：读锁和写锁，都允许线程在获取锁期间发生中断。
  - **Condition支持**：
    - 写锁提供了一个 {@link Condition} 实现，它的行为方式与写锁相同，就像 {@link ReentrantLock#newCondition} 为 {@link ReentrantLock} 提供的{@link Condition} 实现一样。因此， **该{@link Condition} 只能与写锁一起使用**。
    - **读锁不支持 {@link Condition}**， 并且 {@code readLock().newCondition()} 抛出 {@code UnsupportedOperationException}。
  - **仪表监控**：
    - ReentrantReadWriteLock，支持确定是持有锁还是争用锁的方法。
    - 这些方法设计用于监视系统状态，而不是用于同步控制。
- ReentrantReadWriteLock，可用于在某些类型的集合的某些用途中**提高并发性**。这通常只有在**预期集合很大**、**访问的读取线程比写入线程多**、**读取时需要的开销超过同步的开销**时才有意义。

###### Sync

- **tryAcquire**：实现钩子方法，尝试获取独占资源。
  1. 先获取当前锁状态，根据锁状态获取独占线程的个数（int的低16位）。
  2. 如果锁状态不为0，且独占线程个数为0或者当前独占线程不为当前线程，则返回false，代表存在**读线程不允许获取写锁**。
  3. 如果锁状态不为0，且当前独占线程为当前线程，说明是当前线程获取到了写锁，则累加低16位累加锁次数，然后返回true，代表**写锁重入**。
  4. 否则锁状态为0，如果**获取写锁需要等待**，或者CAS更新累加后的锁状态失败，则返回false，代表**写锁获取失败**。
  5. 如果写锁不需要等待，且CAS更新累加后的锁状态成功后，则设置当前独占线程为当前线程，然后返回true，代表**写锁获取成功**。
- **tryRelease**：实现钩子方法，尝试释放独占资源。
  1. 判断当前独占线程是否为当前线程，如果不是，则抛出监视器异常IllegalMonitorStateException。
  2. 如果当前独占线程为当前线程，则获取锁次数**低16位扣减**需要的锁次数，然后根据扣减后的锁次数，获取写锁次数，如果写锁次数为0，说明**写锁被释放了**，则清空当前独占线程。
  3. 如果写锁仍未释放，或者被释放且清空了独占线程，则更新锁状态，然后返回写锁释放结果，**如果为true，代表写锁释放成功，如果为false，代表写锁仍未释放**。
- **tryAcquireShared**：实现钩子方法，尝试获取共享资源。
  1. 先获取锁状态，根据锁状态获取写锁次数，如果写锁次数不为0，且当前独占线程不为当前线程，则返回-1，代表共享资源获取失败。
  2. 如果写锁次数为0，或者当前独占线程为当前线程，说明没有写线程，或者当前线程已经持有了写锁，则根据锁状态获取读锁次数，如果**获取读锁不需要被阻塞**，则CAS更新**高16位读锁次数**，更新成功后，则再更新当前线程缓存的读锁次数（如果一级缓存命中则获取一级缓存，如果命中二级缓存则获取二级缓存），最后返回1，代表读锁获取成功。
  3. 如果**获取读锁需要等待**，或者CAS更新**高16位读锁次数**失败，则开始自旋，如果自旋先判断到独占线程存在，且不为当前线程，则返回-1，代表读锁获取失败，因为**其他线程持有了写锁**。
  4. 否则写锁没被持有，或者写锁被当前线程持有，如果确实在**获取读锁需要等待**（需要在缓存读锁计数失效时返回false），如果缓存中的读锁次数为0（如果一级缓存命中则获取一级缓存，如果命中二级缓存则获取二级缓存），说明当前线程的读锁已经释放了，不可继续重入，则返回-1，代表读锁获取失败，因为当前线程不为第一个获得读锁的线程，所以需要重新排队，从而实现**排队获取读锁**。
  5. 如果判断过了独占线程，以及缓存读锁次数后依然没问题，说明当前线程可以继续重入，则CAS更新**高16位读锁次数**，更新成功后，则再更新当前线程缓存的读锁次数（如果一级缓存命中则获取一级缓存，如果命中二级缓存则获取二级缓存），最后返回1，代表读锁获取成功；如果CAS更新失败，在继续自旋判断+重新更新。
- **tryReleaseShared**：实现钩子方法，尝试释放共享资源。
  1. 判断当前线程是否命中一级缓存，缓存读锁次数为1，说明当前线程为最后一个释放读锁的线程，则清空f一级缓存，使得该线程下次能够继续获取读锁；如果读锁次数不为1，说明当前线程还可以继续重入，则减少读锁次数。
  2. 如果当前线程没有命中二级缓存，则更新缓存的读锁次数-1（如果一级缓存命中则获取一级缓存，如果命中二级缓存则获取二级缓存）。
  3. 缓存的读锁次数更新完毕后，则开始自旋，**高16位减少一单位的锁状态**，然后CAS更新，如果CAS更新失败则继续自旋计算+更新。
  4. 否则，CAS更新成功，如果剩余锁状态为0，则返回true，代表读锁释放成功；如果剩余锁状态不为0，则返回false，代表读锁仍未释放成功。
- **tryWriteLock**：尝试以非公平、非阻塞方式获取独占资源（写锁），相比tryAcquire少了writerShouldBlock的调用，即在锁状态为0时，CAS更新累加后的锁状态前，**不需要校验写锁是否需要等待**。
  1. 先获取当前锁状态，根据锁状态获取独占线程的个数（int的低16位）。
  2. 如果锁状态不为0，且独占线程个数为0或者当前独占线程不为当前线程，则返回false，代表存在**读线程不允许获取写锁**。
  3. 如果锁状态不为0，且当前独占线程为当前线程，说明是当前线程获取到了写锁，则累加低16位累加锁次数，然后返回true，代表**写锁重入**。
  4. 否则锁状态为0，如果CAS更新累加后的锁状态失败，则返回false，代表**写锁获取失败**。
  5. 如果CAS更新累加后的锁状态成功后，则设置当前独占线程为当前线程，然后返回true，代表**写锁获取成功**。
- **tryReadLock**：尝试以非公平、非阻塞方式获取共享资源（读锁），相比tryAcquireShared少了对readerShouldBlock的调用，即无需在缓存读锁计数失效时返回false，这些缓存是用于**获取当前线程的读锁重入次数**，以及在控制公平获取读锁时，用于判断线程是可以重入，还是已经释放过了再获取则需要**重新排队**。
  1. 先获取锁状态，根据锁状态获取写锁次数，如果写锁次数不为0，且当前独占线程不为当前线程，则返回-1，代表共享资源获取失败。
  2. 如果写锁次数为0，或者当前独占线程为当前线程，说明没有写线程，或者当前线程已经持有了写锁，则根据锁状态获取读锁次数，如果CAS更新**高16位读锁次数**成功后，则再更新当前线程缓存的读锁次数（如果一级缓存命中则累加一级缓存，如果命中二级缓存则累加二级缓存），最后返回1，代表读锁获取成功。
  3. 如果CAS更新**高16位读锁次数**失败，则继续自旋判断+更新。

###### NonfairSync

- **writerShouldBlock**：获取写锁时是否需要等待。
  - 由于为非公平抢锁，所以返回false，代表**永远不需要等待写锁**，实现**非公平抢锁**的功能。
- **readerShouldBlock**：获取读锁时是否需要等待。
  - 如果AQS主等待队列中头结点后继为独占结点，则返回true，说明第一个线程为独占线程，此时获取读锁需要等待。
  - 如果AQS主等待队列中头结点后继为共享结点，则返回false，说明第一个线程不为独占线程，此时获取读锁不需要等待，实现**非公平抢锁**的功能。

###### FairSync

- **writerShouldBlock**：获取写锁时是否需要等待。
  - 由于为公平抢锁，所以需要判断AQS主等待队列头结点后继的线程实例是否存在，如果存在则返回true，代表需要**公平排队获取写锁**。
- **readerShouldBlock**：获取读锁时是否需要等待。
  - 由于为公平抢锁，所以需要判断AQS主等待队列头结点后继的线程实例是否存在，如果存在则返回true，代表需要**公平排队获取读锁**。

###### 阻塞式获取读锁

```java
// 共享方式获取读锁, 如果写锁未被另一个线程持有，则获取读锁并立即返回; 如果写锁被另一个线程持有, 则当前线程会阻塞
public void lock() {
    sync.acquireShared(1);
}
```

###### 可中断式获取读锁

```java
// 可中断共享方式获取读锁, 如果写锁未被另一个线程持有，则获取读锁并立即返回; 如果写锁被另一个线程持有, 则当前线程会阻塞
public void lockInterruptibly() throws InterruptedException {
    sync.acquireSharedInterruptibly(1);
}
```

###### 快速失败获取读锁

```java
// 非公平、共享方式尝试获取锁, 如果写锁未被另一个线程持有, 则获取读锁并立即返回值 {@code true}; 如果写锁被另一个线程持有，则此方法将立即返回值 {@code false}
public boolean tryLock() {
    return sync.tryReadLock();
}
```

###### 定时式获取读锁

```java
// 非公平、定时、可中断、共享方式尝试获取锁, 如果写锁未被另一个线程持有, 则获取读锁并立即返回值 {@code true}; 如果写锁被另一个线程持有，则此方法将立即返回值 {@code false}
public boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException {
    return sync.tryAcquireSharedNanos(1, unit.toNanos(timeout));
}
```

###### 释放读锁

```java
// 共享方式释放锁, 如果读锁同步器状态现在为0, 则会尝试共享方式释放读锁
public void unlock() {
    sync.releaseShared(1);
}
```

###### 阻塞式获取写锁

```java
// 独占方式获取写锁, 如果读锁和写锁都没有被另一个线程持有, 则获取写锁并立即返回, 将写锁持有计数设置为1; 如果当前线程已经持有写锁, 则持有计数加一并且该方法立即返回(重入性); 如果该锁由另一个线程持有，则当前线程将阻塞, 直到获得写锁为止, 然后写锁持有计数设置为1
public void lock() {
    sync.acquire(1);
}
```

###### 可中断式获取写锁

```java
// 独占、可中断方式获取写锁, 如果读锁和写锁都没有被另一个线程持有, 则获取写锁并立即返回, 将写锁持有计数设置为1; 如果当前线程已经持有写锁, 则持有计数加一并且该方法立即返回(重入性); 如果该锁由另一个线程持有，则当前线程将阻塞, 直到获得写锁为止, 然后写锁持有计数设置为1
public void lockInterruptibly() throws InterruptedException {
    sync.acquireInterruptibly(1);
}
```

###### 快速失败式获取写锁

```java
// 非公平、独占方式获取写锁, 如果读锁和写锁都没有被另一个线程持有, 则获取写锁并立即返回, 将写锁持有计数设置为1; 如果当前线程已经持有写锁, 则持有计数加一并且该方法立即返回(重入性); 如果该锁由另一个线程持有，则当前线程将阻塞, 直到获得写锁为止, 然后写锁持有计数设置为1
public boolean tryLock( ) {
    return sync.tryWriteLock();
}
```

###### 定时式获取写锁

```java
// 非公平、定时、可中断、独占方式获取写锁, 如果读锁和写锁都没有被另一个线程持有, 则获取写锁并立即返回, 将写锁持有计数设置为1; 如果当前线程已经持有写锁, 则持有计数加一并且该方法立即返回(重入性); 如果该锁由另一个线程持有，则当前线程将阻塞, 直到获得写锁为止, 然后写锁持有计数设置为1
public boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException {
    return sync.tryAcquireNanos(1, unit.toNanos(timeout));
}
```

###### 释放写锁

```java
// 独占方式释放锁, 如果当前线程是此锁的持有者, 则持有计数递减; 如果保持计数现在为零, 则锁定被释放; 如果当前线程不是此锁的持有者, 则抛出 {@link IllegalMonitorStateException}
public void unlock() {
    sync.release(1);
}
```

##### Semaphore

- Semaphore，计数信号量，从概念上讲，**信号量维护一组许可**。
  - 每个{@link#acquire}会直到许可证是可用的，然后获取。
  - 每个{@link#RELEASE}添加一个许可，释放阻塞的获取线程。
  - 实际上{@code Semaphore}有维护许可对象，它只是让计数可用，从而采取相应的行动。
- 信号量通常用于限制可以访问某些（物理或逻辑）资源的线程数。 
- Semaphore，可以初始化为 1 的信号量，并且使用时最多只有一个许可可用，用作互斥锁。 这通常被称为**二元信号量**，因为它只有两种状态：一个许可可用，或零个许可可用。以这种方式使用时，二进制信号量具有属性（与许多 {@link java.util.concurrent.locks.Lock} 实现不同），即“锁”可以由所有者以外的线程释放（因为**信号量具有没有所有权的概念**），这在某些特定上下文中很有用，例如**死锁恢复**。
- Semaphore的构造函数可以选择接受公平参数：
  - 当公平性设置为 false 时，为非公平模式，则不保证线程获取许可的顺序。特别是，允许插入，即调用 {@link #acquire} 的线程可以在一直等待的线程之前，分配一个许可（逻辑上，新线程将自己置于等待线程队列的头部）。
  - 当公平性设置为true时，为公平模式，信号量保证调用任何 {@link #acquire()acquire} 方法的线程被选择以按照它们对这些方法的调用的处理顺序（先进先出）获得许可。
  - 请注意，未计时的 {@link #tryAcquire() tryAcquire} 方法**不遵守公平设置**，但会采用任何可用的许可。
  - 通常，**用于控制资源访问的信号量应初始化为公平的**，以确保没有线程因访问资源而饿死。 当使用信号量进行其他类型的同步控制时，**非公平排序的吞吐量大于公平性排序的吞吐量**。

###### Sync

- **tryReleaseShared**：实现钩子方法，自旋尝试释放共享资源，直到释放到许可则返回true。这里的共享释放，做的是**许可的累加，而非传播**。
  1. 开始自旋，获取最新的许可数量，计算累加需要释放的许可数量，如果累加后的结果反而还小了，则抛出错误，因为**不能释放负数的许可**。
  2. 否则CAS更新许数量为累加后的结果，如果CAS成功，则返回true，代表获取许可成功，否则继续自旋。
- **nonfairTryAcquireShared**：非钩子方法，非公平自旋方式获取共享资源，直到获取到才返回剩余许可。
  1. 开始自旋，获取最新的许可数量，扣减需要获取的许可数量，如果扣减后的剩余许可数量小于0，则返回负数，代表资源获取失败。
  2. 如果剩余许可数量大于等于0，则CAS更新许可数量，更新成功则**返回剩余许可数量**，否则继续自旋。

###### NonfairSync

- **tryAcquireShared**：实现钩子方法，底层调用Sync#nonfairTryAcquireShared方法，会非公平自旋方式获取共享资源。

###### FairSync

- **tryAcquireShared**：实现钩子方法，对比Sync#nonfairTryAcquireShared方法，扣减许可前会先判断AQS主队列头结点后继线程是否已经存在，如果是则返回-1，代表获取共享资源失败，需要去排队。

###### 可中断阻塞式获取许可

```java
// 以阻塞、共享可中断模式获取同步器状态
public void acquire() throws InterruptedException {
    sync.acquireSharedInterruptibly(1);
}
```

###### 不可中断阻塞式获取许可

```java
// 以阻塞、不可中断、共享模式获取同步器状态
public void acquireUninterruptibly() {
    sync.acquireShared(1);
}
```

###### 快速失败式获取许可

```java
// 非公平快速失败方式获取同步器状态
public boolean tryAcquire() {
    return sync.nonfairTryAcquireShared(1) >= 0;
}
```

###### 定时式获取许可

```java
// 以阻塞、共享、定时、可中断模式获取同步器状态
public boolean tryAcquire(long timeout, TimeUnit unit) throws InterruptedException {
    return sync.tryAcquireSharedNanos(1, unit.toNanos(timeout));
}
```

###### 释放许可

```java
// 尝试释放共享模式的同步器状态, 如果释放成功, 则返回true; 如果释放失败, 则返回false
public void release() {
    sync.releaseShared(1);
}
```

##### ThreadPoolExecutor

###### Worker

线程工人类，实现Runnable本身可以作为一个任务，实现AQS以简化获取和释放围绕每个任务执行的锁，实现了一个简单的**不可重入的、互斥的任务锁**。

- **tryAcquire**：实现钩子方法，尝试获取独占资源，如果更新锁计数为1，则设置当前线程为独占线程，然后返回true，代表获取独占锁成功；否则返回false，代表获取独占锁失败。
- **tryRelease**：实现钩子方法，尝试释放独占资源，清空独占线程，并重置锁计数为0，永远返回true，代表独占锁释放成功。

###### 阻塞式获取锁

```java
// 阻塞式获取锁, 快速失败失败还会进去AQS队列中排队
public void lock()        { acquire(1); }
```

###### 快速失败式获取锁

```java
// 快速失败式获取锁, 只会将0 CAS更新为1(unused没用), CAS成功则设置当前线程为独占线程, 并返回true, 代表获取成功, 否则返回false, 代表获取失败
public boolean tryLock()  { return tryAcquire(1); }
```

###### 释放锁

```java
// 释放锁, 只会将同步状态设置为0, 并清空独占线程以及返回true, 代表释放成功
public void unlock()      { release(1); }
```

##### CyclicBarrier

- CyclicBarrier，一种同步辅助工具，它允许一组线程全部等待彼此到达公共屏障点，在涉及固定大小的线程组的程序中很有用。
- CyclicBarrier屏障之所以被称为Cyclic循环的，因为它可以在等待线程被释放后**重新使用**。
- {@code CyclicBarrier} 支持一个可选的 {@link Runnable} 命令，该命令在每个屏障点运行一次，在参与者中的最后一个线程到达之前，其他参与者线程都会被阻塞。此屏障操作对于在任何一方继续之前更新共享状态很有用，为方便起见，每次调用 {@link #await}（被唤醒后） 都会返回该线程在屏障处的**到达索引**。
- {@code CyclicBarrier}的每次使用都表示为一个生成Generation实例，每当障碍物被触发或重置时，Generation都会发生变化，因此可以有许多Generation与使用{@code CyclicBarrier}的线程相关联。
  - 由于锁以不确定的方式分配给等待线程，但一次只能激活Generation其中之一，因此其余的Generation要么坏了要么被绊倒了。
  - 另外，如果有中断但没有后续复位，则该Generation将损坏。

###### await

1. 先阻塞式获取栅栏入口的ReentrantLock，获取到后则获取分代实例Generation，如果发现Generation已经损坏，则抛出BrokenBarrierException异常。
2. 如果Generation还没损坏，则再检验当前线程的中断状态，如果已中断，则破坏当前Generation，重置栅栏参与线程数，并唤醒所有等待的线程。
3. 如果Generation还没损坏，且当前线程没被中断，说明当前线程达到了栅栏，因此参与数-1。
4. 如果减少到了0，说明当前线程为最后一个到达栅栏的线程，则还需要运行栅栏任务（如果有），生成下一代Generation，重置栅栏参与线程数，并唤醒所有等待的线程，并返回0，代表最后一个到达栅栏的索引。
5. 如果还没减少到0，则开始自旋，如果不需要超时，则调用Condiction#await阻塞当前线程，如果还没发生超时，则调用Condiction#awaitNanos定时阻塞当前线程。
6. 当最后一个线程达到栅栏，或者等待期间任意一个线程发生了中断，会破坏当前Generation，唤醒所有线程，如果唤醒后判断到当前Generation已损坏，则抛出BrokenBarrierException异常。
7. 如果Generation没损坏，但已被更新，说明最后一个线程确实到达了栅栏，则返回当前线程达到索引（越接近0，说明越完到达）。
8. 如果Generation没损坏，但没被更新，说明当前线程发生了等待超时，则破坏当前Generation，唤醒所有线程，并抛出超时异常TimeoutException。
9. 最后释放栅栏入口的ReentrantLock。

### 4.7. synchronized与ReentrantLock与的区别？

| 区别点   | Synchronized                   | ReentrantLock                    |
| -------- | ------------------------------ | -------------------------------- |
| 使用方式 | 是一个关键字                   | 是一个实现类                     |
| 实现方式 | 由JVM实现控制                  | 由AQS实现控制                    |
| 锁的获取 | 如果资源被锁，会一直等待       | 如果资源被锁，可以有多种处理方式 |
| 锁的释放 | 被锁的代码执行完，或者发生异常 | 需要在finally中，手动编程释放    |
| 锁的状态 | 无法判断                       | 可以判断，isLocked（）           |
| 锁的特性 | 可重入、不可中断、非公平锁     | 可重入、可中断、公平锁、非公平锁 |

```java
【ReentrantLock可中断锁】
ReentrantLock中的lockInterruptibly()方法使得线程可以在被阻塞时响应中断，比如一个线程t1通过lockInterruptibly()方法获取到一个可重入锁，并执行一个长时间的任务，另一个线程通过interrupt()方法就可以立刻打断t1线程的执行，来获取t1持有的那个可重入锁。
而通过ReentrantLock的lock()方法或者Synchronized持有锁的线程是不会响应其他线程的interrupt()方法的，直到该方法主动释放锁之后才会响应interrupt()方法。
```

### 4.8. 内存泄露与内存溢出？

#### 内存泄露

内存泄漏，memory leak，指不再用到的内存没有及时释放。

- 对于持续运行的服务进程必须及时释放内存，否则系统可用的内存会越来越小，内存占用率越来越高，轻则影响系统性能，重则导致内存不足、进程崩溃甚至操作系统崩溃。

#### 内存溢出

内存溢出，out of memory，指应用所需要的内存超过了系统所能分配的内存，从而导致应用没法正常工作，直接后果是导致应用程序崩溃，严重了还会导致系统安全问题。

- 内存溢出无法根本解决，只能尽可能避免，比如说在应用程序申请内存之前对可用内存进行检查、增大分配给应用的内存、优化程序代码、减少应用中的大数据复杂操作等。

### 4.9. 详细介绍ThreadLocal？

#### 背景

- 为了保证多个线程对变量的安全访问，将变量放到某个对象中，使变量在每个线程中都有独立值，不会出现一个线程读取变量时，被另一个线程修改的现象，JDK设计了ThreadLocal类，通常被翻译为“**线程本地变量**”类或者“**线程局部变量**”类。
  1. 如果程序创建了一个ThreadLocal实例，那么在访问这个变量的值时，每个线程都会拥有一个独立的、自己的本地值。
  2. “线程本地变量”可以看成专属于线程的变量，不受其他线程干扰，保存着线程的专属数据。
  3. 当线程结束后，每个线程所拥有的那个本地值会被释放。在多线程并发操作“线程本地变量”的时候，线程各自操作的是自己的本地值，从而规避了线程安全问题。

#### 特点

- ThreadLocal，**提供线程局部变量**，依赖于附加到每个线程的Map中，即Thread.threadLocals 和inheritableThreadLocals，使得每个访问的线程都有**自己的、独立的变量副本**。
  - 在该Map中**ThreadLocal 对象充当键**，通过threadLocalHashCode进行搜索，这是一个自定义哈希代码（仅在 ThreadLocalMaps 中有用），它消除了在相同线程使用连续构造的 ThreadLocals 的常见情况下的冲突，同时在不太常见的情况下保持良好行为。
  - 只要线程处于活动状态，并且 {@code ThreadLocal} 实例可访问，每个线程都持有对其线程局部变量副本的**隐式引用**；线程消失后，它的所有线程本地实例副本都将进行**垃圾回收**（除非存在对这些副本的其他引用）。
- ThreadLocal是**解决线程安全问题**的一个较好的方案，通过为每个线程提供一个独立的本地值去解决并发访问的冲突问题。在很多情况下，使用ThreadLocal比直接使用同步机制（如synchronized）解决线程安全问题更简单、更方便，且结果程序**拥有更高的并发性**。其使用场景大致可以分为以下两类：
  - **线程隔离**：
    - ThreadLocal的主要价值在于线程隔离，ThreadLocal中的数据只属于当前线程，其本地值对别的线程是不可见的，在多线程环境下，可以防止自己的变量被其他线程篡改。
    - 另外，由于各个线程之间的数据相互隔离，**避免了同步加锁**带来的性能损失，大大提升了并发性的性能。
    - 常见的案例有：数据库连接独享、Session数据管理等，由于每个线程绑定一个数据库连接，使得这个数据库连接为线程所独享，从而避免数据库连接被混用而导致操作异常问题。
  - **跨函数传递数据**：
    - 因为ThreadLocal的特性，同一线程在某些地方进行设置，在随后的任意地方都可以获取到，线程执行过程中所执行到的函数，都能读写ThreadLocal变量的线程本地值，从而可以方便地实现跨函数的数据传递。
    - 使用ThreadLocal保存函数之间需要传递的数据，在需要的地方直接获取，也能**避免通过参数传递数据带来的高耦合**。
    - 常见的案例有：用来传递请求过程中的用户ID、请求过程中的Session、HTTP的用户请求实例HttpRequest，以及其他在函数之间频繁传递的数据等。

#### 实现原理

##### 早期实现

- 早期版本的ThreadLocal是这样设计的，一个ThreadLocal实例可以形象地理解为一个Map，当工作线程Thread实例向本地变量保持某个值时，会以“Key-Value对"的形式保存在ThreadLocal内部的Map中。
- 其中Key为线程**Thread实例**，Value为待保存的值，当工作线程Thread实例从ThreadLocal本地变量取值时，会以Thread实例为Key，获取其绑定的Value。

![1630407029053](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630407029053.png)

##### 优化后的实现

- 经过后来的优化后（下面的源码都以优化后的数据结构为准），ThreadLocal的内部结构发生了演进，虽然还是使用了Map结构，但是Map结构的拥有者已经发生了变化，其拥有者为Thread实例，**每一个Thread实例拥有一个Map实例**，且Key值也发生了变化，由之前的Thread实例更改为了**ThreadLocal实例**。
- 每一个Thread线程内部都有一个ThreadLocalMap，如果给一个Thread创建多个ThreadLocal实例，然后放置本地数据，那么当前线程的ThreadLocalMap中就会有多个“Key-Value对”，其中ThreadLocal实例为Key，本地数据为Value。
- 每一个线程在获取本地值时，都会将ThreadLocal实例作为Key从自己拥有的ThreadLocalMap中获取值，别的线程无法访问自己的ThreadLocalMap实例，自己也无法访问别人的ThreadLocalMap实例，达到相**互隔离，互不干扰**。

![1630411048268](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630411048268.png)

##### 变化点

| 变化点                         | 早期实现            | 优化后的实现    |
| ------------------------------ | ------------------- | --------------- |
| Key发生了变化                  | Thread实例          | ThreadLocal实例 |
| ThreadLocalMap拥有者发生了变化 | 拥有者为ThreadLocal | 拥有者为Thread  |

##### 优化的好处

- **每个ThreadLocalMap存储的“Key-Value对”数量变少**，早期版本的“Key-Value对”数量与线程个数强关联，如果线程数量多，则ThreadLocalMap存储的“Key-Value对”数量也多，而新版本的ThreadLocalMap的Key为ThreadLocal实例，多线程情况下ThreadLocal实例比线程数少。
- 早期版本ThreadLocalMap的拥有者为ThreadLocal，在Thread实例销毁后，ThreadLocalMap还是存在的；而新版本的ThreadLocalMap的拥有者为Thread，在当Thread实例销毁后，ThreadLocalMap也会随之销毁，**在一定程度上能减少内存的消耗**。

#### 线程本地变量

```java
public class Thread implements Runnable {
    ThreadLocal.ThreadLocalMap threadLocals = null;
    ThreadLocal.ThreadLocalMap inheritableThreadLocals = null;
    ...
}

// ThreadLocal.ThreadLocalMap
public class ThreadLocal<T> {
    static class ThreadLocalMap {
        private static final int INITIAL_CAPACITY = 16;// 初始容量，必须是2的幂
        private Entry[] table;// 散列表，table.length必须始终是2的幂
        private int size = 0;// 散列表实际大小
        private int threshold;// 散列表阈值

        // 条目对象
        static class Entry extends WeakReference<ThreadLocal<?>> {
            Object value;

            // Entry#Key 强引用=> WeakReference 弱引用-> ThreadLocal:v
            Entry(ThreadLocal<?> k, Object v) {
                super(k);
                value = v;
            }
        }
    }
}
```

##### 设置原理

**Thread#set**：设置当前线程的本地变量原理。

1. 先获取当前线程实例，然后调用getMap方法，获取该实例的成员变量**Thread#threadLocals**，即ThreadLocalMap实例。
2. 如果获取到的ThreadLocalMap实例为空，说明本地变量还没初始化，则调用setInitialValue方法设置初始值，先获取初始值, 如果ThreadLocal.ThreadLocalMap threadLocals已被初始化, 则为其赋初值, 否则**惰性构造**默认容量16、默认负载因子16 * 2/3、当前ThreadLocal实例作为第一个Entry#Key的ThreadLocalMap。
3. 如果ThreadLocalMap实例已被初始化，则根据**ThreadLocal中的散列码**来计算散列表索引，来获取ThreadLocalMap中的Entry，如果Entry不为空，且**弱键为当前ThreadLocal实例**，说明该Entry就是要找的Entry，则直接返回即可。
4. 如果获取到的Entry弱键为空，说明存在弱键已失效，则清空该槽设置一个弱键包装后的key-value条目，然后向后清空所有弱键为空的槽。
5. 如果获取到的Entry为null，则新增一个key-value条目，更新实际大小，向后清除清空所有弱键为空的槽。
6. 接着判断**是否需要扩容**，如果当前实际大小大于等于阈值（len *  1/2），说明需要扩容，则调用resize方法进行扩容，否则直接返回即可。
   - **ThreadLocalMap#resize**：ThreadLocalMap扩容方法，创建两倍容量的新散列表 -> 遍历旧表，重新计算该元素在新表上的索引，转移结点到新表 -> 根据新表容量设置阈值（len * 2/3）-> 更新新表、更新实际大小 -> 最后返回。

##### 删除原理

**Thread#remove**：删除当前线程的本地变量原理。

1. 先获取当前线程实例，然后调用getMap方法，获取该实例的成员变量**Thread#threadLocals**，即ThreadLocalMap实例。
2. 如果ThreadLocalMap实例已被初始化，则根据**ThreadLocal中的散列码**来计算散列表索引，来获取ThreadLocalMap中的Entry，如果Entry不为空，且**弱键为当前ThreadLocal实例**，说明该Entry就是要找的Entry，则清空WeakReference.referent（即清空弱键，会导致e.get()返回null），然后清空该槽，并且向后清空所有弱键为空的槽，最后返回。

##### 获取原理

**Thread#get**：获取当前线程的本地变量原理。

1. 先获取当前线程实例，然后调用getMap方法，获取该实例的成员变量**Thread#threadLocals**，即ThreadLocalMap实例。
2. 如果获取到的ThreadLocalMap实例为空，说明本地变量还没初始化，则调用setInitialValue方法设置初始值，先获取初始值, 如果ThreadLocal.ThreadLocalMap threadLocals已被初始化, 则为其赋初值, 否则**惰性构造**默认容量16、默认负载因子16 * 2/3、当前ThreadLocal实例作为第一个Entry#Key的ThreadLocalMap。
3. 如果ThreadLocalMap实例已被初始化，则根据**ThreadLocal中的散列码**来计算散列表索引，来获取ThreadLocalMap中的Entry，如果Entry不为空，且**弱键为当前ThreadLocal实例**，说明该Entry就是要找的Entry，则直接返回即可。
4. 如果根据计算出来的索引获取不到Entry，则调用getEntryAfterMiss查找：如果弱键为空，说明存在弱键已失效，则清空该槽，并且向后清空所有弱键为空的槽；否则向后线性探测下一个Entry，继续遍历；如果**线程探测+清空缓存**后，仍然找不到key匹配的Entry，则返回null。

#### 内存泄露

##### ThreadLocal实例内存泄露

- **场景举例**：线程tn调用funcA()方法新建了一个ThreadLocal实例，使用local局部变量指向这个实例，接着set方法设置100后，调用get方法获取一次。

  ```java
  public void funcA() {
      ThreadLocal<Integer> local = new ThreadLocal<>();
      local.set(100);
      local.get();
  }
  ```

- **弱键的好处**：

  1. local是强引用，在set方法设置后，线程tn的ThreadLocalMap成员内部会新建一个Entry实例，其Key以**弱引用包装**的方式指向ThreadLocal实例。
  2. 当线程tn执行完funcA()方法后，funcA()的方法栈帧将被销毁，栈帧中的强引用local会被回收，而由于线程tn仍在继续，导致Thread#ThreadLocalMap中对应的Entry.Key引用还指向ThreadLocal实例。
  3. **如果Entry的Key引用是强引用**，则会导致Key引用指向的ThreadLocal实例及其Value值，都不能被GC回收，这将造成严重的**内存泄漏**问题。
  4. **如果Entry的Key引用是弱引用**，由于ThreadLocalMap中Entry的Key使用了弱引用，在下次GC发生时，就可以使那些没有被其他强引用指向、仅被Entry的Key所指向的**ThreadLocal实例能被顺利回收**，在Entry的Key引用被回收之后，其Entry的Key值变为null，后续当ThreadLocal的get()、set()或remove()被调用时，ThreadLocalMap的内部代码会**清除这些Key为null的Entry**，从而完成相应的内存释放，**避免ThreadLocal内存泄露**。

  ![1630466341260](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630466341260.png)

##### Entry.value内存泄露

ThreadLocal虽然使用了弱键，但仍导致内存泄漏，发生条件如下（同时满足）：

1. **线程长时间运行而没有被销毁**。

   - 在线程池中的Thread实例很容易满足此条件。

2. ThreadLocal引用被设置为null，且后续在同一Thread实例执行期间，没有发生对其他ThreadLocal实例的**get()、set()或remove()**操作。

   - 如果后续没有调用过任何get()、set()或remove()操作，弱键的ThreadLocal实例被回收后，key为null，但Entry没有被回收，出现了Entry.value的内存泄露。
   - 只要存在一个针对任何ThreadLocal实例的get()、set()或remove()操作，就会触发Thread实例拥有的ThreadLocalMap的Key为null的**Entry清理工作**，释放掉ThreadLocal弱引用为null的Entry，避免**Entry.value的内存泄露**。

   ![1630483124536](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630483124536.png)

##### static & final内存泄露

- **使用static、final的好处**：
  - ThreadLocal实例作为ThreadLocalMap的Key，针对一个线程内的所有操作是共享的，所以建议设置static修饰符，以便被所有的对象共享。由于静态变量会在类第一次被使用时装载，只会分配一次存储空间，此类的所有实例都会共享这个存储空间，所以**使用static修饰ThreadLocal就会节约内存空间**。
  - 为了**确保ThreadLocal实例的唯一性**，除了使用static修饰之外，还会使用final进行加强修饰，以防止其在使用过程中发生动态变更。
  - 此外，ThreadLocal实例常常添加private修饰呢，主要目的是**缩小使用的范围**，尽可能不让他人引用。
- **static、final导致内存泄露的原因**：
  - 使用static、final修饰ThreadLoacl实例，ThreadLoacl实例会以单例形式存在，导致指向该ThreadLoacl实例的ThreadLocalMap#Entry.Key在Thread实例的生命期内始终保持为非null，从而导致Key所在的Entry不会被自动清空，这就会让Entry中的Value指向的对象一直被Entry强引用，于是Value指向的对象在线程生命期内不会被释放，导致发生内存泄漏。
  - 因此，在使用完static、final修饰的ThreadLocal实例之后，必须**调用remove()**来进行显式的释放操作。

##### 避免内存泄露总结

可见，使用ThreadLocal容易发生内存泄漏，如果能保证使用完ThreadLocal**及时调用remove方法**，则可以简单、有效地避免内存泄漏的发生。

