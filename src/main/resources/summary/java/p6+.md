# 六、消息队列篇

### 2.0. 什么是系统可用性？

- **概念**：系统可用性，Availability，是信息工业界用来衡量一个信息系统提供**持续服务**的能力，表示在给定时间区间内，系统或者系统某一能力，在特定环境下能够正常工作的**概率**。
- **公式**：系统可用性 = （平均故障间隔时间 MTBF）/ （平均故障间隔时间 MTBF  + 平均故障修复时间 MTTR）
- **应用**：通常，业界习惯用 N 个 9 来表征系统可用性，表示系统可以**正常使用时间与 1 年总时间之比**，比如：
  1. **99.9%**：代表 3 个 9 的可用性，意味着全年不可用时间在 `8.76h` 以内，表示该系统在连续运行 1 年时间里，最多可能的业务中断时间是 `8.76h` 。
  2. **99.99%**：代表 4 个 9 的可用性，意味着全年不可用时间在 `52.6min` 以内，表示该系统在连续运行 1 年时间里，最多可能的业务中断时间是 `52.6min` 。
  3. **99.999%**：代表 5 个 9 的可用性，意味着全年不可用时间在 `5.26min` 以内，缺少故障**自动恢复机制**的系统是很难达到 5 个 9 高可用性的。
  4. 而对于 1、2、6 个 9，它们的不可用时长分别为 36.5D、3.65D、31s，均不符合实际情况（时间太久或者实现成本太高），所以用于表征系统可用性并不合适。

### 2.1. 详细介绍RocketMQ？

#### 概念

RocketMQ 是一款分布式、队列模型的消息中间件，由阿里巴巴自主研发的一款适用于高并发、高可靠性、海量数据场景的消息中间件，其消息路由、存储、集群规划上都参考借鉴了 Kafka 的设计思路，并结合双十一场景进行合理的扩展和丰富了API。

- **通用用途**：异步处理、系统解耦、削峰填谷、蓄流压测。
- **特色用途**：
  1. **支持事务消息**：消息发送和 DB 操作保持两方的最终一致性，RabbitMQ 和 Kafka 都不支持。
  2. **支持延迟消息**：RabbitMQ 支持，Kafka 不支持。
  3. **支持 Consumer#tag 过滤**：以减少不必要的网络传输，RabbitMQ 和 Kafka 都不支持。
  4. **支持重复消费**：RabbitMQ 不支持，Kafka 支持。

| 专业术语       | 释义                                                         |
| -------------- | ------------------------------------------------------------ |
| Broker         | 消息核心服务，作为中转角色，用于消息存储与生产消费的转发，通过 Name Server 暴露统一的集群入口给客户端 |
| Name Server    | Name Server 充当路由信息提供者，供生产者与消费者客户端查找 topic，以找到相应的 Broker 列表 |
| Producer       | 消息生产者，负责生产消息，一般由业务系统负责产生消息，完全无状态，可集群部署 |
| Producer Group | 生产者集合，一般用于发送一类消息，防止原始生产者在事务后崩溃 |
| Consumer       | 消息消费者，负责消费消息，一般是后台系统负责异步消费         |
| Consumer Group | 消费者集合，一般用于接受一类消息进行消费，实现消费的负载平衡与容错，消费者组中的消费者必须具有完全相同的 topic 订阅 |
| Push Consumer  | Consumer 的一种，需要向 Broker 注册监听，被动接受推送过来的消息 |
| Pull Consumer  | Consumer 的一种，需要主动请求 Broker 拉取消息                |
| Topic          | 主题，是生产者传递消息和消费者拉取消息的类别，一个主题可能有零个、一个或多个生产者向它发送消息，生产者也可以发送不同主题的消息；一个主题可能被零个、一个或多个消费者组订阅，消费者组也可以订阅一个或多个主题，只要该组的实例保持订阅一致即可 |
| Message        | 消息，是要传递的信息，一条消息必须有一个主题，一条消息也可能有一个可选的标签和额外的键值对 |
| Message Queue  | 消息队列，是逻辑上存储消息的队列，物理上存储消息的是 CommitLog，其中 topic 被划分为一个或多个 tag，每个 tag 对应着一个消息队列 |
| Tag            | 标签，也可以说是子主题，为用户提供了额外的灵活性，使用标签有助于保持代码的整洁和连贯，也方便消息的查询 |

#### 原理

##### 技术架构原理

![1635853277989](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635853277989.png)

###### NameServer 集群

1. 轻量级的配置中心，是一个几乎无状态的节点，只做集群元数据存储和心跳工作。
2. 可集群部署，节点之间没有任何信息同步，不需要保证节点间数据的强一致性。

###### Broker

1. 消息核心服务，作为中转角色，用于消息存储与生产消费的转发，通过 NameServer 暴露统一的集群入口给客户端。
2. Broker 分为 Master 和 Slave，一个Master 可以对应多个 Slave，一个 Slave 只对应一个 Master，可通过指定相同的 BrokerName + 不同的BrokerId 来定义主从关系，即 BrokerId 为 0 的表示 Master 节点，非 0 的表示 Slave 节点。而 Master 也可以多节点部署，从而构成多主多从的集群架构。
3. 每个 Broker 与 Name Server **所有节点**建立长连接，隔 30s 定期地注册 Topic 信息到所有Name Server 节点，Name Server 也会每隔 10s 定时地扫描所有存活的 Broker 连接，如果 Name Server 超过 2min 没有收到 Broker 心跳，则会与 Broker 断开连接。

###### Producer 集群

- Producer，消息生产者，负责生产消息，一般由业务系统负责产生消息，完全无状态，可集群部署。
- Producer 会在 Name Server 集群中**随机选择一个节点**建立长连接，并与提供 Topic 服务的 Master Broker 节点建立长连接。
  1. **每隔 30s 定期地从 Name Server 获取所有 Topic 的最新情况**：可由 `ClientConfig#pollNameServerInterval` 指定。
     - 意味着如果 Broker 不可用，那么 Producer 最多只需要 30s 就能感知到，然后负载均衡到其他 Broker 上，但在此期间内，发往该 Broker 的所有消息都会失败。
  2. **还会每隔 30s 向所有关联的 Broker发送心跳**：可由 `ClientConfig#heartbeatBrokerInterval` 指定。
     - Broker 每隔 10s 会扫描所有存活的连接，如果 Broker 在 2min 内没有收到心跳数据，则会关闭与该 Producer 的连接。

###### Consumer 集群

- 消息消费者，负责消费消息，一般是后台系统负责异步消费，会被标识为 `{IP}@{consumer group}{topic}{tag}`，比如 x.x.x.x@mqtest_producer-group_2m2sTest_tag-zyk，其中任何一个元素不同，都会被认为是不同的 Consumer。
- Consumer 会在 Name Server 集群中**随机选择一个节点**建立长连接，并与提供 Topic 服务的 Master、Slave Broker 节点建立长连接。
  1. **每隔 30s 从 Name server 获取所有 Topic 的最新情况**：
     - 这意味着如果 Broker 不可用，那么 Consumer 最多只需要 30s 就能感知到，然后负载均衡到其他 Broker 上，但在此期间内，消费该 Broker 的所有消息都会失败。
  2. **还会每隔 30s 向所有关联的 Broker发送心跳**：可由 `ClientConfig#heartbeatBrokerInterval` 指定。
     - Broker 每隔 10s 会扫描所有存活的连接，如果 Broker 在 2min 内没有收到心跳数据，则会关闭与该 Consumer 的连接，并向该 Consumer Group 的所有 Consumer 发出通知，让 Group 内的 Consumer 重新分配队列，然后继续消费。
- Consumer 既可以从 Master 订阅消息，也可以从 Slave 订阅消息，订阅规则由 Broker 配置决定。
  - 当 Consumer 收到 Master 宕机通知后，会转向 Slave 进行消费，由于 Slave 不能保证 Master 消息100% 的同步，因此会有少量的消息丢失，但是一旦 Master 恢复，未同步过去的消息最终还是会被消费掉。

##### 消息存储原理

RocketMQ 消息存储是由 Comsume Queue + cimmitlog 配合完成的。

###### CommitLog

1. 在 RocketMQ 中，所有 Topic 消息都存储在一个称为 CommitLog 的文件中，该文件默认最大为 1GB，超过 1GB 后消息就会写到下一个 CommitLog 文件。
2. 通过 CommitLog，RocketMQ 把所有消息存储在一起，以顺序 I/O 的方式写入磁盘，充分利用了磁盘
   顺序写，减少了 I/O 争用，提高了数据存储的性能。
3. 消息在 CommitLog 中的存储格式如下：

![1635674975794](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635674975794.png)

| 属性                       | 长度  | 作用                                                         |
| -------------------------- | ----- | ------------------------------------------------------------ |
| msgLen                     | 4字节 | 消息的长度，为整个消息体所占用的字节数大小                   |
| magicCode                  | 4字节 | 魔数，固定值，分为 MESSAGE_MAGIC_CODE 和 BLANK_MAGIC_CODE    |
| bodyCRC                    | 4字节 | 消息体校验码，用于防止网络、硬件等故障导致数据与发送时不一样带来的问题 |
| queueId                    | 4字节 | 消息 ID，表示消息发到了哪个 MessageQueue，相当于 Kafka#partition |
| flag                       | 4字节 | 消息 flag，是创建 Message 时由 Producer 通过构造器设置的 flag 值 |
| queueOffset                | 8字节 | 消息在 queue 中的偏移量                                      |
| physicalPosition           | 8字节 | 消息在存储文件中的偏移量                                     |
| sysFlag                    | 4字节 | 生产者相关的信息标识                                         |
| msg born timestamp         | 8字节 | 消息创建时间                                                 |
| msg host                   | 8字节 | 消息 Producer 主机地址                                       |
| store timestamp            | 8字节 | 消息存储时间                                                 |
| store host                 | 8字节 | 消息存储机器的主机地址                                       |
| reconsume times            | 4字节 | 消息被重复消费的次数                                         |
| prepare transaction offset | 8字节 | 消息事务相关的偏移量                                         |
| body length                | 4字节 | 消息体长度                                                   |
| msg body                   | N字节 | 消息体，非固定长度，实际长度等于前面标示的4字节消息体长度    |
| topic length               | 1字节 | topic 长度，<= 127 字节，由于有前置校验，超过存储则会报错    |
| topic                      | N字节 | 存储 topic，非固定长度，实际长度等于前面标示的1字节 topic 长度 |
| properties length          | 2字节 | properties 长度，添加在消息中的 poperties 不能太多太大，要保证所有的properties 的 KV 对在拼接成 string 后，占用的字节数 <= 2^15-1 |
| properties                 | N字节 | properties 内容，非固定长度，实际长度等于前面标示的2字节 properties 长度 |

###### Consume Queue

1. 消息逻辑队列，相当于字典的目录，类似于二级索引，用来指定消息在物理文件 commit log 上的位置。
2. 在 Consumer 第一次连接时创建，每个 Consumer 都会拥有一份自己的 Consume Queue，新挂载的 Consume Queue 会拥有 CommitLog 中所有的数据。
3. 一个 Consume Queue 表示 topic#queue，类似于 Kafka#partition，但是 RocketMQ 在消息存储上与 Kafka 有着非常大的不同，RocketMQ#ConsumeQueue 不存储具体的消息，具体的消息由 CommitLog 存储，Consume Queue 中只存储路由到该 queue 中消息在 CommitLog 中的 offset，整个数据包如下，一共只占20个字节：

![1635678633250](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635678633250.png)

| 属性    | 长度  | 作用                                           |
| ------- | ----- | ---------------------------------------------- |
| offset  | 8字节 | 消息，在 CommitLog 中的 offset，类似于二级索引 |
| size    | 4字节 | 消息大小                                       |
| tagCode | 8字节 | 消息所属的 tagCode，即 tag 的 hash             |

###### 消息存储方式

RocketMQ 消息存储，由 CommitLog + ConsumeQueue 两部分组成，其中 CommitLog 用于存储原始的消息，ConsumeQueue 用于存储投递到某一个 queue 中消息的位置信息，其消息存储方式如下图所示：

1. Consumer 在读取消息时，会先读取 Consume Queue，再通过 Consume Queue 中的 offset，读取
   CommitLog，从而得到原始的消息。

![1635678882127](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635678882127.png)

###### 对比 Kafka

1. 在 Kafka 中，每个 Partition 有独立的消息存储，投递到每个 Partition 的消息，会存储在 Partition 自己的文件中，示意图如下：

   ![1635741207241](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635741207241.png)

2. RocketMQ 与 Kafka 消息存储上的对比：

|          | RocketMQ                                                     | Kafka                                                        |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 消息存储 | 将所有消息存储在同一个 CommitLog 中，且 Consume Queue 中只存储20个字节每个消息的位置信息 | 将每个 Partition的消息分开存储                               |
| 影响     | 单个 Broker 能支持更多的 Topic 和 Consume Queue，单机支持最高5w个队列，并且Load不会发生明显变化 | 单机超过64个 Partition，Load 会发生明显的飙高，发送消息的响应时间变长，但对于少 Partition 场景， 由于利用了 Partition 并行处理，使得此时的写性能高于 RocketMQ |
| 原因     | 所有消息都存储在同一个文件中，使得消息存储是磁盘顺序写       | 将消息按 Partition 存储在不同的文件中，使得消息存储是磁盘随机写，当 Partition 数量非常大时，随机IO非常多，导致所有 Broker 性能明显下降 |

##### 生产者消息投递原理

Producer 轮询某 Topic 下所有队列的方式来实现**发送方的负载均衡**，如下图所示：

```java
private SendResult sendDefaultImpl(Message msg,......) {
    // 检查Producer状态是否是RUNNING
    this.makeSureStateOK();
    // 检查msg是否合法：是否为null、topic & body是否为空、body是否超长
    Validators.checkMessage(msg, this.defaultMQProducer);
    // 获取topic路由信息
    TopicPublishInfo topicPublishInfo = 
this.tryToFindTopicPublishInfo(msg.getTopic());
    // 从路由信息中选择一个消息队列
    MessageQueue mq = topicPublishInfo.selectOneMessageQueue(lastBrokerName);
    // 将消息发送到该队列上去
    sendResult = this.sendKernelImpl(msg, mq, communicationMode, sendCallback, 
timeout);
}
```

![1635741977981](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635741977981.png)

##### 消费者消息消费原理

- **消费方的负载均衡**：

  1. 遍历 Consumer 下所有的 Topic，然后根据 Topic 订阅消息。
  2. 获取同一 Topic 和 Consume Group 下所有的 Consumer。
  3. 然后根据具体的分配策略，来分配消费队列，比如平均分配、消费端配置等。

  ![1635742208490](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635742208490.png)

- **两种消息订阅模式**：但无论是哪种模式，在实现上都是 Consumer 到 Broker 上主动拉取消息。

  1. 一种是 push 模式，即 Broker 主动向 Consumer 推送消息，是通过**长轮询**方式来实现的：

     1. Consumer 通过一个线程，将 LinkBlockingQueue 中的 PullRequest，每隔一段时间主动发送到Broker 去拉取消息（这样可以防止 Consumer 一直被阻塞）。
     2. Broker 在收到 PullRequest 时，如果队列里有消息，就立即返回数据，Consumer 在收到消息后，然后回调设置的 Listener 方法。
     3. Broker 在收到 PullRequest 时，如果队列里没有数据，则 Broker 会**阻塞该请求**，把 PullRequest 扔到 ConcurrentHashMap 中缓存起来，后台会有个线程不停地从 ConcurrentHashMap 取出PullRequest 进行检查，直到有数据投递过来或者发生超时后，才响应返回给 Consumer。

     ![1635742907979](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635742907979.png)

  2. 另一种是 pull 模式，即 Consumer **在需要时**，主动到 Broker 上拉取消息。

##### 事务消息实现原理

###### 单机事务

![1635743492688](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635743492688.png)

- **优点**：可以保证数据一致性，耗时正常。
- **缺点**：系统规模变大时，可能会演变成分布式事务。

###### 分布式事务

![1635743625395](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635743625395.png)

- **优点**：满足系统规模变大的需求。
- **缺点**：数据一致性的实现复杂，且需要经过网络，导致整体耗时成倍增加。

###### 小事务 + 异步消息

![1635743951460](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635743951460.png)

- **优点**：将大事务拆分成多个小事务，然后异步执行，基本把分布式事务的执行效率优化到与单机的一致。

- **注意点**：图中执行本地事务（Bob 账户扣款）和发送异步消息，应该保证同时成功或者同时失败，也就是扣款成功了，发送消息也一定要成功，如果扣款失败了，就不能发送消息。那么问题来了，是要先扣款还是先发送
  消息呢？

  1. **先发送消息**：

     - **存在问题**：如果消息发送成功，但是扣款失败，Consumer 就会消费此消息，进而向 Smith 账户加钱，不可取。

     ![1635744186519](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635744186519.png)

  2. **先扣款**：

     - **存在问题**：如果扣款成功，发送消息失败，就会出现 Bob 扣钱了，但是 Smith 账户未加钱。
       - **解决方案**：把发消息放到 Bob 扣款的事务中去，如果发送失败，就抛出异常，事务回滚，从而保证 Bob 钱是正确的，也没有发送到消息。

     ![1635744257263](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635744257263.png)

###### 事务消息

![1635744412458](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635744412458.png)

事务消息，可以被认为是**两阶段提交**的消息实现，确保本地事务的执行和消息的发送可以原子地执行，实现分布式系统的最终一致性，使用事务消息，可以从框架 API 层面，实现分布式事务的需求，减少编程工作量。

- **事务消息执行时，程序分三个阶段**：
  1. **第一阶段**：发送 Prepared 消息，拿到消息的地址。
     1. RocketMQ 会定期扫描消息集群中的事务消息，如果发现了 Prepared 消息，则会向 Producer 确认，Bob 钱到底是减了还是没减，如果没减，则会继续等待。
     2. 如果减了，还要根据 Producer 设置的策略，来决定到底是回滚，还是会继续发送确认消息。
     3. 从而保证消息发送与本地事务同时成功或者同时失败。
  2. **第二阶段**：执行本地事务。
  3. **第三阶段**：通过第一阶段拿到的地址，去访问消息，并修改消息的状态。
- **存在问题**：Consumer 消费可能会失败，或者发生超时。

###### 事务消息 + 消费重试

![1635747736118](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635747736118.png)

- **优点**：通过不断消费重试，可以解决消费超时的问题，但需要注意重复消费的问题。
- **缺点**：但解决不了消费失败的问题。
  - **解决方案**：人工解决。
    1. 如果按照事务的流程，这种由于 Smith 加款失败，是需要回滚整个流程的。
    2. 而消息系统要实现这个回滚流程，系统的复杂度将大大提升，并且很容易出现 Bug，估计出现 Bug 的概率，都会比消费失败的概率大很多，这也是 RocketMQ 目前暂时没有解决这个问题的原因。
    3. 在设计实现消息系统时，需要衡量是否值得花这么大的代价，来解决一个出现概率非常小的问题。
    4. 因此，对于这种小概率问题人工解决就好，比如阿里的网上银行，发生补钱的流程通过自家的银行人工补偿就好。

#### API

##### 原生 POM 依赖

```xml
<dependency>
    <groupId>org.apache.rocketmq</groupId>
    <artifactId>rocketmq-client</artifactId>
    <version>4.9.2</version>
</dependency>
```

##### Producer | HelloWorld

###### 同步发送

可靠的同步发送，应用于广泛的场景，比如重要通知消息、短信通知、短信营销系统等。

| SendResult.sendStatus | 释义                                                         |
| --------------------- | ------------------------------------------------------------ |
| SEND_OK               | 消息发送成功，不过发送成功并不意味着可靠性投递，要想确保不丢，还要启用同步刷盘 `SYNC_FLUSH` 与同步复制 `SYNC_MASTER` |
| FLUSH_DISK_TIMEOUT    | 消息发送成功但 Master 刷盘超时，当 Master 启用了同步刷盘，却未能在同步刷盘时间内（默认 5s）完成刷盘，则会返回该状态 |
| FLUSH_SLAVE_TIMEOUT   | 消息发送成功但 Slave 同步超时，当 Master 启用了同步复制，Slave 却未能在同步复制时间内（默认 5s）完成与 Master 的同步，则会返回该状态 |
| SLAVE_NOT_AVAILABLE   | 消息发送成功但 Slave 不可用，当 Master 启用了同步复制，却没有配置任何 Slave，则会返回该状态 |

```java
public class SyncProducer {
    public static void main(String[] args) throws Exception {
        DefaultMQProducer producer = new
            DefaultMQProducer("please_rename_unique_group_name");
        producer.setNamesrvAddr("localhost:9876");
        producer.start();
        for (int i = 0; i < 100; i++) {
            Message msg = new Message("TopicTest", "TagA", 
            ("Hello RocketMQ " + i).getBytes(RemotingHelper.DEFAULT_CHARSET));
            // 同步发送: 只要这个方法不抛出任何异常，就代表消息已经发送成功
            SendResult sendResult = producer.send(msg);
            System.out.printf("%s%n", sendResult);
        }
        producer.shutdown();
    }
}
```

###### 异步发送

异步发送，适用于链路耗时较长，对响应时间敏感的业务场景。

```java
public class AsyncProducer {
    public static void main(String[] args) throws Exception {
        ...
        // 异步发送
        producer.send(msg, new SendCallback() {
            @Override
            public void onSuccess(SendResult sendResult) {
                countDownLatch.countDown();
                System.out.printf("%-10d OK %s %n", index, sendResult.getMsgId());
            }

            @Override
            public void onException(Throwable e) {
                countDownLatch.countDown();
                System.out.printf("%-10d Exception %s %n", index, e);
                e.printStackTrace();
            }
        });
        ...
    }
}
```

###### 单向发送

单向发送，用于需要中等可靠性的情况，比如日志收集。

```java
public class OnewayProducer {
    public static void main(String[] args) throws Exception{
        ...
		producer.sendOneway(msg);
        ...
    }
}
```

##### Consumer | HelloWorld

```java
public class Consumer {
    public static void main(String[] args) throws InterruptedException, MQClientException {
        DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("please_rename_unique_group_name");
        consumer.setNamesrvAddr("localhost:9876");
        consumer.subscribe("TopicTest", "*");
        consumer.registerMessageListener(new MessageListenerConcurrently() {
            @Override
            public ConsumeConcurrentlyStatus consumeMessage(List<MessageExt> msgs,
                ConsumeConcurrentlyContext context) {
                System.out.printf("%s Receive New Messages: %s %n", Thread.currentThread().getName(), msgs);
                return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;
            }
        });
        consumer.start();
        System.out.printf("Consumer Started.%n");
    }
}
```

##### Producer | 自定义路由

```java
public class OrderedProducer {
    public static void main(String[] args) throws Exception {
        SendResult sendResult = producer.send(msg, new MessageQueueSelector() {
            @Override
            public MessageQueue select(List<MessageQueue> mqs, Message msg, Object arg) {
                // 自定义路由 -> 队列数量取模
                Integer id = (Integer) arg;
                int index = id % mqs.size();
                return mqs.get(index);
            }
        }, orderId);
    }
}
```

##### Consumer | 广播订阅

```java
public class BroadcastConsumer {
    public static void main(String[] args) throws Exception {
        ...
        // 广播订阅
        consumer.setMessageModel(MessageModel.BROADCASTING);
        consumer.subscribe("TopicTest", "TagA || TagC || TagD");
        ...
    }
}
```

##### Producer | 延迟消息

```java
 public class ScheduledMessageProducer {
     public static void main(String[] args) throws Exception {
         Message message = new Message("TestTopic", ("Hello scheduled message " + i).getBytes());
         // 延迟消息: This message will be delivered to consumer 10 seconds later.
         message.setDelayTimeLevel(3);
         producer.send(message);
     }
        
 }
```

##### Producer | 批量发送

```java
String topic = "BatchTest";
List<Message> messages = new ArrayList<>();
messages.add(new Message(topic, "TagA", "OrderID001", "Hello world 0".getBytes()));
messages.add(new Message(topic, "TagA", "OrderID002", "Hello world 1".getBytes()));
messages.add(new Message(topic, "TagA", "OrderID003", "Hello world 2".getBytes()));
try {
    producer.send(messages);
} catch (Exception e) {
    e.printStackTrace();
}
```

##### Consumer | Tag 过滤器

```java
DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("CID_EXAMPLE");
// 只消费topic为TOPIC，tag为TAGA、TAGB、TAGC的消息
// consumer.subscribe("TOPIC", "TAGA || TAGB || TAGC");
// 只消费topic为TopicTest，tag为 0<=a<=3 的消息
consumer.subscribe("TopicTest", MessageSelector.bySql("a between 0 and 3");
```

##### Producer | 事务消息

```java
public class TransactionProducer {
    public static void main(String[] args) throws MQClientException, InterruptedException {
        TransactionListener transactionListener = new TransactionListenerImpl();
        TransactionMQProducer producer = new TransactionMQProducer("please_rename_unique_group_name");
        ExecutorService executorService = new ThreadPoolExecutor(2, 5, 100, TimeUnit.SECONDS, new ArrayBlockingQueue<Runnable>(2000), new ThreadFactory() {
            @Override
            public Thread newThread(Runnable r) {
                Thread thread = new Thread(r);
                thread.setName("client-transaction-msg-check-thread");
                return thread;
            }
        });

        // 设置自定义线程池来处理检查本地事务状态
        producer.setExecutorService(executorService);
        producer.setTransactionListener(transactionListener);
        producer.start();

        String[] tags = new String[] {"TagA", "TagB", "TagC", "TagD", "TagE"};
        for (int i = 0; i < 10; i++) {
            try {
                Message msg =
                    new Message("TopicTest1234", tags[i % tags.length], "KEY" + i,
                        ("Hello RocketMQ " + i).getBytes(RemotingHelper.DEFAULT_CHARSET));
                SendResult sendResult = producer.sendMessageInTransaction(msg, null);
                System.out.printf("%s%n", sendResult);
                
                Thread.sleep(10);
            } catch (MQClientException | UnsupportedEncodingException e) {
                e.printStackTrace();
            }
        }

        for (int i = 0; i < 100000; i++) {
            Thread.sleep(1000);
        }
        
        producer.shutdown();
    }
}

// 实现TransactionListener接口
public class TransactionListenerImpl implements TransactionListener {
    private AtomicInteger transactionIndex = new AtomicInteger(0);
    // transactionId-status
    private ConcurrentHashMap<String, Integer> localTrans = new ConcurrentHashMap<>();

    // 在发送半消息成功时，执行本地事务，返回三种事务状态之一
    @Override
    public LocalTransactionState executeLocalTransaction(Message msg, Object arg) {
        int value = transactionIndex.getAndIncrement();
        int status = value % 3;
        localTrans.put(msg.getTransactionId(), status);
        return LocalTransactionState.UNKNOW;
    }

    // 检查本地事务状态，并响应MQ检查请求，返回三种事务状态之一
    @Override
    public LocalTransactionState checkLocalTransaction(MessageExt msg) {
        Integer status = localTrans.get(msg.getTransactionId());
        if (null != status) {
            switch (status) {
                case 0:
                    // 中间状态，表示需要MQ回检来确定状态
                    return LocalTransactionState.UNKNOW;
                case 1:
                    // 提交事务，表示允许消费者消费这条消息
                    return LocalTransactionState.COMMIT_MESSAGE;
                case 2:
                    // 回滚事务，表示消息将被删除，不允许消费
                    return LocalTransactionState.ROLLBACK_MESSAGE;
            }
        }
        return LocalTransactionState.COMMIT_MESSAGE;
    }
}
```

#### 常见问题解决

##### 如何保证数据不丢失？

参考《RabbitMQ - 如何保证数据不丢失》，数据丢失的场景有：生产端丢数据、MQ 丢数据、消费端丢数据：

- **解决方案**：

  1. **生产端丢数据**：生产消息可以通过 Comfirm 机制解决，消息状态打标 + 消息落库 + 定时重发 + 人工介入/失败补偿（同步发送时根据 SendResult.sendStatus 判断，异步发送时在回调函数里判断）。

     ```java
     // 同步发送消息的重试次数, 默认为2
     producer.setRetryTimesWhenSendFailed(3);
     // 异步发送消息的重试次数, 默认为2
     // producer.setRetryTimesWhenSendAsyncFailed(3);
     ```

  2. **MQ 丢数据**：同步刷盘 + 同步复制（也叫同步双写）。

     1. **同步刷盘**：
        1. 默认情况下，刷盘策略为 `flushDiskType=ASYNC_FLUSH`，表示异步刷盘，消息到了 Broker 将会优先保存到内存中，然后立刻返回确认响应给 Producer。
        2. Broker 再定期批量地把一组消息，从内存异步刷入到磁盘中。
        3. 虽然这种方式减少了 I/O 次数，可以取得更好的性能，但如果机器发生断电、异常、宕机等情况，导致消息未能及时刷入磁盘，就会出现消息丢失。
        4. 如果要 Broker 不丢消息，保证消息可靠性，需要把刷盘机制修改为同步刷盘方式 `flushDiskType = SYNC_FLUSH`，当消息存入到磁盘成功后，才返回响应给 Producer。
     2. **同步复制**：
        1. 为了保证 RocketMQ 的可用性，Broker 通常采用一主多从的部署方式，Slave 会从 Master 复制消息。
        2. 默认情况下，复制策略为 `brokerRole=ASYNC_MASTER`，表示异步复制，消息写入 Master 成功后，就可以返回响应给 Producer 了，接着消息才会异步复制到 Slave 节点。
        3. 但如果复制期间，Master 发生宕机且不可恢复，那么未复制到 Slave 的消息将可能会发生丢失。
        4. 为了进一步提高消息可靠性，可以采用同步复制的方式 `brokerRole=SYNC_MASTER`，消息写入 Master 时，会同步等待 Slave 节点复制完成后，才返回响应给 Producer。

  3. **消费端丢数据**：

     1. Consumer 从 Broker 拉取消息，然后执行相应的业务逻辑。
     2. 如果执行成功，将返回 `ConsumeConcurrentlyStatus.CONSUME_SUCCESS` 消费成功状态给 Broker。
     3. 而如果执行失败或者发生异常，则返回 `ConsumeConcurrentlyStatus.RECONSUME_LATER` 稍后重试状态给 Broker。
     4. 如果 Broker 未收到 Consumer 响应，或者收到除消费成功外的其他状态，那么 Consumer 下次还会再次拉取到该条消息进行重试。
     5. 这样有效避免了 Consumer 消费过程发生异常，或者消息在网络传输中丢失的情况。

- **总结**：

  1. 虽然提高了消息的可靠性，但会降低 MQ 的性能，生产实践中需要综合实际情况进行选择。
  2. 另外，还可能导致消息重发、Consumer 重复消费的情况，所以，对于 Consumer 而言，需要注意保证消费的**幂等性**。

##### 如何防止重复消费？

RocketMQ 不保证消息不重复，如果业务系统需要保证严格的不重复消息，需要自己在业务端进行去
重，具体解决方案参考《RabbitMQ - 如何防止重复消费》。

##### 一致性与可用性保障？

###### 集群部署方案

1. **单 Master 模式**：也就是只有一个 Master 节点。

   - **缺点**：如果 Master 节点挂了，则会导致整个服务不可用，线上不宜使用，只适用于个人学习使用。
2. **多 Master 模式**：多个 Master 节点组成集群。 
   - **优点**：所有模式中性能最高，且单个 Master 节点宕机或者重启，对应用没有影响。
   - **缺点**：单个 Master 节点宕机期间，未被消费的消息在节点恢复之前不可用，消息实时性受到影响。
     - **解决方案**：使用同步刷盘可以保证消息不丢失，同时 Topic 相对应的 Queue 应该分布在集群中各个 Master 节点，而不是只在某 Master 节点上，否则，该节点宕机会对订阅该 Topic 的应用造成影响。
3. **多 Master 多 Slave + 异步复制**：在多 Master 模式的基础上，让每个 Master 节点都有至少一个对应的 Slave，其中 Master 节点可读可写，而 Slave 只能读不能写，类似于 MySQL 的主从模式。
   - **优点**： 在 Master 宕机时，消费者可以从 Slave 读取消息，消息实时性不会受影响，性能几乎和多 Master 一样。 
   - **缺点**：使用异步复制有可能会发生消息丢失的问题。
4. **多 Master 多 Slave + 同步双写**： 与多 Master 多 Slave + 异步复制类似，但区别在于 Master 和 Slave 之间的数据同步方式为同步复制。
   - **优点**：同步双写能保证数据不丢失。 
   - **缺点**：发送单个消息 RT 会略长，性能相比异步复制低10%左右，即要保证数据可靠，需要采用同步刷盘和同步复制的方式，但性能会较其他方式低。

###### 可用性

![1635853268227](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635853268227.png)

1. **Name Server 高可用**：
   1. 由于 Name Server 节点是无状态的，且各个节点直接的数据是一致的，没有任何节点通信，每个 Broker 对接所有 Name Server 节点。
   2. 因此，在多个 NameServer 节点的情况下，当部分 Name Server 不可用时，也可以保证 MQ 服务正常运行。
2. **Broker 高可用**：
   1. Broker 通过 Master 和 Slave 的配合，达到模块的高可用，即一个 Master 可以配置多个 Slave 的一主多从模式，以及配置多个 Master-Slave 的多主多从模式。
   2. 当其中一个 Master 出现问题时：
      - **一主多从模式**：由于 Slave 只负责读，当 Master 不可用时，Slave 仍能保证消息被正常消费。
      - **多主多从模式**：由于配置了多组 Master-Slave，当 Master 不可用时，其他的 Master-Slave 还会保证消息的正常发送和消费。
   3. 新版本的 RocketMQ，支持 Slave 自动转成 Master，而老版本则要手动停止 Slave Broker，更改配置文 件，用新的配置文件启动 Broker，把 Slave 转成 Master。
3. **Consumer 高可用**：
   1. Consumer 高可用依赖于 Master-Slave 配置，由于 Master 能够支持读写消息，Slave 支持读消息，当 Master 不可用时， Consumer 会被自动切换到从 Slave 进行读取。
   2. 此时，Master 发生机器故障时，消息仍可从 Slave 中被消费。
4. **Producer 高可用**：
   1. 在创建 Topic 时，把 Topic 的多个 Message Queue 创建在多个 Master Broker 上。
   2. 这样，当其中一个 Master Broker 不可用后，其他的 Master Broker 仍然可用，Producer 可以继续发送消息。

###### 一致性（可靠性）

**4 可靠性**

- **异步刷盘**：Broker 在返回 Producer 消息写成功时，消息可能只是被写入了内存的 PageCahce，这样的写操作返回快，吞吐量大，当内存里的消息量积累到一定程度时，会统一触发写磁盘操作，快速批量写入。
  - **优点**：减少了多次写磁盘的 I/O，性能高。
  - **缺点**：当 Master 宕机，或者在磁盘损坏的情况下，会丢失少量的消息，导致 MQ 的消息状态和 Producer、Consumer 的消息状态不一致。
  - **配置参数**：默认，`flushDiskType=ASYNC_FLUSH` 。
- **同步刷盘**：Broker 在返回 Producer 消息写成功前，消息已经被真正地写入了磁盘，具体流程是，消息写入内存的 PageCache后，会立刻通知刷盘线程进行刷盘，在等待刷盘线程刷盘完成后，才唤醒等待的写消息线程，返回消息写成功的状态给 Producer。
  - **优点**：可以保持 MQ 的消息状态和 Producer、Consumer 的消息状态一致。
  - **缺点**：性能比异步的低。
  - **配置参数**：`flushDiskType = SYNC_FLUSH` 。
- **异步复制**：Broker 只要在 Master 写成功后，即可返回消息写成功状态给 Producer。
  - **优点**：复制具有较低的延迟和较高的吞吐量。
  - **缺点**：如果 Master 出现故障，有些数据会因为没有被写入 Slave，而丢失少量消息。
  - **配置参数**：默认，`brokerRole=ASYNC_MASTER` 。
- **同步复制**：Broker 会等到 Master 和 Slave 都写成功后，才返回消息写成功状态给 Producer。
  - **优点**：如果 Master 出现故障，Slave 有全部的备份数据，消息不丢失，容易实现完整恢复。
  - **缺点**：增大了复制的写入延迟，降低了系统吞吐量，性能比异步复制略低，大约低 10% 左右。
  - **配置参数**：`brokerRole=SYNC_MASTER` 。

###### 总结

1. **Master + Slave + 同步刷盘 + 同步双写**是最安全的方案，但会引入额外的延迟。
2. 事实证明，在所有的故障模式下，分布式系统不可能同时保证无数据丢失的最终一致性，以及时刻都接受读取和写入的高可用性。
3. 因此需要做的是，选择要针对其中的一些进行优化，让一致性和可用性处于一个范围的两端。

##### 如何实现 Consumer Rebalance？

###### 概念

Consumer Rebalance，消费者再均衡机制，指将一个 Topic 下的多个 Queue，在同一个 Consumer Group下的多个 Consumer 之间进行重新分配。 

1. Consumer Rebalance 机制，目的是为了提升消息的并行处理能力。
2. 比如，一个 Topic 下有 5 个 Queue，在只有 1 个 Consumer 的情况下，那么该 Consumer 将会负责处理这 5 个队列的消息。
3. 如果此时增加了一个 Consumer，那么则可以给其中一个 Consumer 分配 2 个 Queue，给另一个分配 3 个 Queue，从而提升消息的并行处理能力。

![1635925188314](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635925188314.png)

###### 局限与危害

- **空消费**：由于一个 Queue 最多分配给一个 Consumer，因此，当某个 Consumer Group 下的 Consumer#size > Queue#size 时，多余的 Consumer 将分配不到任何 Queue。
- **消费暂停**：
  1. 在只有 Consumer 1 负责消费所有 5 个 Queue 的情况下，在新增 Consumer 2，触发 Consumer Rebalance 时，需要分配 2 个Queue 给 Consumer 2 消费。
  2. 那么，此时 Consumer 1 就需要停止这 2 个 Queue 的消费，等到这 2 个 Queue 分配给 Consumer 2 后，这 2 个 Queue 才能被继续消费。
- **重复消费**：
  1. Consumer 2 在消费分配给自己的 2 个 Queue 时，必须接着从 Consumer 1 之前已经消费到的 offset 继续开始消费。
  2. 然而，默认情况下，offset 是异步提交的，比如 Consumer 1 当前消费到 offset=10，但是异步提交给Broker#offset=8。
  3. 那么，如果 Consumer 2 从 offset=8 开始消费，那么就会有 2 条消息被重复消费，也就是说，Consumer 2 并不会等待 Consumer 1 提交完 offset 后，再进行 Rebalance，因此，Consumer 1 提交间隔越长，可能造成的重复消费就越多。 
- **消费突刺**：由于 Consumer Rebalance 可能导致重复消费，如果需要重复消费的消息过多，或者因为Consumer Rebalance 暂停时间过长，导致 MQ 积压了较多消息，导致在 Consumer Rebalance 结束之后的瞬间，Consumer 需要消费很多消息。

###### 触发条件

Consumer Rebalance 的触发条件有 2 个：

1. 订阅了 Topic 的 Queue 信息发生变化。
2. Consumer Group 信息发生变化。

| 订阅了 Topic 的 Queue 信息发生变化 | Consumer Group 信息发生变化                |
| ---------------------------------- | ------------------------------------------ |
| Broker 宕机                        | Consumer 发布过程中的停止与启动            |
| Broker 升级等运维操作              | Consumer 异常宕机                          |
| Queue 扩容或者缩容                 | 网络异常，导致 Consumer 与 Broker 断开连接 |
|                                    | Consumer 扩容或者缩容                      |
|                                    | Topic 订阅信息发生变化                     |

###### 场景分析

对于 Consumer Group 信息发生变化，具体场景有，Consumer 在启动/运行/停止时，都有可能触发 Consumer Rebalance。

- **启动时**：
  1. Consumer 启动后会立即向所有 Broker 发送一次发送 `HEART_BEAT` 心跳请求。
  2. Broker 收到该请求后，则会将 Consumer 添加到由 ConsumerManager 维护的某个消费者组中。
  3. 然后该 Consumer 自己会触发一次 Consumer Rebalance。
- **运行时**：Consumer 接收到 Broker Rebalance 通知后，会立即触发一次 Consumer Rebalance，同时为了避免 Rebalance 通知丢失，还会周期性触发 Consumer Rebalance。
- **停止时**：
  1. Consumer 向所有 Broker 发送 `UNREGISTER_CLIENT` 命令。
  2. Broker 收到后，则将 Consumer 从 ConsumerManager 中移除，并通知其他 Consumer 进行 Consumer Rebalance。

###### 执行过程

- **单 Topic 队列分配**：调用 `RebalanceImpl#rebalanceByTopic`。

  1. 获得 Rebalance 元数据信息：即当前消费者组订阅 Topic 的 Queue 信息，以及当前消费者组所有 Consumer#ID 集合。

  2. 调用 `AllocateMessageQueueStrategy#allocate` 根据具体的**队列分配策略**，进行队列分配，此时需要传入消费者组、当前 Consumer#ID、当前消费者组可分配的队列集合、当前消费者组所有 Consumer#ID 集合。

     | 队列分配策略实现                      | 实现原理       |
     | ------------------------------------- | -------------- |
     | AllocateMessageQueueAveragely         | 默认，平均分配 |
     | AllocateMessageQueueAveragelyByCircle | 循环分配       |
     | AllocateMessageQueueConsistentHash    | 一致性哈希分配 |
     | AllocateMessageQueueByConfig          | 根据配置分配   |
     | AllocateMessageQueueByMachineRoom     | 根据机房分配   |
     | AllocateMachineRoomNearby             | 就近分配       |

  3. 调用 `RebalanceImpl#updateProcessQueueTableInRebalance` 进行队列的实际分配。

- **多 Topic 分配**：每个 Topic 都会调用一次 `RebalanceImpl#rebalanceByTopic`，触发一次队列分配策略。

###### 对比 Kafka Consumer Rebalance

- **相同点**：二者的 Rebalance 都是在 Consumer 客户端进行。
- **不同点**：
  1. Kafka 会在 Consumer Group 中，选出一个 Consumer 作为 Group Leader，再由这个 Group Leader 来进行分区分配，最后的分配结果通过 Cordinator Broker 同步给其他 Consumer，即 Kafka 分区分配只有一个大脑 Group Leader。
  2. 而 RocketMQ 则是，通过 Broker 通知每个 Consumer 各自进行 Consumer Rebalance，每个Consumer 自己给自己重新分配队列，而不是 Broker 将分配好的结果告知 Consumer，即每个 Consumer 都是一个大脑。
- **RocketMQ#Consumer Rebalance 一致性保证原理**：
  - **定时 Rebalance 保证**：每个 Consumer 都会定时触发 Rebalance，避免 Rebalance 通知丢失。
  - **队列分配策略保证**：
    1. 每个 Consumer 在调用 `AllocateMessageQueueStrategy#allocate` 时，会传入自身实例#ID，用于获取在当前消费组所有 Consumer#ID 集合中的索引位置。
    2. 然后根据具体的策略实现，可以使得每个索引所处的 Consumer 分配的队列各不相同。
    3. 这样可以让 RocketMQ 即使没有一个中心节点做统一的队列分配，也能完成 Consumer Rebalance。

##### 如何保证顺序消费？

- **问题场景**：

  - 业务上产生三条消息，分别是对数据的 add、update 和 delete，如果没有保证顺序消费，结果可能是delete ->  update -> add，本来数据最终是要 delete 掉的，结果却变成 add。
  - 再如电商平台，先付钱，然后生成订单，最后通知物流，如果顺序改变了，则可能出现不用先付钱了，却通知物流送货。

- **解决思路**：必须要使用**单消费者消费单个队列**，类似于 Kafka 的单消费者单个分区，目的是防止消费者争抢消息导致乱序消费的情况发生。

  1. 与 RabbitMQ 保证单消费者消费单个队列不同，但与 Kafka 类似，由于 RocketMQ#Consumer Group 机制，天然就能保证单个消费者消费，虽然 RocketMQ 针对的是 Queue，但却类似于一个 Kafka#Partition。

- **解决方案**：参考《RabbitMQ - 如何保证顺序消费》。

  - **多队列、多消费者**：类似于 RabbitMQ#一致性哈希交换机 `x-consistent-hash Exchange`，通过业务ID进行 hash，保证同一个业务 ID 的消息只落在同一个 Queue 中，然后被同一个 Consumer 顺序消费。

    - **局限**：消息不是全局保证顺序的，而只是相关消息才保证顺序；如果确实要保证顺序消费，则需要并发同步，比如搞分布式锁。

    - **实现方法**：

      ```java
      // 通过MessageQueueSelector中实现的算法，来确定消息发送到哪一个Queue上
      SendResult sendResult = producer.send(msg, new MessageQueueSelector() {
          @Override
          public MessageQueue select(List<MessageQueue> mqs, Message msg, Object arg) 
      {
              Integer id = (Integer) arg;
              int index = id % mqs.size();
              return mqs.get(index);
          }
      }, orderId);
      ```

  - **单队列、多消费者**：多个 Consumer 消费。

    - **局限**：需要保证并发同步，引入了同步机制，可能会降低消费速度。

  - **单消费者、多线程**：一个 Consumer + 一个内存队列 + 多线程消费，即 Master - Worker 模式。

    - **局限**：与单分区、多消费者模式类似，只不过在同一个 Consumer 进程中，处理并发同步的成本可能要比不同进程的更低一些。

  - **单队列、单消费者**：始终保证使用 一个 Queue + 一个 Consumer 消费。

    - **局限**：Consumer 不能水平扩展，消费能力有限。

    - **实现方法**：

      ```shell
      # -b : broker地址,表示topic建在该broker
      # -c : cluster名称,表示topic建在该集群
      # -n : nameserve服务地址列表
      # -r : 可读队列数(默认为8)
      # -w : 可写队列数(默认为8)
      # -t： topic名称
      mqadmin updateTopic -b localhost:10911 -t TopicTest
      ```

##### 如何优化消息积压？

###### 影响后果

1. 消息积压越多，Consumer 寻址的性能就越慢，最差的情况则会导致整个 RocketMQ 对外提供服务的性能很差，从而影响其他服务的访问速度，最后造成雪崩效应。
2. 而且还可能发生磁盘被堆满，导致 Producer 消息无法写入磁盘，一直报错，然后引发连锁反应，同样会造成雪崩效应。

###### 原因分析

如果 Consumer 消费速度跟不上 Producer 生产速度，就会造成消息积压。

- Consumer 消费速度跟不上 Producer 生产速度，一般是业务逻辑没设计好，导致 Consumer 和 Producer 之间的效率不平衡。
  - **解决思路**：增加 Partition、增加 Consumer 等，以提高消费速度。
- Consumer 出现异常，导致一直无法接收新的消息。
  - **解决思路**：优化消费程序，解决异常。

###### 优化思路

一定要保证 Consumer 的消费性能要高于 Producer 的发送性能，这样系统才能健康、持续地运行。

###### 优化方案

这里的优化方案，针对的是**消费速度低于生产速度**，而不是 Consumer 异常。

1. **Consumer 批量拉取消息**：这里要考虑的是，Consumer 有没有已经发挥出其本身真正的后端处理能力，如果没有，可以从**减少 I/O 的角度**入手。

   1. 对于每条消息分别拉取的情况，Consumer 需要多次从 Broker 上拉取消息，这样的多次 I/O，浪费了服务器性能与增加了带宽的占用。
   2. 通过批量拉取消息的方式，减少多次发起 Broker 请求，可以减少很多 I/O 浪费和带宽占用。

   ```java
   // 每次拉取10条
   consumer.setConsumeMessageBatchMaxSize(10);
   ```

2. **多线程并发消费**：接着还要考虑的是，Consumer 有没有已经发挥出其本身真正的后端处理能力，如果没有，可以**从多线程并发消费**入手，参考《Kafka API - Consumer 多线程消费》。

   1. 多线程并发消费，不需要建立多个 RocketMQ 客户端连接，在收到消息后，可以将其放入不同的线程中进行消费，这样进程中就会同时消费多个消息，增加了消费的吞吐量，从而提升消费速度。
   2. **小结**：与增加 Consumer 类似，存在并发冲突和顺序消费的问题，只不过在多线程并发消费是在同一个 Consumer 进程中，处理并发同步的成本可能要比不同进程的更低一些。

3. **增加 Consumer**：这个道理比较容易理解，多个人搬砖的速度肯定比一个人要快很多，不过实际情况还需要面对一些技术挑战，比如后端处理能力瓶颈、并发消费冲突，以及保持顺序消费三个问题。

   - **后端处理能力瓶颈**：
     1. 比如多个 Consumer 都要操作数据库，那么数据库连接的并发数和读写吞吐量就是后端处理能力。
     2. 如果达到了**数据库的最大处理能力**，出现了瓶颈，增加再多的 Consumer 也没有用，甚至会因为加剧了数据库拥塞，从而导致整体消费速度的进一步下降。
   - **并发消费冲突**：
     1. 比如两个 Consumer 都要去修改用户的积分，如果同时取出了相同的数据，并发处理的话就会出现并发安全问题。
     2. 此时需要保证**并发同步**，比如可以搞一个分布式锁，对于具体的某个用户，确保同时只能有一个消费者来处理其积分。
   - **保持顺序消费**：
     1. 由于增加了多个 Consumer，不再是单个 Consumer 消费单个队列，可能会出现乱序消费的情况。
     2. 如果仍需要保证顺序消费，那么可以参考一致性哈希的做法，搞成多队列、多消费者模式，不过只能保证相关消息顺序消费；如果确实要保证顺序消费，则需要并发同步，比如搞分布式锁。
   - **小结**：
     1. 解决并发消费冲突、保持顺序消费两个问题，常常需要引入多个 Consumer 之间的**并发同步**机制，如果这些机制设计得不好，还会给消费速度带来很大的影响。
     2. 因此，多人搬砖速度快的前提，是多个人搬砖时不需要大家频繁的坐下来协调谁搬哪块砖，否则，就会浪费很多时间在相互协调上，反而不能提升搬砖的速度。
     3. 所以，想要通过增加 Consumer，来提升消费速度，需要确保 Consumer **并发处理能力**要留有余地，Consumer 依赖的**后端服务处理能力**也要留有余地。

###### 方案总结

- **优化思路**：通过分析上边的这些方法，在进行消费优化时，可以遵循这样一个路径，以保证最大消费速度。
  1. 先单个 Consumer 消费，**1 次只接收 1 条消息**，消息消费完毕后再消费下一条，避免并发冲突和顺序消费的问题，减少同步机制的消耗。
  2. 如果消费速度不满足要求，则提高 `consumer.setConsumeMessageBatchMaxSize(10)`，**1 次接收多条消息**，单线程按顺序消费，避免并发冲突和顺序消费的问题，减少同步机制的消耗。
  3. 如果消费速度还是不满足要求，则 **1 次接收多条 + 多线程消费**，但要注意并发冲突和顺序消费的问题。
  4. 如果消费速度还是不满足要求，则**多个消费者并发消费**，但要注意并发冲突和顺序消费的问题。
  5. 如果消费速度还是不满足要求，则考虑**改需求**，或者**换别的中间件**。
- **优化注意点**：
  1. **程序性能优化优先**：需要始终优先优化 Consumer 处理能力，以及其依赖的后端程序处理能力，比如要去优化 SQL 语句、使用缓存、使用负载均衡等，来加快消费速度，因为消息积压常常都是程序处理太耗时导致的。
  2. **幂等性消费**：由于不只 Producer 可能会重复发送消息，Consumer 也可能会触发消息的重复投递，所以，Consumer 要保证幂等性消费。
  3. **并发同步**：如果使用了多线程消费，或者多 Consumer 消费，则会存在并发冲突以及顺序消费的问题，此时需要保证并发同步，比如使用分布式锁。
  4. **顺序消费**：最好能做到无需顺序消费，否则需要在多线程消费，或者多 Consumer 消费时保证并发同步。

###### 【线上】如何紧急处理消息积压？

参考《RabbitMQ - 【线上】如何紧急处理消息积压》。

对于 RocketMQ 而言，由于 Queue 机制与 RabbitMQ Queue 并不相同，所以除了参考，还有以下的处理方式：

1. **提高 Queue 队列数**：队列数足够，可以提高消费的并行度，并且也决定者同组 Consumer 的最大数量。
2. **临时 Topic + Consumer 进行消息转储消费**：
   1. 如果消息积压的 Topic 在建立时，就没有建立足够多的 Queue，导致同一个 Consumer Group 中的 Consumer#size 已经增加到 Queue#size，却仍有大量的消息积压。
   2. 此时，可以建立一个临时的 Topic、设置足够多的 Queue 以及上线一批足够多的 Consumer ，然后把所有原 Topic 的 Consumer 拉取到的消息统统转储到临时 Topic 上，供新上线那批 Consumer 去消费。
   3. 这样，消费速度上去了，消息积压的问题才有机会解决掉，解决掉后再视实际情况，来决定是否恢复原状。



# 	七、Spring篇 

### 设计思想&Beans

#### **1、IOC 控制反转**

​		IoC（Inverse of Control:控制反转）是⼀种设计思想，就是将原本在程序中⼿动创建对象的控制权，交由Spring框架来管理。 IoC 在其他语⾔中也有应⽤，并⾮ Spring 特有。 

​		IoC 容器是 Spring⽤来实现 IoC 的载体， IoC 容器实际上就是个Map（key，value）,Map 中存放的是各种对象。将对象之间的相互依赖关系交给 IoC 容器来管理，并由 IoC 容器完成对象的注⼊。这样可以很⼤程度上简化应⽤的开发，把应⽤从复杂的依赖关系中解放出来。 IoC 容器就像是⼀个⼯⼚⼀样，当我们需要创建⼀个对象的时候，只需要配置好配置⽂件/注解即可，完全不⽤考虑对象是如何被创建出来的。



**DI 依赖注入**

​	DI:（Dependancy Injection：依赖注入)站在容器的角度，将对象创建依赖的其他对象注入到对象中。



#### **2、AOP 动态代理**

​		AOP(Aspect-Oriented Programming:⾯向切⾯编程)能够将那些与业务⽆关，却为业务模块所共同调⽤的逻辑或责任（例如事务处理、⽇志管理、权限控制等）封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可拓展性和可维护性。

​		Spring AOP就是基于动态代理的，如果要代理的对象，实现了某个接⼝，那么Spring AOP会使⽤JDKProxy，去创建代理对象，⽽对于没有实现接⼝的对象，就⽆法使⽤ JDK Proxy 去进⾏代理了，这时候Spring AOP会使⽤基于asm框架字节流的Cglib动态代理 ，这时候Spring AOP会使⽤ Cglib ⽣成⼀个被代理对象的⼦类来作为代理。



#### **3、Bean生命周期** 

**单例对象：** singleton                    

总结：单例对象的生命周期和容器相同        

**多例对象：** prototype           

出生：使用对象时spring框架为我们创建            

活着：对象只要是在使用过程中就一直活着            

死亡：当对象长时间不用且没有其它对象引用时，由java的垃圾回收机制回收

<img src="https://s0.lgstatic.com/i/image3/M01/89/0C/Cgq2xl6WvHqAdmt4AABGAn2eSiI631.png" alt="img" style="zoom:67%;" />

IOC容器初始化加载Bean流程：

```java
@Override
public void refresh() throws BeansException, IllegalStateException { synchronized (this.startupShutdownMonitor) {
  // 第一步:刷新前的预处理 
  prepareRefresh();
  //第二步: 获取BeanFactory并注册到 BeanDefitionRegistry
  ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory();
  // 第三步:加载BeanFactory的预准备工作(BeanFactory进行一些设置，比如context的类加载器等)
  prepareBeanFactory(beanFactory);
  try {
    // 第四步:完成BeanFactory准备工作后的前置处理工作 
    postProcessBeanFactory(beanFactory);
    // 第五步:实例化BeanFactoryPostProcessor接口的Bean 
    invokeBeanFactoryPostProcessors(beanFactory);
    // 第六步:注册BeanPostProcessor后置处理器，在创建bean的后执行 
    registerBeanPostProcessors(beanFactory);
    // 第七步:初始化MessageSource组件(做国际化功能;消息绑定，消息解析); 
    initMessageSource();
    // 第八步:注册初始化事件派发器 
    initApplicationEventMulticaster();
    // 第九步:子类重写这个方法，在容器刷新的时候可以自定义逻辑 
    onRefresh();
    // 第十步:注册应用的监听器。就是注册实现了ApplicationListener接口的监听器
    registerListeners();
    //第十一步:初始化所有剩下的非懒加载的单例bean 初始化创建非懒加载方式的单例Bean实例(未设置属性)
    finishBeanFactoryInitialization(beanFactory);
    //第十二步: 完成context的刷新。主要是调用LifecycleProcessor的onRefresh()方法，完成创建
    finishRefresh();
	}
  ……
} 
```

总结：

**四个阶段**

- 实例化 Instantiation
- 属性赋值 Populate
- 初始化 Initialization
- 销毁 Destruction

**多个扩展点**

- 影响多个Bean
  - BeanPostProcessor
  - InstantiationAwareBeanPostProcessor
- 影响单个Bean
  - Aware

**完整流程**  

1. 实例化一个Bean－－也就是我们常说的**new**；
2. 按照Spring上下文对实例化的Bean进行配置－－**也就是IOC注入**；
3. 如果这个Bean已经实现了BeanNameAware接口，会调用它实现的setBeanName(String)方法，也就是根据就是Spring配置文件中**Bean的id和name进行传递**
4. 如果这个Bean已经实现了BeanFactoryAware接口，会调用它实现setBeanFactory(BeanFactory)也就是Spring配置文件配置的**Spring工厂自身进行传递**；
5.  如果这个Bean已经实现了ApplicationContextAware接口，会调用setApplicationContext(ApplicationContext)方法，和4传递的信息一样但是因为ApplicationContext是BeanFactory的子接口，所以**更加灵活**
6.  如果这个Bean关联了BeanPostProcessor接口，将会调用postProcessBeforeInitialization()方法，BeanPostProcessor经常被用作是Bean内容的更改，由于这个是在Bean初始化结束时调用那个的方法，也可以被应用于**内存或缓存技**术
7.  如果Bean在Spring配置文件中配置了init-method属性会自动调用其配置的初始化方法。
8.   如果这个Bean关联了BeanPostProcessor接口，将会调用postProcessAfterInitialization()，**打印日志或者三级缓存技术里面的bean升级**
9.   以上工作完成以后就可以应用这个Bean了，那这个Bean是一个Singleton的，所以一般情况下我们调用同一个id的Bean会是在内容地址相同的实例，当然在Spring配置文件中也可以配置非Singleton，这里我们不做赘述。
10.   当Bean不再需要时，会经过清理阶段，如果Bean实现了DisposableBean这个接口，或者根据spring配置的destroy-method属性，调用实现的destroy()方法





#### **4**、Bean作用域

| 名称           | 作用域                                                       |
| -------------- | ------------------------------------------------------------ |
| **singleton**  | **单例对象，默认值的作用域**                                 |
| **prototype**  | **每次获取都会创建⼀个新的 bean 实例**                       |
| request        | 每⼀次HTTP请求都会产⽣⼀个新的bean，该bean仅在当前HTTP request内有效。 |
| session        | 在一次 HTTP session 中，容器将返回同一个实例                 |
| global-session | 将对象存入到web项目集群的session域中,若不存在集群,则global session相当于session |

默认作用域是singleton，多个线程访问同一个bean时会存在线程不安全问题

**保障线程安全方法：**

1. 在Bean对象中尽量避免定义可变的成员变量（不太现实）。

2. 在类中定义⼀个ThreadLocal成员变量，将需要的可变成员变量保存在 ThreadLocal 中

  **ThreadLocal**：

  ​		每个线程中都有一个自己的ThreadLocalMap类对象，可以将线程自己的对象保持到其中，各管各的，线程可以正确的访问到自己的对象。

  ​		将一个共用的ThreadLocal静态实例作为key，将不同对象的引用保存到不同线程的ThreadLocalMap中，然后**在线程执行的各处通过这个静态ThreadLocal实例的get()方法取得自己线程保存的那个对象**，避免了将这个对象作为参数传递的麻烦。



#### 5、循环依赖

​	循环依赖其实就是循环引用，也就是两个或者两个以上的 Bean 互相持有对方，最终形成闭环。比如A 依赖于B，B又依赖于A

Spring中循环依赖场景有: 

- prototype 原型 bean循环依赖

- 构造器的循环依赖（构造器注入）

- Field 属性的循环依赖（set注入）

  其中，构造器的循环依赖问题无法解决，在解决属性循环依赖时，可以使用懒加载，spring采用的是提前暴露对象的方法。

**懒加载@Lazy解决循环依赖问题**

​	Spring 启动的时候会把所有bean信息(包括XML和注解)解析转化成Spring能够识别的BeanDefinition并存到Hashmap里供下面的初始化时用，然后对每个 BeanDefinition 进行处理。普通 Bean 的初始化是在容器启动初始化阶段执行的，而被lazy-init=true修饰的 bean 则是在从容器里第一次进行**context.getBean() 时进行触发**。



**三级缓存解决循环依赖问题**

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1glv7ivru2lj31980qcn13.jpg" alt="循环依赖问题" style="zoom: 33%;" />

1. Spring容器初始化ClassA通过构造器初始化对象后提前暴露到Spring容器中的singletonFactorys（三级缓存中）。

2. ClassA调用setClassB方法，Spring首先尝试从容器中获取ClassB，此时ClassB不存在Spring 容器中。

3. Spring容器初始化ClassB，ClasssB首先将自己暴露在三级缓存中，然后从Spring容器一级、二级、三级缓存中一次中获取ClassA 。

4. 获取到ClassA后将自己实例化放入单例池中，实例 ClassA通过Spring容器获取到ClassB，完成了自己对象初始化操作。

5. 这样ClassA和ClassB都完成了对象初始化操作，从而解决了循环依赖问题。

   

### Spring注解

#### 1、@SpringBoot 

​	**声明bean的注解**

​	**@Component** 通⽤的注解，可标注任意类为  Spring 组件

​	**@Service** 在业务逻辑层使用（service层）

​	**@Repository** 在数据访问层使用（dao层）

​	**@Controller** 在展现层使用，控制器的声明（controller层）

​	**注入bean的注解**

​	**@Autowired**：默认按照类型来装配注入，**@Qualifier**：可以改成名称

​	**@Resource**：默认按照名称来装配注入，JDK的注解，新版本已经弃用



**@Autowired注解原理** 

​		 @Autowired的使用简化了我们的开发，

​				实现 AutowiredAnnotationBeanPostProcessor 类，该类实现了 Spring 框架的一些扩展接口。
​				实现 BeanFactoryAware 接口使其内部持有了 BeanFactory（可轻松的获取需要依赖的的 Bean）。
​				实现 MergedBeanDefinitionPostProcessor 接口，实例化Bean 前获取到 里面的 @Autowired 信息并缓存下来；
​				实现 postProcessPropertyValues 接口， 实例化Bean 后从缓存取出注解信息，通过反射将依赖对象设置到 Bean 属性里面。



**@SpringBootApplication**

```java
@SpringBootApplication
public class JpaApplication {
    public static void main(String[] args) {
        SpringApplication.run(JpaApplication.class, args);
    }
}
```

**@SpringBootApplication**注解等同于下面三个注解：

- **@SpringBootConfiguration：** 底层是**Configuration**注解，说白了就是支持**JavaConfig**的方式来进行配置
- **@EnableAutoConfiguration：**开启**自动配置**功能
- **@ComponentScan：**就是**扫描**注解，默认是扫描**当前类下**的package

其中`@EnableAutoConfiguration`是关键(启用自动配置)，内部实际上就去加载`META-INF/spring.factories`文件的信息，然后筛选出以`EnableAutoConfiguration`为key的数据，加载到IOC容器中，实现自动配置功能！

它主要加载了@SpringBootApplication注解主配置类，这个@SpringBootApplication注解主配置类里边最主要的功能就是SpringBoot开启了一个@EnableAutoConfiguration注解的自动配置功能。

 **@EnableAutoConfiguration作用：**

它主要利用了一个

EnableAutoConfigurationImportSelector选择器给Spring容器中来导入一些组件。

```java
@Import(EnableAutoConfigurationImportSelector.class)
public @interface EnableAutoConfiguration 
```





#### **2、@SpringMVC**

```java
@Controller 声明该类为SpringMVC中的Controller
@RequestMapping 用于映射Web请求
@ResponseBody 支持将返回值放在response内，而不是一个页面，通常用户返回json数据
@RequestBody 允许request的参数在request体中，而不是在直接连接在地址后面。
@PathVariable 用于接收路径参数
@RequestMapping("/hello/{name}")申明的路径，将注解放在参数中前，即可获取该值，通常作为Restful的接口实现方法。
```

**SpringMVC原理** 

<img src="https://img-blog.csdn.net/20181022224058617?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2F3YWtlX2xxaA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" style="zoom: 50%;" />

1. 客户端（浏览器）发送请求，直接请求到  DispatcherServlet 。
2. DispatcherServlet 根据请求信息调⽤  HandlerMapping ，解析请求对应的  Handler 。
3. 解析到对应的  Handler （也就是  Controller 控制器）后，开始由HandlerAdapter 适配器处理。
4. HandlerAdapter 会根据  Handler 来调⽤真正的处理器开处理请求，并处理相应的业务逻辑。
5. 处理器处理完业务后，会返回⼀个  ModelAndView 对象， Model 是返回的数据对象
6. ViewResolver 会根据逻辑  View 查找实际的  View 。
7. DispaterServlet 把返回的  Model 传给  View （视图渲染）。
8. 把  View 返回给请求者（浏览器）



#### 3、@SpringMybatis

```java
@Insert ： 插入sql ,和xml insert sql语法完全一样
@Select ： 查询sql, 和xml select sql语法完全一样
@Update ： 更新sql, 和xml update sql语法完全一样
@Delete ： 删除sql, 和xml delete sql语法完全一样
@Param ： 入参
@Results ： 设置结果集合@Result ： 结果
@ResultMap ： 引用结果集合
@SelectKey ： 获取最新插入id 
```

**mybatis如何防止sql注入？**

​	简单的说就是#{}是经过预编译的，是安全的，**$**{}是未经过预编译的，仅仅是取变量的值，是非安全的，存在SQL注入。在编写mybatis的映射语句时，尽量采用**“#{xxx}”**这样的格式。如果需要实现动态传入表名、列名，还需要做如下修改：添加属性**statementType="STATEMENT"**，同时sql里的属有变量取值都改成**${xxxx}**



**Mybatis和Hibernate的区别** 

**Hibernate 框架：** 

​    **Hibernate**是一个开放源代码的对象关系映射框架,它对JDBC进行了非常轻量级的对象封装,建立对象与数据库表的映射。是一个全自动的、完全面向对象的持久层框架。

**Mybatis框架：**

​    **Mybatis**是一个开源对象关系映射框架，原名：ibatis,2010年由谷歌接管以后更名。是一个半自动化的持久层框架。

**区别：**

  **开发方面**

​    在项目开发过程当中，就速度而言：

​      hibernate开发中，sql语句已经被封装，直接可以使用，加快系统开发；

​      Mybatis 属于半自动化，sql需要手工完成，稍微繁琐；

​    但是，凡事都不是绝对的，如果对于庞大复杂的系统项目来说，复杂语句较多，hibernate 就不是好方案。

  **sql优化方面**

​    Hibernate 自动生成sql,有些语句较为繁琐，会多消耗一些性能；

​    Mybatis 手动编写sql，可以避免不需要的查询，提高系统性能；

  **对象管理比对**

​    Hibernate 是完整的对象-关系映射的框架，开发工程中，无需过多关注底层实现，只要去管理对象即可；

​    Mybatis 需要自行管理映射关系；



#### 4、@Transactional

```java
@EnableTransactionManagement 
@Transactional
```

注意事项：

​	①事务函数中不要处理耗时任务，会导致长期占有数据库连接。

​	②事务函数中不要处理无关业务，防止产生异常导致事务回滚。

**事务传播属性**

**1) REQUIRED（默认属性）** 如果存在一个事务，则支持当前事务。如果没有事务则开启一个新的事务。 

2) MANDATORY  支持当前事务，如果当前没有事务，就抛出异常。 

3) NEVER  以非事务方式执行，如果当前存在事务，则抛出异常。 

4) NOT_SUPPORTED  以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 

5) REQUIRES_NEW  新建事务，如果当前存在事务，把当前事务挂起。 

6) SUPPORTS  支持当前事务，如果当前没有事务，就以非事务方式执行。 

**7) NESTED** （**局部回滚**） 支持当前事务，新增Savepoint点，与当前事务同步提交或回滚。 **嵌套事务一个非常重要的概念就是内层事务依赖于外层事务。外层事务失败时，会回滚内层事务所做的动作。而内层事务操作失败并不会引起外层事务的回滚。**



### Spring源码阅读

#### **1、Spring中的设计模式** 

参考：[spring中的设计模式](https://mp.weixin.qq.com/s?__biz=Mzg2OTA0Njk0OA==&mid=2247485303&idx=1&sn=9e4626a1e3f001f9b0d84a6fa0cff04a&chksm=cea248bcf9d5c1aaf48b67cc52bac74eb29d6037848d6cf213b0e5466f2d1fda970db700ba41&token=255050878&lang=zh_CN%23rd)

**单例设计模式 :** Spring 中的 Bean 默认都是单例的。

**⼯⼚设计模式 :** Spring使⽤⼯⼚模式通过  BeanFactory 、 ApplicationContext 创建bean 对象。

**代理设计模式 :** Spring AOP 功能的实现。

**观察者模式：** Spring 事件驱动模型就是观察者模式很经典的⼀个应⽤。

**适配器模式：**Spring AOP 的增强或通知(Advice)使⽤到了适配器模式、spring MVC 中也是⽤到了适配器模式适配 Controller 。











# 八、SpringCloud篇

#### Why SpringCloud

> ​	Spring cloud 是一系列框架的有序集合。它利用 spring boot 的开发便利性巧妙地简化了分布式系统基础设施的开发，如**服务发现注册**、**配置中心**、**消息总线**、**负载均衡**、**断路器**、**数据监控**等，都可以用 spring boot 的开发风格做到一键启动和部署。

| SpringCloud（微服务解决方案）    | Dubbo（分布式服务治理框架） |
| -------------------------------- | --------------------------- |
| Rest API （轻量、灵活、swagger） | RPC远程调用（高效、耦合）   |
| Eureka、Nacos                    | Zookeeper                   |
| 使用方便                         | 性能好                      |
| 即将推出SpringCloud2.0           | 断档5年后17年重启           |

​	SpringBoot是Spring推出用于解决传统框架配置文件冗余,装配组件繁杂的基于Maven的解决方案,**旨在快速搭建单个微服务**，SpringCloud是依赖于SpringBoot的,而SpringBoot并不是依赖与SpringCloud,甚至还可以和Dubbo进行优秀的整合开发

​	MartinFlower 提出的微服务之间是通过RestFulApi进行通信，具体实现

- RestTemplate：基于HTTP协议
- Feign：封装了ribbon和Hystrix 、RestTemplate 简化了客户端开发工作量
- RPC：基于TCP协议，序列化和传输效率提升明显
- MQ：异步解耦微服务之间的调用

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gmawejgpgwj30ht0bnt9d.jpg" alt="img" style="zoom:67%;" />

#### Spring Boot

> Spring Boot 通过**简单的步骤**就可以创建一个 Spring 应用。
>
> Spring Boot 为 Spring 整合第三方框架提供了**开箱即用功能**。
>
> Spring Boot 的核心思想是**约定大于配置**。

**Spring Boot 解决的问题**

- 搭建后端框架时需要手动添加 Maven 配置，涉及很多 XML 配置文件，增加了搭建难度和时间成本。

- 将项目编译成 war 包，部署到 Tomcat 中，项目部署依赖 Tomcat，这样非常不方便。

- 应用监控做的比较简单，通常都是通过一个没有任何逻辑的接口来判断应用的存活状态。

**Spring Boot 优点**

**自动装配：**Spring Boot 会根据某些规则对所有配置的 Bean 进行初始化。可以减少了很多重复性的工作。

​	比如使用 MongoDB 时，只需加入 MongoDB 的 Starter 包，然后配置  的连接信息，就可以直接使用 MongoTemplate 自动装配来操作数据库了。简化了 Maven Jar 包的依赖，降低了烦琐配置的出错几率。

**内嵌容器：**Spring Boot 应用程序可以不用部署到外部容器中，比如 Tomcat。

​	应用程序可以直接通过 Maven 命令编译成可执行的 jar 包，通过 java-jar 命令启动即可，非常方便。

**应用监控：**Spring Boot 中自带监控功能 Actuator，可以实现对程序内部运行情况进行监控，

​	比如 Bean 加载情况、环境变量、日志信息、线程信息等。当然也可以自定义跟业务相关的监控，通过Actuator 的端点信息进行暴露。

```java
spring-boot-starter-web          //用于快速构建基于 Spring MVC 的 Web 项目。
spring-boot-starter-data-redis   //用于快速整合并操作 Redis。
spring-boot-starter-data-mongodb //用于对 MongoDB 的集成。
spring-boot-starter-data-jpa     //用于操作 MySQL。
```

**自定义一个Starter**

1. 创建 Starter 项目，定义 Starter 需要的配置（Properties）类，比如数据库的连接信息；

2. 编写自动配置类，自动配置类就是获取配置，根据配置来自动装配 Bean；

3. 编写 spring.factories 文件加载自动配置类，Spring 启动的时候会扫描 spring.factories 文件，；

4. 编写配置提示文件 spring-configuration-metadata.json（不是必须的），在添加配置的时候，我们想要知道具体的配置项是什么作用，可以通过编写提示文件来提示；

5. 在项目中引入自定义 Starter 的 Maven 依赖，增加配置值后即可使用。

**Spring Boot Admin**（将 actuator 提供的数据进行可视化）

- 显示应用程序的监控状态、查看 JVM 和线程信息

- 应用程序上下线监控  

- 可视化的查看日志、动态切换日志级别

- HTTP 请求信息跟踪等实用功能



#### GateWay / Zuul

> GateWay⽬标是取代Netflflix Zuul，它基于Spring5.0+SpringBoot2.0+WebFlux等技术开发，提供**统⼀的路由**⽅式（反向代理）并且基于 **Filter**(定义过滤器对请求过滤，完成⼀些功能) 链的⽅式提供了⽹关基本的功能，例如：鉴权、流量控制、熔断、路径重写、⽇志监控。

**组成：**

- **路由route：** ⽹关最基础的⼯作单元。路由由⼀个ID、⼀个⽬标URL、⼀系列的断⾔（匹配条件判断）和Filter过滤器组成。如果断⾔为true，则匹配该路由。

- **断⾔predicates：**参考了Java8中的断⾔Predicate，匹配Http请求中的所有内容（类似于nginx中的location匹配⼀样），如果断⾔与请求相匹配则路由。

- **过滤器filter：**标准的Spring webFilter，使⽤过滤器在请求之前或者之后执⾏业务逻辑。

  请求前`pre`类型过滤器：做**参数校验**、**权限校验**、**流量监控**、**⽇志输出**、**协议转换**等，

  请求前`post`类型的过滤器：做**响应内容**、**响应头**的修改、**⽇志的输出**、**流量监控**等。

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gmc49l9babj31do0n7n13.jpg" alt="image-20210105001419761" style="zoom: 50%;" />

**GateWayFilter** 应⽤到单个路由路由上 、**GlobalFilter** 应⽤到所有的路由上











#### Eureka / Zookeeper

> 服务注册中⼼本质上是为了解耦服务提供者和服务消费者，为了⽀持弹性扩缩容特性，⼀个微服务的提供者的数量和分布往往是动态变化的。

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gmawwm3k7bj30o80ecq3u.jpg" alt="image-20210103231405882" style="zoom: 50%;" />

| 区别   | Zookeeper        | Eureka                       | Nacos              |
| ------ | ---------------- | ---------------------------- | ------------------ |
| CAP    | CP               | AP                           | CP/AP切换          |
| 可用性 | 选举期间不可用   | 自我保护机制，数据不是最新的 |                    |
| 组成   | Leader和Follower | 节点平等                     |                    |
| 优势   | 分布式协调       | 注册与发现                   | 注册中心和配置中心 |
| 底层   | 进程             | 服务                         | Jar包              |

**Eureka**通过**⼼跳检测**、**健康检查**和**客户端缓存**等机制，提⾼系统的灵活性、可伸缩性和可⽤性。

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gmaxc493qyj30ji0a6mxx.jpg" alt="image-20210103232900353" style="zoom:67%;" />

1. us-east-1c、us-east-1d，us-east-1e代表不同的机房，**每⼀个Eureka Server都是⼀个集群**。
2. Service作为服务提供者向Eureka中注册服务，Eureka接受到注册事件会在**集群和分区中进⾏数据同步**，Client作为消费端（服务消费者）可以从Eureka中获取到服务注册信息，进⾏服务调⽤。
3. 微服务启动后，会周期性地向Eureka**发送⼼跳**（默认周期为30秒）以续约⾃⼰的信息
4. Eureka在⼀定时间内**（默认90秒）没有接收**到某个微服务节点的⼼跳，Eureka将会注销该微服务节点
5. Eureka Client**会缓存Eureka Server中的信息**。即使所有的Eureka Server节点都宕掉，服务消费者依然可以使⽤缓存中的信息找到服务提供者



**Eureka缓存**

> 新服务上线后，服务消费者**不能立即访问**到刚上线的新服务，需要过⼀段时间后才能访问？或是将服务下线后，服务还是会被调⽤到，⼀段时候后**才彻底停⽌服务**，访问前期会导致频繁报错！

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gmaxmk97q0j30vw0j6gmu.jpg" alt="image-20210103233902439" style="zoom:50%;" />

​	服务注册到注册中⼼后，服务实例信息是**存储在Registry表**中的，也就是内存中。但Eureka为了提⾼响应速度，在内部做了优化，加⼊了两层的缓存结构，将Client需要的实例信息，直接缓存起来，获取的时候直接从缓存中拿数据然后响应给 Client。 

- 第⼀层缓存是**readOnlyCacheMap**，采⽤**ConcurrentHashMap**来存储数据的，主要负责定时与readWriteCacheMap进⾏数据同步，默认同步时间为 **30** 秒⼀次。

- 第⼆层缓存是**readWriteCacheMap**，采⽤**Guava**来实现缓存。缓存过期时间默认为**180**秒，当服务**下线、过期、注册、状态变更**等操作都会清除此缓存中的数据。

- 如果两级缓存都无法查询，会**触发缓存的加载**，从存储层拉取数据到缓存中，然后再返回给 Client。

  Eureka之所以设计⼆级缓存机制，也是为了**提⾼ Eureka Server 的响应速度**，缺点是缓存会导致 Client**获取不到最新的服务实例信息**，然后导致⽆法快速发现新的服务和已下线的服务。

**解决方案**

- 我们可以**缩短读缓存的更新时间**让服务发现变得更加及时，或者**直接将只读缓存关闭**，同时可以缩短客户端如ribbon服务的定时刷新间隔，多级缓存也导致C层⾯（数据⼀致性）很薄弱。
- Eureka Server 中会有**定时任务去检测失效**的服务，将服务实例信息从注册表中移除，也可以将这个失效检测的**时间缩短**，这样服务下线后就能够及时从注册表中清除。

**自我保护机制开启条件**

- 期望最小每分钟能够续租的次数（实例* 频率 * 比例==10* 2 *0.85）
- 期望的服务实例数量（10）

**健康检查**

- Eureka Client 会定时发送心跳给 Eureka Server 来证明自己处于健康的状态

- 集成SBA以后可以把所有健康状态信息一并返回给eureka

  

#### Feign / Ribbon

- Feign 可以与 Eureka 和 Ribbon 组合使用以支持负载均衡，
- Feign 可以与 Hystrix 组合使用，支持熔断回退
- Feign 可以与ProtoBuf实现快速的RPC调用

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gmbxsh2rfnj30uo0fgmxz.jpg" alt="img" style="zoom:80%;" />

- **InvocationHandlerFactory 代理**

  采用 JDK 的动态代理方式生成代理对象，当我们调用这个接口，实际上是要去调用远程的 HTTP API

- **Contract 契约组件**

  比如请求类型是 GET 还是 POST，请求的 URI 是什么

- **Encoder 编码组件 \ Decoder 解码组件**

  通过该组件我们可以将请求信息采用指定的编码方式进行编解码后传输

- **Logger 日志记录**

  负责 Feign 中记录日志的，可以指定 Logger 的级别以及自定义日志的输出

- **Client 请求执行组件**

  负责 HTTP 请求执行的组件，Feign 中默认的 Client 是通过 JDK 的 HttpURLConnection 来发起请求的，在每次发送请求的时候，都会创建新的 HttpURLConnection 链接，Feign 的性能会很差，可以通过扩展该接口，使用 Apache HttpClient 等基于连接池的高性能 HTTP 客户端。

- **Retryer 重试组件**

  负责重试的组件，Feign 内置了重试器，当 HTTP 请求出现 IO 异常时，Feign 会限定一个最大重试次数来进行重试操作。

- **RequestInterceptor 请求拦截器**

  可以为 Feign 添加多个拦截器，在请求执行前设置一些扩展的参数信息。

**Feign最佳使用技巧**

- 继承特性

- 拦截器

  比如添加指定的请求头信息，这个可以用在服务间传递某些信息的时候。

- GET 请求多参数传递

- 日志配置

  FULL 会输出全部完整的请求信息。

- 异常解码器

  异常解码器中可以获取异常信息，而不是简单的一个code，然后转换成对应的异常对象返回。

- 源码查看是如何继承Hystrix

  HystrixFeign.builder 中可以看到继承了 Feign 的 Builder，增加了 Hystrix的SetterFactory， build 方法里，对 invocationHandlerFactory 进行了重写， create 的时候**返回HystrixInvocationHandler**， 在 invoke 的时候**会将请求包装成 HystrixCommand** 去执行，这里就自然的集成了 Hystrix



**Ribbon**

<img src="http://s0.lgstatic.com/i/image2/M01/93/96/CgotOV2Nux-AO2PcAAEcl4M1Zi4629.png" alt="img" style="zoom: 50%;" />



**使用方式**

- **原生 API**，Ribbon 是 Netflix 开源的，没有使用 Spring Cloud，需要使用 Ribbon 的原生 API。

- **Ribbon + RestTemplate**，整合Spring Cloud 后，可以基于 RestTemplate 提供负载均衡的服务

- **Ribbon + Feign**

  <img src="http://s0.lgstatic.com/i/image2/M01/93/76/CgoB5l2NuyCALoefAAAdV1DlSHY088.png" alt="img" style="zoom: 67%;" />

**负载均衡算法**

- RoundRobinRule 是**轮询的算法**，A和B轮流选择。

- RandomRule 是**随机算法**，这个就比较简单了，在服务列表中随机选取。

- BestAvailableRule 选择一个最**小的并发请求 server**

**自定义负载均衡算法**

- 实现 Irule 接口
- 继承 AbstractLoadBalancerRule 类

**自定义负载均衡使用场景**（核心）

- **灰度发布**

  灰度发布是能够平滑过渡的一种发布方式，在发布过程中，先发布一部分应用，让指定的用户使用刚发布的应用，等到测试没有问题后，再将其他的全部应用发布。如果新发布的有问题，只需要将这部分恢复即可，不用恢复所有的应用。

- **多版本隔离**

  多版本隔离跟灰度发布类似，为了兼容或者过度，某些应用会有多个版本，这个时候如何保证 1.0 版本的客户端不会调用到 1.1 版本的服务，就是我们需要考虑的问题。

- **故障隔离**

  当线上某个实例发生故障后，为了不影响用户，我们一般都会先留存证据，比如：线程信息、JVM 信息等，然后将这个实例重启或直接停止。然后线下根据一些信息分析故障原因，如果我能做到故障隔离，就可以直接将出问题的实例隔离，不让正常的用户请求访问到这个出问题的实例，只让指定的用户访问，这样就可以单独用特定的用户来对这个出问题的实例进行测试、故障分析等。



#### Hystrix / Sentinel

**服务雪崩场景**

自己即是服务消费者，同时也是服务提供者，同步调用等待结果导致资源耗尽

**解决方案**

服务方：扩容、限流，排查代码问题，增加硬件监控

消费方：使用Hystrix资源隔离，熔断降级，快速失败

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gmby7y9ykzj30wr0ehac5.jpg" alt="img" style="zoom:150%;" />

**Hystrix断路保护器的作用**

- **封装请求**会将用户的操作进行统一封装，统一封装的目的在于进行统一控制。
- **资源隔离限流**会将对应的资源按照指定的类型进行隔离，比如**线程池**和**信号量**。
  - 计数器限流，例如5秒内技术1000请求，超数后限流，未超数重新计数
  - 滑动窗口限流，解决计数器不够精确的问题，把一个窗口拆分多滚动窗口
  - 令牌桶限流，类似景区售票，售票的速度是固定的，拿到令牌才能去处理请求
  - 漏桶限流，生产者消费者模型，实现了恒定速度处理请求，能够绝对防止突发流量
- **失败回退**其实是一个备用的方案，就是说当请求失败后，有没有备用方案来满足这个请求的需求。
- **断路器**这个是**最核心**的，，如果断路器处于打开的状态，那么所有请求都将失败，执行回退逻辑。如果断路器处于关闭状态，那么请求将会被正常执行。有些场景我们需要手动**打开断路器强制降级**。
- **指标监控**会对请求的生**命周期进行监控**，请求成功、失败、超时、拒绝等状态，都会被监控起来。

**Hystrix使用上遇到的坑**

- 配置可以对接**配置中心**进行动态调整

  Hystrix 的配置项非常多，如果不对接配置中心，所有的配置只能在代码里修改，在集群部署的难以应对紧急情况，我们项目只设置一个 CommandKey，其他的都在配置中心进行指定，紧急情况如需隔离部分请求时，只需在配置中心进行修改以后，强制更新即可。

- 回退逻辑中可以**手动埋点**或者通过**输出日志**进行告警

  当请求失败或者超时，会执行回退逻辑，如果有大量的回退，则证明某些服务出问题了，这个时候我们可以在回退的逻辑中进行埋点操作，上报数据给监控系统，也可以输出回退的日志，统一由日志收集的程序去进行处理，这些方式都可以将问题暴露出去，然后通过实时数据分析进行告警操作

- 用 **ThreadLocal**配合**线程池隔离**模式需当心

  当我们用了线程池隔离模式的时候，被隔离的方法会包装成一个 Command 丢入到独立的线程池中进行执行，这个时候就是从 A 线程切换到了 B 线程，ThreadLocal 的数据就会丢失

- **Gateway中**多用信号量隔离

  网关是所有请求的入口，路由的服务数量会很多，几十个到上百个都有可能，如果用线程池隔离，那么需要创建上百个独立的线程池，开销太大，用信号量隔离开销就小很多，还能起到限流的作用。
  
  

[^常见问题]: Hystrix的超时时间要⼤于Ribbon的超时时间，因为Hystrix将请求包装了起来，特别需要注意的是，如果Ribbon开启了重试机制，⽐如重试3 次，Ribbon 的超时为 1 秒，那么Hystrix 的超时时间应该⼤于 3 秒，否则就会出现 Ribbon 还在重试中，⽽ Hystrix 已经超时的现象。



**Sentinel** 

> Sentinel是⼀个⾯向云原⽣微服务的流量控制、熔断降级组件。
>
> 替代Hystrix，针对问题：服务雪崩、服务降级、服务熔断、服务限流

Hystrix区别：

- 独⽴可部署Dashboard（基于 Spring Boot 开发）控制台组件
- 不依赖任何框架/库，减少代码开发，通过UI界⾯配置即可完成细粒度控制

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gmbza4zixbj30kl09sq4p.jpg" alt="image-20210104212151598" style="zoom:80%;" />

**丰富的应⽤场景**：Sentinel 承接了阿⾥巴巴近 10 年的双⼗⼀⼤促流量的核⼼场景，例如秒杀、消息削峰填⾕、集群流量控制、实时熔断下游不可⽤应⽤等。

**完备的实时监控**：可以看到500 台以下规模的集群的汇总也可以看到单机的秒级数据。

**⼴泛的开源⽣态：**与 SpringCloud、Dubbo的整合。您只需要引⼊相应的依赖并进⾏简单的配置即可快速地接⼊ Sentinel。

**区别：**

- Sentinel不会像Hystrix那样放过⼀个请求尝试⾃我修复，就是明明确确按照时间窗⼝来，熔断触发后，时间窗⼝内拒绝请求，时间窗⼝后就恢复。
- Sentinel Dashboard中添加的规则数据存储在内存，微服务停掉规则数据就消失，在⽣产环境下不合适。可以将Sentinel规则数据持久化到Nacos配置中⼼，让微服务从Nacos获取。

| #              | Sentinel                                       | Hystrix                       |
| -------------- | ---------------------------------------------- | ----------------------------- |
| 隔离策略       | 信号量隔离                                     | 线程池隔离/信号量隔离         |
| 熔断降级策略   | 基于响应时间或失败比率                         | 基于失败比率                  |
| 实时指标实现   | 滑动窗口                                       | 滑动窗口（基于 RxJava）       |
| 扩展性         | 多个扩展点                                     | 插件的形式                    |
| 限流           | 基于 QPS，支持基于调用关系的限流               | 不支持                        |
| 流量整形       | 支持慢启动、匀速器模式                         | 不支持                        |
| 系统负载保护   | 支持                                           | 不支持                        |
| 控制台         | 开箱即用，可配置规则、查看秒级监控、机器发现等 | 不完善                        |
| 常见框架的适配 | Servlet、Spring Cloud、Dubbo、gRPC             | Servlet、Spring Cloud Netflix |





#### Config / Nacos

> Nacos是阿⾥巴巴开源的⼀个针对微服务架构中**服务发现**、**配置管理**和**服务管理平台**。
>
> Nacos就是**注册中⼼+配置中⼼**的组合（Nacos=Eureka+Confifig+Bus）
>

**Nacos**功能特性

- 服务发现与健康检查
- 动态配置管理
- 动态DNS服务
- 服务和元数据管理

**保护阈值：**

当服务A健康实例数/总实例数 < 保护阈值 的时候，说明健康实例真的不多了，这个时候保护阈值会被触发（状态true），nacos将会把该服务所有的实例信息（健康的+不健康的）全部提供给消费者，消费者可能访问到不健康的实例，请求失败，但这样也⽐造成雪崩要好，牺牲了⼀些请求，保证了整个系统的⼀个可⽤。

**Nacos** 数据模型（领域模型）

- **Namespace** 代表不同的环境，如开发dev、测试test、⽣产环境prod
- **Group** 代表某项⽬，⽐如爪哇云项⽬
- **Service** 某个项⽬中具体xxx服务
- **DataId** 某个项⽬中具体的xxx配置⽂件

可以通过 Spring Cloud 原⽣注解 `@RefreshScope` 实现配置⾃动更新



#### Bus / Stream

> Spring Cloud Stream 消息驱动组件帮助我们更快速，更⽅便的去构建**消息驱动**微服务的
>
> 本质：屏蔽掉了底层不同**MQ**消息中间件之间的差异，统⼀了**MQ**的编程模型，降低了学习、开发、维护**MQ**的成本，⽬前⽀持Rabbit、Kafka两种消息



#### **Sleuth / Zipkin**

**全链路追踪**

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gmc3avezqrj30xb0lw76z.jpg" alt="image-20210104234058218" style="zoom:67%;" />

**Trace ID**：当请求发送到分布式系统的⼊⼝端点时，Sleuth为该请求创建⼀个唯⼀的跟踪标识Trace ID，在分布式系统内部流转的时候，框架始终保持该唯⼀标识，直到返回给请求⽅

**Span ID**：为了统计各处理单元的时间延迟，当请求到达各个服务组件时，也是通过⼀个唯⼀标识SpanID来标记它的开始，具体过程以及结束。

Spring Cloud Sleuth （追踪服务框架）可以追踪服务之间的调⽤，Sleuth可以记录⼀个服务请求经过哪些服务、服务处理时⻓等，根据这些，我们能够理清各微服务间的调⽤关系及进⾏问题追踪分析。

**耗时分析**：通过 Sleuth 了解采样请求的耗时，分析服务性能问题（哪些服务调⽤⽐较耗时）

**链路优化**：发现频繁调⽤的服务，针对性优化等

**聚合展示**：数据信息发送给 Zipkin 进⾏聚合，利⽤ Zipkin 存储并展示数据。



### **安全认证**

- Session

  认证中最常用的一种方式，也是最简单的。存在**多节点session丢失**的情况，可通过**nginx粘性Cookie**和Redis集中式Session存储解决

- HTTP Basic Authentication 

  服务端针对请求头中base64加密的Authorization 和用户名和密码进行**校验**。

- Token

  Session 只是一个 key，**会话信息存储在后端**。而 Token 中会存储用户的信息，然后通过加密算法进行加密，只有服务端才能解密，**服务端拿到 Token 后进行解密获取用户信息**。

- JWT认证

> JWT（JSON Web Token）用户提供用户名和密码给认证服务器，服务器验证用户提交信息的合法性；如果验证成功，会产生并返回一个 Token，用户可以使用这个 Token 访问服务器上受保护的资源。

<img src="http://s0.lgstatic.com/i/image2/M01/AB/87/CgotOV3WUG2ARl98AAD_xcd-ElM857.png" alt="img" style="zoom:70%;" />

1. 认证服务提供认证的 API，校验用户信息，返回认证结果
2. 通过JWTUtils中的RSA算法，生成JWT token，token里封装用户id和有效期
3. 服务间参数通过请求头进行传递，服务内部通过 ThreadLocal 进行上下文传递。
4. Hystrix导致ThreadLocal失效的问题可以通过，重写 Hystrix 的 Callable 方法，传递需要的数据。

**Token最佳实践**

- 设置**较短（合理）的过期时间**。

- 注销的 Token **及时清除**（放入 Redis 中做一层过滤）。

  虽然不能修改 Token 的信息，但是能在验证层面做一层过滤来进行处理。

- 监控 Token 的**使用频率**。

  为了防止数据被别人爬取，最常见的就是监控使用频率，程序写出来的爬虫程序访问频率是有迹可循的 

- 核心功能敏感操作可以使用**动态验证**（验证码）。

  比如提现的功能，要求在提现时再次进行验证码的验证，防止不是本人操作。

- **网络环境、浏览器**信息等识别。

  银行 APP 对环境有很高的要求，使用时如果断网，APP 会自动退出，重新登录，因为网络环境跟之前使用的不一样了，还有一些浏览器的信息之类的判断，这些都是可以用来保证后端 API 的安全。

- **加密密钥**支持动态修改。

  如果 Token 的加密密钥泄露了，也就意味着别人可以伪造你的 Token，可以将密钥存储在配置中心，以支持动态修改刷新，需要注意的是建议在流量低峰的时候去做更换的操作，否则 Token 全部失效，所有在线的请求都会重新申请 Token，并发量会比较大。



### 灰度发布

**痛点：**

- 服务数量多，业务变动频繁，一周一发布

- 灰度发布能降低发布失败风险，**减少影响范围**

  通过灰度发布，先让一部分用户体验新的服务，或者只让测试人员进行测试，等功能正常后再全部发布，这样能降低发布失败带来的影响范围。 

- 当发布出现故障时，可以**快速回滚**，不影响用户

  灰度后如果发现这个节点有问题，那么只需回滚这个节点即可，当然不回滚也没关系，通过灰度策略隔离，也不会影响正常用户

可以通过Ribbon的负载均衡策略进行灰度发布，可以使用更可靠的Discovery

**Discovery**

> 基于Discovery 服务注册发现、Ribbon 负载均衡、Feign 和 RestTemplate 调用等组件的企业级微服务开源解决方案，包括灰度发布、灰度路由、服务隔离等功能

<img src="https://s0.lgstatic.com/i/image3/M01/54/41/CgpOIF3nXSaAB9bRAAE8rktrUyY037.png" alt="img" style="zoom:50%;" />

1. 首先将需要发布的服务从转发过程中移除，等流量剔除之后再发布。

2. 部分机器中的版本进行升级，用户默认还是请求老的服务，通过版本来支持测试请求。

3. 测试完成之后，让新的版本接收正常流量，然后部署下一个节点，以此类推。

```java
grayVersions = {"discovery-article-service":["1.01"]}
```



### 多版本隔离



<img src="https://s0.lgstatic.com/i/image3/M01/54/41/Cgq2xl3nXSeAZMTOAAE2sCaIhPE668.png" alt="img" style="zoom:50%;" />



**本地复用测试服务**-Eureka Zone亮点

​	**region** 地理上的分区，比如北京、上海等

​	**zone** 可以简单理解为 region 内的具体机房

​	在调用的过程中会优先选择相同的 zone 发起调用，当找不到相同名称的 zone 时会选择其他的 zone 进行调用，我们可以利用这个特性来解决本地需要启动多个服务的问题。

[^]: 当你访问修改的服务 A 时，这个服务依赖了 B、C 两个服务，B 和 C 本地没有启动，B 和 C 找不到相同的 zone 就会选择其他的 zone 进行调用，也就是会调用到测试环境部署的 B 和 C 服务，这样一来就解决了本地部署多个服务的问题。



#### **各组件调优**

当你对网关进行压测时，会发现并发量一直上不去，错误率也很高。因为你用的是默认配置，这个时候我们就需要去调整配置以达到最优的效果。

首先我们可以对容器进行调优，最常见的就是**内置的 Tomcat** 容器了，

```java
server.tomcat.accept-count //请求队列排队数
server.tomcat.max-threads //最大线程数
server.tomcat.max-connections //最大连接数
```

**Hystrix** 的信号量（semaphore）隔离模式，并发量上不去很大的原因都在这里，信号量默认值是 100，也就是最大并发只有 100，超过 100 就得等待。

```java
//信号量
zuul.semaphore.max-semaphores //信号量：最大并发数
//线程池
hystrix.threadpool.default.coreSize //最大线程数
hystrix.threadpool.default.maximumSize //队列的大
hystrix.threadpool.default.maxQueueSize //等参数
```

配置**Gateway**并发信息，

```java
gateway.host.max-per-route-connections //每个路由的连接数 
gateway.host.max-total-connections //总连接数
```

调整**Ribbon** 的并发配置，

```java
ribbon.MaxConnectionsPerHost //单服务并发数
ribbon.MaxTotalConnections   //总并发数
```

修改**Feign**默认的HttpURLConnection 替换成 httpclient 来提高性能

```java
feign.httpclient.max-connections-per-route//每个路由的连接数
feign.httpclient.max-connections //总连接数
```

Gateway+配置中心实现动态路由

Feign+配置中心实现动态日志



# **九、分布式篇**

> 分布式系统是一个硬件或软件组件分布在不同的网络计算机上，彼此之间仅仅通过消息传递进行通信和协调的系统。
>

### **发展历程**

- 入口级负载均衡
  - 网关负载均衡
  - 客户端负载均衡
- 单应用架构
  - 应用服务和数据服务分离
  - 应用服务集群
  - 应用服务中心化SAAS

- 数据库主备读写分离
  - 全文搜索引擎加快数据统计
  - 缓存集群缓解数据库读压力
  - 分布式消息中间件缓解数据库写压力
  - 数据库水平拆分适应微服务
  - 数据库垂直拆分解决慢查询

- 划分上下文拆分微服务
  - 服务注册发现（Eureka、Nacos）
  - 配置动态更新（Config、Apollo）
  - 业务灰度发布（Gateway、Feign）
  - 统一安全认证（Gateway、Auth）
  - 服务降级限流（Hystrix、Sentinel）
  - 接口检查监控（Actuator、Prometheus）
  - 服务全链路追踪（Sleuth、Zipkin）



### CAP

- **一致性**（2PC、3PC、Paxos、Raft）
  - 强一致性：**数据库一致性**，牺牲了性能
    - **ACID**：原子性、一致性、隔离性、持久性
  - 弱一致性：**数据库和缓存**，**延迟双删、重试**
  - 单调读一致性：**缓存一致性**，ID或者IP哈希
  - 最终一致性：**边缘业务**，消息队列
- **可用性**（多级缓存、读写分离）
  - **BASE** 基本可用：限流导致响应速度慢、降级导致用户体验差
    - Basically Availabe 基本可用  
    - Soft state 软状态
    - Eventual Consistency 最终一致性
- 分区容忍性（一致性Hash解决扩缩容问题）



### 一致性

#### XA方案

**2PC**协议：两阶段提交协议，P是指**准备**阶段，C是指**提交**阶段

- 准备阶段：询问是否可以开始，写Undo、Redo日志，收到响应
- 提交阶段：执行Redo日志进行**Commit**，执行Undo日志进行**Rollback** 



**3PC**协议：将提交阶段分为**CanCommit**、**PreCommit**、**DoCommit**三个阶段

**CanCommit**：发送canCommit请求，并开始等待

**PreCommit**：收到全部Yes，写Undo、Redo日志。超时或者No，则中断

**DoCommit**：执行Redo日志进行**Commit**，执行Undo日志进行**Rollback** 

区别是第二步，参与者**自身增加了超时**，如果**失败可以及时释放资源**



#### **Paxos算法**

> 如何在一个发生异常的分布式系统中，快速且正确地在集群内部对某个数据的值达成一致

​	参与者（例如Kafka）的一致性可以由协调者（例如Zookeeper）来保证，**协调者的一致性就只能由Paxos保证了**

Paxos算法中的角色：

- **Client**：客户端、例如，对分布式文件服务器中文件的写请求。
- **Proposer**：提案发起者，根据Accept返回选择最大N对应的V，发送[N+1,V]
- **Acceptor**：决策者，Accept以后会拒绝小于N的提案，并把自己的[N,V]返回给Proposer
- **Learners**：最终决策的学习者、学习者充当该协议的复制因素

```java
//算法约束
P1:一个Acceptor必须接受它收到的第一个提案。
//考虑到半数以上才作数，一个Accpter得接受多个相同v的提案
P2a:如果某个v的提案被accept，那么被Acceptor接受编号更高的提案必须也是v
P2b:如果某个v的提案被accept，那么从Proposal提出编号更高的提案必须也是v
//如何确保v的提案Accpter被选定后，Proposal都能提出编号更高的提案呢
针对任意的[Mid,Vid]，有半数以上的Accepter集合S，满足以下二选一：
  S中接受的提案都大于Mid
  S中接受的提案若小于Mid，编号最大的那个值为Vid
```

![image-20210112225118095](https://tva1.sinaimg.cn/large/008eGmZEly1gmlato63bnj319m0u0wmi.jpg)

面试题：如何保证Paxos算法活性

​	假设存在这样一种极端情况，有两个Proposer依次提出了一系列编号递增的提案，导致最终陷入死循环，没有value被选定

- **通过选取主Proposer**，规定只有主Proposer才能提出议案。只要主Proposer和过半的Acceptor能够正常网络通信，主Proposer提出一个编号更高的提案，该提案终将会被批准。
- 每个Proposer发送提交提案的时间设置为**一段时间内随机**，保证不会一直死循环



#### **ZAB算法**

#### Raft算法

> Raft 是一种为了管理复制日志的一致性算法

Raft使用**心跳机制**来触发选举。当server启动时，初始状态都是**follower**。每一个server都有一个定时器，超时时间为election timeout（**一般为150-300ms**），如果某server**没有超时的情况下收到**来自领导者或者候选者的任何消息，**定时器重启**，如果超时，它就**开始一次选举**。

**Leader异常**：异常期间Follower会超时选举，完成后Leader比较彼此步长

**Follower异常：**恢复后直接同步至Leader当前状态

**多个Candidate：**选举时失败，失败后超时继续选举



#### 数据库和Redis的一致性

**全量缓存保证高效读取**

<img src="/Users/suhongliu/Library/Application Support/typora-user-images/image-20210418185425386.png" alt="image-20210418185425386" style="zoom:50%;" />

所有数据都存储在缓存里，读服务在查询时不会再降级到数据库里，所有的请求都完全依赖缓存。此时，因降级到数据库导致的毛刺问题就解决了。但全量缓存并**没有解决更新时的分布式事务**问题，反而把问题放大了。因为全量缓存**对数据更新要求更加严格**，要求所有数据库**已有数据和实时更新**的数据必须完全同步至缓存，不能有遗漏。对于此问题，一种有效的方案是采用**订阅数据库的 Binlog** 实现数据同步

<img src="/Users/suhongliu/Library/Application Support/typora-user-images/image-20210418185457610.png" alt="image-20210418185457610" style="zoom:50%;" />

​	现在很多开源工具（如**阿里的 Canal**等）可以模拟主从复制的协议。通过模拟协议读取主数据库的 Binlog 文件，从而获取主库的所有变更。对于这些变更，它们开放了各种接口供业务服务获取数据。

<img src="/Users/suhongliu/Library/Application Support/typora-user-images/image-20210418185516743.png" alt="image-20210418185516743" style="zoom:50%;" />

​	将 Binlog 的中间件挂载至目标数据库上，就可以**实时获取该数据库的所有变更数据**。对这些变更数据解析后，便可**直接写入缓存里**。优点还有：

- 大幅提升了读取的速度，降低了延迟

- Binlog 的主从复制是基于 **ACK** 机制， 解决了分布式事务的问题

  如果同步缓存失败了，被消费的 Binlog 不会被确认，下一次会重复消费，数据最终会写入缓存中

**缺点**不可避免：1、增加复杂度 2、消耗缓存资源 3、需要筛选和压缩数据 4、极端情况数据丢失

<img src="/Users/suhongliu/Library/Application Support/typora-user-images/image-20210418185549520.png" alt="image-20210418185549520" style="zoom:50%;" />

可以通过异步校准方案进行补齐，但是会损耗数据库性能。但是此方案会隐藏中间件使用错误的细节，线上环境前期更重要的是记录日志排查在做后续优化，不能本末倒置。



### 可用性

#### **心跳检测**

> 以**固定的频率**向其他节点汇报当前节点状态的方式。收到心跳，说明网络和节点的状态是健康的。心跳汇报时，一般会携带一些附加的**状态、元数据，以便管理**

**周期检测心跳机制**：超时未返回

**累计失效检测机制**：重试超次数



#### **多机房实时热备**

<img src="/Users/suhongliu/Library/Application Support/typora-user-images/image-20210418185610597.png" alt="6.png" style="zoom:50%;" />

两套缓存集群可以分别部署到不同城市的机房。读服务也相应地部署到不同城市或不同分区。在承接请求时，不同机房或分区的读服务只依赖同样属性的缓存集群。此方案有两个好处。

1. **提升了性能。**读服务不要分层，读服务要尽可能地和缓存数据源靠近。
2. **增加了可用。**当单机房出现故障时，可以秒级将所有流量都切换至存活的机房或分区

此方案虽然带来了性能和可用性的提升，但代价是资源成本的上升。











### 分区容错性

> 分布式系统对于错误包容的能力

通过限流、降级、兜底、重试、负载均衡等方式增强系统的健壮性

#### 日志复制

![image-20210114154435003](https://i.loli.net/2021/01/14/fmYEJy9N7Zjp2Xd.png)

1. **Leader**把指令添加到日志中，发起 RPC 给其他的服务器，让他们复制这条信息
2. **Leader**会不断的重试，直到所有的 Follower响应了ACK并复制了所有的日志条目
3. 通知所有的**Follower**提交，同时Leader该表这条日志的状态，并返回给客户端



#### **主备（Master-Slave）**

​	主机宕机时，备机接管主机的一切工作，主机恢复正常后，以自动（**热备**）或手动（**冷备**）方式将服务切换到主机上运行，**Mysql**和**Redis**中常用。

​	MySQL之间数据复制的基础是**二进制日志文件**（binary log fifile）。它的数据库中所有操作都会以**“事件”**的方式记录在二进制日志中，其他数据库作为slave通过一个**I/O线程与主服务器保持通信**，并**监控**master的二进制日志文件的变化，如果发现master二进制日志文件**发生变化**，则会把变化复制到自己的**中继日志**中，然后slave的一个SQL线程会把相关的“事件”**执行**到自己的数据库中，以此实现从数据库和主数据库的**一致性**，也就实现了**主从复制**



#### **互备（Active-Active）**

​	指两台主机**同时运行**各自的服务工作且**相互监测**情况。在数据库高可用部分，常见的互备是**MM**模式。MM模式即**Multi-Master**模式，指一个系统存在多个master，每个master都具有**read-write**能力，会根据**时间戳**或**业务逻辑**合并版本。



#### **集群（Cluster）模式**

​	是指有多个节点在运行，同时可以通过主控节点**分担服务**请求。如Zookeeper。集群模式需要解决主控节点**本身的高可用**问题，一般采用主备模式。



### 分布式事务

#### XA方案 

**两阶段提交** | **三阶段提交**

- 准备阶段的资源锁定，存在性能问题，严重时会造成死锁问题
- 提交事务请求后，出现网络异常，部分数据收到并执行，会造成一致性问



#### TCC方案 

**Try Confirm Cancel / 短事务**

- **Try** 阶段：这个阶段说的是对各个服务的资源做检测以及对资源进行**锁定或者预留**

- **Confirm** 阶段：这个阶段说的是在各个服务中**执行实际的操作**

- **Cancel** 阶段：如果任何一个服务的业务方法执行出错，那么就需要**进行补偿**/回滚

  

#### **Saga方案** 

事务性补偿 / 长事务

- 流程**长**、流程**多**、调用第三方业务

  

#### **本地消息表（eBay）**

#### **MQ最终一致性**	

<img src="https://tva1.sinaimg.cn/large/008eGmZEly1gmr1k3dfbxj31h00pkjy8.jpg" alt="image-20210117220405706" style="zoom:50%;" />

比如阿里的 RocketMQ 就支持消息事务（核心：**双端确认，重试幂等**）

1. A**(订单)** 系统先发送一个 **prepared** 消息到 mq，prepared 消息发送失败则取消操作不执行了
2. 发送成功后，那么执行本地事务，执行成功和和失败发送**确认和回滚**消息到mq
3. 如果发送了确认消息，那么此时 B**(仓储)** 系统会接收到确认消息，然后执行本地的事务
4. mq 会自动**定时轮询**所有 prepared 消息回调的接口，确认事务执行状态
5.  B 的事务失败后自动**不断重试**直到成功，达到一定次数后发送报警由人工来**手工回滚**和**补偿**



#### 最大努力通知方案（订单 -> 积分）

1. 系统 A 本地事务执行完之后，发送个消息到 MQ；
2. 这里会有个专门消费 MQ 的**最大努力通知服务**，接着调用系统 B 的接口；
3. 要是系统 B 执行失败了，就定时尝试重新调用系统 B，**反复 N 次**，最后还是不行就**放弃**



你找一个严格**资金**要求绝对不能错的场景，你可以说你是用的 **TCC 方案**；

如果是一般的分布式事务场景，例如**积分**数据，可以用可靠消息**最终一致性方案**

如果分布式场景**允许不一致**，可以使用最大努力通知方案



### 面试题

#### 分布式Session实现方案

- 基于JWT的Token，数据从cache或者数据库中获取
- 基于Tomcat的Redis，简单配置conf文件
- 基于Spring的Redis，支持SpringCloud和Springb

