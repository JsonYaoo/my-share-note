# **十六、Hadoop 篇**

### 1.1. 什么是大数据？

大数据，big data，或称巨量资料，指的是基于规模巨大的资料，使得可以在合理时间内，通过撷取、管理、处理、整理成为更有价值的信息，典型的例子有，今日头条的个性推荐、百度地图的拥塞推断、《买披萨的故事》等。

### 1.2. 大数据产生的背景？

- 信息技术的进步。
- 云计算的兴起。
- 数据越来越被重视，数据资源化的趋势。

### 1.3. 大数据的 4V 特征？

1. Volume 量大：存储量、计算量大。
2. Variety 多样：来源多、格式多。
3. Velocity 快速：数据增长速度快、处理速度要求快。
4. Value 价值：价值密度低、和数据总量成反比。

### 1.4. 什么是 Hadoop？

Hadoop，是一个由 Apache 开发的分布式系统基础架构，充分利用集群的威力进行分布式存储和计算，用户可以在不了解分布式底层细节的情况下，开发分布式程序。

### 1.5. Hadoop 三大核心组件？

1. HDFS：负责海量数据的分布式存储。
2. MapReduce：是一个计算模型，负责海量数据的分布式计算。
3. YARN：负责集群资源的管理和调度。

### 1.6. Hadoop 集群架构？

| 部署方式     | 部署详情            |
| ------------ | ------------------- |
| 伪分布式集群 | 使用一台 Linux 机器 |
| 分布式集群   | 使用多台 Linux 机器 |

#### 1）伪分布式集群

![1660391039404](D:\Users\yaocs2\AppData\Roaming\Typora\typora-user-images\1660391039404.png)

其中，由于 MapReduce 是一个计算框架，所以不会在图中展示，只在运行时其相关进程才会创建。

1. HDFS 默认端口 9870
2. YARN 默认端口 8088

#### 2）分布式集群

![1660394757523](D:\Users\yaocs2\AppData\Roaming\Typora\typora-user-images\1660394757523.png)

#### 3）客户端节点

Hadoop 客户端节点，允许在业务机器上，操作 Hadoop 集群，避免直接把 Hadoop 集群的节点，暴露给开发人员，造成不安全的问题发生。

![1660397090440](D:\Users\yaocs2\AppData\Roaming\Typora\typora-user-images\1660397090440.png)

### 1.7. 什么是 HDFS？

HDFS，Hadoop Distributed File System，是一种允许通过网络，在多台主机上分享文件的文件系统，可以让多台机器上的多个用户分享文件和存储空间，即共享的分布式文件系统。

- HDFS 只适合大文件，不适合存储小文件。

![1660467170051](D:\Users\yaocs2\AppData\Roaming\Typora\typora-user-images\1660467170051.png)

### 1.8. HDFS Shell 操作？

####  1）命令格式

```
bin/hdfs dfs -XXX     hdfs://bigdata01:9000
			(具体命令) (core-site.xml#fs.defaultFS)
```

#### 2）基本命令 - 查询指定路径

```shell
# 全路径
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -ls hdfs://bigdata01:9000/
[root@bigdata01 hadoop-3.2.0]# 

# 配置了HADOOP_HOME和core-site.xml时, 可简写
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -ls /
[root@bigdata01 hadoop-3.2.0]# 

# 递归查询多级目录
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -ls -R /
-rw-r--r--   2 root supergroup       1361 2022-08-14 16:05 /README.txt
drwxr-xr-x   - root supergroup          0 2022-08-14 16:16 /abc
drwxr-xr-x   - root supergroup          0 2022-08-14 16:16 /abc/xyz
drwxr-xr-x   - root supergroup          0 2022-08-14 16:13 /test
```

#### 3）基本命令 - 上传文件

```shell
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -put README.txt /
put: `/README.txt': File exists
[root@bigdata01 hadoop-3.2.0]#
```

#### 4）基本命令 - 查看文件内容

```shell
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -cat /README.txt
For the latest information about Hadoop, please visit our website at:
...
```

#### 5）基本命令 - 下载文件

```shell
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -get /README.txt README.txt_bak_20220814
[root@bigdata01 hadoop-3.2.0]# ll
总用量 188
drwxr-xr-x. 2 1001 1002    203 1月   8 2019 bin
drwxr-xr-x. 3 1001 1002     20 1月   8 2019 etc
drwxr-xr-x. 2 1001 1002    106 1月   8 2019 include
drwxr-xr-x. 3 1001 1002     20 1月   8 2019 lib
drwxr-xr-x. 4 1001 1002   4096 1月   8 2019 libexec
-rw-rw-r--. 1 1001 1002 150569 10月 19 2018 LICENSE.txt
-rw-rw-r--. 1 1001 1002  22125 10月 19 2018 NOTICE.txt
-rw-rw-r--. 1 1001 1002   1361 10月 19 2018 README.txt
-rw-r--r--. 1 root root   1361 8月  14 16:12 README.txt_bak_20220814
drwxr-xr-x. 3 1001 1002   4096 8月  13 21:12 sbin
drwxr-xr-x. 4 1001 1002     31 1月   8 2019 share
```

#### 6）基本命令 - 创建文件夹

```shell
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -mkdir /test
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -ls /
Found 2 items
-rw-r--r--   2 root supergroup       1361 2022-08-14 16:05 hdfs://bigdata01:9000/README.txt
drwxr-xr-x   - root supergroup          0 2022-08-14 16:13 hdfs://bigdata01:9000/test

# 递归创建文件目录
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -mkdir -p /abc/xyz
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -ls /
Found 3 items
-rw-r--r--   2 root supergroup       1361 2022-08-14 16:05 /README.txt
drwxr-xr-x   - root supergroup          0 2022-08-14 16:16 /abc
drwxr-xr-x   - root supergroup          0 2022-08-14 16:13 /test
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -ls /abc
Found 1 items
drwxr-xr-x   - root supergroup          0 2022-08-14 16:16 /abc/xyz
```

#### 7）基本命令 - 删除文件/文件夹

```shell
# 删除文件
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -rm /README.txt
Deleted /README.txt
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -ls /
Found 2 items
drwxr-xr-x   - root supergroup          0 2022-08-14 16:16 /abc
drwxr-xr-x   - root supergroup          0 2022-08-14 16:13 /test

# 删除文件夹
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -rm -R /test
Deleted /test
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -ls /
Found 1 items
drwxr-xr-x   - root supergroup          0 2022-08-14 16:16 /abc
```

#### 8）高级命令 - 修改权限

```shell
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -ls -R /
-rw-r--r--   2 root supergroup     150569 2022-08-14 16:22 /LICENSE.txt
-rw-r--r--   2 root supergroup      22125 2022-08-14 16:22 /NOTICE.txt
-rw-r--r--   2 root supergroup       1361 2022-08-14 16:22 /README.txt
drwxr-xr-x   - root supergroup          0 2022-08-14 16:28 /abc
drwxr-xr-x   - root supergroup          0 2022-08-14 16:28 /abc/xyz

[root@bigdata01 hadoop-3.2.0]# hdfs dfs -chmod 777 /README.txt
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -chmod 777 /abc
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -ls -R /
-rw-r--r--   2 root supergroup     150569 2022-08-14 16:22 /LICENSE.txt
-rw-r--r--   2 root supergroup      22125 2022-08-14 16:22 /NOTICE.txt
-rwxrwxrwx   2 root supergroup       1361 2022-08-14 16:22 /README.txt
drwxrwxrwx   - root supergroup          0 2022-08-14 16:28 /abc
drwxr-xr-x   - root supergroup          0 2022-08-14 16:28 /abc/xyz
```

#### 9）高级命令 - 查询文件/文件夹大小

```shell
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -ls -R /
-rw-r--r--   2 root supergroup     150569 2022-08-14 16:22 /LICENSE.txt
-rw-r--r--   2 root supergroup      22125 2022-08-14 16:22 /NOTICE.txt
-rwxrwxrwx   2 root supergroup       1361 2022-08-14 16:22 /README.txt
drwxrwxrwx   - root supergroup          0 2022-08-14 16:28 /abc
drwxr-xr-x   - root supergroup          0 2022-08-14 16:28 /abc/xyz

# 查询文件/文件夹大小，单位为字节，第1列表示原始大小，第2列表示所有副本大小之和（这里副本因子配置了2）
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -du /
150569  301138  /LICENSE.txt
22125   44250   /NOTICE.txt
1361    2722    /README.txt
0       0       /abc

# 便于识别地查询文件/文件夹大小，第1列表示原始大小，第2列表示所有副本大小之和（这里副本因子配置了2）
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -du -h /
147.0 K  294.1 K  /LICENSE.txt
21.6 K   43.2 K   /NOTICE.txt
1.3 K    2.7 K    /README.txt
0        0        /abc

# 汇总查询整个目录的大小，第1列表示原始大小，第2列表示所有副本大小之和（这里副本因子配置了2）
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -du -s /
174055  348110  /
```

#### 10）高级命令 - 回收站相关

开启回收站功能：设置文件删除后，还能存活的生命周期，超过这个周期，则会被永久删除。

```xml
<!-- hadoop-xxx/etc/hadoop/core-site.xml -->
<property>
    <name>fs.trash.interval</name>
    <value>1440</value>
</property>
```

```shell
# 删除文件 => 文件不再直接删除，而是进入到回收站中
[root@bigdata01 hadoop]# hdfs dfs -ls /
Found 8 items
-rw-r--r--   2 root supergroup     150569 2022-08-14 16:22 /LICENSE.txt
-rw-r--r--   2 root supergroup      22125 2022-08-14 16:22 /NOTICE.txt
-rwxrwxrwx   2 root supergroup       1361 2022-08-14 16:22 /README.txt
drwxrwxrwx   - root supergroup          0 2022-08-14 16:28 /abc
-rw-r--r--   2 root supergroup  345625475 2022-08-14 18:30 /hadoop-3.2.0.tar.gz
-rw-r--r--   2 root supergroup          0 2022-08-14 18:30 /hello.txt
-rw-r--r--   2 root supergroup          0 2022-08-14 16:48 /k.txt
-rw-r--r--   2 root supergroup          0 2022-08-14 16:48 /m.txt
[[A[root@bigdata01 hadoop]# hdfs dfs -rm /README.txt
2022-08-14 20:26:34,372 INFO fs.TrashPolicyDefault: Moved: 'hdfs://bigdata01:9000/README.txt' to trash at: hdfs://bigdata01:9000/user/root/.Trash/Current/README.txt
# 直接删除文件 => 跳过回收站
[root@bigdata01 hadoop]# hdfs dfs -rm -skipTrash /m.txt
Deleted /m.txt

# 查询回收站
[root@bigdata01 hadoop]# hdfs dfs -ls /user/root/.Trash
Found 2 items
drwx------   - root supergroup          0 2022-08-14 20:26 /user/root/.Trash/220814202900
drwx------   - root supergroup          0 2022-08-14 20:32 /user/root/.Trash/Current

# 清空回收站
[root@bigdata01 hadoop]# hdfs dfs -expunge
2022-08-14 20:32:39,202 INFO fs.TrashPolicyDefault: TrashPolicyDefault#deleteCheckpoint for trashRoot: hdfs://bigdata01:9000/user/root/.Trash
2022-08-14 20:32:39,202 INFO fs.TrashPolicyDefault: TrashPolicyDefault#deleteCheckpoint for trashRoot: hdfs://bigdata01:9000/user/root/.Trash
2022-08-14 20:32:39,220 INFO fs.TrashPolicyDefault: TrashPolicyDefault#createCheckpoint for trashRoot: hdfs://bigdata01:9000/user/root/.Trash
2022-08-14 20:32:39,228 INFO fs.TrashPolicyDefault: Created trash checkpoint: /user/root/.Trash/220814203239
[root@bigdata01 hadoop]# hdfs dfs -ls /user/root/.Trash
Found 2 items
drwx------   - root supergroup          0 2022-08-14 20:26 /user/root/.Trash/220814202900
drwx------   - root supergroup          0 2022-08-14 20:32 /user/root/.Trash/220814203239
```

#### 11）高级命令 - 查询文件头部内容

```shell
# 查询尾部内容
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -head /README.txt

# head本身不支持动态查询
```

#### 12）高级命令 - 查询文件尾部内容

```shell
# 查询尾部内容
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -tail /README.txt
...

# 动态查询尾部内容
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -tail -f /README.txt
...
```

#### 13）高级命令 - 检查文件/文件夹是否存在

```shell
# -e，检查文件是否存在，如果存在，则返回0
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -test -e /README.txt
[root@bigdata01 hadoop-3.2.0]# echo $?
0
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -test -e /README123.txt
[root@bigdata01 hadoop-3.2.0]# echo $?
1

# -z，检查文件是否0字节，如果是，则返回0
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -test -z /README.txt
[root@bigdata01 hadoop-3.2.0]# echo $?
1
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -test -z /abc
[root@bigdata01 hadoop-3.2.0]# echo $?
0

# -d，如果路径是个目录，则返回0，否则返回1
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -test -d /abc
[root@bigdata01 hadoop-3.2.0]# echo $?
0
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -test -d /README.txt
[root@bigdata01 hadoop-3.2.0]# echo $?
1
```

#### 14）高级命令 - 创建空文件

```shell
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -ls /
Found 4 items
-rw-r--r--   2 root supergroup     150569 2022-08-14 16:22 /LICENSE.txt
-rw-r--r--   2 root supergroup      22125 2022-08-14 16:22 /NOTICE.txt
-rwxrwxrwx   2 root supergroup       1361 2022-08-14 16:22 /README.txt
drwxrwxrwx   - root supergroup          0 2022-08-14 16:28 /abc

# touch创建空文件，可用于生成标记文件，比如_SUCCESS等
[[A[root@bigdata01 hadoop-3.2.0]# hdfs dfs -touch /k.txt
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -ls /
Found 5 items
-rw-r--r--   2 root supergroup     150569 2022-08-14 16:22 /LICENSE.txt
-rw-r--r--   2 root supergroup      22125 2022-08-14 16:22 /NOTICE.txt
-rwxrwxrwx   2 root supergroup       1361 2022-08-14 16:22 /README.txt
drwxrwxrwx   - root supergroup          0 2022-08-14 16:28 /abc
-rw-r--r--   2 root supergroup          0 2022-08-14 16:48 /k.txt

# touchz创建空文件，等同于touch，可用于生成标记文件，比如_SUCCESS等
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -touchz /m.txt
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -ls /
Found 6 items
-rw-r--r--   2 root supergroup     150569 2022-08-14 16:22 /LICENSE.txt
-rw-r--r--   2 root supergroup      22125 2022-08-14 16:22 /NOTICE.txt
-rwxrwxrwx   2 root supergroup       1361 2022-08-14 16:22 /README.txt
drwxrwxrwx   - root supergroup          0 2022-08-14 16:28 /abc
-rw-r--r--   2 root supergroup          0 2022-08-14 16:48 /k.txt
-rw-r--r--   2 root supergroup          0 2022-08-14 16:48 /m.txt
```

#### 15）高级命令 - 查看文件内容（text）

```shell
# 效果类似于cat，但text还可以查看zip和TextRecordInputStream格式的数据，而cat只能查看普通文件
[root@bigdata01 hadoop-3.2.0]# hdfs dfs -text /README.txt
For the latest information about Hadoop, please visit our website at:
...
```

#### 16）高级命令 - 安全模式

集群刚启动时，HDFS 先会进入安全模式，此时无法执行写操作，当自检完毕后，则会自动退出安全模式。

```shell
# 查看当前安全模式状态
[root@bigdata01 hadoop]# hdfs dfsadmin -safemode get
Safe mode is OFF

# 刚重启时，删除文件，则删除失败
[root@bigdata02 ~]# hdfs dfs -rm -R /abc
2022-08-14 20:37:53,157 WARN fs.TrashPolicyDefault: Can't create trash directory: hdfs://bigdata01:9000/user/root/.Trash/Current
org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /user/root/.Trash/Current. Name node is in safe mode.

# 手动强制退出安全模式
[root@bigdata01 hadoop]# hdfs dfsadmin -safemode leave
Safe mode is OFF
```

### 1.9. HDFS Java 客户端操作？

#### 1）POM 依赖

```xml
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-client</artifactId>
    <version>3.2.0</version>
</dependency>
```

#### 2）获取配置

```java
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.*;
import org.apache.hadoop.io.IOUtils;

import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;

/**
 * JAVA代码操作HDFS：上传文件、下载文件、删除文件
 *
 * @author yaocs2
 * @since 2022-08-14
 */
public class HdfsOp {

    /**
     * org.apache.hadoop.security.AccessControlException: Permission denied:
     * 解决方案：
     *
     *     <property>
     *         <name>dfs.permissions.enabled</name>
     *         <value>false</value>
     *     </property>
     *
     * @param args
     * @throws IOException
     */
    public static void main(String[] args) throws IOException {
        // 获取配置
        Configuration conf = new Configuration();
        conf.set("fs.defaultFS", "hdfs://bigdata01:9000");// 即core-site.xml中配置的fs.defaultFS

        // 获取HDFS文件系统对象
        FileSystem fileSystem = FileSystem.get(conf);

        // 上传文件
//        put(fileSystem);

        // 下载文件
//        get(fileSystem);

        // 删除文件: true表示递归删除
//        delete(fileSystem);

        // 获取文件所有block块
//        list(fileSystem);
    }
}
```

#### 3）上传文件

```java
    /**
     * 上传文件
     *
     * @param fileSystem
     * @throws IOException
     */
    private static void put(FileSystem fileSystem) throws IOException {
        FileInputStream fis = new FileInputStream("D:\\Users\\yaocs2\\Desktop\\00 test.txt");
        FSDataOutputStream fos = fileSystem.create(new Path("/00 test.txt"));
        IOUtils.copyBytes(fis, fos, 1024, true);
    }
```

#### 4）下载文件

```java
    /**
     * 下载文件
     *
     * @param fileSystem
     * @throws IOException
     */
    private static void get(FileSystem fileSystem) throws IOException {
        FSDataInputStream fis = fileSystem.open(new Path("/README.txt"));
        FileOutputStream fos = new FileOutputStream("D:\\README.txt");
        IOUtils.copyBytes(fis, fos, 1024, true);
    }
```

#### 5）删除文件

```java
    /**
     * 删除文件
     *
     * @param fileSystem
     * @throws IOException
     */
    private static void delete(FileSystem fileSystem) throws IOException {
        if (fileSystem.delete(new Path("/00 test.txt"), true)) {
            System.out.println("删除成功!");
        } else {
            System.out.println("删除失败!");
        }
    }
```

#### 6）获取文件所有 block 块

```java
    /**
     * 获取文件所有block块
     *
     * path: hdfs://bigdata01:9000/hadoop-3.2.0.tar.gz
     * 分块数量：3
     * block_0_location: bigdata02	134217728
     * block_0_location: bigdata03	134217728
     * block_1_location: bigdata03	134217728
     * block_1_location: bigdata02	134217728
     * block_2_location: bigdata02	77190019
     * block_2_location: bigdata03	77190019
     * ==========
     *
     * @param fileSystem
     * @throws IOException
     */
    private static void list(FileSystem fileSystem) throws IOException {
        RemoteIterator<LocatedFileStatus> iterator = fileSystem.listFiles(new Path("/hadoop-3.2.0.tar.gz"), true);
        while (iterator.hasNext()) {
            LocatedFileStatus fileStatus = iterator.next();
            BlockLocation[] blockLocations = fileSystem.getFileBlockLocations(fileStatus, 0, fileStatus.getLen());

            int blockLen = blockLocations.length;
            System.out.println("path: " + fileStatus.getPath().toString());
            System.out.println("分块数量：" + blockLen);

            for (int i = 0; i < blockLen; i++) {
                String[] hosts = blockLocations[i].getHosts();
                long blkLength = blockLocations[i].getLength();
                for(int j = 0; j < hosts.length; j++) {
                    System.out.println("block_" + i + "_location: " + hosts[j] + "\t" + blkLength);
                }
            }

            System.out.println("==========");
        }
    }
```

### 2.0. HDFS 架构原理？

![1660475210252](D:\Users\yaocs2\AppData\Roaming\Typora\typora-user-images\1660475210252.png)

HDFS 主从架构：

1. 主节点称为 NameNode，可以支持多个，相当于老板。
2. 从节点称为 DataNode，也支持多个，相当于工人。
3. 以及 SecondaryNameNode 进程，相当于老板的秘书。

#### 1）NameNode

NameNode 是整个文件系统的管理节点，主要维护整个文件系统的文件目录树、文件/目录的信息、每个文件对应的数据块列表，以及负责接收用户操作请求。

NameNode 维护了 2 份关系，这样 NameNode 就可以找到对应的关系了，即 File -> Block -> DataNode 了：

1. 第一份关系是，File 与 Block list 的关系，对应的关系信息存储在 fsimage 和 edits 文件中，这些文件中的元数据信息，会在 NameNode 启动时加载到内存中。
2. 第二份关系是，DataNode 与 Block 之间的关系，当 DataNode 启动时，会把 DataNode 节点上的 Block 和节点信息，上报给 NameNode。

但是，无论是大文件还是小文件，每一份文件的这些关系数据，都会占用 NameNode 150 字节的内存空间，所以 NameNode 不适合存储小文件。

![1660476267007](D:\Users\yaocs2\AppData\Roaming\Typora\typora-user-images\1660476267007.png)

##### 1、配置路径

```xml
<!-- hadoop-hdfs-xxx.jar#hdfs-default.xml -->
<property>
  <name>dfs.namenode.name.dir</name>
  <value>file://${hadoop.tmp.dir}/dfs/name</value>
  <description>Determines where on the local filesystem the DFS name node
      should store the name table(fsimage).  If this is a comma-delimited list
      of directories then the name table is replicated in all of the
      directories, for redundancy. </description>
</property>

<!-- hadoop-xxx/etc/hadoop/core-site.xml -->
<property>
    <name>hadoop.tmp.dir</name>
    <value>/data/hadoop_repo</value>
</property>
```

##### 2、实际文件

```shell
[root@bigdata01 current]# pwd
/data/hadoop_repo/dfs/name/current

[root@bigdata01 current]# ll
总用量 6184
-rw-r--r--. 1 root root 1048576 8月  13 21:15 edits_0000000000000000001-0000000000000000001
-rw-r--r--. 1 root root 1048576 8月  13 21:20 edits_0000000000000000002-0000000000000000002
-rw-r--r--. 1 root root 1048576 8月  13 21:28 edits_0000000000000000003-0000000000000000003
-rw-r--r--. 1 root root 1048576 8月  13 21:33 edits_0000000000000000004-0000000000000000004
-rw-r--r--. 1 root root    2838 8月  14 16:48 edits_0000000000000000005-0000000000000000042
-rw-r--r--. 1 root root     288 8月  14 17:48 edits_0000000000000000043-0000000000000000046
-rw-r--r--. 1 root root 1048576 8月  14 17:48 edits_0000000000000000047-0000000000000000047
-rw-r--r--. 1 root root      42 8月  14 18:10 edits_0000000000000000048-0000000000000000049
# edits事务文件：记录正在上传的文件信息，以及每个block的上传状态，上传完毕后，则定期合并到fsimage中，包含操作动作、事务ID、块ID、临时路径地址等信息
-rw-r--r--. 1 root root    1643 8月  14 19:10 edits_0000000000000000050-0000000000000000072
-rw-r--r--. 1 root root 1048576 8月  14 19:10 edits_inprogress_0000000000000000073
# fsimage文件和Block之间的关系：包含事务ID、类型(文件/文件夹)、名称、副本数量、修改时间、访问时间、默认块大小、权限、块ID、实际块大小、存储策略等信息
-rw-r--r--. 1 root root     875 8月  14 18:10 fsimage_0000000000000000049
-rw-r--r--. 1 root root      62 8月  14 18:10 fsimage_0000000000000000049.md5
-rw-r--r--. 1 root root    1062 8月  14 19:10 fsimage_0000000000000000072
-rw-r--r--. 1 root root      62 8月  14 19:10 fsimage_0000000000000000072.md5
# seen_txid：记录最后一个事务ID，重启时会从头读取到这个事务ID的文件，读取失败则报错
-rw-r--r--. 1 root root       3 8月  14 19:10 seen_txid
# VERSION：集群版本信息
-rw-r--r--. 1 root root     217 8月  14 15:59 VERSION
```

#### 2）SecondaryNameNode

SecondaryNameNode，主要负责定期地把 NameNode#edits 文件中的内容，合并到 NameNode#fsimage 文件中，这个合并的操作称为 checkpoint，在合并时，会对 edits 中的内容进行转换，生成新的内容保存到 NameNode#fsimage 中。

- 不过要注意的是，在 NameNode 的 HA 高可用架构中，由于同时存在主、备 NameNode，所以文件合并操作会有 StandbyNameNode 备用节点负责，此时不会有 SecondaryNameNode 进程。

#### 3）DataNode

1. DataNode，负责提供真实的文件数据存储服务，它会按照固定的大小，顺序地对文件进行划分并编号，划分好地每一块称为 Block，默认 Block 大小是 128 MB，而如果一个文件小于一个数据块的大小，那么并不会占用整个数据块的存储空间，实际多大就占多大。
2. DataNode 有两个核心概念，一个是 Block，另外一个是 Replication。
3. Replication，多副本机制，默认副本数量为 3，用于保证系统数据的高可靠。

##### 1、Block 配置路径

```xml
<!-- hadoop-hdfs-xxx.jar#hdfs-default.xml -->
<property>
  <name>dfs.datanode.data.dir</name>
  <value>file://${hadoop.tmp.dir}/dfs/data</value>
  <description>Determines where on the local filesystem an DFS data node
  should store its blocks.  If this is a comma-delimited
  list of directories, then data will be stored in all named
  directories, typically on different devices. The directories should be tagged
  with corresponding storage types ([SSD]/[DISK]/[ARCHIVE]/[RAM_DISK]) for HDFS
  storage policies. The default storage type will be DISK if the directory does
  not have a storage type tagged explicitly. Directories that do not exist will
  be created if local filesystem permission allows.
  </description>
</property>

<!-- hadoop-xxx/etc/hadoop/core-site.xml -->
<property>
    <name>hadoop.tmp.dir</name>
    <value>/data/hadoop_repo</value>
</property>
```

##### 2、Block 实际文件

```shell
[root@bigdata02 subdir0]# pwd
/data/hadoop_repo/dfs/data/current/BP-562300349-192.168.56.106-1660396543488/current/finalized/subdir0/subdir0

[root@bigdata02 subdir0]# ll
总用量 340364
-rw-r--r--. 1 root root    150569 8月  14 16:22 blk_1073741826
-rw-r--r--. 1 root root      1187 8月  14 16:22 blk_1073741826_1002.meta
-rw-r--r--. 1 root root     22125 8月  14 16:22 blk_1073741827
-rw-r--r--. 1 root root       183 8月  14 16:22 blk_1073741827_1003.meta
-rw-r--r--. 1 root root      1361 8月  14 16:22 blk_1073741828
-rw-r--r--. 1 root root        19 8月  14 16:22 blk_1073741828_1004.meta
-rw-r--r--. 1 root root 134217728 8月  14 18:30 blk_1073741830
-rw-r--r--. 1 root root   1048583 8月  14 18:30 blk_1073741830_1006.meta
-rw-r--r--. 1 root root 134217728 8月  14 18:30 blk_1073741831
-rw-r--r--. 1 root root   1048583 8月  14 18:30 blk_1073741831_1007.meta
# 存储内容
-rw-r--r--. 1 root root  77190019 8月  14 18:30 blk_1073741832
# 校验文件
-rw-r--r--. 1 root root    603055 8月  14 18:30 blk_1073741832_1008.meta
```

##### 3、Replication 配置路径

```xml
<!-- hadoop-hdfs-xxx.jar#hdfs-default.xml -->
<property>
  <name>dfs.replication</name>
  <value>3</value>
  <description>Default block replication. 
  The actual number of replications can be specified when the file is created.
  The default is used if replication is not specified in create time.
  </description>
</property>

<!-- hadoop-xxx/etc/hadoop/hdfs-site.xml -->
<property>
    <name>dfs.replication</name>
    <value>2</value>
</property>
```

#### 4）HDFS 高可用架构

1. HDFS HA，HDFS 的高可用架构，指一个集群中存在多个 NameNode，只有一个 NameNode 是 Active 状态，表示主节点，其他的都是 Standby 状态，表示备用节点。
2. 主节点负责所有的客户端操作，备用节点负责同步主节点的状态信息，以提供快速故障恢复能力。
3. 因此，在使用 HA 时，不能启动 SecondaryNameNode，否则会报错。

![1660482228528](D:\Users\yaocs2\AppData\Roaming\Typora\typora-user-images\1660482228528.png)

#### 5）HDFS 高扩展架构

Federation 高扩展架构，可以解决单一命名空间的内存不足问题，以支持集群的高扩展性、更高效的性能、以及良好的业务隔离性。

![1660482562065](D:\Users\yaocs2\AppData\Roaming\Typora\typora-user-images\1660482562065.png)

### 2.1. HDFS 源码解析？

#### 1）读数据过程

![1660473872633](D:\Users\yaocs2\AppData\Roaming\Typora\typora-user-images\1660473872633.png)

从 HDFS 中，读数据的过程是这样的： 

1. 客户端向 NameNode 请求下载指定文件， NameNode 获取文件的元数据信息（主要
  是 Block 的存放位置信息），然后返回给客户端。
2. 客户端根据返回的 Block 位置信息，请求对应的 DataNode 节点，获取数据。
3. DataNode 会向客户端传输数据，客户端会以 Packet 为单位接收，先在本地缓
  存，然后写入对应的文件。 

其中，要注意的是：

1. 如果在读数据时，DFSInputStream 和 DataNode 的通讯发生异常，就会尝试正
   在读的 Block 的排第二近的 DataNode，并且会记录哪个 DataNode 发生错误，剩余的 Blocks
   读时，就会直接跳过这些坏的 DataNode 。

1. 而 DFSInputStream 也会检查 Block 数据校验和，如果发现一个坏的 Block，就会先报告到 NameNode 节点，然后 DFSInputStream 在其他的 DataNode 上读该 Block 的镜像。 

因此，设计的原则就是：

1. 客户端直接连接 DataNode 来检索数据。
2. NameNode 来负责为每一个 Block 提供最优的 DataNode，且仅仅处理 Block Location 的请求，这些信息都加载在 NameNode 的内存中，所以，HDFS 通过 DataNode 可以承受大量客户端的并发访问。 

#### 2）写数据过程

![1660474705407](D:\Users\yaocs2\AppData\Roaming\Typora\typora-user-images\1660474705407.png)

向 HDFS 中，写数据的过程是这样的：
1. 客户端首先向 NameNode 发送请求上传文件， NameNode 会做一些基础检查，例
  如：目标文件是否已存在，父目录是否存在等。

2. NameNode 如果检查通过，则会返回该文件的所有 Block 块，以及对应存储的 DataNode 列表，如果检查没通过，则抛异常结束。

  ```java
  blk-001:bigdata01,bigdata02,bigdata03
  blk-002:bigdata01,bigdata02,bigdata03
  ...
  ```

3. 客户端收到 Block 块，以及对应存储的 DataNode 列表之后，就开始向对应的节点中上传数据。

4. 客户端首先上传第一个 Block 块：blk-001 ，它对应的 DataNode 列表是 bigdata01、bigdata02、bigdata03，所以客户端就把 blk-001 先上传到 bigdata01，bigdata01 再把 blk-001 上传到 bigdata02 ， bigdata02 再把 blk-001 上传到 bigdata03 。

5. 客户端接着再上传第二个 Block 块： blk-002 ，流程如上。

6. 当所有的 Block 块都传完以后，客户端会给 NameNode 返回一个状态信息，表示数据已经全部写入成功，或者失败。

7. NameNode 根据客户端返回的状态信息，来判断本次写入数据是成功还是失败，如果成功，则需要对应更新元数据信息。 



