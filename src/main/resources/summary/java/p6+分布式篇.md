# **十二、分布式篇**

### 1.1. 分布式系统？

1. 分布式系统，是一个硬件或者软件分布在不同的计算机上，彼此之间仅仅通过消息传递，进行通信和协调的系统。
2. 提到分布式架构，就一定绕不开 `一致性` 问题，而 `一致性` 又包含了 `数据一致性` 和 `事务一致性` 两种情况，其解决方案在后面都有给出。

### 1.2. CAP 定理？

1. **CAP 定理**，又叫布鲁尔定理，指的是，在一个分布式系统中，最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）三项中的两项。
   - CAP 的适用场景是副本性数据，业务间的不一致性（比如订单和库存的不一致）不在 CAP 的讨论范畴。
2. **C：一致性（Consistency）**，数据在多个副本中保持一致，可以理解成两个用户访问两个系统 A 和 B，当 A系统数据有变化时，及时同步给 B 系统，让两个用户看到的数据是一致的。
   - 强调的是，对某个读操作，必须保证能够返回最新的写操作结果，要求的是**数据强一致性**，要和弱一致性、最终一致性、缓存一致性、业务一致性区分开来。
   - **保证方案**：分布式一致性算法。
3. **A：可用性（Availability）**，系统对外提供服务必须一直处于可用状态，在任何故障下，客户端都能在合理时间内，获得服务端非错误的响应。
   - 强调的是，对某次请求，必须保证在合理的时间内，返回合理的响应（不是错误和超时的响应），要求的是**返回及时**。
   - **保证方案**：接口高性能相关，比如缓存、限流、降级、熔断。
4. **P：分区容错性（Partition tolerance）**，在分布式系统中，遇到任何网络分区故障，系统仍然能对外提供服务。其中，网络分区是指，由于某些原因，子节点之间的网络出现故障，导致不同的节点分布在不同的子网络中，有可能子网络中只有一个节点，这就是网络分区。
   - 强调的是，当出现网络分区后，系统能够继续履行职责，要求的是**分布式和数据同步**。
   - **保证方案**：集群、日志复制、主从复制。

证明，为什么只能满足其中两个，而不能 3 个都满足：

![1647171096774](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647171096774.png)

- 假设，系统 A 和系统 B 是可以通过网络，进行同步数据的。
- 此时，用户 1 和用户 2 分别要访问系统 A 和系统 B，理想情况下，用户 1 访问系统 A 对数据进行修改，将 data1 改成了 data2，同时用户 2 访问系统 B，拿到的数据应该是 data2。
- 但是，由于网络总是不可靠的，涉及网络调用，就需要一一进行分析：
  1. 当网络发生故障时，系统 A 和系统 B 没法进行数据同步，也就是不能满足 P，同时两个系统依然可以访
     问，那么此时其实相当于是单机系统，就不是分布式系统了，因此，对于分布式系统，P 是必须满足的。
  2. 当 P 满足时，如果用户 1 通过系统 A 对数据进行了修改，将 data1 改成了 data2，如果也想让用户 2 通过系统 B 正确地拿到 data2，那么此时就满足了 C，由于 P 的存在，可能会导致一致性同步的时间无限延长，在同步期间，任何人不能访问系统 B，从而导致系统 B 不可用，以保证数据一致性，此时满足的是 CP。
  3. 当 P 满足时，如果用户 1 通过系统 A 对数据进行了修改，将 data1 改成了 data2，如果也想让系统 B 能继续提供服务，那么此时，由于 P 的存在，一致性同步可能需要更多的时间，所以只能牺牲掉一致性，接受系统 A 没有将 data2 同步给系统B，此时满足的就是 AP。
- 因此，分布式系统只能同时满足 CAP 定理中其中 2 个，比如注册中心 Eureka 满足的是 AP，并不能保证 C，Zookeeper 保证了 CP，但不满足 A，而在生产中，A 和 C 的选择，没有正确的答案，应该要取决于自己的业务，比如 12306 系统满足 CP，是因为买票必须满足数据的一致性，不然一个座位多卖了，对铁路运输都是不可以接受的。

### 1.3. Base 理论？

由于 CAP 中一致性 C 和可用性 A 无法兼得，eBay 的架构师，提出了 BASE 理论，它是通过牺牲数据的强一致性，来获得可用性，属于对 CAP 中 AP 方案的一个补充，BASE 理论并没有要求数据的强一致性，而是允许数据在一定的时间段内是不一致的，但在最终某个状态会达到一致。

它具有如下 3 种特征：

1. **基本可用**：Basically Available，分布式系统在出现不可预知故障时，允许损失部分可用性，保证核心功能的可用性。
2. **软状态**：Soft state，软状态也称为弱状态，和硬状态相对，是指允许系统中的数据存在中间状态，并认为该
   中间状态的存在，不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间，进行数据同步的过程存在延时。
3. **最终一致性**：Eventually consistent，最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是，需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。

在生产环境中，很多公司，会采用 BASE 理论来实现数据的一致，因为产品的可用性相比强一致性来说，更加重要。

1. 比如在电商平台中，当用户对一个订单发起支付时，往往会调用第三方支付平台，比如支付宝支付或者微信支付。
2. 调用第三方成功后，第三方并不能及时通知我方系统，在第三方没有通知我方系统的这段时间内，我们给用户的订单状态显示支付中，等到第三方回调之后，我们再将状态改成已支付。
3. 虽然订单状态在短期内存在不一致，但是用户却获得了更好的产品体验。

### 1.4. 分布式数据一致性？

#### 1、数据不一致性产生原因

复制，是导致数据一致性问题的唯一原因。

如果只用一台数据库实例，来处理所有的写入和读取请求，就一定不存在数据一致性的问题。但在中大型项目中，却经常需要将一份数据，存储在超过一台数据库中（即复制），原因有三：

1. **高可用**：即使一部分数据库出现故障，系统也能正常工作。
2. **降低延迟**：使数据与用户在地理上接近。
3. **可扩展性、提高读取吞吐量**：可以扩展处理读请求的机器数量。

这里假设数据集非常小，每台机器的空间都足够保存整个数据集，否则将会引入一个新的话题 `数据分区`。

#### 2、强一致性与弱一致性 | 强弱角度

其实只有两类数据一致性，**强一致性与弱一致性**。

1. **强一致性**：也叫做线性一致性，除此以外，所有其他的一致性都是弱一致性的特殊情况。
2. **弱一致性**：所谓强一致性，即复制是同步的，弱一致性，即复制是异步的。

用户更新网站头像，在某个时间点，用户向主库发送更新请求，不久之后主库就收到了请求。在某个时刻，主库又会将数据变更转发给自己的从库。最后，主库通知用户更新成功。

1. **强一致性举例**：如果在返回“更新成功”并使新头像对其他用户可见之前，主库需要等待从库的确认，确保从库已经收到写入操作，那么复制是同步的，即**强一致性**。
2. **弱一致性举例**：如果主库写入成功后，不等待从库的响应，直接返回“更新成功”，则复制是异步的，即弱一致性。

**优点**：强一致性，可以保证从库有与主库一致的数据。如果主库突然宕机，仍可以保证数据完整。

**缺点**：但如果从库宕机或网络阻塞，主库就无法完成写入操作。

**结论**：

1. 在实践中，通常使一个从库是同步的，而其他的则是异步的。如果这个同步的从库出现问题，则使另一个异步从库同步。
2. 这可以确保永远有两个节点拥有完整数据：主库和同步从库。 这种配置称为**半同步。**

#### 3、最终一致性 | 强弱角度

除了强一致性以外，所有其他的一致性都是弱一致性的特殊情况，最终一致性就是其中一种特例。

- **最终一致性举例**：当用户从异步从库读取时，如果此异步从库落后，他可能会看到过时的信息。

这种不一致只是一个暂时的状态，如果等待一段时间，从库最终会赶上并与主库保持一致吗，这就是最终一致性。

- **最终**：这两个字用得很微妙，反映了从写入主库到同步至从库之间的延迟，可能仅仅是几分之一秒，也可能是几个小时，时延是不确定的。

#### 4、读写一致性 | 读写角度

读写一致性，也称为读己之写一致性。它可以保证，如果用户刷新页面，总会看到自己刚提交的任何更新，虽然不能保证其他用户的写入，他们的更新可能稍等才会看到，但它保证用户自己提交的数据能马上被自己看到。

- **读写不一致性举例**：手机刷虎扑的时候经常遇到，回复某人的帖子然后想马上查看，但我刚提交的回复可能尚未到达从库，看起来好像是刚提交的数据丢失了，很不爽。

如何实现读写一致性？最简单的方案，对于某些特定的内容，都**从主库读**。

- **举例**：知乎个人主页信息只能由用户本人编辑，而不能由其他人编辑。因此，永远从主库读取用户自己的个人主页，从从库读取其他用户的个人主页。

还有一种更好的方法是：

1. 客户端可以在本地记住**最近一次写入的时间戳**，发起请求时带着此时间戳。
2. 从库提供任何查询服务前，需确保该时间戳前的变更都已经同步到了本从库中。
3. 如果当前从库不够新，则可以从另一个从库读，或者等待从库追赶上来。

#### 5、单调一致性 | 多次读角度

单调读，比强一致性更弱，比最终一致性更强，意味着如果一个用户进行多次读取时，绝对不会遇到时光倒流，即如果先前读取到较新的数据，后续读取不会得到更旧的数据。

- **多次读不一致性举例**：用户在从某从库查询到了一条记录，再次刷新后发现此记录不见了，就像遇到了**时光倒流**。如果用户从不同从库进行多次读取，就可能发生这种情况。

实现单调读取的一种方式是：

1. 确保每个用户总是从**同一个节点**进行读取（不同的用户可以从不同的节点读取）。
2. 比如，可以基于用户 ID 哈希值来选择节点，而不是随机选择节点。

#### 6、因果一致性 | 数据分区角度

数据分区（分片）后，每个节点并不包含全部数据。不同的节点独立运行，不存在全局写入顺序。

1. 如果用户A提交一个问题，用户B提交了回答。
2. 问题写入了节点A，回答写入了节点B。
3. 由于同步延迟，发起查询的用户可能会先看到回答，再看到问题。

为了防止这种异常，需要保证**因果一致性**， 即如果一系列写入按某个逻辑顺序发生，那么任何人读取这些写入时，会看见它们以正确的逻辑顺序出现。

- **解决方案举例**：由应用来保证，将问题和对应的回答写入**相同的分区**。

### 1.5. 分布式一致性算法？

#### 1、背景

1. 分布式系统，对分区容错性的一般解决方案是 `state machine replication` 状态机复制。
2. 分布式一致性算法，本质上就是一种 `state machine replication` 状体机复制的共识算法。
3. 分布式系统，有多个节点就会存在节点间通信的问题，存在着两种节点通讯模型：共享内存（Shared memory）和消息传递（Messages passing）传递模型。
4. 以下谈到的分布式一致性算法，都是基于**消息传递**通讯模型实现的，从而保证在分布式系统中，进程间基于消息传递就某个值达成一致。

#### 2、一致性模型

1. 现阶段工业，有 2 种一致性模型：弱一致性和强一致性。
2. 弱一致性中最主要的是最终一致性，对于**最终一致性**最好的体现是 DNS（Domain Name System） 和 Gossip （Cassandra、Redis 的通信协议）。
3. **强一致性**主要有：同步模型（主从同步）和多数派机制模型（Paxos、Raft、Zab）。

#### 3、同步模型

**基本思想**：

1. Master 接受写请求。
2. Master 复制日志至 Slave。
3. Master 等待，直到所有从库返回后，才响应客户端。

**存在的问题**：任意一个从节点返回失败，都会导致 Master 阻塞，导致整个集群不可用，虽然保证了一致性，但可用性大大降低。

#### 4、多数派模型

**基本思想**：

1. 每次写，都保证写入大于 N / 2 个节点。
2. 每次读，都保证从大于 N / 2 个节点中读。

**相关算法**：

![1647235115496](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647235115496.png)

Paxos 算法，是莱斯利 · 兰伯特(Leslie Lamport) 于 1990 年提出的一种基于消息传递的一致性算法，其发展分类有：Basic Paxos、Multi Paxos 和 Fast Paxos，其中工业界用得最多的是 Raft 和 ZAB。

##### 1）Basic Paxos

**算法角色**：

- **Client**： 系统外部角色，请求发起者，像民众。
- **Proposer**： 接受 Client 请求，向集群提出提议（propose），并在冲突发生时，起到冲突调解的作用，像提议员，替民众提出议案。
- **Acceptor**：提议投票和接受者，只有在形成法定人数（Quorum，一般即为 majority 多数派）时，提议才会最终被接受，像国会议员。
- **Learner**：提议接受者，负责 backup 备份工作，对集群一致性没什么影响，像记录员。

![1647235532293](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647235532293.png)

**算法阶段**，有 2 个阶段，每个阶段有 2 个分支阶段：

1. **Prepare**：提出一个提案，编号为N，只有 N 大于此 Propser 之前提出的提案编号（全局递增的一种编号）， 请求才会被 Accpetor 的 Quorum 多数派接受。
2. **Promise**：接受发过来的请求，前提是该请求编号 N 大于之前任何提案的编号。
3. **Accept**：如果 Propser  确认达到了多数派，则会发出 Accept 请求，该请求包含提案编号 N 和提案内容。
4. **Accpeted**：如果 Acccetor 集群在此期间，没有收到任何编号还大于 N 的提案，则接受刚刚的提案，否则忽略。

**算法问题**：

1. **活锁问题**：在议案还没有被接受时，如果再出现新议案编号，那么就会不断出现 Prepare + Promise 讨论新提案，而不是 Accept 接受一个提案。
   - **解决方案**：提供一个 random timeout，其他的提案需要等待一段随机时间，才被讨论。
2. **效率较低**：提交提案、接收提案进行了 2 轮的 RPC 操作，效率较低。
3. **实现难度大，且不容易理解**。

![1647235817187](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647235817187.png)

##### 2）Multi Paxos

**目的**：为了减少角色，简化步骤。

**解决方案**：

1. 由于 Basic Paxos 存在活锁问题，其根因是多个 Proposer 导致的，所以，Multi Paxos 提出了一个新的概念 -> Leader，Leader 是唯一的 Proposer，所有请求都需经过此 Leader。
2. 由于 Basic Paxos 存在两轮 RPC 导致的效率低下问题，所以，Multi Paxos 则通过 Leader 角色 + 在消息中增加一个随机的 Term 任期，使得两轮 RPC 的情况，只在竞选 Leader （选举）时才出现，其余情况（复制）只需要进行一轮 RPC。

![1647236688258](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647236688258.png)

##### 3）Raft

Raft，可以认为是比 Multi Paxos 更简单的一致性算法。

其**算法角色**有：

1. **Leader**：主节点，整个集群只有一个 Leader，所有的写请求都通过 Leader 发送给 Follower。
   - **Term**：在每一个 Leader 的任期期间，都有唯一表示该任期的一个 Term。
2. **Follower**：从节点（跟随角色）。
3. **Candidate**：在 Leader 消息发送失败或宕机，整集群没有 Leader 时，Follower 接收 Leader 的心跳包超时，则它们以 Candidate 身份，开始竞选 Leader。Candidate 只是个中间状态，不会长期存在。

Raft 将分布式问题划分成 3 个小问题：

1. **Leader Election**：主节点选举，集群启动、或者 Leader 心跳包消息无法发送给 Follower 时，触发选主操作。

   集群启动：

   ![1647238020422](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647238020422.png)

2. **Log Replication**：日志复制。

   1. 客户端请求 Leader 写入数据.
   2. Leader 将数据分发到 Follower 中，然后数据被写入 Follower 的内存.
   3. Follower 向 Leader 发送确认消息，Leader 首先提交自己的数据，响应客户端，然后向 Follower 发送提交数据请求。
   4. Follower 收到提交请求后，提交数据。

   ![1647238328150](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647238328150.png)

3. **Safety**：安全恢复。

   - **1）Leader 宕机感知**：

     1. 通过 timeout，来保证 Follower 能正确感知 Leader 宕机或消息丢失的事件，并触发 Follower 竞选Leader。

        ![1647238709444](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647238709444.png)

     2. Leader 需要给 Follower 发送心跳包（heartbeats），数据也是携带在心跳包中，发送给 Follower 的。

        ![1647238617190](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647238617190.png)

   - **2）选主平票情况**：Leader Election 平票时，两个 Candidates 会产生一个随机的 timewait，继续发送下一个竞选消息。

     ![1647238893242](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647238893242.png)

   - **3）脑裂（大小集群）情况**：

     1. 小集群由于没有得到多数派的回复，写操作失败。
     2. 大集群会发生重新选主的过程，且新 Leader 拥有自己新的 Term(任期)，写操作成功。
     3. 当小集群回到大集群时，由于小集群的 Term 小于新集群的 Term，则会同步大集群的信息。

     ![1647238545305](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647238545305.png)

##### 4）ZAB

ZAB，全称是 Zookeeper atomic broadcast protocol，是 ZK 内部用到的一致性协议，基本与 Raft 相同：

1. Zab 将任期 Term 换成了epoch。
2. 用于保证日志连续性的心跳检测，方向 ZAB 改由 Follower 发送至 Leader。

**算法角色**：

1. **Leader**：负责投票的发起和决议、更新系统状态。
2. **Follower**：接受客户端请求、响应客户端结果、参与投票。
3. **Observer**：可以接受客户端请求，将其转发给 Leader，但是不参与投票过程，只同步 Leader 状态，目的是为了扩展系统，提高读取速度。

ZAB 有两种模式，它们分别是恢复模式（选主）和广播模式（同步）。

- **1）恢复模式（选主）**：
  1. 当服务启动或者在领导者崩溃后，ZAB 就进入了恢复模式。
  2. 当领导者被选举出来，且大多数 Follower 完成了和Leader 的状态同步以后，恢复模式就结束了。
  3. 状态同步保证了 Leader 和 Follower 具有相同的系统状态。
  4. ZK 的选举算法有两种：一种是基于 basic paxos 实现的，另外一种是基于 fast paxos 算法实现的，系统默认的选举算法为 fast paxos。
- **2）广播模式（同步）**：
  1. 选完 leader 以后，ZK 就进入状态同步过程。
  2. leader 等待 server 连接。
  3. Follower 连接 leader，将最大的 ZXID（全局递增的事务 ID）发送给leader。
  4. Leader 根据 Follower 的 ZXID 确定同步点。
  5. 完成同步后，通知 Follower 已经成为 uptodate 状态。
  6. Follower 收到 uptodate 消息后，又可以重新接受 client 的请求，继续进行服务了。

### 1.6. 分布式事务一致性？

#### 1）XA 方案 - 2PC 协议 | 数据库实现 | 非 100% 一致

**XA 方案**：

1. XA，是由 X/Open 组织提出的分布式事务规范，由一个事务管理器（TM）和多个资源管理器（RM）组成。
2. 当一个事务跨越多个节点时，为了保持事务 ACID 的特性，需要引入协调者（TM），来统一掌控所有参与者节点（RM）的操作结果，并最终指示这些节点是否要把操作结果进行真正的提交或者回滚。

**两阶段提交**，Two phaseCommit，是指在计算机网络以及数据库领域中，为了使基于分布式系统架构下的所有节点，在进行事务提交时，保持一致性而设计的一种算法，整体思路可以概括为：

1. 参与者将操作成败通知协调者。
2. 再由协调者根据所有参与者的反馈情报，决定各参与者是否要提交操作还是回滚操作。

所谓的两个阶段是指：**准备阶段和提交阶段**。

1. **准备阶段**：

   1. 事务协调者（TM）给每个参与者（RM）发送Prepare消息。
   2. 每个参与者（RM），要么直接返回失败，比如权限验证失败等，要么在执行本地事务，各自写本地的 redo log 和 undo log，但不提交事务。

   ![1647248864385](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647248864385.png)

2. **提交阶段**：

   1. 如果协调者（TM），收到了参与者（RM）的失败通知或者超时，则直接给每个参与者（RM）发送回滚（Rollback）通知；否则，发送提交（Commit）通知。
   2. 参与者（RM），根据协调者（TM）的指令，执行提交或者回滚操作，释放所有事务处理过程中使用的锁资源，注意的是，2 PC 协议是必须在最后阶段，才会释放锁资源。

   ![1647250694317](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647250694317.png)

**缺点**：

1. **同步阻塞问题**：执行过程中，所有参与节点（RM）都是事务阻塞的，当参与者（RM）占有公共资源时，其他第三方节点访问公共资源，将不得不处于阻塞状态。

2. **存在单点故障风险**：一旦协调者（TM）发生故障，参与者（RM）将会一直阻塞下去，尤其在第二阶段，协调者发生故障，那么所有的参与者（RM）还都处于锁定事务资源的状态中，而无法继续完成事务操作。
  - **解决方案**：如果是协调者（TM）挂了，可以重新选举一个协调者（TM），但是无法解决因为协调者（TM）宕机，导致的参与者（RM）处于阻塞状态的问题。
3. **数据不一致**：
  1. 在两阶段提交的二阶段中，当协调者（TM）向参与者（RM）发送 commit 请求后，如果发生局部的网络异常，或者在发送 commit 请求过程中，协调者（TM）发生故障，则会导致只有一部分参与者（RM）接受到了 commit 请求。
  2. 而在这部分参与者（RM）接到 commit 请求之后，就会执行 commit 操作。
  3. 但是，其他部分未接到 commit 请求的参与者（RM），则无法执行事务提交。
  4. 于是，整个分布式系统便出现了数据不一致性的情况。

#### 2）XA 方案 - 3PC 协议 | 数据库实现 | 非 100% 一致

![1647250732842](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647250732842.png)

**3PC 算法流程**：

1. **`can commit` 阶段**：准备阶段，3PC 的 `can commit` 阶段其实和 2PC 的准备阶段很像。协调者（TM）向参与者（RM）发送 commit 请求，如果参与者（RM）可以提交就返回 Yes 响应，否则返回 No 响应。
2. **`pre commit` 阶段**：预提交阶段，协调者（TM），根据参与者（RM）的反应情况，来决定是否可以进行事务的 `preCommit` 操作：
   1. **参与者（RM）情况判断**：如果协调者（TM）从所有的参与者（RM），获得的反馈都是 Yes 响应，那么就会执行事务的预执行。
   2. **发送预提交请求**：协调者（TM）向参与者（RM），发送 `pre commit` 请求，并进入 prepared 阶段。
   3. **事务预提交**：参与者（RM）接收到 `pre commit` 请求后，会执行事务操作，并将 undo log 和 redo log记录到本地的事务日志中。
   4. **响应反馈**：如果参与者（RM）成功执行了事务操作，则返回 ACK 响应给协调者（TM），同时开始等待其最终的指令。
   5. **事务中断**：但是，如果有任何一个参与者（RM），向协调者（TM）发送了 No 响应，或者协调者（TM）等待超时，没有接到参与者（RM）的响应，那么就执行事务的中断：
      - **1）发送中断请求**：协调者（TM）发送中断 abort 请求给所有参与者（RM）。
      - **2）事务中断**：参与者（RM），收到来自协调者（TM）的 abort 请求后，或超时没收到协调者（TM）的任何请求，则执行事务中断，**什么都不用做**。
3. **`do commit` 阶段**：提交阶段，此阶段进行真正的事务提交，也需要分为以下 2 种情况：
   1. **发送提交请求**：协调者（TM），收到参与者（RM）返回的 ACK 响应，则会从预提交状态进入到提交状态，并向所有参与者（RM）发送 `do commit` 请求。
   2. **事务提交**：参与者（RM），接收到 `do commit` 请求后，执行正式的事务提交，并在完成事务提交后，释放所有事务资源。
   3. **响应反馈**：参与者（RM）事务提交完之后，向协调者（TM）再次返回 ACK 响应。
   4. **完成事务**：协调者（TM），再次接收到所有参与者（RM）的 ACK 响应之后，代表本次分布式事务完成。
   5. **中断事务**：但是，如果协调者（TM），没有接收到任意一个参与者（RM）返回的 ACK 响应，可能是响应的不是 ACK，也可能发生了超时，那么就会执行中断事务：
      - **1）发送中断请求**：协调者（TM），向所有参与者（RM）发送 `abort` 请求。
      - **2）事务回滚**：参与者（RM），接收到 `abort` 请求后，利用其在 `pre commit` 阶段记录的 undo log 来执行本地事务的回滚操作，并在完成回滚之后，释放所有的事务资源；但如果超时没收到协调者（TM）的任何请求，则参与者（RM）会进行**事务提交**（因为都到了这一阶段了，大概率是可以提交的）。
      - **3）反馈结果**：参与者（RM），完成事务回滚之后，向协调者（TM）发送 ACK 响应。
      - **4）中断事务**：协调者（TM），接收到参与者（RM）反馈的 ACK 响应之后，执行最终的事务中断。

**2PC 和 3PC 的区别**：

1. **3PC 能够及时释放资源**：3PC 比 2PC，多了一个 `can commit` 阶段，减少了不必要的资源浪费。
   - 因为 2PC 在第一阶段会占用资源，而 3PC 在 `can commit` 阶段不占用资源，只是校验一下 sql，如果不能执行，则直接返回，减少了资源占用。
2. **3PC 引入了参与者（RM）超时机制**：在协调者（TM）和参与者（RM），都引入超时机制。
   - **2PC**：只有协调者（TM）有超时机制，超时后，发送会发送回滚指令。
   - **3PC**：协调者（TM）和参与者（RM）都有超时机制：
     1. **协调者（TM）超时**：`can commit`、`pre commit` 阶段，如果收不到参与者（RM）的反馈，则协调者（TM）会向参与者（RM）发送中断指令，参与者（RM）进行事务中断，什么都不用做。
     2. **参与者（RM）超时**：`pre commit` 阶段，参与者（RM）进行回滚；`do commit` 阶段，参与者（RM）进行事务提交（因为都到了这一阶段了，大概率是可以提交的）。

**总结**：

1. 3PC 相对于 2PC 做了一定的改进，引入了参与者（RM）超时机制，并且增加了 `pre commit` 预提交阶段，使得故障恢复之后，协调者（TM）的决策复杂度降低，但整体的交互过程更长了，性能会有所下降，并且同样还会存在数据不一致的问题。
2. 所以，2PC 和 3PC 都**不能保证数据 100% 一致**，因此，一般都需要有**定时扫描补偿机制**来兜底。
3. 3PC 只是纯理论上的东西，相比于 2PC 只是做了一些优化，但是效果甚微，所以只做了解即可。

#### 3）AT 方案 | Seata 实现 | 默认读未提交，读已提交性能低下

![1647251563240](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647251563240.png)

AT 模式，基于支持本地 ACID 事务的关系型数据库实现：

1. **一阶段 `prepare` 行为**：在本地事务中，一并提交业务数据更新，和相应回滚日志记录。
2. **二阶段 `commit` 行为**：马上成功结束，自动异步批量清理回滚日志。
3. **二阶段 `rollback` 行为**：通过回滚日志，自动解析生成补偿 SQL，完成数据回滚。

**缺点**：

1. 解析回滚日志，生成 SQL 损耗性能。
2. 默认事务隔离级别是读未提交，无法解决脏读。如果设定读已提交（`SELECT FOR UPDATE` 申请全局锁），则性能会直线下降。

#### 4）TCC 方案 | Seata 实现 | 默认读未提交，读已提交性能低下 

TCC，Try-Confirm-Cancel，是一种 Seata 的分布式事务解决方案，它将一个事务拆分成 3 个步骤：

1. **T**：try，业务检查阶段，主要进行业务校验，以及检查或者资源预留，也可以进行一些业务处理。
2. **C**：confirm，业务确认阶段，主要是对 `try` 阶段校验过的业务，或者预留的资源进行确认。
3. **C**：cancel，业务回滚阶段，主要进行与上面相反的业务操作，释放 `try` 阶段预留的资源或者业务。

![1647254230571](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647254230571.png)

**TCC  方案 VS AT 方案**：

1. TCC 方案，相当于 AT 方案的人工版，属于事务补偿型。
2. AT 方案的回滚，是自动解析回滚日志，解析出反向 SQL。
3. 而 TCC 方案则是完全把 `perpare`、`rollback` 和 `commit` 三个方法全都交给开发者来实现。

**TCC 空回滚是解决什么问题的？**

1. 在没有调用 `try` 方法的情况下，调用了二阶段的 `cancel` 方法。
2. 比如，当 `try` 请求由于网络延迟或故障等原因，没有执行，且结果返回了异常时， `cancel` 不能正常执行，只能进行空回滚，因为 `try` 并没有对数据进行修改，如果 `cancel` 正常执行，对数据进行反向修改，那就会导致数据的不一致。
3. **解决思路**：
   1. 关键是要识别出这个空回滚，也就是需要知道 `try` 阶段到底是否执行，如果  `try`  执行了，那就正常回滚，如果  `try`  没有执行，那就空回滚。
   2. 可以让协调者（TM）在发起全局事务时，生成全局事务记录以及分支事务记录， `try` 阶段插入一条记录，表示 `try` 阶段执行了，`cancel` 阶段再读取该记录，如果该记录存在，则正常回滚，如果该记录不存在，则进行空回滚。

**如何解决 TCC 幂等问题？**

1. 为了保证 TCC 二阶段提交，重试机制不会引发数据的不一致，就要求 TCC 的二阶段 `confirm` 和 `cancel` 方法保证幂等性。
2. 这样才不会重复使用或者释放资源，但如果幂等性控制没有做好，很可能会导致数据不一致等严重问题。
3. **解决思路**：
   1. 在上述所说的分支事务记录中，增加执行状态，每次执行前都查询该状态是否已执行，执行过了就不再执行。
   2. 也可以用分布式锁解决。

**如何解决 TCC 中悬挂问题？**

1. 悬挂，就是对于一个分布式事务，其二阶段 `cancel` 方法比 一阶段的 `try` 方法先执行。
2. **出现原因**是，在调用分支事务 `try` 时，由于网络发生拥堵，造成了超时，协调者（TM）就会通知参与者（RM）回滚该分布式事务，可能回滚完成后，`try` 请求才到达参与者（RM）被真正执行。
3. **造成的后果**是，由于一个 `try` 方法预留的业务资源，只有该分布式事务才能使用，此分布式事务最后才执行的 `try`，导致业务资源预留后，无法被继续处理。
4. **解决思路**是：
   1. 如果二阶段执行完成，那一阶段就不能再继续执行。
   2. 在执行一阶段事务时，判断在该全局事务下，分支事务记录表中，是否已经有二阶段的事务记录，如果有，则不再执行 `try` 。

**缺点**：

1. 默认事务隔离级别读未提交，还是会出现脏读问题。
2. 所要编写的代码量，可能会恶心死人。

#### 5）Saga 方案 | Seata 实现 | 默认读未提交，读已提交性能低下

Saga，长事务解决方案，事务一旦 start，各个参与者（RM）按照顺序，一个一个执行自己的逻辑，当其中有一个参与者（RM）执行失败，那么之前已经执行的参与者（RM）都需要**反向逆序**进行回滚。

![1647255581291](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647255581291.png)

**优点**：

1. 一阶段提交本地事务，无锁，高性能。
2. 事件驱动架构，参与者（RM）可异步执行，高吞吐。
3. 补偿服务易于实现。

**缺点**：默认事务隔离级别读未提交，还是会出现脏读问题，不保证隔离性。

**适用场景**：

1. **业务流程长、业务流程多时**。
2. **调用第三方业务**：参与者（RM）包含其它公司或遗留系统服务，无法提供 TCC 模式要求的三个接口时。

#### 6）本地消息表 | MQ 实现 | 最终一致性

本地消息表，其实就是利用了**各系统本地的事务**来实现分布式事务。

1. 本地消息表，顾名思义就是，会有一张存放本地消息的表，一般都是放在数据库中。
2. 然后，在执行业务时，将业务的执行和将消息放入消息表中的操作，包在同一个事务中，以保证消息放入本地表后，业务肯定能够执行成功。
3. 然后，再去调用下一个操作，如果下一个操作调用成功了，那么消息表的消息状态直接改成已成功。
4. 如果调用失败了，那也没事，会有后台任务定时去读取本地消息表，筛选出还未成功的消息，再调用对应的服务，服务更新成功了再变更对应消息的状态。
5. 其中，有可能消息对应的操作不成功，所以，还需要进行重试（要保证服务接口幂等），在超过最大次数时，还需要记录下来，报警，进行人工处理。

可以看到，本地消息表，其实实现的是最终一致性，容忍了数据暂时不一致的情况。

#### 7）可靠消息最终一致性方案 | MQ 实现 | 最终一致性	

可靠消息最终一致性方案，指的是：当事务发起方（消息发送者）执行完本地事务后，发出一条消息，保证事务参与方（消息的消费者）一定能够接受消息，并可以成功处理他自己的事务。

1. **可靠消息**：发起方一定得把消息传递到消费者。
2. **最终一致性**：最终，发起方的业务处理，和消费方的业务处理都得完成，达到数据的最终一致性。

![1647242994399](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647242994399.png)

**解决方案**：比如，阿里 RocketMQ 的消息事务（**双端都要进行确认，重试要保证幂等性**）

1. A（比如订单）系统，先发送一个 `prepared` 消息到 Broker。
2. 如果 `prepared` 消息发送失败，则取消操作，不再执行。
3. 如果 `prepared`  消息发送成功后，则执行 A 的本地事务，执行成功，则发送确认消息到 Broker，执行失败，则发送回滚消息到 Broker，其中，Broker 会**定时轮询**所有 `prepared` 消息回调的接口，以确认事务的执行状态。
4. 如果 Broker 收到了确认消息，则 B （比如仓储） 系统会接收到该事务消息，然后执行 B 的本地事务。
5. 如果 B 事务执行失败，则会不断重试，直到成功，或者达到一定次数后，发送报警，人工介入，来手工回滚或者补偿。

![1647256631363](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647256631363.png)

#### 8）最大努力通知方案 | MQ 实现 | 最终一致性

**概念**：

1. 其实，本地消息表，也可以算作最大努力通知，事务消息也可以算最大努力通知。
2. 本地消息表来说，会有后台任务定时去查看未完成的消息，然后去调用对应的服务，当一个消息多次调用都失败时，可以记录下来，然后人工介入，也可以直接舍弃掉，算最大努力通知。
3. 事务消息也是一样，当 half message 被 commit 了之后，确实就是普通消息了，如果订阅者一直不消费，或者消费不了，则会一直重试，最后进入死信队列，也算是最大努力通知。
4. 所以，最大努力通知，其实就是一种**柔性事务**的思想，我已经尽力我最大的努力，想达成事务的最终一致了。

**执行流程**：

1. 系统 A 本地事务执行完后，发送消息到 MQ。
2. 然后有个专门消费 MQ 的**最大努力通知服务**，去调用系统 B 的接口。
3. 要是系统 B 执行失败了，就会定时尝试重新调用系统 B 的接口，**反复 N 次** 。
4. 最后还是不行的话，就**放弃**。

**注意要点**：

1. **消息重复通知机制**：由于消费可能没有接收到通知，所以需要有一定的机制，对消息进行重复通知。
2. **消息校对机制**：如果尽最大努力，也没有通知到消费方，或者消费方消费消息后要再次消费，就可由消费方，主动向生产方，查询消息信息来满足需求。

**适用场景**：适用于对时间不敏感的业务，比如短信通知。

#### 9）分布式事务总结

1. 可以看出 2PC 和 3PC 是一种**强一致性事务**，不过还是有数据不一致、事务阻塞等风险，且只能用在数据库层面。
2. 而 TCC 则是一种**补偿性事务**的思想，适用范围更广，由于在业务层面实现，所以对业务的侵入性较大，每个分布式事务，都需要实现对应的三个方法。
3. 本地消息表、事务消息和最大努力通知，都属于**最终一致性事务**，所以，适用于一些对时间不敏感的业务。

**比如**：

1. 如果是一个严格资金绝对不能错的场景，可以用 **TCC 方案**。
2. 如果是一个一般的分布式事务场景，比如积分数据，可以用**事务消息**方案。
3. 如果分布式场景允许不一致，可以使用**本地消息表、最大努力通知**等最终一致性方案。

### 1.7. 分布式消息队列？

见《消息队列篇》。

### 1.8. 分布式缓存？

见《Redis篇》。

#### 1、数据库、缓存双写一致性？

##### 1、总

1. 首先，从理论上来讲，给缓存设置**过期时间**，所有写操作以数据库的为准，对缓存操作只尽最大努力更新的话，如果数据库写成功，缓存更新失败，只要缓存到达了过期时间，那么后面的读请求自然会从数据库中，读取到新值，然后回填到缓存，是可以实现**最终一致性**的。
2. 而如果要给出依赖过期时间的方案的话，可以从更新/删除缓存的角度去思考，它们的大前提是，先读缓存，如果缓存没有，才从数据库读取：
   1. 先更新缓存，再更新数据库。（不可取，数据丢失）
   2. 先更新数据库，再更新缓存。（不可取，后者脏数据覆盖）
   3. 先删除缓存，再更新数据库。（不可取，延迟双删、异步双删依然不能保证一致性）
   4. 先更新数据库，再删除缓存。（可取，Cannal + MQ 实现业务解耦、以及最终一致性）

##### 2、分

###### 1）先更新缓存，再更新数据库 | 数据丢失

1. 这个方案会出现，同一个缓存被频繁写入，但还没来得及更新到数据库，造成数据丢失的问题。
2. 故，放弃。

###### 2）先更新数据库，再更新缓存 | 后者脏数据覆盖

1. 这个方案，也是有问题的，如果有请求 A 和请求 B 并发进行更新操作，那么就会出现：
   - （1）线程 A 更新了数据库。
   - （2）线程 B 更新了数据库。
   - （3）线程 B 更新了缓存。
   - （4）线程 A 更新了缓存。
2. 理论上，请求 A 更新缓存，应该比请求 B 更新缓存早才对，但是由于网络等原因，B 却比 A 更早更新了缓存，导致 A 最后才把脏数据刷到缓存中，造成数据不一致。
3. 故，放弃。

显然，删除缓存才是更好的选择。

###### 3）先删除缓存，再更新数据库 | 延迟双删、异步双删依然不能保证一致性

1. 这个方案，也还是有问题的，如果在请求 A 更新时，请求 B 并发查，会出现：
   - （1）请求 A 进行写操作，删除缓存。
   - （2）请求 B 查询，发现缓存不存在，则去数据库查询得到旧值。
   - （3）请求 B 将旧值填入缓存。
   - （4）请求 A 将新值写入数据库。
2. 上述情况就会导致不一致的情形出现，如果不给缓存设置过期时间，则该数据永远都是脏数据。
3. 此时，解决方案可以采用**延时双删**策略：
   - （1）先淘汰缓存。
   - （2）再写数据库（这两步和原来一样） 。
   - （3）关键来了，再**休眠** n 秒，然后淘汰缓存，这么做，可以把 n 秒内，所产生的缓存脏数据，再次删除掉。
4. 但是，这个 n 秒怎么确定，可以在写数据后，休眠的时间在读数据业务逻辑的耗时基础上，加几百 ms 即可，这么做的目的，就是确保读请求结束后，写请求可以删除读请求造成的缓存脏数据。
5. 然而，如果 MySQL 读写分离架构下，还是会出现不一致的情况：
   - （1）请求 A 进行写操作，删除缓存。
   - （2）请求 A 将数据写入数据库了。
   - （3）请求 B 查询缓存，发现缓存没有值，然后去从库查询，但是此时，还没有完成主从同步，因此，查询到的还是旧值。
   - （4）请求 B 将旧值写入缓存。
   - （5）数据库完成主从同步，从库变为新值 。
6. 上述情况也产生了数据不一致的现象，解决方法还是使用**延时双删**策略，只不过，休眠时间 n，需要在主从同步的延时时间基础上，加几百 ms，而不是读耗时加几百 ms。
7. 但是，采用这种同步淘汰策略，由于设计到阻塞休眠 n s，接口吞吐量将会降低很多，此时可以把第二次休眠后删除的步骤，改为**异步**的操作，即起一个线程做异步删除。这样，写请求就不用沉睡一段时间后才返回响应。
8. 如果采用**异步双删**，虽然保证了吞吐量，但第二次可能会**删除失败**，比如：
   - 为了方便，假设是单库。
   - （1）请求 A 进行写操作，删除缓存。
   - （2）请求 B 查询发现缓存不存在，则去数据库查询得到旧值。
   - （3）请求 B 将旧值写入缓存。
   - （4）请求 A 将新值写入数据库。
   - （5）请求 A 试图去删除，但由于某种原因失败了，导致缓存中一直存在 B 放入的旧值。
9. 这种情况下，如果第二次删除缓存失败，还是会出现后面缓存和数据库不一致的现象。
10. 所以，更新数据库前的缓存删除，起不到任何作用，一致性是由第二次缓存删除来保证的。
11. 故，放弃更新前缓存删除方案。

###### 4）先更新数据库，再删除缓存

1. 这种情况，也还是会存在并发问题：
   - （1）缓存刚好失效。
   - （2）请求 A 查询数据库，得一个旧值。
   - （3）请求 B 将新值写入数据库。
   - （4）请求 B 删除缓存。
   - （5）请求 A 将查到的旧值写入缓存。 
2. 上述情况，确实还是有脏数据，解决方案有：
   1. **过期时间**：给缓存设有效时间是一种方案。
   2. **异步删除**：采用上面的异步删除策略，保证读请求完成以后，再进行删除操作也可以，但同样，也存在缓存删除失败导致数据不一致的情况，此时可以提供一个**重试删除**机制即可解决。

##### 3、总

1. 因此，要保证数据库、缓存双写一致性的关键在于，**先更新数据库 + 再删除缓存 + 异步重试删除**，实现方案如下：

2. **方案一**：

   - （1）更新数据库后，再删除缓存。
   - （2）如果缓存删除失败，则将需要删除的 key，丢到消息队列中。
   - （3）然后消费消息，获取到要删除的 key。
   - （5）接着根据 key 继续重试删除操作，直到成功。

   **缺点**：对业务线代码，造成了大量的侵入。

   ![1647257594150](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647257594150.png)

3. **方案二**： 启动一个订阅程序，去订阅数据库的 **binlog**，获得需要操作的数据，然后另起一个程序，获得这个订阅程序传来的数据，进行删除缓存操作。

   - （1）更新数据库数据。
   - （2）数据库则会把更新操作的信息，写入 binlog 日志当中。
   - （3）binlog 日志被订阅程序订阅到，则提取出所需要的数据以及 key。
     - 这个订阅 binlog 程序，在 MySQL 有现成的中间件（阿里# Canal），至于 Oracle 目前好像还没有现成的中间件。
   - （4）调用另一段非业务代码，获得 key。
   - （5）根据这个 key，尝试执行缓存删除操作。
   - （6）如果缓存删除失败，则将需要删除的 key，丢到消息队列中。
   - （7）然后消费消息，获取到要删除的 key。
   - （8）接着根据 key 继续重试删除操作，直到成功。

   ![1647257612877](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647257612877.png)

=> 以上，就是我对数据库、缓存双写一致性的一些实现方案的理解，请问有什么细节需要补充的吗？

### 1.9. 分布式锁？

#### 1、总

分布式锁指的是，在不同的系统或者同一个系统的不同主机之间，共享访问某个资源时，用来互斥地防止彼此干扰保证一致性的锁实现，其实现方式有：

- **基于数据库实现**：通过乐观锁，或者唯一索引实现。
- **基于分布式缓存实现**： 典型的有，通过 Redis 实现。
- **基于分布式一致性算法实现**：典型的有，通过 ZK 实现。

#### 2、分

##### 1）基于数据库实现 | 负担大

- **基于乐观锁实现**：原理是，根据版本号，来判断更新之前有没有其他线程更新过，如果被更新过，则获取锁失败。
- **基于唯一索引实现**：原理是，在表上建立唯一索引，当想要获得锁时，向表中插入一条记录，释放锁时则删除这条记录。
  - **缺点**：
    1. **锁没有失效时间**，解锁失败会导致死锁，此时该唯一索引所有 insert 都会返回失败，其他线程无法再获取到锁。
    2. **不可重入**，同一线程在没有释放锁之前无法再获取到锁。

##### 2）基于分布式缓存实现 | 锁失效

- **基于 Redis 单机实现**：使用 `SET NX EX` 指令加锁，保证**原子性**地给锁设置**过期时间，防止死锁**，使用 LUA 脚本 `redis.call` 指令，保证 key 值判断与删除键指令原子性执行，且防止由于 STW 时间过长，锁被其他进程误删。

  - **缺点**：会出现由于**时钟漂移** 或者 任务执行时间过长，导致的锁被提前释放的问题。

- **基于 Redisson 实现**：它是一个 Redis 的客户端，其分布式锁的实现原理是，让获得锁的线程开启一个定时守护线程，每隔 expireTime / 3 的时间就去检查一下，该线程持有的锁是否还存在，如果存在，则对锁的过期时间重新设置为 expireTime，完成守护线程对**锁的续约**，防止锁由于过期提前释放。

  - **缺点**：这些只是在 Redis 单机实现的分布式锁，加锁时只作用在一个 Redis 节点上，即使通过了 Sentinel 保证了高可用，但由于 Redis 是**异步复制**的，如果在 Master 节点获取到锁后，在未完成数据同步的情况下，发生了故障转移，那么其他客户端上的线程依然可以获取到锁，丧失了锁的安全性。

- **基于 RedLock 算法实现**：红锁算法的原理是，

  1. 先获取当前时间 `t1`，然后按顺序依次尝试从 n 个 Redis 实例，使用相同的 key 和具有唯一性的 value（例如 UUID）来获取锁，当向 Redis 请求获取锁时，除了设置锁的失效时间 `expire`，还应该设置超时时间 `timeout`，且这个超时时间 **<** 锁的失效时间 `expire` ，这样可以避免 Redis 已经挂掉的情况下，客户端不用一直等待响应结果，而是尽快地去尝试另外一个 Redis 实例来获取锁。
  2. 客户端通过使用当前时间 `t3` 减去开始获取锁时间 `t1`，就得到获取锁花费的总时间 `T`，当且仅当从过半数（N/2+1 个）的 Redis 节点都取到锁，并且获取锁花费的总时间 `T` **<** 锁失效时间 `expire` 时，锁才算获取成功，如果获取到了锁，那么 key 的真正有效时间 `real_expire` 等于锁失效时间 `expire` **减去**锁花费的总时间 `T`。
  3. 如果获取锁失败，客户端则应该在所有的 Redis 实例上使用 **Lua 脚本进行解锁**，原因是可能存在某个节点加锁成功后，**返回客户端时**的响应包丢失了，即客户端到服务器的通信是正常的，但反方向却是有问题的，虽然对客户端而言，由于响应超时导致加锁失败，但是对 Redis节点而言，`SET` 指令执行成功，意味着加锁成功，因此，释放锁时，客户端也应该对当时获取锁失败的那些 Redis 节点同样发起解锁请求。

  - **缺点**：
    1. **性能过重**：使用 RedLock 需要维护那么多的 Redis 实例，提升了系统的维护成本。
    2. **仍然不安全**：RedLock 严重依赖系统时钟，如果 Master 系统时间发生回调，则会导致它持有的锁提前过期释放，还是不能保证锁的安全性，这个是基于时间来实现自动释放的分布式锁，都无法解决的问题。

##### 3）基于分布式一致性算法实现 | 强一致

基于 ZK 实现：可以利用**顺序临时节点**的特性，结点在创建时，会自动在结点名后加一个数字后缀，以保证有序，同时，如果客户端连接失效，则还会立即删除结点，再利用 **watcher 监视器**的特性，注册某个结点的监视器，当节点状态发生改变时，watcher 被触发时，ZK 会向客户端发送一条通知。其分布式锁的实现原理是，

1. 创建一个锁目录 lock，希望获得锁的线程 A 在 lock 目录下，将创建**顺序临时结点**。
2. A 先获取锁目录下所有的子结点，判断是否存在序号比自己小的结点，如果不存在，则说明当前线程的顺序号最小，则线程 A 获得锁。
3. 当另外一个线程 B 获取锁时，判断到 B 自己不是最小的结点，存在有更小的线程 A 结点，则设置  watcher 监听器，只监听比自己**次小**的结点 A。
4. 当线程 A 处理完业务后，会删除结点 A，释放掉分布式锁，然后线程 B 监听到节点状态变更事件后，判断自己已经是最小的结点了，则成功获得锁。

#### 3、总

以上，就是我对分布式锁一些实现方案的理解，总结一下就是，

- **基于数据库实现**：
  - **优点**：直接使用数据库，使用简单。
  - **缺点**：但这样会增加数据库的负担。
- **基于分布式缓存实现**：
  - **优点**：属于 AP 模型，性能高，实现起来较为方便，在允许偶发性的锁失效情况发生，不影响系统正常使用时，可以采用分布式缓存来实现锁。
  - **缺点**：通过过期时间实现的锁超时机制不是十分可靠，当业务必须要数据的**强一致性**，不允许重复获得锁时，比如金融场景的重复下单与重复转账场景下，就不能使用分布式缓存来实现锁了，此时可以使用 CP 模型来实现，比如 Zookeeper。
- **基于分布式一致性算法实现**：
  - **优点**：不依靠过期时间来释放锁，可靠性高，当系统要求高可靠性时，可以采用分布式一致性算法来实现锁。
  - **缺点**：性能比不上分布式缓存实现的锁，因为 ZK 需要频繁的创建和删除结点。

### 2.0. 分布式全局 ID？

#### 1、总

1. 在分库分表环境中，由于表中数据同时存在不同数据库中，平时使用的自增主键 ID 将无用武之地，因为某个分区数据库自生成的 ID 无法保证全局唯一。
2. 因此，需要单独设计全局主键，来避免跨库主键重复的问题，我了解到的方案有：UUID、MyISAM ID 表、高可用 ID 服务器、Snowflake 分布式自增 ID 算法、以及美团的 Leaf 分布式 ID 生成系统。

#### 2、分

##### 1）UUID

UUID 标准形式包含 32 个 16 进制数字，分为 5 段，形式为 8­4­4­4­12 的 36 个字符，比如：550e8400­e29b­41d4­a716­446655440000。

- **优点**：方案最简单，且本地生成，性能高，没有网络耗时。
- **缺点**：
  1. 由于 UUID 非常长，会占用大量的存储空间。
  2. UUID 作为主键，建立索引和基于索引进行查询时，都会存在性能问题，在 InnoDB 下，UUID 的无序性会引起数据位置频繁变动，导致页分裂。

##### 2）MyISAM ID 表

```sql
-- 使用MyISAM存储引擎建立ID表
CREATE TABLE `sequence` (  
  ìd` bigint(20) unsigned NOT NULL auto_increment,  
  `stub` char(1) NOT NULL default '', 
  PRIMARY KEY  (ìd`),  
  UNIQUE KEY `stub` (`stub`)  
) ENGINE=MyISAM;

-- 先删除再获取自增ID
REPLACE INTO sequence (stub) VALUES ('a');  
SELECT LAST_INSERT_ID();
```

- **概念**：
  1. `stub` 字段（存根）设置为**唯一索引**，同一 `stub` 值在 `sequence` 表中只有一条记录，支持同时给多张表生成全局 ID。
  2. 使用 MyISAM 存储引擎而不是 InnoDB，可以获取更高的性能，因为 MyISAM 使用的是表级锁，对表的读写是串行的，不用担心在并发时两次读取同一个 ID 值的问题。
  3. 使用 `REPLACE INTO + SELECT` 来获取自增 ID，保证两操作在同一事务内，它会先删除旧数据，再生成新数据，从而实现主键自增。
- **优点**：实现简单。
- **缺点**：
  1. 存在单点问题，且强依赖 DB，当 DB 异常时，会导致整个分布式系统都不可用。
  2. 虽然可以配置主从来增加可用性，但当主库挂了，主从切换时，数据一致性难以得到保证。
  3. 因此，整个系统的性能瓶颈，被限制在单台 MySQL 的读写性能上。

##### 3）高可用 ID 服务器

![1631524708834](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631524708834.png)

- **背景**：Flickr（弗里克，雅虎的一个图片分享网站）团队使用的一种主键生成策略，与上面的 `sequence` 表方案类似，但可以更好地解决了单点故障和性能瓶颈的问题。

- **思想**：

  1. 建立 2 个以上的全局 ID 生成的服务器，每个服务器上只部署一个数据库，每个库有一张 `sequence` 表用于记录当前全局 ID。
  2. 表中 ID 增长的**步长相同，等于库的数量，起始值依次错开**，这样能将 ID 的生成散列到各个数据库上，比如第一台为（1，3，5，7，...）以及第二台为（2，4，6，8，...）等等。

- **优点**：生成 ID 的压力，能够均匀分布在多台机器上，同时提高了系统的容错能力，当第一台出现了错误，可以自动切换到第二台机器，来获取 ID。

- **缺点**：

  1. 系统添加机器水平扩展时，需要停止原本正在运行的 ID 服务器，以**修改步长**。
  2. 每次获取 ID 都要读写一次 DB，DB 的压力还是很大，只能靠堆机器来提升性能。

- **优化方案 **：**批量获取 ID**。

  - 使用批量获取的方式，可以降低数据库的写压力，每次获取**一段**区间的 ID 号段，用完之后再去数据库获取，可以大大减轻数据库的压力。
    - 1）比如，还是使用 2 台 DB 保证可用性，数据库中只存储当前的最大 ID。
    - 2）ID 生成服务，每次批量获取 6 个ID，可以先将 max_id 修改为 5，当应用访问 ID 生成服务时，就不需要访问数据库，从**号段缓存**中依次派发 0~5 的 ID。
    - 3）当这些 ID 发完后，再将 max_id 修改为 11，下次就能派发 6~11 的ID。
    - 4）这样，数据库的压力降低为原来的 1/6。
  - **缺点**：ID 生成服务需要维护最大 ID 值，再下次生成 ID 时，需要告诉 DB M1、DB M2 各自的初始值。

  ![1631525249951](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631525249951.png)

##### 4）Snowflake 分布式自增 ID 算法

Twitter 的 snowflake 算法，解决了分布式系统生成全局 ID 的需求，可以生成 64 位的 long 类型的数值。

![1631525530303](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631525530303.png)

- **概念**：1 + 41 + 10 + 12  = 64位。
  1. 首先是，第 1 位不使用。
  2. 接下来是， 41 位的毫秒级时间戳，最大可以表示 **69年** 的时间。
  3. 然后是，5 位的 `datacenterId`，5位的 `workerId`，这 10 位长度，最多支持部署**1024 个节点**。
  4. 最后是，12 位是毫秒内的计数值，最大支持每个节点、每毫秒产生**4096 个 ID 序列**。
- **优点**：
  1. 毫秒数在高位，生成的 ID 整体上按时间趋势是**递增**的，还可以根据自身业务灵活分配 bit 位。
  2. 不依赖第三方系统，稳定、效率高，理论上 QPS 约为 409.6 w/s（2^12 * 1000 ms），且整个分布式系统中不会产生 ID 碰撞。
- **缺点**：强依赖于机器时钟，如果时钟回拨，则可能导致生成的 ID 重复。

##### 5）美团点评分布式ID生成系统 - Leaf

Leaf，服务美团点评公司内部产品，包含金融、支付交易、餐饮、外卖、酒店旅游、猫眼电影等众多业务线，其性能在 4 C 8 G 的机器上，QPS 能压测到近 5 w/s，TP 999 1ms，能够满足大部分的业务需求。

![1631532240042](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631532240042.png)

###### 1、Leaf - segment ID 服务器方案

- **思想**：
  1. 获取 ID 时，向 proxy server 代理服务器批量获取 ID，每次获取一个 segment 号段（由 `step` 决定大小）的值。
  2. 用完之后，再去获取新的号段，可以大大减轻数据库的压力。
- **实现**：
  1. `biz_tag` 用来区分业务，`max_id` 表示该 `biz_tag` 目前所被分配的 ID 号段的最大值，`step` 表示每次分配的号段长度。
  2. 比如，`test_tag` 在第 1 台 Leaf 机器上是 1~1000 的号段，当这个号段用完时，会去加载另一个长度为step=1000 的号段，而如果另外机器的号段都没有更新的话，此时第 1 台机器会重新加载 3001~4000 的号段，同时，数据库对应的 `biz_tag` 这条数据的 `max_id` 会从 3000 被更新成 4000。
  3. 这样，各业务不同的发号需求用 `biz_tag` 字段来区分，每个 `biz-tag` 的 ID 相互隔离，互不影响，如果以后有性能要求，需要对数据库进行扩容时，则不用复杂的扩容操作，只需要对 `biz_tag` 分库分表即可。
  4. 而且，对比原来获取 ID 每次都需要写数据库，现在只需要把 `step` 设置得足够大，比如 1000，那么只有当 1000 个号被消耗完了之后，才会去重新读写一次数据库，此时读写数据库的频率从1  减小到了 **1 / step** 。
- **优点**：
  1. Leaf 服务可以很方便的进行**线性扩展**，性能完全能够支撑大多数业务场景。
  2. 生成的 ID 是**趋势递增**的 8 byte 的数字，满足上述数据库存储的主键要求。
  3. 容灾性高，Leaf 服务内部有**号段缓存**，即使 DB 宕机，短时间内，Leaf 仍能正常对外提供服务。
  4. 可以自定义 `max_id` 的大小，非常方便业务从原有的 ID 方式上**迁移**过来。
- **缺点**：
  1. **TP 999 数据波动大**：当号段使用完之后，还是会 hang 在更新数据库的 I/O 上，TP 999 数据会出现偶尔的尖刺。
  2. **高可用得不到保障**：DB 宕机会造成整个系统不可用。
  3. **生成的 ID 不够随机**：会泄露发号数量的信息，不太安全。

###### 2、双 buffer 优化方案

目的是，优化第 1 个缺点，当号段使用完、线程取号时阻塞的问题。

![1631532711485](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631532711485.png)

- **背景**：
  1. Leaf 取号段的时机，是在号段消耗完时进行的，意味着号段临界点的 ID 下发时间，取决于下一次从 DB 取回号段的时间，并且在这期间，进来的请求也会因为 DB 号段没有取回来，导致线程阻塞。
  2. 假如 Leaf 服务取 DB 时，网络发生抖动，或者 DB 发生慢查询，就会导致整个系统的响应时间变慢。
- **思想**：
  1. 为了让 DB 取号段的过程能够做到无阻塞，不会在 DB 取号段时阻塞请求线程，可以让发号段消费到某个点时，就**异步**的把下一个号段加载到内存中，而不需要等到号段用尽时，才去更新号段。
- **实现**：
  1. 采用双 buffer 的方式，Leaf 服务内部有两个号段缓存区 segment。
  2. 当前号段已下发 10% 时，如果下一个号段未更新，则**异步**另启一个更新线程去更新下一个号段。
  3. 当前号段全部下发完后，如果下个号段准备好了，则切换到下个号段为当前 segment 接着下发，循环往复。

###### 3、高可用容灾方案

目的是，解决第 2 个缺点，DB 宕机会造成整个系统不可用的问题。

![1631543929139](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631543929139.png)

**DB 高可用方案**：

1. 采用 1 主 2 从的方式，同时分机房部署，Master 和 Slave 之间采用半同步复制的方式，进行数据同步，同时使用公司的 DBProxy（原 Atlas）数据库中间件，做主从切换。
2. 当然，这种方案在一些情况会退化成**异步模式**，甚至在非常极端情况下仍然会造成**数据不一致**的情况，但是出现的概率非常小。
3. 如果系统确实要保证 100% 的数据强一致，可以选择使用**类Paxos算法**，实现强一致 MySQL 的方案，但这样运维成本和精力都会相应的增加，应该需要根据实际情况进行选型。

**应用高可用方案**：

1. Leaf 服务分 IDC 部署，内部的服务化框架是 `MTthrift RPC`。
2. 服务调用时，根据负载均衡算法，优先调用同机房的 Leaf 服务。
3. 如果该 IDC 内，Leaf 服务不可用，则会选择其他机房的 Leaf 服务。
4. 同时，服务治理平台 `OCTO` ，还提供了针对服务的过载保护、一键截流、动态流量分配等服务保护措施。

###### 4、Leaf-snowflake 方案

目的是，优化第 3 个缺点，生成的 ID不够随机，会泄露发号数量信息，不太安全的问题。

![1631544317384](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631544317384.png)

Leaf-snowflake 方案，完全沿用了 snowflake 方案的 bit 位设计，即是 `1+41+10+12` 的方式组装 ID 号。

1. 对于 `workerID` 的分配，当服务集群数量较小时，完全可以手动配置。
2. 但当 Leaf 服务规模较大时，动手配置成本太高，此时可以使用 ZK 持久顺序节点的特性，自动对 snowflake 节点配置 `wokerID`。

**对接 ZK 的步骤**：

1. 启动 Leaf-snowflake 服务时，会去连接 ZK，在 `leaf_forever` 父节点下，检查自己是否已经注册过，是否有该节点的持久化顺序子节点。
2. 如果有注册过，则直接取回自己的 `workerID`，启动服务。
3. 如果没有注册过，则在该节点下，创建一个持久化顺序节点，创建成功后，取回顺序号当做自己的 `workerID` 号，启动服务。
4. 除了每次会去 ZK 拿数据以外，也会在本机文件系统上，缓存一个 `workerID` 文件，当 ZK 出现问题且恰好 Leaf-snowflake 服务机器也需要重启时，还能保证服务正常启动，做到了对 ZK 的**弱依赖**，一定程度上提高了 SLA 。

![1631544564754](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631544564754.png)

**解决 snowflake 时钟回退问题**：

由于 snowflake 强依赖机器时间，如果其发生了回拨，则可能会生成重复的 ID，解决方案为：

1. 首先，服务启动时，先检查自己是否写过 `ZK#leaf_forever` 节点。
   - 1）如果写过，则用自身系统时间与 `leaf_forever/#{self}` 节点记录时间做比较，且小于 `leaf_forever/​#{self}` 的时间，则认为当前机器时间发生了回拨，服务启动失败并报警。
   - 2）如果没写过，则证明是新服务节点，此时需要创建持久顺序节点 `leaf_forever/#{self}` ，并写入自身系统的时间。
2. 接下来，综合对比其余 Leaf 节点的系统时间，来判断自身系统时间是否准确，具体做法是：
   - 1）取 `leaf_temporary` 下的所有临时节点（**所有运行中的** Leaf-snowflake节点）的服务 IP+端口。
3. 然后，通过 RPC 请求，得到它们各自的系统时间，计算 `at = sum(time) / nodeSize`。
4. 如果计算结果 `at < 阈值`，认为当前系统时间准确，可以正常启动服务，同时写临时节点 `leaf_temporary/#{self}` ，每隔一段时间（3s），上报自身系统时间，并写入到 `leaf_forever/#{self}` 中，以维持租约。
5. 否则，则认为本机系统时间发生大步长的偏移，启动失败并报警。

#### 3、总

以上，就是我对分布式全局 ID 的一些实现方案的理解，请问有什么细节需要补充的吗？

### 2.1. 分布式幂等性？

#### 1、总

1. 幂等，idempotence，是一个数学与计算机学的概念，常见于抽象代数中。
2. 在编程中，一个幂等操作的特点是，**该操作被任意多次执行，其所产生的影响，都与一次执行的影响相同**。
3. 所以，幂等性的方法是指，可以使用相同参数，重复执行，并还能获得相同结果的方法，这些方法不会影响系统的状态，也不用担心重复执行后，是否会对系统造成多余的改变。
4. 比如，`getUsername()` 和 `setTrue()` 方法就是一个幂等性方法。
5. 因此，我的理解是，幂等就是一个操作，无论执行多少次，返回的结果或者产生的效果都是一样的。

#### 2、分

1. 那么，保证幂等性有哪些方案呢？
2. **查询操作**：查询一次和查询多次，在数据不变的情况下，结果都是一样的。select 是天然的幂等操作。
3. **删除操作**：删除一次和删除多次，都是把数据删除，产生的效果都是一样的，但是返回的结果可能不一样，因为删除的数据不存在时，会返回 0，存在时，则返回 1。
4. **唯一索引**：可以防止新增脏数据，比如，给资金账户表中的用户 ID 加唯一索引，使得并发时，一个用户新增一个资金账户记录，只会让一个新增成功，后新增的则报错，从而保证了只插入一条记录。
   - **注意**：为了幂等性友好反馈，一定要先查询一下，判断是否处理过该笔业务，如果不查询，直接插入业务的话，由于该业务实际已经处理过了，从而导致大量报错的发生。
5. **token 机制**：防止页面重复提交，比如，由于重复点击、或者网络重发等情况，会页面表单被重复提交，解决办法是 token + 分布式锁保证幂等性。

#### 3、总

以上，就是我对分布式幂等性的一些理解，请问有什么细节需要补充的吗？

### 2.2. 分布式会话？

#### 1、为什么 cookie 无法防止 CSRF 攻击，而 token 却可以？

1. CSRF，Cross Site Request Forgery，跨站请求伪造，简单来说，就是用你的身份，去发送一些对你不友好的请求。
   - 1）比如，小壮登录了某网上银行，他来到了网上银行的帖子区，看到一个帖子下面有一个链接写着“科学理财，年盈利率过万”，于是小壮就好奇地点开了这个链接，结果发现自己的账户少了10000元。
   - 2）这是这么回事呢？原来黑客在链接中，藏了一个请求 `<a src=http://www.mybank.com/Transfer?bankId=11&money=10000>科学理财，年盈利率过万</> `，这个请求直接利用小壮的身份，给银行发送了一个转账请求，也就是通过小壮的 cookie 向银行发出请求。
2. 使用 session 认证时，一般使用 cookie 来存储 sessionid，在登陆后，后端生成这个 sessionid，放在 cookie 中，返回给客户端，而服务端通过 Redis 或者其他存储工具，记录保存着这个 sessionid。
3. 客户端登录以后，每次请求都会带上这个 sessionId，服务端就可以用这个 sessionId，来标识哪些请求来源于你个人。
4. 但是，如果别人通过 cookie，拿到了你的 sessionId 后，就可以代替你的身份访问系统了，在登录后的客户端中，攻击者可以通过让用户误点攻击链接，从而利用每次请求都会带上这个 sessionId 的特性，以达到攻击效果。
5. 而如果使用 token 的话，就不会存在这个问题，因为登录成功获得 token 后，一般存放在 local storage 中，然后是前端通过某些方式，在每个发到后端的请求中，都加上这个 token。
6. 而如果点击了个非法链接，由于是非法链接，不是脚本，没有查询 local storage#token 的能力，此时发送给服务端的请求也，是不会携带 token 的，也就是这个请求是非法的，这样，就不会出现 CSRF 漏洞的问题了。

#### 2、什么是 token? 什么是 JWT ? 如何基于 token 进行身份验证？

![1647336741900](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647336741900.png)

1. 由于 session 需要先保存一份在服务器端，使得服务存在状态，这给后端服务带来一定的麻烦，比如，需要保证有 session 信息的服务器的可用性、在扩容时还要为其做额外的考虑、由于依赖于 cookie，所以不适合移动端等等。
2. JWT ，JSON Web Token，可以让服务器端不用保存 session 信息，只需在登录后返回给客户端，由客户端进行保存，以后每次请求都带上这个 token，服务端解析 token 即可拿到 session 信息，从而使后端服务的扩展性得到提升，所以，**token 就相当于一个通行令牌**。
3. JWT，本质上就一段签名后的 JSON 数据，接收者可以根据签名，验证它的真实性，它由 3 部分构成：
   - **1）Header**：token 头，描述 JWT 的元数据，主要定义了，生成签名的算法，以及 token 的类型。
   - **2）Payload**：token 体，主要用来存放实际传递的数据。
   - **3）Signature**：token 签名，服务器通过 Payload、Header 和一个 secret 密钥，使用 Header 指定的签名算法来生成 token 令牌，默认是 `HMAC SHA256`，登录生成后，则把 token 令牌返回给客户端。
4. 客户端收到 token 后，可以保存在 cookie，或者 local storage 里面，在以后发出的所有请求，都携带这个令牌，以标识请求来源，然后服务端检查 JWT，并从中获取用户相关信息，从而替代掉 sessionId。
   - 1）如果放在 cookie 里，让请求发送时自动带上，则不能跨域问题，因为非子域下的 token 将会失效。
   - 2）所以，最好的做法是，放在 `HTTP Header#Authorization` 字段中。

#### 3、token 使用的最佳实践？

1. **设置合理的过期时间**。
2. **注销的 token，如果后端保存过，则需要及时清除**。
3. **监控 token 的使用频率**：防止数据被别人爬取，通过监控使用频率，从请求痕迹中，分析出是否为爬虫程序访问。 
4. **核心功能、敏感操作可以使用动态验证码**：比如提现功能，就要求在提现时，再次进行验证码校验，以防止非本人操作。
5. **识别网络环境、浏览器等信息**：比如，网络环境跟之前使用的不一样时，则需要 APP 重新登录。
6. **secret 加密密钥支持动态修改**：
   - 1）如果 token 的 secret  加密密钥泄露了，意味着别人可以伪造这些 token，这需要能够立刻修改密钥，以更改 token 生成和校验机制。
   - 2）此时，可以将 secret 密钥存储在配置中心，以支持动态修改刷新。
   - 3）但需要注意的是，建议在流量低峰时，再去做 secret 加密密钥的更换操作，否则会导致 token 全部失效，使得所有在线的请求，都需要重新申请 token，导致并发量突增。

#### 4、session 共享方案？

1. **集中式 session 服务器**：还是使用 sessionid 和 cookie 的机制，统一由一个集中式服务器进行 session 管理，缺点是，sessionid  存储在客户端本地有风险，且要保证集中式 session 服务器的高可用。
2. **session 同步**：对各服务器进行 session 数据同步，虽然可以保证每个服务器上都有全部的 session 信息，但当服务器数量过多时，同步会带来很大延迟，甚至同步失败，故放弃。
3. **负载均衡 IP 绑定**：通过负载均衡，比如 nginx 对客户端 IP 与某个服务实例进行绑定，让同一个 IP 只能在指定的同一个机器访问，缺点是对应服务实例宕机后，session 将会丢失，且失去了对请求进行负载均衡的意义。
4. **Redis 共享存储**：把 session 存放到 Redis 中，缺点是需要多访问一次 Redis，但真正实现了 session 共享，不仅可以跨服务器 session 共享，还可以跨平台 session 共享，比如网页端和 APP 端；同时，还支持水平扩展、无状态化后端服务，即使服务重启了，存在 Redis 中的 session 也至于丢失。

#### 5、认证与授权的区别？

1. 认证，Authentication，身份验证，是验证你的身份凭据，比如用户名、用户ID 和密码是否合法，通过这个凭据，系统可以得以知道你就是你，是**身份维度**。
2. 授权，Authorization，发生在认证之后，主要是掌控用户访问系统的权限，有些特定资源只能具有特定权限的人才能访问，比如 admin，是**权限维度**。
3. 认证和授权，一般在系统中都被结合使用，目的是为了保护系统安全。

#### 6、单点登录 SSO？

SSO，Single Sign On，单点登录，是一种会话共享的技术，指用户只需要在某一个网站登录后，那么他所产生的同一次会话，就共享给了其他网站，也就是说，实现了单点登录后，间接地也登录了其他网站。

- **会话**，session，代表的是客户端与服务器的一次交互过程，这个过程可以是连续的，也可以是断断续续的。

根据浏览器的跨域与 cookie 特性，二级域名可同享一级域名的 cookie， 其他不同域名间不共享 cookie，实现基于会话共享的单点登录，可以分为 2 种不同方案：相同一级域名的单点登录和不同一级域名的单点登录。

##### 1、相同一级域名的单点登录 | cookie + Redis

二级域名共享一级域名的 cookie，即 `www.abc.test.com` 和 `www.def.test.com` ，两者共享 `www.test.com` 域名下的 cookie。

**具体流程为**：

1. 首先，前端方面，登录后，后端可以把标志当前会话的 userId，设置到 `www.test.com` 域名下的 cookie 中。
2. 然后，后端方面，则把 userId 作为 key，把会话信息放到 Redis 中，实现分布式会话。

这样，用户在 `www.abc.test.com` 登录后，userId 放入了 `www.test.com`  的 cookie 中，会话信息放到了 Redis 中，只要会话没结束（浏览器没关闭），则在用户下次访问 `www.def.test.com` 时，后端照样能够拿到 cookie 中的 userId，从而去 Redis 查到有会话信息，则认为本次请求有效，无需重新登录，从而实现单点登录。

##### 2、不同一级域名的单点登录 | cookie + CAS + Redis

（图片仅供参考，与下面流程不符）

![1647352168308](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647352168308.png)

不同一级域名间的 cookie 无法共享，即 `www.abc.com` 和 `www.def.com`，两者间的 cookie 无法共享。

这样，就需要引入一个中心节点，用于专门承接单点登录工作，以实现以上两个域名间的 cookie 共享，这个中心节点就是 CAS。

- **CAS**，Center Authentication Service，中央认证服务，是一个单点登录的解决方案，用于不同一级域名之间的单点登录。

**具体流程为**：

![img](file:///D:/MyData/yaocs2/Desktop/%E5%A4%87%E6%B3%A8/5%20SSO/SSO_%E5%8D%95%E7%82%B9%E7%99%BB%E5%BD%95_%E6%97%B6%E5%BA%8F%E5%9B%BE_20220315.png)

假设 CAS 服务端域名为 `www.cas.com`，其他一级域名都是它的客户端，这里有两个，分别为 `www.abc.com` 和 `www.def.com` 。

1. 一个用户在某个时间，访问了 `www.mtv.com` 服务 A。
2. 此时，A 需要校验请求，由于 A 没有认证能力，则把请求重定向到 CAS，让中央认证中心去做登录判断。
3. CAS 收到请求后，发现请求并没有会话标记，代表没有登录，则重定向用户到登录页面，需要用户进行登录。
4. 用户在登录成功后（账号密码校验通过），则 CAS 为用户创建会话，生成全局门票和临时门票，把会话标记 userId，塞到 CAS 域名 `www.cas.com` 的 cookie 中，然后返回临时门票给 A。
5. A 拿到临时门票后，需要再次请求 CAS，让 CAS 去校验临时门票。
6. CAS 门票校验通过，撕毁临时门票，兑换成会话信息，设置到 CAS 域名 `www.cas.com` 的 cookie 中，然后显示 A 登录成功。
7. 接着，用户跑去访问 `www.music.com` 服务 B。
8. 此时，B 需要校验请求，由于 B 没有认证能力，则把请求重定向到 CAS，让中央认证中心去做登录判断。
9. CAS 收到请求后，发现请求有会话标记，然后去查 Redis 是否存在全局门票，如果存在，则说明会话已共享，认为用户已登录，然后生成临时门票，把会话标记 userId，塞到 CAS 域名 `www.cas.com` 的 cookie 中，然后返回临时门票给 B。
10. B 拿到临时门票后，需要再次请求 CAS，让 CAS 去校验临时门票。
11. CAS 门票校验通过，撕毁临时门票，兑换成会话信息，设置到 CAS 域名 `www.cas.com` 的 cookie 中，然后显示 B 登录成功。
12. 至此，一次不同一级域名间的单点登录流程完毕。

其原理在于，利用了两网站间接共享了 CAS 同一个域名的 cookie，使用其中的 userId 作为当前会话的标记，以让后端能够拿着这个标记，去 Redis 中判断是否已生成分布式会话，从而免去第二次的重复登录，实现单点登录。

**全局门票 VS 临时门票**：

1. **全局门票是分布式会话**，是判断当前会话是否已经登录的核心所在。
2. **临时门票是安全性保证**，用于跟 CAS 换取会话信息。
   - **1）假设不用临时门票**，而是直接把全局门票返回给客户端，这样就会导致，如果该全局门票被他人知道了，那么即使用户的当前会话关闭了，劫持者依然能够通过全局门票进行登录操作，就保证不了单点登录是同一次会话。
   - **2）而如果用的是临时门票**，就可以保证本次登录只在当前会话有效，会话结束后，cookie 中的 userId 过期，再次请求 CAS 的话，就需要重新登录和设置了，并且，就算临时门票被劫持了，只需在临时门票兑换会话信息的操作中，增加额外的校验（比如网站来源，或者校验 cookie 中有没有 userId），返回校验不通过，就可以避免掉上面这种劫持风险了。

### 2.3. 分布式限流？

#### 1、分布式限流维度？

1. **时间**：限流基于某段时间范围，或者某个时间点，也就是常说的时间窗口，比如每分钟、每秒钟的时间窗口做限定。
2. **资源**：基于可用资源的限制，比如设定最大访问次数，或者最高可用连接数。

=> 故，

1. **限流就是在某个时间窗口内，对资源访问做限制**，比如设定每秒最多放行 100 个访问请求。
2. 分布式限流，区别于单机限流的场景，它是把整个分布式环境中，所有的服务器当做一个整体进行考量。
3. 比如针对了某个 IP 的限流，限制其每秒最多 10 个访问，那么不管这个 IP 的请求，落在了分布式中的哪台机器上，只要是访问了集群中的服务节点，那么都会受到限流规则的制约。

#### 2、分布式限流规则？

在真正的场景里，往往不止设置一种限流规则，而是会设置多个限流规则共同作用，主要以下 4 种规则：

1. **QPS 和连接数控制**：Nginx、Gateway / Zuul、Redis + Lua、Sentinel、Guava#RateLimiter 等。
2. **传输速率**：Nginx 限制下载速度。
3. **黑白名单**：布隆过滤器。

![1647393890625](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647393890625.png)

#### 3、分布式限流实现方案？

1. 分布式限流，区别于单机限流的场景，它是把整个分布式环境中，所有的服务器当做一个整体进行考量。
2. 所以，需要将限流信息，保存在一个中心化的组件上，这样它就可以获取到集群中所有机器的访问状态，从而实现分布式限流。

目前比较主流的**实现方案**有：

1. **网关层限流**：把限流规则应用在所有流量的入口处。
2. **中间件限流**：把限流规则存储在分布式环境中的某个中间件里，比如 Redis 缓存中，每个组件都可以从这里，获取到当前时刻的流量统计信息，从而决定是拒绝服务，还是放行流量。

#### 4、分布式限流算法 - 计数器算法？

![1647393051245](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647393051245.png)

**概念**：

计数器算法，是指在指定的时间周期内，累加访问次数，如果达到设定的阈值，则触发限流策略，在下一个时间周期进行访问时，再将访问次数清零。

**实现**：此算法无论在单机，还是分布式环境下实现都非常简单，使用 `redis#incr` 原子自增性，再结合 key
的过期时间，可以轻松实现。

**缺点**：

1. 这个算法有一个临界问题，比如在上图中，在 0:00 到 1:00 内，只在 0:50 有 60 个请求，而在 1:00 到 2:00 之间，只在 1:10 有 60 个请求。
2. 虽然在 2 个一分钟的时间内，都没有超过 100 个请求，但是在 0:50 到 1:10 这 20 秒内，却有 120 个请求。
3. 虽然在每个周期内，都没超过阈值，但是在这 20 秒内，却已经远远超过了我们原来设置的 1 分钟内 100 个请求的阈值。

#### 5、分布式限流算法 - 滑动时间窗口算法？

![1647393095141](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647393095141.png)

**概念**：

1. 为了解决计数器算法的临界值问题，于是就发明了滑动时间窗口算法，而在 TCP 网络通信协议中，就采用滑动时间窗口算法来解决网络拥堵问题的。
2. 滑动时间窗口，是将计数器算法中的实际周期，切分成多个小的时间窗口，分别在每个小的时间窗口中，记录访问次数，然后根据时间将窗口往前滑动，并删除过期的小时间窗口，最终只需要统计滑动窗口范围内的小时间窗口的总的请求数即可，本质上也是一种计数器算法。

**优点**：

1. 在上图中，假设我们设置一分钟的请求阈值是 100，则将一分钟拆分成 4 个小时间窗口，这样，每个小的时间窗口只能处理 25 个请求，用虚线方框表示滑动时间窗口，当前窗口的大小是 2，也就是在窗口内最多能处理50 个请求。
2. 随着时间的推移，滑动窗口也随着时间往前移动，比如上图开始时，窗口是 0:00 到 0:30 的这个范围，过了15 秒后，窗口右移到 0:15 到 0:45 的这个范围，窗口中的请求重新清零，很大地减少了计数器算法临界值问题出现的概率。
3. 在滑动时间窗口算法中，我们的小窗口划分的越多，滑动窗口的滚动就越平滑，限流的统计就会越精确。

**缺点**：滑动时间窗口算法，本质上还是计数器算法，所以极端情况下，还是会出现临界值问题，使用令牌桶算法则可以很好地解决这个问题。

#### 6、分布式限流算法 - 漏桶算法？

![1647394890225](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647394890225.png)

**概念**：

1. 漏桶算法的原理，就像它的名字一样，维持一个漏斗，拥有恒定的流出速度，不管水流流入的速度有多快，漏斗出水的速度始终保持不变。
2. 类似于消息中间件，不管消息的生产者请求量有多大，消息的处理能力取决于消费者。
3. `漏桶的容量 = 漏桶的流出速度 * 可接受的等待时长`，在这个容量范围内的请求，可以排队等待系统的处理，超过这个容量的请求，才会被抛弃。

**限流规则**：

在漏桶限流算法中，存在下面几种情况：

1. 当请求速度大于漏桶的流出速度时，也就是请求量大于当前服务所能处理的最大极限值时，会触发限流策略。
2. 当请求速度小于或等于漏桶的流出速度时，也就是服务的处理能力大于或等于请求量时，则正常执行。s

**缺点**：当系统在短时间内，有突发的大流量时，漏桶算法处理不了。

**优点**：桶末端流量匀速流出，能够保证系统平稳运行，不用应对突发流量。

#### 7、分布式限流算法 - 令牌桶算法？

![1647394814391](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647394814391.png)

**概念**：

1. 令牌桶算法，是增加一个大小固定的容器，也就是令牌桶，系统以**恒定的速率**向令牌桶中放入令牌，当令牌桶满时，再向令牌桶生成令牌时，令牌会被抛弃。
2. 如果有客户端来请求，先需要从令牌桶中拿一个令牌，拿到令牌，才有资格访问系统，这时令牌桶中少一个令牌。

**匀速要生成令牌原因**

1. **提高令牌利用率**：
   - 1）如果前 1 秒，一把梭哈了 10 个，但桶里都有 9 个了，此时只利用了 1 个令牌，其余 9 个令牌都会被丢弃。
   - 2）而如果匀速生成的话，就可以先 10 个令牌一个一个的发放，这样令牌也不至于被梭哈式的丢弃掉，提高了令牌的利用率。
2. **避免服务雪崩**：
   - 1）计数器算法和滑动时间窗口算法，在极端情况下，都会存在一个**临界值的问题**，即虽然某个统计窗口内没超过系统阈值，但前后区间总和却远远超过了系统阈值，从而有可能造成服务响应超时，甚至发生服务雪崩。
   - 2）采用匀速令牌生成，则可以避免某个区间前后之和大于系统阈值，因为**这个和 = 桶容量大小 + 令牌生成速度 * 区间跨度**，可见，令牌生成速度如果是匀速的话，那么这个和就会被限定在一个很小的值，从而可以很好的解决，上述那种人造区间洪峰流量的攻击，避免服务雪崩。

**限流规则**：

在令牌桶算法中，存在以下几种情况：
1. **请求速度大于令牌的生成速度**：那么令牌桶中的令牌会被取完，后续再进来的请求，由于拿不到令牌，会被限
  流。
2. **请求速度等于令牌的生成速度**：那么此时系统处于平稳状态。
3. **请求速度小于令牌的生成速度**：那么此时系统的访问量远远低于系统的并发能力，请求可以被正常处理。

**优点**：令牌桶算法，由于有一个桶的存在，可以处理短时间大流量的场景，这是令牌桶算法和漏桶算法的区别。

**缺点**： 由于可能会存在突发的大量，对系统设计时，需要保留一定的余力去面对。

### 2.5. 分布式文件存储？

#### 1、什么是分布式文件系统？

![1647397927726](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647397927726.png)

1. 随着文件数据的越来越多，通过 tomcat 或者 nginx 虚拟化的静态资源文件，在单一的一个服务器内已经存不下了，如果用多个节点存存储又不利于管理和维护，所以，需要一个系统来管理多台计算机节点上的文件数据，这个系统就是分布式文件系统。
2. 分布式文件系统，是一个允许文件，通过网络在多台节点上分享的文件系统，多台计算机节点，共同组成一个整体，为用户提供更大的文件存储空间。

#### 2、分布式文件系统的优点？

1. **海量文件数据存储**。
2. **文件数据高可用（冗余备份）**。
3. **读写性能好和支持负载均衡**。

#### 3、什么是 Fast DFS？

1. Fast DFS，是一个开源的轻量级的分布式文件系统，用于对文件进行管理，包括：文件存储、文件同步、文件上传、文件下载等。
2. 解决了大容量存储和负载均衡的问题，特别适合以文件为载体的在线服务，比如相册网站、视频网站等。

#### 4、Fast DFS 核心组件？

1. **tracker**：追踪者服务器，主要用于协调调度，起负载均衡的作用，记录 storage 的相关状态信息。
2. **storage**：存储服务器，用于保存文件，以及文件的元数据信息。
3. **group**：组，同组节点提供冗余备份，不同组用于扩容。
4. **mata data**：文件的元数据信息，比如长宽信息、图片后缀，视频帧数等信息。

#### 5、Fast DFS 文件上传流程？

1.  Fast DFS 启动后，storage 会定期向 tracker 发送心跳续约。
2. 此时，一个客户端发起一个文件上传请求给 Fast DFS。
3. Fast DFS#tracker 收到请求后，则首先检查目前是否还有可用的 storage。
4. 检查到有，则返回可用的 storage 地址给客户端。
5. 客户端到后，则携带文件数据，到目标 storage 地址上传文件。
6. storage 服务接收文件，把文件写入磁盘中。
7. storage 文件写入磁盘成功后，则组装返回**文件对象信息**给客户端，包括文件相对路径、组名、文件名、文件 ID 等。
8. 客户端收到文件对象信息后，则做关于文件的一些业务处理，比如保存好文件存储的 ID 和路径等。

![1647398273896](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647398273896.png)

#### 6、Fast DFS 文件下载流程？

1.  Fast DFS 启动后，storage 会定期向 tracker 发送心跳续约。
2. 此时，一个客户端发起一个文件下载请求给 Fast DFS。
3. Fast DFS#tracker 收到请求后，则首先根据**文件对象信息**（比如文件存储 ID），查找对应的 storage 地址。
4. 查找到，则返回对应的 storage 地址给客户端。
5. 客户端收到后，则到目标 storage 地址，文件进行下载。
6. storage 服务根据客户端给到的文件存储地址，查找所要下载的文件。
7. storage 找到文件后，则输出文件字节流给客户端。
8. 客户端收到文件流数据后，则可以进行文件下载或者展示。

![1647398629900](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647398629900.png)

#### 7、Fast DFS vs HDFS？

- **HDFS**：Hadoop 的默认存储方式，主要解决并行计算中大数据存储的问题，采用分块存储技术，适用于**大文件存储**。
- **Fast DFS**：主要是为文件上传和下载提供在线服务，支持负载均衡和动态扩容，适用于**中小文件存储**，比如用户头像、小视频等。

### 2.6. 分布式搜索？

#### 1、什么是分布式搜索？

分布式搜索，可以将更大范围分布的异构数据联合起来，形成一个逻辑整体，为用户提供强大的**全文检索**能力。

![1647400157225](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647400157225.png)

#### 2、Lucene、Solr、ElasticSearch？

- **Lucene**：是一个基于 Java 的类库，集成了许多 API 方法，相当于一个 Jar 包，只能给 Java 使用，实现集群十分复杂。
- **Solr**：Apche 的开源项目，也是用的 Java 开发，基于 Lucene 的开源搜索引擎，本质上是对 Lucene 进行的一层封装，可以实现集群，可靠性，容错性高。
- **ElasticSearch**：也叫 ES，基于 Lucene 开发，提供很多 RestFul 风格的接口，可供其他语言使用，可扩展性更高，支持 PB 级别的近实时搜索。

#### 3、倒排索引 vs 正排索引？

- **正排索引**：文档 => 关键词，但在检索关键词时很费力，要一个文档一个文档，挨个遍历查找。
- **倒排索引**：关键词 => 文档，可以根据关键词，立马找到其出现过的所有文档。

#### 4、ES 核心概念？

1. **Near Realtime**：NRT，近实时，两个意思：

   - 1）从写入数据到数据可以被搜索到有一个小延迟（大概1秒）。
   - 2）基于es执行搜索和分析可以达到秒级。

2. **Cluster**：

   - 1）集群，包含多个节点，通过配置集群名称，来划分每个节点所属集群。
   - 2）对于中小型应用来说，刚开始一个集群只有一个节点很正常。

3. **Node**：节点，集群中的一个节点，节点也有一个名称，默认随机分配。

4. **index**：

   - 1）索引，包含一堆有相似结构的文档数据，比如客户索引、商品分类索引、订单索引。
   - 2）索引有一个名称，一个 index 包含很多 document，一个 index 就代表了一类类似的或者相同的document 。
   - 3）在 ES 5.x 版本以前，可以在一个索引中定义多个类型，6.x 之后一个索引只能创建一个类型，在 7~8.x 版本中，已经被彻底移除了。

5. **type**：类型，每个索引里都可以有一个或多个 type，type 是 index 中的一个逻辑数据分类，一个 type 下的document，都有相同的 field。

6. **document**：文档，是 ES 中的最小数据单元，一个 document 可以是一条客户数据、一条商品分类数据、一条订单数据，通常用 json 数据结构来表示，每个 index 下的 type 中，都可以去存储多个 document。

7. **field**：属性，一个 document 里面有多个 field，每个 field 就是一个数据字段。

8. **shard**：

   - 1）分区，单台机器无法存储大量数据，ES 就把一个索引中的数据，切分为多个 shard，分布在多台服务器上存储。
   - 2）有了 shard 就可以横向扩展，以存储更多数据，让搜索和分析等操作，分布到多台服务器上去执行，提升吞吐量和性能。

9. **replica**：

   - 1）副本，任何一个服务器随时都可能故障或宕机，导致 shard 可能就会丢失，因此，ES 为每个 shard 创建多个 replica 副本。

   - 2）replica 可以在 shard 故障时，提供备用服务，保证数据不丢失。

| ES       | 类比于 MySQL 中的     |
| -------- | --------------------- |
| index    | 库                    |
| type     | 表                    |
| document | 行                    |
| field    | 列                    |
| mappings | 表结构                |
|          |                       |
| **ES**   | **类比于 Kafka 中的** |
| Node     | Broker                |
| shard    | Partition             |
| replica  | Partition#Replica     |

#### 5、ES 读写原理？

**写原理**：

1. 把索引拆分成多个 shard 分区，每个 shard 存储部分数据。
2. 其中，一个 shard 可以有多个备份，也就是说，每个 shard 都会有一个 primary shard 主分区，负责写入数据，还有几个 replica shard 副本分区。
3. primary shard 主分区写入数据之后，会将数据同步到其他几个 replica shard 副本上去。
4. 通过这个分区 + 副本机制，可以保证每个 shard 都有多个备份，如果某个机器宕机了，则还会有别的副本在别的机器上，从而实现高可用。
5. 如果某台 primary shard 主分区宕机，那么会由 master 节点，让那个宕机节点上的 primary shard 主分区的身份，转移到其他机器上的 replica shard 副本分区中。
6. 当宕机的那台机器修复完，并重启之后，master 节点会控制将缺失的 replica shard 分配过去，同步后续修改的数据，让集群恢复正常。
7. 而如果 master 节点宕机了，那么则会重新选举一个节点为 master 节点。

**读原理**：

1. 客户端发送一个搜索请求，协调节点会把请求发送给所有的 shard。
2. 然后这些 shard 会去自己里面查，找到可能会匹配到的 doc，返回给协调节点。
3. 协调节点再去 doc 里面去做匹配，找到最符合查询需要的那些 document。
4. 最后再给客户端返回。

#### 6、ES API 使用？

以下操作都是基于《elasticsearch-6.4.3》进行记录的~

##### 一、集群操作

###### 1）查询集群健康信息

```json
GET     /_cluster/health
```

##### 二、索引操作

###### 1）查看索引

```json
GET     _cat/indices?v
```

###### 2）创建索引

```json
PUT     /index_test

{
    "settings": {
        "index": {
            "number_of_shards": "2",
            "number_of_replicas": "0"
        }
    }
}
```

###### 3）删除索引

```json
DELETE      /index_test
```

###### 4）创建索引同时创建 mappings

```
PUT     /index_str

{
    "mappings": {
        "properties": {
            "realname": {
            	"type": "text",
            	"index": true
            },
            "username": {
            	"type": "keyword",
            	"index": false
            }
        }
    }
}
```

###### 5）为已存在的索引新增 mappings

1. 注意，某个 mappings 的属性一旦被建立，就不能再修改了，但还可以追加额外的属性。
2. **主要数据类型**有：
   - 1）**字符串型**：text、keyword。
   - 2）**整型**：long、Integer、short、byte。
   - 3）**浮点型**： double、short。
   - 4）**布尔型**：boolean。
   - 5）**日期型**：date。
   - 6）**对象型**：object（{ } 格式）。
   - 7）**数组型**：[ ] 格式，但里面的类型要一致。
3. **text vs keyword**：
   - **text**：会对内容进行分词，可以用于商品名称、商品详情、商品介绍。
   - **keyword**：不会对内容进行分词，需要精确匹配，可以用于订单状态、qq 号、微信号、手机号等。

```json
POST        /index_str/_mapping

{
    "properties": {
        "id": {
        	"type": "long"
        },
        "age": {
        	"type": "integer"
        },
        "nickname": {
            "type": "keyword"
        },
        "money1": {
            "type": "float"
        },
        "money2": {
            "type": "double"
        },
        "sex": {
            "type": "byte"
        },
        "score": {
            "type": "short"
        },
        "is_teenager": {
            "type": "boolean"
        },
        "birthday": {
            "type": "date"
        },
        "relationship": {
            "type": "object"
        }
    }
}
```

##### 三、文档操作

这些查询方式，称为 QueryString 查询方式。

###### 1）添加文档数据

注意，如果索引没有手动建立 mappings，那么插入文档时，则会根据文档类型自动设置属性类型，这就是 ES 的动态映射。

```json
POST /{索引名}/_doc/{索引ID}

{
    "id": 1001,
    "name": "imooc-1",
    "desc": "imooc is very good, 慕课网非常牛！",
    "create_date": "2019-12-24"
}
```

###### 2）删除文档

注意，文档删除不是立即删除，删除后文档还是会保存在磁盘上，当索引积累得越来越多时，ES 才会把那些曾经标识为删除的清理掉，此时才会真正的从磁盘上移出去。

```json
DELETE /my_doc/_doc/1
```

###### 3）局部更新文档

每次修改后，隐藏的 `version` 字段都会被更改。

```json
POST /my_doc/_doc/1/_update

{
    "doc": {
        "name": "慕课"
    }
}
```

###### 4）全量更新文档

每次修改后，隐藏的 `version` 字段都会 +1。

```json
PUT /my_doc/_doc/1

{
     "id": 1001,
    "name": "imooc-1",
    "desc": "imooc is very good, 慕课网非常牛！",
    "create_date": "2019-12-24"
}
```

###### 5）查询文档

```json
GET /index_demo/_doc/1
GET /index_demo/_doc/_search
```

**查询得到的结果**：

1. **_index**：文档数据所属索引。
2. **_type**：文档数据属于哪个类型。
3. **_id**：文档数据的唯一标识，类似于数据库中某张表的主键，可以自动生成或者手工指定。
4. **_score**：查询相关度，表示与用户查询条件的契合程度，分数越高，用户搜索体验越好。
5. **_version**：版本号。
6. **_source**：真实的文档数据，json 格式表示。

```json
{
    "_index": "my_doc",
    "_type": "_doc",
    "_id": "2",
    "_score": 1.0,
    "_version": 9,
    "_source": {
        "id": 1002,
        "name": "imooc-2",
        "desc": "imooc is fashion",
        "create_date": "2019-12-25"
    }
}
```

###### 6）定制文档查询结果

```json
GET /index_demo/_doc/1?_source=id,name
GET /index_demo/_doc/_search?_source=id,name
```

###### 7）查询文档是否存在

```json
HEAD /index_demo/_doc/1
```

##### 四、分词检索

###### 1）内置分词检索

**ES 内置分词器**：

1. **standard**：默认分词，单词会被拆分，大小会转换为小写。
2. **simple**：按照非字母分词，大小转为小写。
3. **whitespace**：按照空格分词，忽略大小写。
4. **stop**：去掉无意义的单词，比如 the / a / an / is 等等。
5. **keyword**：不做分词，把整个文本作为一个单独的关键词。

```json
POST /my_doc/_analyze

{
    "analyzer": "standard",
    "field": "name",
    "text": "text文本"
}
```

###### 2）IK 分词检索

```json
POST /_analyze

{
    "analyzer": "ik_max_word",
    "text": "上下班车流量很大"
}
```

##### 五、DSL 操作

DSL，Domain Specific Language，领域特定语言，是 ES 基于 JSON 格式的数据查询语言，相比于 Query String 方式，DSL 查询更加灵活，有利于复杂查询。

###### 1）查询所有文档

```json
POST     /shop/_doc/_search

{
    "query": {
        "match_all": {}
    },
    "_source": ["id", "nickname", "age"]
}
```

###### 2）term vs match

1. match 会对 `慕课网` 先进行分词，然后再查询。
2. 而 term 则不会，直接把 `慕课网` 作为一个 整的词汇去搜索。

```json
POST     /shop/_doc/_search

{
    "query": {
        "term": {
            "desc": "慕课网"
        }
    }
}

对比

{
    "query": {
        "match": {
            "desc": "慕课网"
        }
    }
}
```

###### 3）连续分词匹配查询

1. match 分词后，只要有匹配就返回。
2. match_phrase：分词结果必须在字段中包含，顺序还要与 query 中的相同才返回，而且必须是连续的。

```json
POST     /shop/_doc/_search

{
    "query": {
        "match_phrase": {
            "desc": {
            	"query": "大学 毕业 研究生",
            	"slop": 2
            }
        }
    }
}
```

###### 4）逻辑匹配查询

and / or 

```json
POST     /shop/_doc/_search

{
    "query": {
        "match": {
            "desc": "慕课网"
        }
    }
}

+

{
    "query": {
        "match": {
            "desc": {
                "query": "xbox游戏机",
                "operator": "or"
            }
        }
    }
}

=> 相当于 select * from shop where desc='xbox' or|and desc='游戏机'
```

###### 5）匹配度查询

minimum_should_match，最低匹配精度，至少有 n% * query 中分词个数的文档才会被匹配。

```json
POST     /shop/_doc/_search

{
    "query": {
        "match": {
            "desc": {
                "query": "女友生日送我好玩的xbox游戏机",
                "minimum_should_match": "60%"
            }
        }
    }
}
```

###### 6）主键查询

```json
POST     /shop/_doc/_search

{
    "query": {
        "ids": {
            "type": "_doc",
            "values": ["1001", "1010", "1008"]
        }
    }
}

```

###### 7）多条件匹配

multi_match，多个字段同时满足条件，才会被匹配。

```json
POST     /shop/_doc/_search

{
    "query": {
        "multi_match": {
                "query": "皮特帕克慕课网",
                "fields": ["desc", "nickname"]

        }
    }
}
```

###### 8）权重查询 

权重，为某个字段设置权重，权重越高，文档相关性得分就越高，比如商品名称的权重，一般都比商品简介的权重要高。

```json
POST     /shop/_doc/_search

{
    "query": {
        "multi_match": {
            "query": "皮特帕克慕课网",
            -- nickname^10，表示搜索权重提升10倍相关性，搜索时以nickname为主，desc为辅
            "fields": ["desc", "nickname^10"]
        }
    }
}
```

###### 9）布尔查询

1. **must**：查询必须匹配搜索条件，类似于 and。
2. **should**：查询需匹配满足 1 个以上条件，类似于 or。
3. **must_not**：一个搜索条件都不能匹配。

```json
{
    "query": {
        "bool": {
            "must": [
                {
                	"match": {
                		"desc": "慕"
                	}	
                },
                {
                	"match": {
                		"nickname": "慕"
                	}	
                }
            ],
            "should": [
                {
                	"match": {
                		"sex": "0"
                	}	
                }
            ],
            "must_not": [
                {
                	"term": {
                		"birthday": "1992-12-24"
                	}	
                }
            ]
        }
    }
}
```

###### 10）过滤器

1. 过滤器，可以对搜索出来的结果进行数据过滤，可以和全文检索一起结合使用。
2. 过滤器类型：
   - **gte**：大于等于。
   - **lte**：小于等于。
   - **gt**：大于。
   - **lt**：小于。

```json
POST     /shop/_doc/_search

{
	"query": {
		"match": {
			"desc": "慕课网游戏"
		}	
    },
    "post_filter": {
		"range": {
			"money": {
				"gt": 60,
				"lt": 1000
			}
		}
	}	
}
```

###### 11）排序

类似于 sql 中的排序，asc 表示顺序，desc 表示逆序。

```json
POST     /shop/_doc/_search
{
	"query": {
		"match": {
			"desc": "慕课网游戏"
		}
    },
    "post_filter": {
    	"range": {
    		"money": {
    			"gt": 55.8,
    			"lte": 155.8
    		}
    	}
    },
    "sort": [
        {
            "age": "desc"
        },
        {
            "money": "desc"
        }
    ]
}
```

###### 12）高亮标签

```json
POST     /shop/_doc/_search

{
    "query": {
        "match": {
            "desc": "慕课网"
        }
    },
    "highlight": {
        "pre_tags": ["<tag>"],
        "post_tags": ["</tag>"],
        "fields": {
            "desc": {}
        }
    }
}
```

###### 13）前缀匹配查询

```json
POST     /shop/_doc/_search

{
    "query": {
        "prefix": {
            "desc": "imo"
        }
    }
}
```

###### 14）错误纠偏查询

```json
POST     /shop/_doc/_search

{
  "query": {
    "fuzzy": {
      "desc": "imoov.coom"
    }
  }
}
```

###### 15）占位符查询

- **?**：占 1 个字符。
- *****：占 1 个或者多个字符。

```json
POST     /shop/_doc/_search

{
  "query": {
    "wildcard": {
      "desc": "*oo?"
    }
  }
}
```

###### 16）分页查询

```json
POST     /shop/_doc/_search

{
	"query": {
		"match_all": {}
	},
	"_source": [
		"id",
		"nickname",
		"age"
	],
	"from": 5,
	"size": 5
}
```

###### 17）滚动分页查询 | 解决深度分页问题

scroll=1m，相当于一个 session 会话时间，设置搜索保持的上下文为 1 分钟。

```json
-- 滚动分页第一次查询，开启滚动分页
POST    /shop/_search?scroll=1m
{
    "query": { 
    	"match_all": {
    	}
    },  
    "sort" : ["_doc"], 
    "size":  5
}

-- 滚动分页第二次查询
POST    /_search/scroll
{
    "scroll_id": "DnF1ZXJ5VGhlbkZldGNoBQAAAAABrDnhFnFiRFN6VFZEVDRLNXNCTFJKeHZkTkEAAAAAAaw55BZxYkRTelRWRFQ0SzVzQkxSSnh2ZE5BAAAAAAGsOeMWcWJEU3pUVkRUNEs1c0JMUkp4dmROQQAAAAABrDniFnFiRFN6VFZEVDRLNXNCTFJKeHZkTkEAAAAAAaw55RZxYkRTelRWRFQ0SzVzQkxSSnh2ZE5B",
    "scroll": "1m"
}
```

##### 六、批量操作

###### 1）批量获取文档

```json
GET http://10.18.12.149:9200/shop/_doc/_mget
{
    "ids": [
        "1001",
        "1003",
        "10015"
    ]
}
```

###### 2）批量插入文档及 mappings

```json
POST http://10.18.12.149:9200/_bulk

{"create": {"_index": "shop2", "_type": "_doc", "_id": "2004"}}
{"id": "2004", "nickname": "name-2004"}
{"create": {"_index": "shop2", "_type": "_doc", "_id": "2005"}}
{"id": "2002", "nickname": "name-2005"}
{"create": {"_index": "shop2", "_type": "_doc", "_id": "2003"}}
{"id": "2003", "nickname": "name-2003"}
```

###### 3）批量插入文档到已有索引中

```json
POST http://10.18.12.149:9200/shop2/_doc/_bulk

{"create": {"_id": "2007"}}
{"id": "2004", "nickname": "name-2004"}
{"create": {"_id": "2008"}}
{"id": "2002", "nickname": "name-2005"}
{"create": {"_id": "2009"}}
{"id": "2003", "nickname": "name-2003"}
```

###### 4）批量更新或保存文档

```json
POST http://10.18.12.149:9200/shop2/_doc/_bulk

{"index": {"_id": "2004"}}
{"id": "2004", "nickname": "index-2004"}
{"index": {"_id": "2007"}}
{"id": "2007", "nickname": "name-2007"}
{"index": {"_id": "2008"}}
{"id": "2008", "nickname": "name-2008"}
```

###### 5）批量更新文档

```json
POST http://10.18.12.149:9200/shop2/_doc/_bulk

{"update": {"_id": "2004"}}
{"doc": {"id": "3304"}}
{"update": {"_id": "2007"}}
{"doc": {"nickname": "name-update"}}
```

###### 6）批量删除文档

```json
POST http://10.18.12.149:9200/shop2/_doc/_bulk

{"delete": {"_id": "2001"}}
{"delete": {"_id": "2003"}}
{"create": {"_id": "8008"}}
{"id": "8008", "nickname": "name-8088"}
{"update": {"_id": "2002"}}
{"doc": {"id": "2222"}}
```



