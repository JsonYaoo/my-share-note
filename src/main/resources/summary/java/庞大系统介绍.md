### 十三、庞大系统介绍

| 考察维度 | 精讲部分（必须掌握、背熟）                                   |
| -------- | ------------------------------------------------------------ |
| Redis    | 分布式锁、Redis 常用数据结构                                 |
| 消息队列 | 可靠性保证                                                   |
| MySQL    | MVCC、Explain、**SQL 语句调优**、分布式 ID 实现方案          |
| JVM      | 运行时数据区、垃圾收集器、**垃圾回收算法**                   |
| Spring   | Spring 启动原理、SpringBoot  自动装配、MVC 原理、**IOC 原理**、AOP 原理、Spring @Transactional 原理、Eureka 原理 |
| Dubbo    | 架构图、Provider 服务注册原理、Consumer 服务引用原理、调用原理 |
| Java     | ThreadLocal、HashMap、ConcurrentHashMap、ThreadPoolExecutor、Synchronized、ArrayList、Volatile、AQS 原理、Queue |

#### 1.1.1.1. 项目上用到了 SpringCloud 哪些组件？

##### 1、总

用到了 Eureka 做**服务注册**，Feign 做**服务通信**，Ribbon 做**负载均衡**，Sleuth+Zipkin 做**链路追踪**，Config 做**配置中心**、Stream+Bus 做**消息驱动**。

##### 2、分

1. 服务注册的意思就是，在微服务中，由于服务可能会比较多，其 IP 也经常会发生变化，如果**手动维护**每个服务的 IP+端口 的话，那么就太耗时耗力了，此时可以通过服务启动时，**上报**包括自己当前的 IP 地址、端口、健康状态等信息，到一个注册中心中，由注册中心来统一收集并管理它们，从而减轻人工维护的成本，且信息也比较及时和准确。
2. 服务通信的意思就是，在微服务中，由于模块与模块之间，常常需要相互获取数据，那么就需要进行通信，此时可以到注册中心那里，获取一份想要访问的服务器的**注册表**，得到它的 IP+端口，然后发起比如 HTTP 的远程调用，从而获取到某个参数下远程服务器的执行结果。
3. 负载均衡的意思就是，在微服务中，由于服务实例常常不止一个，如果没有一个合适的访问策略，那么很可能会导致某个实例**访问过热**，此时就需要一种策略来**打散这些流量**，这就是负载均衡，在 SpringCloud 中的实现是 Ribbon 组件，其原理是服务实例启动时，通过获取注册中心中所有的服务列表，然后缓存到机器内存中，在要发起服务调用时，会经过 Ribbon 配置的负载算法，来保证调用的均衡性。
4. 链路追踪的意思就是，在微服务中，由于一个前端请求，可能会拉起多个服务间的调用，导致排查问题可能需要跨机器的挨个挨个排查，浪费人力精力，此时可以通过使用一个**全局 ID** 来标记当前请求，在需要跨机器请求时，全局 ID 不变，但生成一个新的**局部 ID**，以及**时间戳**，传递到下一台机器，这样全局 ID 标记也就跟着过去，同时，还可以通过使用当前时间戳减去传过去的时间戳，从而计算出本次跨机器请求所花费的时间，完成链路追踪。
5. 配置中心的意思就是，在微服务中，由于服务实例可能会有很多，且每个模块的业务、参数等配置可能会不一样，导致配置可能会有多个版本，如果以人工的方式去登记每个模块配置的不同之处，识别哪个版本是哪个服务的配置的话，很容易出错，且难以维护和管理，此时可以抽象出一个配置中心，通过**按路径、按标签**地存放所有服务模块的配置信息，在每个服务启动时，再去配置中心**拉取**对应自己的配置，填入自己配置的**占位符**中，然后再去加载容器，从而实现**远端配置**的统一管理与控制。
6. 消息驱动的意思就是，在微服务中，常常需要用到消息中间件，但不同中间件的 API 使用方式不同，比如业务代码基于 RabbitMQ 做了实现，如果某一天想要更换底层为 Kafka，就需要对原有的业务代码进行修改，此时，可以抽取出一个统一的消息驱动组件，来**屏蔽底层的实现差异**，只负责输入输出消息即可，从而实现一套从上层来看是以**消息作为事件驱动**的架构。

##### 3、总

综上，就是我对 Spring Cloud 微服务的一个理解，请问有什么细节需要补充的吗？

#### 1.1.1.2. 如何在 Feign 调用时增强一下 Header？

关键字：RequestInterceptor#apply()、RequestTemplate、ThreadLocal。

##### 1、总

可以实现 `RequestInterceptor#apply()` 方法，该方法会传入一个 `RequestTemplate`，然后从上下文，比如 `ThreadLocal` 中取出需要增强的属性，然后设置到 `RequestTemplate#header` 中即可，比如这样：

```java
@Component
@Slf4j
public class FeignRequestInterceptor implements RequestInterceptor {
    @Override
    public void apply(RequestTemplate requestTemplate) {
        // Member User信息
        MemberUser memberUser = GlobalMemberContext.getMemberUser();
        if(memberUser != null){
            try {
                requestTemplate.header(ProjectConstant.MEMBER_USER_INFO, URLEncoder.encode(JSON.toJSONString(memberUser), Charsets.UTF_8.name()));
            } catch (UnsupportedEncodingException e) {
                log.info("member user编码失败");
            }
        }
    }
}
```

##### 2、分

其中，Feign 的底层原理是这样的，

- **注入原理**：
  1. 由于 SpringBootApplication 上打了 @EnableFeignClients 注解，所以会回调注解导入的 **ImportBeanDefinitionRegistrar**#registerBeanDefinitions 方法，该方法可以扫描并注册 FeignClient。
  2. 其中注册 FeignClient 时， 会在 BeanDefinitionRegistry 中添加 FeignClientFactoryBean#beanDefinition。
  3. 由于 FeignClientFactoryBean 实现了 FactoryBean 接口，所以在对应的 FeignClient 被注入时，则会调用 **FactoryBean#getObject** 方法。
  4. 然后该 FeignClientFactoryBean#getObject 方法在构造 Feign.Builder 时，获取到容器中所有打了 @Component 注解，且实现 **RequestInterceptors** 接口的拦截器，然后注入到 Feign.Builder#requestInterceptors 属性中。
  5. 接着就是使用 Feign.Builder 来构建 LoadBalancer 的动态代理，返回对应的动态代理对象。

```java
Map<String, RequestInterceptor> requestInterceptors = context
				.getInstances(this.contextId, RequestInterceptor.class);
if (requestInterceptors != null) {
    builder.requestInterceptors(requestInterceptors.values());
}
```

- **执行原理**：
  1. 当业务方法调用注入的 FeignClient 实例对应的接口方法时，则会触发 JDK 动态代理，回调到 **InvocationHandler** 的 invoke 方法。
  2. 在 invoke 方法中，会去遍历所有的 **requestInterceptors**，并执行他们的 **apply** 方法，从而实现 Header 的增强。

```java
final class SynchronousMethodHandler implements MethodHandler {
    @Override
    public Object invoke(Object[] argv) throws Throwable {
        ...
        return executeAndDecode(template, options);
    }
    
    Object executeAndDecode(RequestTemplate template, Options options) throws Throwable {
    	Request request = targetRequest(template);
        ...
    }
    
  	Request targetRequest(RequestTemplate template) {
    	for (RequestInterceptor interceptor : requestInterceptors) {
      		interceptor.apply(template);
    	}
    	return target.apply(template);
  	}
}
```

##### 3、总

以上，就是我对 Feign Client 的一个理解，请问有什么细节需要补充的吗？

#### 1.1.1.3. Feign 的调优参数有哪些，以及HTTP 连接池优化？ 

| 参数解释                               | 具体配置                                                     |
| -------------------------------------- | ------------------------------------------------------------ |
| 具体服务的连接超时时间（ms）           | xxx.ribbon.ConnectionTimeout                                 |
| 具体服务的读取超时时间（ms）           | xxx.ribbon.ReadTimeout                                       |
| 具体服务的重试开关（true / false）     | xxx.ribbon.okToRetryOnAllOperations                          |
| 具体服务的超时重试次数（不包括首次）   | xxx.ribbon.MaxAutoRetries                                    |
| 具体服务的超时重试机器数（不包括首台） | xxx.ribbon.MaxAutoRetriesNextServer                          |
| 更换 Feign 的客户端（池化连接对象）    | feign.okhttp（新起之秀） / .httpClient（老牌 HC），两者性能差距不大，默认为  JDK 的 HttpUrlConnection |
| 日志调用配置                           | feign.client.config.default.loggerLevel：full                |
| 拦截器配置                             | 实现 RequestInterceptor#apply()                              |
| 自定义解码器                           | 实现 feign.codec.Decoder#decode()                            |
| 降级配置                               | @FeignClient fallback：继承接口自定义 Handler，或者实现 FallBackFactory#create() |

#### 1.1.1.4. SpringBoot 自动装配和自定义 starter？

##### 1、总

Starter 是什么：
1. Starter 就是一个 jar 包，写一个 @Configuration 的配置类，把这些 Bean 的定义都包含在其中，然后在 Starter 包下的 `META-INF/spring.factories` 中写入该配置类，SpringBoot 程序在启动时，就会按照约定来加载该配置类。
2. 开发人员只需要将相应的 Starter 包依赖进应用中，然后进行相关的属性配置，就可以进行代码开发，而不需要再单独对 Bean 进行配置。

##### 2、分

原理：
1. 当启动 SpringBoot 应用时，会先创建 SpringApplication 的对象，在对象的构造方法中，会进行某些参数的初始化工作，最主要的是判断当前应用的类型（比如 Servlet 类型），以及通过 SPI 的方式加载整个应用的 `spring.factories` 文件中的初始化器和监听器的 Class。
2. SpringApplication 对象创建完成之后，开始执行 `run()` 方法，来完成整个启动，启动过程中最主要的有两个方法，第一个叫做 `prepareContext()`，第二个叫做 `refreshContext()`，在这两个步骤中完成了自动装配的核心功能，而其他方法的处理逻辑包含了上下文对象的创建、Banner 的打印、异常报告期的准备等各个准备工作，方便后续来进行调用。
3. 在 `prepareContext()` 方法中，主要完成了对上下文对象的初始化工作，包括比如 `Enviroment`对象属性值的设置，在整个过程中，有一个非常重要的方法 `load()`，`load()` 主要完成一件事，那就是将**启动类**作为BeanDefinition 注册到 Registry 中，方便后续在进行 BeanFactoryPostProcessor 调用执行时，找到对应的启动类来完成 `@SpringBootApplication` 和 `@EnableAutoConfiguration` 等注解的解析工作。
4. 在 `refreshContext()` 方法中，会进行整个容器的刷新过程，会调用 Spring 中的启动流程，即`AbstractApplicationContext#refresh()`，有 13 个关键方法，来完成整个 Spring 应用的启动，其中会调用 `invokeBeanFactoryPostProcessor()` 方法，主要是对`ConfigurationClassPostProcessor` 的处理，会先调用实现 `BeanDefinitionRegistryPostProcessor` 接口的 `postProcessBeanDefinitionRegistry()` 方法，然后再调用自己实现的 `postProcessBeanFactory()` 方法，处理各种包括 @PropertySource、@ComponentScan、@Import、@ImportResource、@Bean 等注解。
5. 其中，在解析 @Import 注解时，会有一个 `getImports()` 的方法，会从**启动类**开始递归解析注解，把所有包含 @Import 注解都收集到，然后在 `processImport()` 方法中，对 Import 导入的类进行分类，这里主要起识别作用的是 `ImportSelect` 的实现类  `AutoConfigurationImportSelect`，来调用 `selectImports()` 方法使用 SPI 的方式，来获取并加载 `spring.factories` 中的 `EnableAutoConfiguration` 自动装配配置类的 Class，完成自动装配。
6. 接着，还调用子类上下文 `ServletWebServerApplicationContext#onRefresh` 方法，来拉起嵌入式的 Tomcat 容器。
7. 最后，就是实例化 Bean，即 `finishBeanFactoryInitialization()` 方法，主要是实例化所有剩下非懒加载的单例 Bean，并完成它们的初始化和依赖注入，通过遍历所有 beanNames，然后挨个判断是走 FactoryBean 的流程，还是走 BeanFactory 的流程，其中主要步骤总结起来分为 3 步，分别是 NewInstance 实例化、Populate 属性赋值 和 Initialization 初始化，从而完成自动装配配置 Bean 的注入。

##### 3、总

自定义 Starter：

1. 创建 Starter 项目，定义 Starter 需要的 Properties 配置类，比如数据库连接信息等。
2. 然后编写自动配置类，自动配置类就是获取配置，根据配置来自动装配 Bean。
3. 编写 `META-INF/spring.factories` 文件，以让 SpringBoot 在启动时加载自动配置类。
4. 然后在项目中，引入自定义 Starter 的 Maven 依赖，增加对应的配置值后，然后即可直接使用。

#### 1.1.1.5. SpringMVC 请求前改写 @RequestBody 参数？

##### 1、总

有三种方法，分别是 (Filter + HttpServletRequestWrapper) + HandlerInterceptor + WebMvcInterceptorConfig、RequestBodyAdvice + @ControllerAdvice 和 Controller AOP。

| Spring MVC 请求扩展点                 | 执行顺序 | 方法                                                         | 作用                                                         |
| ------------------------------------- | -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Filter                                | 1        | doFilter(ServletRequest, ServletResponse)                    | 根据 params、header 过滤非法请求、包装 HttpServletRequest，不做改写 Body |
| HandlerInterceptor                    | 2        | preHandle(HttpServletRequest, HttpServletResponse, Object)   | Handler Method 处理前拦截                                    |
| RequestBodyAdvice + @ControllerAdvice | 3        | afterBodyRead(Object, HttpInputMessage, MethodParameter, Type, Class) | HandlerAdapter#handleInternal 执行过程时解析 Method 参数     |
| Controller AOP                        | 4        | @Before、@Around                                             | point.proceed(args) 前改写 args 参数                         |

##### 2、分

###### HandlerInterceptor | 实现最麻烦

**实现最麻烦**，需要在 Filter 处添加 RequestWrapper 包装，然后在包装类里替换掉 Reader 读取的输入流，最后在 MVC 的拦截器 preHandle 实现 body 替换。

```java
@Component
public class UserFilter implements Filter {
    @Override
    public void init(FilterConfig filterConfig) throws ServletException {
        System.err.println("init");
    }

    @Override
    public void destroy() {
        System.err.println("destroy");
    }

    @Override
    public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException {
        ((ResponseFacade) response).addHeader("sign", request.getParameter("sign"));
        System.err.println("doFilter");
        
        // 1、结合UserBodyWrapper包装request
        chain.doFilter(new UserBodyWrapper((HttpServletRequest) request), response);
    }
}

public class UserBodyWrapper extends HttpServletRequestWrapper {

    private String body;

    public String getBody() {
        return body;
    }

    public void setBody(String body) {
        this.body = body;
    }

    public UserBodyWrapper(HttpServletRequest request) throws IOException {
        super(request);

        // 2、先读取一次输入流, 获取body内容
        this.body = IOUtils.toString(request.getInputStream(), StandardCharsets.UTF_8);
    }

    @Override
    public ServletInputStream getInputStream() throws IOException {
        ByteArrayInputStream bais = new ByteArrayInputStream(body.getBytes(StandardCharsets.UTF_8));

        // 3、以后读取输入流时, 只读取内存中的这个body
        return new ServletInputStream() {
            @Override
            public boolean isFinished() {
                return false;
            }

            @Override
            public boolean isReady() {
                return false;
            }

            @Override
            public void setReadListener(ReadListener listener) {

            }

            @Override
            public int read() throws IOException {
                return bais.read();
            }
        };
    }

    @Override
    public BufferedReader getReader() throws IOException {
        // 4、以后读取输入流时, 只读取内存中的这个body
        return new BufferedReader(new InputStreamReader(getInputStream()));
    }
}

public class UserHandlerInterceptor implements HandlerInterceptor {

    @Override
    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {
        System.err.println("preHandle");

        UserBodyWrapper userBodyWrapper = (UserBodyWrapper) request;
        String body = userBodyWrapper.getBody();
		
        // 5、获取body，改写body
        User user = JSONObject.parseObject(body, User.class);
        user.setUsername("测试添加前缀" + user.getUsername());
        ((UserBodyWrapper) request).setBody(JSONObject.toJSONString(user));
        
        return true;
    }

    @Override
    public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception {
        System.err.println("postHandle");
    }

    @Override
    public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception {
        System.err.println("afterCompletion");
    }
}

@Configuration
public class WebMvcInterceptorConfig implements WebMvcConfigurer {

    @Override
    public void addInterceptors(InterceptorRegistry registry) {
        // 6、添加WebMvc拦截器配置类
        registry.addInterceptor(new UserHandlerInterceptor()).addPathPatterns("/**");
    }
}
```

###### RequestBodyAdvice | 时机最适合

```java
@ControllerAdvice
public class UserRequestBodyAdvice implements RequestBodyAdvice {

    @Override
    public boolean supports(MethodParameter methodParameter, Type targetType, Class<? extends HttpMessageConverter<?>> converterType) {
        System.err.println("supports");
        return true;
    }

    @Override
    public HttpInputMessage beforeBodyRead(HttpInputMessage inputMessage, MethodParameter parameter, Type targetType, Class<? extends HttpMessageConverter<?>> converterType) throws IOException {
        System.err.println("beforeBodyRead");
        return inputMessage;
    }

    // 1、在 HandlerAdpater#resolveArgments 时，HttpMessageConverter读取完body后，进行回调改写body
    @Override
    public Object afterBodyRead(Object body, HttpInputMessage inputMessage, MethodParameter parameter, Type targetType, Class<? extends HttpMessageConverter<?>> converterType) {
        System.err.println("afterBodyRead");

        JSONObject jsonObject = JSONObject.parseObject(JSONObject.toJSONString(body));
        Long id = jsonObject.getLong("id");
        String username = jsonObject.getString("username");
        String password = jsonObject.getString("password");

        try {
            Constructor constructor = ((Class) targetType).getConstructor(Long.class, String.class, String.class);
            return constructor.newInstance(id, username + "测试添加后缀", password);
        } catch (Exception e) {
            e.printStackTrace();
        }

        return body;
    }

    @Override
    public Object handleEmptyBody(Object body, HttpInputMessage inputMessage, MethodParameter parameter, Type targetType, Class<? extends HttpMessageConverter<?>> converterType) {
        System.err.println("handleEmptyBody");
        return body;
    }
}
```

###### Controller AOP | 实现简单强大

```java
@Aspect
@Component
public class AspectJConfig {

    @Pointcut("execution(* com.jsonyao.cs.controller.*.*(..))")
    private void pointcut() {

    }

    @Around("pointcut()")
    public Object around(ProceedingJoinPoint point) throws Throwable {
        System.err.println("around");
        long start = System.currentTimeMillis();
        
        // 1、业务方法调用前，改写body
        Object[] args = point.getArgs();
        User user = User.class.cast(args[0]);
        user.setUsername(user.getUsername() + "测试AOP后缀");
        
        Object res = point.proceed(new Object[] {user});
        
        long end = System.currentTimeMillis();
        System.err.println("执行结果: " + res + ", 消耗时间: " + (end - start));
        return res;
    }
}
```

##### 3、总

综上，HandlerInterceptor  实现最麻烦，但处理位置比较通用，适合一些公共的设置，RequestBodyAdvice 回调的位置**最适合改写 @RequestBody**，适合统一对 @RequestBody 进行设置，Controller AOP 更强大，实现也简单，适合一些通用方法级别的拦截。

#### 1.1.1.6. Spring MVC 拦截器和过滤器的区别？

##### 1、总

拦截器 HandlerInterceptor，过滤器 Filter，虽然可以对请求进行一定处理，但：

1. **实现人不同**：HandlerInterceptor 属于 Spring#Web 包下，Filter 属于 Tomcat#javax 包下。
2. **配置方式不同**：HandlerInterceptor 在 Spring#WebMvcConfigurer 中配置，而 Filter 在 web.xml 中配置。
3. **处理时机不同**：HandlerInterceptor 作用更强大，方法处理前的拦截、处理后的拦截、返回前的拦截，而 Filter 则是在进入DispatcherServlet 前进行一定的请求过滤处理。

##### 2、分

![1645947452956](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1645947452956.png)

Spring MVC 客户端请求的生命周期管理：

1. **Filter#doFilter** 执行链过滤客户端请求。
2. DispatcherServlet，表示中心控制器，在客户端发出请求后，经过 Web 容器（比如 Tomcat）后，会打到 DispatcherServlet 上，并由其来处理请求。
3. HandlerMapping，表示处理器映射，DispatcherServlet 收到请求后，会调用 HandlerMapping，HandlerMapping 会根据请求 url 去查找对应的 Handler，即一个 Handler Method 对象，指的是 url 对应 Controller 中的对应方法。
4. HandlerExecutionChain，表示处理器执行链，Handler 解析完 url  后，会返回一个处理器执行链给DispatcherServlet，其中就包括一堆 HandlerInterceptor。
5. HandlerAdapter，表示处理器适配器，DispatcherServlet 会按照规则去匹配对应的 HandlerAdapter。
6. 再由对应的 Handler Method 去处理请求，其中就包括调用去执行我们编写的 Controller 业务逻辑。
   - 但在执行前，会先调用 **HandlerInterceptor#preHandle** 进行  handler 方法处理前的拦截。
7. Controller 代理对象，则会去访问数据库，填充好模型后，再把 ModelAndView 返回给 HandlerAdapter。
   - 其中，在 HandlerAdpater#resolveArgments 时，HttpMessageConverter读取完 body 前/后，可以对 **RequestBodyAdvice#beforeBodyRead/afterBodyRead** 进行回调，从而改写 body。
   - 而我们平常使用的 @ResponseBoday 的方法，返回的 ModelAndView 为空，也就是不会返回视图给客户端，而是经过 `RequestResponseBodyMethodProcessor`，把 JSON 串写入到 Response 的 Body 中。
8. HandlerAdapter 收到后，再将 ModelAndView  传递给 DispatcherServlet。
9. DispatcherServlet 收到后，则调用视图解析器 ViewResolver，来解析 ModelAndView。
10. ViewResolver 会解析逻辑视图名，根据逻辑的 View 找到实例的 View，并返回给 DispatcherServlet。
11. DispatcherServlet 收到后，则根据 ViewResolver 解析出的 View，调用对应的实际视图，结合 Model 进行渲染。
    - 但在执行前，会先调用 **HandlerInterceptor#postHandle** 进行 handler 方法处理后的拦截。
12. 最后，DispatcherServlet 再将渲染后的 View 作为结果，响应给客户端。
    - 在执行后，返回结果前，会调用 **HandlerInterceptor#afterCompletion** 进行响应前的拦截。

##### 3、总

以上，就是我对 Spring MVC 拦截器和过滤器的一个理解，请问有什么细节需要补充的吗？

#### 1.1.1.7. Kafka 的高可靠保证？

##### 1、总

对于 Kafka 高可靠，需要从 3 个角度进行考虑，分别是生产端、Broker 端以及消费端的高可靠保证。

##### 2、分

1. **生产者端**：通过 @Transactional 事务管理 + 本地消息表打标 + `acks=all`（或者 `acks=1`）  + 定时重发 + 失败补偿来保证，即保证本地消息表和业务表属于同一个事务，业务落库时消息也落库，使用 `acks=all` 保证消息成功写入所有 ISR，使用定时重发 + 失败补偿，可以保证消息最终会写入 Broker。
2. **Broker 端**：通过 ISR 机制来保证，配合生产端 `acks=all` 来保证每次写入消息，必须在 ISR 集合中的所有 Broker 写入成功后才认为消息写入成功，响应 ACK 给生产端。
3. **消费端**：通过 `enable-auto-commit=false` 关闭自动 ACK，`ack-mode=manual` 打开手工 ACK，在处理消息完毕后，才进行一次手工 ACK `acknowledgement.acknowledge()`，避免处理异常后，不能再次重复消费的情况。

##### 3、总

以上，就是我对 Kafka 高可靠保证的一个理解，请问有什么细节需要补充的吗？

#### 1.1.1.8. RabbitMQ 的高可靠保证？

##### 1、总

与 Kafka 高可靠类似，对于 RabbitMQ 高可靠，也需要从 3 个角度进行考虑，分别是生产端、Broker 端以及消费端的高可靠保证。

##### 2、分

1. **生产者端**：通过 @Transactional 事务管理 + 本地消息表打标 +  `publisher-confirms=true` 以及实现 ConfirmCallback 函数开启 Broker 消息 Confirm 机制  + 定时重发 + 失败补偿来保证，即保证本地消息表和业务表属于同一个事务，业务落库时消息也落库，使用 Broker Confirm 机制，保证消息成功写入 Broker，使用定时重发 + 失败补偿，可以保证消息最终会写入 Broker。
2. **Broker 端**：通过 `druable=true` 开启持久化队列 + 生产端消息设置 `deliveryMode=2` 开启持久化消息 ，来保证消息写入后，如果未被消费前，会保存在 Broker 中，防止 MQ 丢消息。
3. **消费端**：通过 `acknowledge-mode=mannul` 关闭自动 ACK，打开手工 ACK，在处理消息完毕后，才 `channel.basicAck` 进行一次手工 ACK，避免处理异常后，RabbitMQ 立即把消息删除，丢失消息的情况。

##### 3、总

以上，就是我对 RabbitMQ 高可靠保证的一个理解，请问有什么细节需要补充的吗？

#### 1.1.1.9. JDK 的新特性、新 JVM？

| JDK 版本 | 新特性                                                       |
| -------- | ------------------------------------------------------------ |
| JDK 5    | 自动拆装箱、 Foreach 循环、枚举类、泛型、JUC                 |
| JDK 6    | 对脚本语言 Ruby、Groovvy、JS 的支持                          |
| JDK 7    | switch 支持 String，开始转移永久代，静态变量、字符串常量池转移到了堆中 |
| JDK 8    | 函数式接口，Lambda 表达式，Stream API、接口支持 default 方法、HashMap 和 ConcurrentHasMap 性能提升、元空间完全取代永久代 |
| JDK 9    | 集合添加 List.of(xxx, xxx) 等工厂方法 、接口支持 private 方法，默认使用 G1垃圾收集器 |
| JDK 10   | G1 多线程并行 Full GC，降低 G1 STW 时间                      |
| JDK 11   | 新增 ZGC，比 G1 更细粒度的内存管理，采用并行回收策略         |
| JDK 12   | 新增 Shenandoah GC 算法、优化 G1 将垃圾分为强制部分和可选部分，强制部分会被回收，可选部分可能不会被回收，提高 GC 效率 |
| JDK 13   | ZGC 优化，将标记长时间空闲的堆内存返还给操作系统，只要保证堆大小不会小于 -Xms 即可 |
| JDK 14   | 删除 CMS 、弃用  Parallel Scavenge + SerialOld 的 GC 组合、将 ZGC 应用到 MacOS 和 Win 中 |
| JDK 15   | 新增隐藏类、密封类（避免抽象类被滥用）                       |
| JDK 16   | ZGC 性能优化，此版本相当于是对 JDK 14 和 15 的一些特性进行了正式的引入 |
| JDK 17   | 正式引入密封类 sealed class，限制抽象类的实现                |

#### 1.2.1.1. JDK 8 VS JDK 7？

| JDK 8 新特性                          | 解释                                                         |
| ------------------------------------- | ------------------------------------------------------------ |
| 函数式接口，Lambda 表达式，Stream API | 在需要一个函数，但又不想费神去命名一个函数时使用，也就是匿名函数，同时，还可以把函数作为参数，传递进某个方法中 |
| 接口支持 default 方法                 | 接口的默认实现                                               |
| 元空间完全取代永久代                  | 元空间使用本地内存，永久代使用 JVM 内存，从而根本上解决了永久代溢出的问题 |
| HashMap 性能提升                      | 拉链 >= 8 时会准换为红黑树，<= 6 时转换为链表、弃用哈希种子、扩容时无需重新 rehash，只需要分高低位转移链表即可、改用尾插法解决并发插入时的 CPU 100% 问题 |
| ConcurrentHasMap 性能提升             | 拉链 >= 8 时会准换为红黑树，<= 6 时转换为链表、弃用哈希种子、Node+CAS+synchronzied+TreeBin 读写锁，替换掉 Segment+HashEntry+ ReentrantLock 分段锁、采用 CAS+synchronzied 实现渐进式并发扩容 |

#### 1.2.1.2. JDK 8 函数式接口的实现原理？

##### 1、总

1. 函数式接口，指**有且只有一个抽象方法的接口**，接口中的 static 方法、default 方法、Object 方法都不算抽象方法，且有个专门的注解 `@FunctionInterface`，但它不是必须的，如果接口符合以上函数式编程的语义，那么加不加这个注解都不影响，加上只是为了编译器进行检查而已，但如果不符合语义，又加上了该注解，那么编译器则会报错。
2. 有且只有一个抽象方法的原因是，由于函数式接口 ` () -> {} ` 的写法，主要是为了简化代码，相当于是一个**匿名内部类的匿名函数**，如果有多个抽象方法，编译器则不知道是重写哪个方法了，所以只能有且只有一个抽象方法。

##### 2、分

在 JDK 8 中，函数式接口主要分为 4 类，分别是**供给型、消费型、断言型和方法型**，其中它们的方法又可以和定义的 default 方法进行**连用**。

| 接口           | 方法               | 说明                                          |
| -------------- | ------------------ | --------------------------------------------- |
| Supplier< T >  | T get();           | 供给型，无参，返回一个函数返回的一个泛型对象  |
| Consumer< T >  | void accept(T t);  | 消费型，传入一个泛型对象，但没有返回值        |
| Predicate< T > | boolean test(T t); | 断言型，传入一个泛型对象，但返回 boolean 类型 |
| Function< T >  | R apply(T t);      | 方法型，传入一个泛型对象，返回另一个泛型结果  |

##### 3、总

以上就是我对 JDK 8 函数式接口的理解，它是 Lambda 表达式和 Stream API 的基础，使用这种方式来编码，既简洁又高效。

#### 1.2.1.3. 线程池核心参数，以及选取原则？

##### 1、总

对于这个问题，我打算从线程池的概念、线程复用原理、线程淘汰原理、构造参数、数据结构、生命周期、工作原理和调优原则这几个方面进行回答。

##### 2、分

1. **线程池的概念**：线程池，ThreadPoolExecutor，允许使用多个线程之一，来执行每个提交的任务，通过**线程复用**，来降低线程创建和销毁所带来的开销，同时任务到达时，可以无需等待线程的创建就能立即执行，从而提高任务的处理速度。

2. **线程复用原理**：通过设计一个**任务队列**，来承放并缓冲更多的执行任务，使得**本已存在的线程**，在处理完它们手上的任务后，可以立马从任务队列的**另一端取出执行任务**，接着继续往下执行，周而复始，从而不用每次都构建一个新的线程再执行，实现线程复用。

3. **线程池的构造参数**：但是，这种线程复用太过于简单暴力，为了让线程池稳定可控，还需要其他参数进行优化：

   - 考虑到，那些本已存在的线程应该有个上限，就需要指定一个 `corePoolSize` 核心线程数，核心线程的意思就是，默认情况下没有保活时间，不会被回收，在不超 `corePoolSize` 上限且收到新任务时会被创建。
   - 考虑到，在任务过多，核心线程处理不过来的情况，就需要指定一个 `maximumPoolSize` 最大线程数。
   - 考虑到，在任务峰值过后，非核心线程可能会空闲一段时间，但仍然占据系统资源的情况，就需要指定一个 `keepAliveTime` 非核心线程的最大空闲时间以及 `TimeUnit` 时间单位。
     - 这也是**线程淘汰**的原理所在，如果在从任务队列中取任务的时间，超过了指定的 `keepAliveTime` 还没取到任务，则可以认为队列中没有多余的任务了，也就是轮训任务队列的这个线程空闲了 `keepAliveTime` 这么久的时间，那么就要对这个线程进行淘汰处理，以节省系统资源。
   - 考虑到，任务需要装入任务队列，就需要指定一个 `BlockingQueue` 阻塞队列接口的具体实现。
   - 考虑到，在构建核心或者非核心线程时，可能需要对线程本身进行一些，比如线程名称等参数设置的情况，就需要指定一个 `ThreadFactory` 的具体实现。
   - 考虑到，任务队列和最大线程都超上限，即线程池超负载时，任务还源源不断到来的情况，就需要指定一种 `RejectedExecutionHandler` 的具体实现，来执行相应的拒绝策略逻辑。

4. **线程池的数据结构**：

   - 根据以上分析，可以容易得到，如果构建一个好点的线程池，就至少需要持有核心线程数、最大线程数、非核心线程的最大空闲时间、任务队列、线程工厂、拒绝策略程序的引用。
   - 另外，JDK 在实现方面，还抽象了一个 `Worker` 类，通过使用 `Worker` 自己持有的 `Thread` 实例，在轮训任务队列时进行任务消费。
   - 同时还持有了一个 `AtomicInteger` 原子类型的 `ctl  ` 线程池控制位，来管理线程池的生命周期及有效线程的数量。

5. **线程池的生命周期**：线程池控制位 ctl，JDK 把 Integer 的高 3 位作为**线程池的状态**，低 29 位作为**有效线程的数量**，前者一个存在 5 种状态，规定：

   -  **-1 为 RUNNING 运行态**，此状态下能够接收新提交的任务，同时还能处理任务队列中的任务。
   -  **0 为 SHUTDOWN 关闭态**，此状态下不会再接受新提交的任务，但还可以继续处理任务队列中的任务。
   -  **1 为 STOP 停止态**，此状态下不会再接收新提交的任务，也不能继续处理任务队列中的任务，并且还会尝试中断正在处理任务的线程。
   -  **2 为 TIDYING 整理态**，此状态下所有的线程都已终止了，此时有效工作数量为 0。
   -  **3 为 TERMINATED 终止态**，在 TIDYING 整理态回调完 `terminated()` 钩子函数以后，会进行此状态。

   ![1645968027493](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1645968027493.png)

6. **线程池的工作原理**：

   1. 首先根据线程控制位 `ctl`，判断当前有效线程数是否小于设定的核心线程数，如果是小于，那么就可以把当前任务作为首个任务 `firstTask`，去**创建核心线程**并执行任务。
   2. 如果发现超出了核心线程数，或者并发下核心线程创建失败了，那么就在检查完线程池还在运行状态后，就尝试**把任务投递到任务队列中**。
   3. 如果任务投递成功，则还要做线程池状态的双重检查，防止线程池突然被关闭，如果发现线程池确实不再是运行态了，那么就需要把刚才投递成功的任务给取出来，再**执行拒绝策略程序**；而如果发现还是运行态，则视有效线程数量是否为 0，来决定是否需要**补充非核心线程**，以保证任务队列中的任务不会永远停留在内存中。
      1. 注意这里**补充非核心线程**的操作，它可以保证 `corePoolSize=0 & maximumPoolSize > 0` 且任务队列还没达到上限时，仍能生成 1 个非核心线程去消费任务队列，避免队列内存溢出的发生，关于这里的细节就可以说说我们的一段生产事故了~
      2. 我们系统某个模块中常常需要使用别的模块的数据源，比如在 POS 模块中需要去查询 PRICE 库，此时，由于主线程默认绑定的是 POS 数据源，要想使用 PRICE 数据源的话，就需要开辟子线程去做切换，所以就为每个模块抽象了数据源切换的工具类。
      3. 由于正常都是调用一次切换方法，也就是把 Lambda 表达式提交给线程池，主线程再去做 `FutureTask.get()` 的监听，拿到结果后返回，这是正常情况，因此我们一开始也没怎么留意。
      4. 但问题就来了，有一次某段代码对连续进行了两次的切换，就发现接口超时，以及往后的切换都超时了，那时排查了很久，最后在这个切换工具类里看到，生产上跑的参数为 `corePoolSize=0、maximumPoolSize=32、workQueue=LinkedBlockingQueue`，按照原理，本以为是错误的配置，会导致任务都阻塞在队列中直到 OOM 发生的，但改成一次切换又可以了，这就很奇怪了，然后就去研究源码，发现刚才上面所说的那段**补充非核心线程**的逻辑，也就是即使 `corePoolSize=0`，只要最大线程数不为 0，那么还是会保证至少创建一个线程去消费任务队列的，所以切换一次就可以拿到任务的运行结果。
      5. 那么为什么切换两次就不行了呢？这是因为，在第一次切换时，是主线程在做 `FutureTask.get()`，阻塞的是主线程，而如果还做一次连续切换，那么就会导致线程池中仅有的 1 个线程也做了 `FutureTask.get()`，从而进入了阻塞，但线程池认为有效线程数还是 0，所以新任务进来也不会去创建新线程消费，会直到 OOM 的发生为止~
      6. 最后，更正了合理的构造参数 `corePoolSize=Runtime.getRuntime().availableProcessors() + 1`，`maximumPoolSize=2 * Runtime.getRuntime().availableProcessors()`，`keepAliveTime=120s`，`LinkedBlockingQueue=1024` 保证不会发生内存溢出，`Google#ThreadFactoryBuilder`来设构建线程工厂、在线程创建时设定名称前缀和递增线程号，`RejectedExecutionHandler=CallerRunsPolicy` 保证任务不丢失，同时设置 `allowCoreThreadTimeOut=true` 允许核心线程空闲时也会被回收。
   4. 如果任务投递失败了，则**尝试补充非核心线程**，如果因为线程池不是为运行态，或者超出了最大核心线程数，导致非核心线程补充失败的话，那么就需要**执行拒绝策略程序**，因为此时要么是 SHUTDOWN 关闭状态不能接受新任务了，要么就是任务队列满了需要拒绝添加新任务了。

   ![1645967491556](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1645967491556.png)

7. **线程池的调优原则**：

   - **线程池大小的设置**：需要先确定任务的类型，分为 CPU 密集型、IO 密集型以及混合型的任务：

      | 类型       | 概念                                                         | 目的                                                         | 合理的经验公式                                               |
      | ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
      | CPU 密集型 | 任务需要大量的运算，中间没有阻塞，CPU 一直全速运行           | 需要尽可能少的线程数量，以减少线程上下切换的次数，提高 CPU 的利用率 | CPU 核数 + 1                                                 |
      | IO 密集型  | 任务大量时间都花在 IO 的阻塞上，希望 CPU 尽可能去调度其他任务，而不是在等待阻塞的线程、浪费 CPU 资源，所以需要更多的线程数以供 CPU 调度 | 需要尽可能多的线程数，但过多的线程也会带来过多的上下文切换   | CPU 核数 * 2                                                 |
      | 混合型     | 既有 CPU 密集型的特点，又有 IO 密集型的特点                  | 需要看平均线程等待时间，和平均线程运行时间，来决定对应的线程数，可通过 Github#PoolSize Calculator 工具类进行粗略的计算 | CPU 核心数 * 目标 CPU 利用率 * （1 + 平均线程等待时间 / 平均线程运行时间），可见，平均线程等待时间越长，平均线程运行时间越短，则需要的线程就越多 |

      => 但这些只是经验公式，最优的参数还需要根据实际环境不断压测、调优才能得到。

   - **任务队列的设置**：控制任务队列容量，实际上就是在考量**内存占用**和**任务的排队策略**，常用的阻塞队列实现有：

      | 实现类              | 特性                                                         | 排队策略   | 优点                                                         | 缺点                                                         |
      | ------------------- | ------------------------------------------------------------ | ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
      | SynchronousQueue    | 无界同步队列，容量为 0，不存储任何元素，每个插入操作都会阻塞等到另一个线程进行相应的删除操作才会恢复（利用 CAS 自旋 + LockSupport 方式实现阻塞） | 直接交接   | 最小的内存花销                                               | 当任务到达速度大于处理速度时，如果搭配无界池，可能会出现无限线程增长的问题 |
      | LinkedBlockingQueue | 单向链表有界阻塞队列，容量可选，空参构造时为 Integer.MAX_VALUE，相当于无界队列 | "无界"队列 | 核心线程繁忙时，任务会在队列中排队，适合用于平滑瞬间爆发的流量 | 如果任务到达速度大于处理速度，可能会导致任务队列元素无限增长，占用较大的内存花销 |
      | ArrayBlockingQueue  | 数组有界阻塞队列，初始时必须先指定容量大小，一旦指定，就不能再修改了 | 有界队列   | 与最大线程数一起使用，可以防止资源被耗尽                     | 任务队列初始化好了后，就难以再动态的调整和控制了             |

   - **拒绝策略程序的设置**：当线程池被关闭，或者任务队列和线程都已经饱和时，新提交的任务会走到拒绝策略程序的处理逻辑中，默认的拒绝策略程序都是定义在 ThreadPoolExecutor 的内部类中：

      | 实现类              | 特性                                                         | 适用场景                                                     |
      | ------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
      | AbortPolicy         | 默认的拒绝策略程序，在任务被拒绝时，会抛出 RejectedExecutionException 异常 | 可以阻止系统正常运行下去                                     |
      | CallerRunsPolicy    | 调用 Runnable#run，当前主线程会自己去运行任务                | 不会造成任务的丢失，可以让线程池有一定的缓冲时间             |
      | DiscardPolicy       | 任务会被简单的丢弃掉                                         | 允许任务丢失时，这是最好的一种丢弃策略                       |
      | DiscardOldestPolicy | 丢弃任务队列头部的任务，然后重新执行一开始的工作流程         | 会丢弃最老的一个任务，也就是可能马上就被执行的任务，然后重新提交当前任务 |

##### 3、总

以上，就是我对线程池核心参数及原理的一个理解，请问有什么细节需要补充的吗？

#### 1.2.1.4. 分布式锁的实现方案，以及红锁的缺点？

##### 1、总

分布式锁指的是，在不同的系统或者同一个系统的不同主机之间，共享访问某个资源时，用来互斥地防止彼此干扰保证一致性的锁实现，其实现方式有：

- **基于数据库实现**：通过乐观锁，或者唯一索引实现。
- **基于分布式缓存实现**： 典型的有，通过 Redis 实现。
- **基于分布式一致性算法实现**：典型的有，通过 ZK 实现。

##### 2、分

###### 1）基于数据库实现 | 负担大

- **基于乐观锁实现**：原理是，根据版本号，来判断更新之前有没有其他线程更新过，如果被更新过，则获取锁失败。
- **基于唯一索引实现**：原理是，在表上建立唯一索引，当想要获得锁时，向表中插入一条记录，释放锁时则删除这条记录。
  - **缺点**：
    1. **锁没有失效时间**，解锁失败会导致死锁，此时该唯一索引所有 insert 都会返回失败，其他线程无法再获取到锁。
    2. **不可重入**，同一线程在没有释放锁之前无法再获取到锁。

###### 2）基于分布式缓存实现 | 锁失效

- **基于 Redis 单机实现**：使用 `SET NX EX` 指令加锁，保证**原子性**地给锁设置**过期时间，防止死锁**，使用 LUA 脚本 `redis.call` 指令，保证 key 值判断与删除键指令原子性执行，且防止由于 STW 时间过长，锁被其他进程误删。

  - **缺点**：会出现由于**时钟漂移** 或者 任务执行时间过长，导致的锁被提前释放的问题。
- **基于 Redisson 实现**：它是一个 Redis 的客户端，其分布式锁的实现原理是，让获得锁的线程开启一个定时守护线程，每隔 expireTime / 3 的时间就去检查一下，该线程持有的锁是否还存在，如果存在，则对锁的过期时间重新设置为 expireTime，完成守护线程对**锁的续约**，防止锁由于过期提前释放。

  - **缺点**：这些只是在 Redis 单机实现的分布式锁，加锁时只作用在一个 Redis 节点上，即使通过了 Sentinel 保证了高可用，但由于 Redis 是**异步复制**的，如果在 Master 节点获取到锁后，在未完成数据同步的情况下，发生了故障转移，那么其他客户端上的线程依然可以获取到锁，丧失了锁的安全性。

- **基于 RedLock 算法实现**：红锁算法的原理是，

  1. 先获取当前时间 `t1`，然后按顺序依次尝试从 n 个 Redis 实例，使用相同的 key 和具有唯一性的 value（例如 UUID）来获取锁，当向 Redis 请求获取锁时，除了设置锁的失效时间 `expire`，还应该设置超时时间 `timeout`，且这个超时时间 **<** 锁的失效时间 `expire` ，这样可以避免 Redis 已经挂掉的情况下，客户端不用一直等待响应结果，而是尽快地去尝试另外一个 Redis 实例来获取锁。
  2. 客户端通过使用当前时间 `t3` 减去开始获取锁时间 `t1`，就得到获取锁花费的总时间 `T`，当且仅当从过半数（N/2+1 个）的 Redis 节点都取到锁，并且获取锁花费的总时间 `T` **<** 锁失效时间 `expire` 时，锁才算获取成功，如果获取到了锁，那么 key 的真正有效时间 `real_expire` 等于锁失效时间 `expire` **减去**锁花费的总时间 `T`。
  3. 如果获取锁失败，客户端则应该在所有的 Redis 实例上使用 **Lua 脚本进行解锁**，原因是可能存在某个节点加锁成功后，**返回客户端时**的响应包丢失了，即客户端到服务器的通信是正常的，但反方向却是有问题的，虽然对客户端而言，由于响应超时导致加锁失败，但是对 Redis节点而言，`SET` 指令执行成功，意味着加锁成功，因此，释放锁时，客户端也应该对当时获取锁失败的那些 Redis 节点同样发起解锁请求。

  - **缺点**：
    1. **性能过重**：使用 RedLock 需要维护那么多的 Redis 实例，提升了系统的维护成本。
    2. **仍然不安全**：RedLock 严重依赖系统时钟，如果 Master 系统时间发生回调，则会导致它持有的锁提前过期释放，还是不能保证锁的安全性，这个是基于时间来实现自动释放的分布式锁，都无法解决的问题。

###### 3）基于分布式一致性算法实现 | 强一致

基于 ZK 实现：可以利用**顺序临时节点**的特性，结点在创建时，会自动在结点名后加一个数字后缀，以保证有序，同时，如果客户端连接失效，则还会立即删除结点，再利用 **watcher 监视器**的特性，注册某个结点的监视器，当节点状态发生改变时，watcher 被触发时，ZK 会向客户端发送一条通知。其分布式锁的实现原理是，

1. 创建一个锁目录 lock，希望获得锁的线程 A 在 lock 目录下，将创建**顺序临时结点**。
2. A 先获取锁目录下所有的子结点，判断是否存在序号比自己小的结点，如果不存在，则说明当前线程的顺序号最小，则线程 A 获得锁。
3. 当另外一个线程 B 获取锁时，判断到 B 自己不是最小的结点，存在有更小的线程 A 结点，则设置  watcher 监听器，只监听比自己**次小**的结点 A。
4. 当线程 A 处理完业务后，会删除结点 A，释放掉分布式锁，然后线程 B 监听到节点状态变更事件后，判断自己已经是最小的结点了，则成功获得锁。

##### 3、总

以上就是我对分布式锁一些实现方案的理解，总结一下就是，

- **基于数据库实现**：
  - **优点**：直接使用数据库，使用简单。
  - **缺点**：但这样会增加数据库的负担。
- **基于分布式缓存实现**：
  - **优点**：属于 AP 模型，性能高，实现起来较为方便，在允许偶发性的锁失效情况发生，不影响系统正常使用时，可以采用分布式缓存来实现锁。
  - **缺点**：通过过期时间实现的锁超时机制不是十分可靠，当业务必须要数据的**强一致性**，不允许重复获得锁时，比如金融场景的重复下单与重复转账场景下，就不能使用分布式缓存来实现锁了，此时可以使用 CP 模型来实现，比如 Zookeeper。
- **基于分布式一致性算法实现**：
  - **优点**：不依靠过期时间来释放锁，可靠性高，当系统要求高可靠性时，可以采用分布式一致性算法来实现锁。
  - **缺点**：性能比不上分布式缓存实现的锁，因为 ZK 需要频繁的创建和删除结点。

#### 1.2.1.5. Redis 数据结构？

##### 1、总

对于这个问题，我打算先介绍 Redis 对象 `RedisObject`，

1. 其数据结构包含一个 4 字节的 `type` 对象类型属性，分为 STRING、HASH、LIST、SET 和 ZSET。
2. 一个 4 字节的 `encoding` 对象编码属性，包括 INT、EMBSTR、RAW、HT、LINKEDLIST、ZIPLIST、INTSET 和 SKIPLIST。
3. 以及一个 `ptr` piont 指针，指向底层实现的数据结构。

然后我再按照上面所说的对象类型（String、Hash、List、Set、ZSet）按顺序进行介绍。

##### 2、分

###### 1）String

1. 首先是 String，常见的 API 有， SET、SET NX EX、GET、APPEND、STRLEN、SETRANGE、GETRANGE、MSET、MGET、INCRBY、DECRBY 等命令，适合存储帖子、评论、热点数据等缓存，其底层的数据结构分为 3 种编码，分别为 INT、EMBSTR 和 RAW：

2. **INT**：只能存储 long 类型的整数，`ptr` 指针指向对应的整数值。

   ![1631946982581](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631946982581.png)

3. **EMBSTR**：（< 3.2 版本时）在字符串值小于等于 39 字节时会使用此编码，其优点是，它是对 `SDS` 的一个小优化，通过将 `RedisObject` 对象头和 `SDS` 存放在一起，采用**连续空间保存**，只需要一次内存分配，避免了它们各自进行空间分配，提高了字符串的内存分配效率，同时还可以减少内存碎片和 `ptr` 指针的占用，节约内存，提高空间的利用率。

   - `SDS`：是 Redis 自己实现一个字符串数据结构，通过持有 `len` 来标识字符串长度，`free` 来记录空闲字符的个数，`buf` 则指向真实的字符数组，能够在 O（1）内获取字符串长度，具有空间预分配、惰性释放内存，以减少分配次数的特点。

   - **缺点**：EMBSTR 是**只读**的形式，要修改时，只能转换为 RAW 编码，才能进行修改。

   ![1631878253168](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631878253168.png)

4. **RAW**：（< 3.2 版本时）在字符串值大于 39 字节时会使用此编码，`ptr` 指针指向一个 `SDS` 数据结构，也就是以 `SDS` 的形式存储，主要为了解决长度计算和追击字符效率的问题。

   ![1631947051873](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631947051873.png)

###### 2）Hash

1. 然后就是 Hash，常见的 API 有，HSET、HGET、HEXSITS、HDEL、HLEN、 HSTRLEN、HINCRBY、HMSET、HMGET、HKEYS、HVALUES 等命令，适合存储结构化的对象数据，其底层的数据结构分为 ZIPLIST 和 HT。

2. **ZIPLIST**：压缩列表，（< 3.2 版本时）在元素大小小于 64 字节且元素个数少于 512 个时会使用此编码，由数组实现，通过 `zlbytes` 记录列表总字节数、`zltail_offset` 记录最后一个节点的偏移量快速定位结尾、`zllength` 记录列表元素个数、`entries` 数组存储列表节点、`zlen` 标志列表结束，以及每个`entry` 节点都持有前一个节点的长度 `prevlous_entry_length`，在 HASH 取值时，可以通过**向后或者向前遍历**找到对应的键和值，其优点是由于内存是连续的，可以减少很多内存碎片和指针内存的占用，进而**节约内存**。

   - **缺点**：ZIPLIST 存在级联更新的问题，由于 `prevlous_entry_length` 存储的长度取决于前一个节点的长度，当前一个节点大于 254 字节时，这个值会从 1 字节变为 5 字节，从而导致后面值可能也会发生变更的连锁反应，此时的时间复杂度非常差为 O（n^2），但好在这种大范围的级联更新出现的概率并不大。

   ![1631950146881](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631950146881.png)

3. **HT**：哈希表，（< 3.2 版本时）在元素大小大于等于 64 字节，或者元素个数大于等于 512 个时会使用此编码，基于 2 张哈希表实现，使用拉链法解决哈希冲突，相对于 Java#HashMap，其特点在于，除了在负载因子大于等于 1  （没有执行 `BGSAVE` 或者 `BGREWARITEAOF` 时）或者 5 （执行 `BGSAVE` 或者 `BGREWARITEAOF` 时）发生 2 倍实际使用长度的扩容外，还会在负载因子小于 0.1 时发生 1 倍实际使用长度的缩容， 并且采用的是渐进式的 `rehash` 机制，即定时执行，或者在客户端每次增删改查操作完成后，让其进行一次 `rehash` 操作，把 ht[0] 对应 `rehashindex` 位置的值搬 ht[1] 上，然后 `rehashIndex+1`，这样避免了集中式扩容带来的性能压力。

   - **缺点**：在 `rehash` 期间，需要同时持有两张哈希表，对内存占用稍大，如果本身内存都不足时，突然的 `rehash` 会使得 Redis 执行缓存淘汰策略，抛弃大量的 Key。

   ![1631950298985](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631950298985.png)

###### 3）List

1. 接着就是 List，常见的 API 有：LPUSH、LPOP、RPUSH、RPOP、LREM、LINSERT、LSET、LINDEX、LRANGE、LTRIM、BLPOP、BRPOP、RPOPLPUSH、BRPOPLPUSH，适合用作评论列表、商品列表、发布与订阅等功能，其底层的数据结构分为 ZIPLIST、LINKEDLIST 和 QUICKLIST 编码。 

2. **ZIPLIST**：压缩列表，（< 3.2 版本时）在元素大小小于 64 字节且元素个数少于 512 个时会使用此编码，由数组实现，通过 `zlbytes` 记录列表总字节数、`zltail_offset` 记录最后一个节点的偏移量快速定位结尾、`zllength` 记录列表元素个数、`entries` 数组存储列表节点、`zlen` 标志列表结束，以及每个`entry` 节点都持有前一个节点的长度 `prevlous_entry_length`，在 LIST 取值时，可以通过**向后或者向前遍历**找到对应的值，其优点是由于内存是连续的，可以减少很多内存碎片和指针内存的占用，进而**节约内存**。

   - **缺点**：ZIPLIST 存在级联更新的问题，由于 `prevlous_entry_length` 存储的长度取决于前一个节点的长度，当前一个节点大于 254 字节时，这个值会从 1 字节变为 5 字节，从而导致后面值可能也会发生变更的连锁反应，此时的时间复杂度非常差为 O（n^2），但好在这种大范围的级联更新出现的概率并不大。

   ![1631952569099](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631952569099.png)

3. **LINKEDLIST**：双向链表，（< 3.2 版本时）在元素大小大于等于 64 字节，或者元素个数大于等于 512 个时会使用此编码，它是 Redis 自己实现的一条双向链表，包含头节点、尾结点、前驱和后继，可以很方便的进行向后或者向前遍历，同时持有 `len` 长度计数器，可以 O（1）内获取到 LIST 的长度 。

   - **缺点**：每个节点都有自己的前后指针，指针这部分所占用的内存较多，且每个节点是单独进行内存分配，当节点过多时，造成的内存碎片会比较多，影响内存管理的效率。

   ![1631952812216](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631952812216.png)

4. **QUICKLIST**：快速列表，大于 3.2 版本后，LIST 统一采用此格式进行存储，是 ZIPLIST 和 LINKEDLIST 的混合体，它将 LINKEDLIST 按段切分，每一段使用 ZIPLIST 来紧凑存储，多个 ZIPLIST 之间使用双向指针串接起来，缓解了 LINKEDLIST 指针内存浪费和内存碎片多的问题，同时解决 ZIPLIST 数据量过大时导致的性能变差问题。

   - **缺点**：ZIPLIST 节点太小的话（比如只存 1 个元素时），快速列表会退化成普通的链表，起不到应有的节省内存的作用，而 ZIPLIST 节点太大的话（比如只有一个 ZIPLIST 节点时），快速列表会退化成压缩列表，还是会出现数据量过大时导致的性能变差问题。
   - 因此，快速列表内部默认定义的单个 ZIPLIST 节点大小为 `8k 字节`，可以由参数 `list-max-ziplist-size` 来控制，其作用是，在分配结点时，如果发现当前 ZIPLIST 节点超过了这个大小，则会重新分配一个 ZIPLIST 节点。

   ![1631953074136](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631953074136.png)

###### 4）Set

1. 再然后就是 Set，常见的 API 有：SADD、SPOP、SREM、SRANGMEMBER、SISMEMBER、SCARD、SMEMBERS、SINTER、SINTERSTORE、SUNION、SUNIONSTORE、SDIFF、SDIFFSTORE，适合用于求交集、并集、差集，比如朋友关系，其底层的数据结构分为 INTSET 和 HT 编码。 

2. **INTSET**：整数集合，（< 3.2 版本时）只存储 long 范围内的整数值，且在元素个数 < 512 个时会使用此编码，持有对应整数的编码类型 `encoding` 总是使用容纳数字的**最小编码进行存储**，以节约内存、集合元素数量 `length` 可以在 O（1） 内获取到对应长度、以及元素使用数组  `contents`  来进行**连续存储**，可以减少内存碎片和指针内存的占用，以节约内存。

   - **缺点**：编码类型只能升级不能降级，在大数字删除后，整数集合还是会使用大类型存储小数字，造成空间的浪费。

   ![1631954240528](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631954240528.png)

3. **HT**：哈希表，（< 3.2 版本时）在元素大小大于等于 64 字节，或者元素个数大于等于 512 个时会使用此编码，基于 2 张哈希表实现，使用拉链法解决哈希冲突，相对于 Java#HashMap，其特点在于，除了在负载因子大于等于 1  （没有执行 `BGSAVE` 或者 `BGREWARITEAOF` 时）或者 5 （执行 `BGSAVE` 或者 `BGREWARITEAOF` 时）发生 2 倍实际使用长度的扩容外，还会在负载因子小于 0.1 时发生 1 倍实际使用长度的缩容， 并且采用的是渐进式的 `rehash` 机制，即定时执行，或者在客户端每次增删改查操作完成后，让其进行一次 `rehash` 操作，把 ht[0] 对应 `rehashindex` 位置的值搬 ht[1] 上，然后 `rehashIndex+1`，这样避免了集中式扩容带来的性能压力。

   - **缺点**：在 `rehash` 期间，需要同时持有两张哈希表，对内存占用稍大，如果本身内存都不足时，突然的 `rehash` 会使得 Redis 执行缓存淘汰策略，抛弃大量的 Key。

   ![1631954355626](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631954355626.png)

###### 5）ZSet

1. 最后就是 ZSet，常见的 API 有：ZADD、ZREM、ZSCORE、ZINCRBY、ZRNAGE（指定偏移，score 从小到大，分数相等，则再按值顺序排序）、 ZRERANGE（指定偏移，score 从大到小，分数相等，则再按值逆序排序）、ZRNAGEBYSOCRE（指定分数）、 ZRERANGEBYSOCRE（指定分数）、ZRANK、ZRERANK、ZCARD、ZCOUNT（区间个数统计） 等命令，可以在去重后进行排序，适合排名的场景，其底层的数据结构分为 ZIPLIST、SKIPLIST 编码。 

2. **ZIPLIST**：压缩列表，（< 3.2 版本时）在元素大小小于 64 字节且元素个数少于 **128** 个时会使用此编码，由数组实现，通过 `zlbytes` 记录列表总字节数、`zltail_offset` 记录最后一个节点的偏移量快速定位结尾、`zllength` 记录列表元素个数、`entries` 数组存储列表节点、`zlen` 标志列表结束，以及每个`entry` 节点都持有前一个节点的长度 `prevlous_entry_length`，在 ZSet 取值时，可以通过**向后或者向前遍历**找到对应的值，其优点是由于内存是连续的，可以减少很多内存碎片和指针内存的占用，进而**节约内存**。

   - **缺点**：ZIPLIST 存在级联更新的问题，由于 `prevlous_entry_length` 存储的长度取决于前一个节点的长度，当前一个节点大于 254 字节时，这个值会从 1 字节变为 5 字节，从而导致后面值可能也会发生变更的连锁反应，此时的时间复杂度非常差为 O（n^2），但好在这种大范围的级联更新出现的概率并不大。

   ![1631954526032](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631954526032.png)

3. **SKIPLIST**：（< 3.2 版本时）在元素大小大于等于 64 字节且元素个数大于等于 **128** 个时会使用此编码，由跳跃表 + 哈希表来实现，通过在有序链表上维护每级 25% 概率生成的索引，从而达到 O（logn）访问元素的目的，通过累加查找路径中的 `SPAN` 跨度，来计算当前节点所处的排名，通过哈希表存储跳跃表节点，以实现在 O（1）内完成根据名称找到对应的得分。

   - **缺点**：由于新节点插入的 LEVEL 是随机的，导致老节点的查找路径可能发生变化，**缓存友好性不如**红黑树，而红黑树则是插入新节点后，大部分老节点仍然处于原查找路径上。
   - **为什么 Redis#ZSet 使用跳跃表而不是红黑树**？
     1. **范围查找效率高**：
        1. 对于 `ZRANGE` 和 `ZREVRANGE` 命令的范围查找，如果使用哈希表，则只能做单值查找，不适合做范围查找。
        2. 如果使用红黑树，需要中序遍历 [ 范围最小的后继，范围最大的前驱 ]，效率低且实现复杂。
        3. 而使用 skiplilst 只需要 O（logn）定位头尾结点，然后遍历链表即可，简单又高效。
     2. **内存占用少**：Redis skiplist 索引的默认生成概率为 25%，即每个结点平均只包含 **1.33** 个指针，内存占用比红黑树的 2 个指针要少。
     3. **实现与调试容易**：使用 skiplist 比红黑树更容易实现与调试。

   ![1631955510770](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631955510770.png)

##### 3、总

以上就是我对 Redis 常用数据结构的一些理解，请问还有什么细节需要补充的吗？

#### 1.2.1.6. 提升技术的方法？

1. **看视频**：入门。
2. **做实验**：踩坑。
3. **看源码**：找答案。
4. **看书**：系统性总结。

#### 1.2.1.7. 技术不足的地方？

某些技术缺乏实际场景应用，久而久之细节就忘却了，以及有许多知识点学完了，但没做系统性整理，比如 ES、Docker、K8S 这些。

#### 2.1.1.1. 项目自我介绍？

##### 1、自我介绍

面试官你好，我叫姚超松，2019 毕业于广东工业大学，读的是电子信息工程专业，毕业时秋招进了美的集团的美云智数事业部，的供应商部门，主要工作内容是负责功能模块的全栈开发，以及架构的优化。

下面我打算以时间倒序的方式，介绍一下我的项目经历：

1. 第一个，也是最近在做的一个项目是，美的集团的 GSRM 产品，也就是供应商关系管理，包括寻源、资质审查、现场评审、供方生效、退出、失效、画像等业务，技术的话用到了 SpringCloud 来拆的微服务，Sharding JDBC 和 MySQL 做的分库分表，中间件的话用到了 Redis 和 Kafka ，以及 MongoDB 做的一个接口日志收集，我在里面主要负责迭代现场评审模块，和供应商画像模块，以及一些外部接口对接的工作。
2. 再往前一点的话，就是一个自研的 Sass 产品，包括抽象了美的 QMS 业务的品质云，抽象了 MES 业务的进销存云，抽象了 SRM 业务的 SRM 云等等，技术的话，重构之前用到的是一个 SpringBoot 做的应用，数据库用的是 MySQL，中间件的话用到了 Redis 和集团的 ESB 企业总线，我在里面主要负责品质和进销存的需求开发，和数据中台的建设，以及后期品质云 SpringCloud 微服务的落地。
3. 再往前的话，就是推广到比亚迪的一个外部产品的迭代，在 SRM 2.0 的基础上级为 SRM 3.0，主要是增加了服务和资产类的交付、验收、结算等业务，技术的话，用到了 SpringMVC + Dubbo 做的应用，数据库用的是 Oracle，中间件的话用到了 Redis 和 RabbitMQ，我在里面主要负责交付单模块的开发，以及和云平台接口对接的工作。 

以上，就是我的自我介绍，请问有什么细节需要补充的吗？

##### 2、项目细节

###### 1）GSRMC

1. **微服务模块**：

   | 模块名 | 模块业务 | 备注                                                         |
   | ------ | -------- | ------------------------------------------------------------ |
   | BASE   | 基础数据 | 字典、物料、采购分类、研发分类等主数据                       |
   | POS    | 生命周期 | 寻源、资质审查、现场评审、供方生效、信息变更、黑名单管理、质保金管理、供方退出、失效等供应商主数据 |
   | PERF   | 考核绩效 | 供方送货不合格、交期、品质、服务考核 -> 考核会影响绩效（品质、成本、综合水平绩效） -> 绩效影响分级 -> 绩效（大头）与分级影响比例（QUO） -> 也可按规则分配比例（也叫配额） -> ERP根据比例决定供货数量 |
   | QUO    | 比例管理 | 绩效（大头）与分级影响比例（QUO） -> 也可按规则分配比例（也叫配额） -> ERP根据比例决定供货数量 |
   | BID    | 集采招标 | BID 招标定价，给物料定价 -> 一揽子价格                       |
   | PRICE  | 价格管理 | 生效之后，研发工程师在 PLM 分解POM，核价员估价，给定价做参考、线下定价以及BID 招标定价，给物料定价 -> 一揽子价格 |
   | CON    | 电子合同 | 电子合同模块，签署框架协议，合作时要遵守的规则               |
   | -      | -        | 还有ESB 交易数据、JOBS/TASK 定时任务、MIP 审批流程、SYNC 数据同步、INDEX 指标工作台等非核心业务模块 |

2. **主要业务流程**：

   1. 研发工程师在 PLM 根据业务，创建新物料 ITEM，跟 SRM 的采购分类进行关联。
   2. 然后选定采购分类和需求图纸等信息，在供应商库中做**寻源匹配**。
   3. 寻源完毕后，送样到供应商 GSC 进行初步**估价**。
   4. 然后 SRM 根据**资质**和价格进行筛选。
   5. 筛选通过后，PLM 进行试用，SRM 对供应商生产环境进行**现场评审**。
   6. 评审通过后生成供应商品类 ASL，此后就可以按照比例进行批量供货了。
   7. 其中， SRM 可以对供方送货不合格、交期、品质、服务等进行考核 -> 考核会影响**绩效**（品质、成本、综合水平绩效） -> 绩效影响分级 -> 绩效（大头）与分级影响比例（QUO） -> 也可按规则分配**比例**（也叫配额） -> ERP根据比例决定供货数量。
   8. 送货完毕后，SRM 需要对品类进行**核价**，包括招标转定价、意向价、直接定价、线下议价等方式。
   9. 最后走价格审批，把价格同步到 ERP，每月 25 号对上个月进行结算。

   => 总的来说，SRM 就是负责供应商从生到死的生命周期管理，以及物料价格和供货数量管理，也就是寻源到生效、需求到生产、定价到付款三个方向的业务。

3. **主要用到的技术组件**：

   | 组件                    | 作用                                                         | 部署           |
   | ----------------------- | ------------------------------------------------------------ | -------------- |
   | Vue + I-View            | 前端                                                         | 8 Apache       |
   |                         | 负载均衡                                                     | F5、Nginx      |
   | SpringCloud             | Eureka 做服务注册，Feign 做服务通信，Ribbon 做负载均衡，Sleuth+Zipkin 做链路追踪，Config 做配置中心、Stream+Bus 做消息驱动 | 126 应用服务器 |
   | Sharding JDBC + MyBatis | Sharding JDBC 分表，MyBatis ORM 映射                         | 126 应用服务器 |
   | MySQL                   | 数据库                                                       | 8 主 8 从      |
   | MongoDB                 | 文档数据库                                                   | 3 主备         |
   | Redis、Redisson         | 缓存中间件                                                   | 3 主备 + 哨兵  |
   | Kafka、ZK               | 消息中间件                                                   | 3 集群架构     |

4. **分表与分片规则的设置**：

   | 分表                   | 数据量     | 表注释             | 数据库 | 分片规则               |
   | ---------------------- | ---------- | ------------------ | ------ | ---------------------- |
   | srm_sys_items_submeter | 3280.19 w+ | 物料表             | BASE   | ${organization_code}   |
   | srm_sys_item_cates_sub | 1613.38 w+ | 物料采购分类关系表 | BASE   | ${organization_code}   |
   | srm_sys_item_keycs     | 2153.11w+  | 物料研发分类关系表 | BASE   | ${organization_code}   |
   | srm_po_receive_det     | 3.0813 E+  | 采购接收表         | PRICE  | ${period_month / year} |
   | srm_po_line_locations  | 1.0220 E+  | 一揽子价格表       | PRICE  | ${bu_code}             |

5. **服务器（虚拟机）的配置**：

   - **微服务**：基本每个虚机部署 2~3 个模块，平常 CPU 10%，内存 60%，所以一共注册了 126 个服务。
   - **数据库**：监控显示，32 C、128 G、3 T，平均响应时间 15ms，平均 30% CPU 利用率，内存利用率 75%。

   | 模块           | 数量              | CPU           | 内存               | 硬盘  |
   | -------------- | ----------------- | ------------- | ------------------ | ----- |
   | 前端 Apache    | 8                 | 4 C           | 16 G               | 50 G  |
   | Eureka         | 3                 | 1 C           | 1 G                | 10 G  |
   | BASE           | 13                | 8 C           | 16 G               | 100 G |
   | POS            | 19                | 8 C           | 16 G               | 100 G |
   | PERF           | 7                 | 8 C           | 16 G               | 100 G |
   | QUO            | 6                 | 8 C           | 16 G               | 100 G |
   | BID            | 3                 | 8 C           | 16 G               | 100 G |
   | PRICE          | 13                | 8 C           | 16 G               | 100 G |
   | CON            | 6                 | 8 C           | 16 G               | 100 G |
   | ESB            | 5                 | 8 C           | 16 G               | 100 G |
   | JOBS           | 9                 | 8 C           | 16 G               | 100 G |
   | TASK           | 5                 | 8 C           | 16 G               | 100 G |
   | MIP            | 4                 | 8 C           | 16 G               | 100 G |
   | SYNC           | 5                 | 8 C           | 16 G               | 100 G |
   | INDEX          | 3                 | 8 C           | 16 G               | 100 G |
   | 其他模块       | 25                | 8 C           | 16 G               | 100 G |
   | MySQL          | 16（8 主 8 从）   | 32 C          | 128 G              | 3 T   |
   | MongoDB        | 3                 | 8 C           | 16 G               | 500 G |
   | Redis          | 3                 | 8 C           | 32 G               | 100 G |
   | Kafka          | 3                 | 8 C           | 16 G               | 100 G |
   | 报表导出 OSS   | 文件数： 500 / 天 | 数据量：10 G  | 增长量：300 M / 天 | -     |
   | 供应商附件 OSS | 文件数：10 w / 年 | 数据量：200 G | 增长量：200 G / 年 | -     |

6. **应用的 JVM 参数**：

   ```bash
   nohup java -Duser.timezone=Asia/Shanghai 
   -Xms2048m -Xmx2048m 
   -XX:OnOutOfMemoryError="sh kkp.sh" 
   -XX:+HeapDumpOnOutOfMemoryError
   -XX:+PrintGCDetails -Xloggc:/apps/svr/srmpos/logs/gc.log
   -Dsun.net.inetaddr.ttl=30 -Dsun.net.inetaddr.negative.ttl=10   
   -jar srm-pos-2.0.0-SNAPSHOT.jar 
   --spring.cloud.bootstrap.location=/apps/svr/srmpos/bootstrap.yml 
   -Xbootclasspath/a:/apps/svr/srmpos/: > /apps/svr/srmpos/logs/srmpos_$(date +%Y-%m-%d).log &
   
   # kkp.sh
   pids=`ps -ef|grep srm-pos|grep -v grep|awk '{print $2}'`
   if [ "$pids" != "" ]; then
    echo "kill -9 $pids"
    kill -9 $pids
   fi
   ```

   `gc.log`：

   ![1648004562016](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1648004562016.png)

   ```shell
   OpenJDK 64-Bit Server VM (25.232-b09) for linux-amd64 JRE (1.8.0_232-b09), built on Oct 22 2019 16:14:06 by "mockbuild" with gcc 4.8.5 20150623 (Red Hat 4.8.5-39)
   
   Memory: 4k page, physical 16268352k(6193040k free), swap 8388604k(8089872k free)
   
   CommandLine flags: -XX:+HeapDumpOnOutOfMemoryError -XX:InitialHeapSize=2147483648 -XX:MaxHeapSize=2147483648 -XX:OnOutOfMemoryError=sh kkp.sh -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseParallelGC 
   
   1.626: [GC (Metadata GC Threshold) [PSYoungGen: 241412K->16509K(611840K)] 241412K->16517K(2010112K), 0.0255457 secs] [Times: user=0.03 sys=0.01, real=0.03 secs] 
   
   # PSYoungGen：指 Parallel Scavenge，ParOldGen：指 Parallel Old
   1.652: [Full GC (Metadata GC Threshold) [PSYoungGen: 16509K->0K(611840K)] [ParOldGen: 8K->15950K(1398272K)] 16517K->15950K(2010112K), [Metaspace: 20697K->20697K(1069056K)], 0.0707538 secs] [Times: user=0.09 sys=0.01, real=0.07 secs] 
   ```

###### 2）云化项目

1. **微服务模块**：（品质云）

   | 模块名 | 模块业务     | 备注                                                |
   | ------ | ------------ | --------------------------------------------------- |
   | BASE   | 基础数据模块 | 字典、IDM 用户、权限、系统附件、企业、客户、组织等  |
   | OEM    | 代工生产模块 | OEM 供方成品下线、抽检、出库等                      |
   | APP    | 综合模块     | SPC 过程检验，PQC  半成品检验、OQC 成品出货检验等   |
   | JOBS   | 定时任务模块 | 包括数据中台、PSI、QMS、GSC、GSRM、MES 等的数据同步 |

2. **品质云主要业务流程**：

   1. 供方原材料上线，MES 进行来料检测后，会走到各制程工序。
   2. 在每个制程工序的 CTQ 品质关键点上，会进行相应的 SPC 过程检验。
   3. 然后，还在工序的检验岗位上，进行对应的 PQC 半成品检验。
   4. 最后，在成品准备入库出货前，则进行 OQC 成品出货检验。
   5. 其中，对于 OEM 供应商，由于是代工生产，成品下线时，还需要进行对应的彩箱、大箱、地台板条码的绑定操作后，才能进行后面的入库出货操作。

   => 总的来说，品质云就是实时管控供方生产的**品质质量**。

3. **进销存云主要业务流程**：

   1. 接收到 GSC 供应商门户 的采购订单，就在进销存为供方生成对应的销售订单。
   2. 然后根据物料 BOM 信息，生成后续的生产订单。
   3. 在生产前，获取原材料时，可以直接在原材料仓进行扣减，也可以发起原材料的采购订单，进行补充原材料。
   4. 在生产完工后，会创建生产入库单，把成品送入成品仓，更新供方成品库存。
   5. 其中，对于 OEM 供应商，由于是代工生产，成品下线时，品质会把彩箱、大箱条码传入进销存，进销存再做对应的地台板绑定、销售出库、以及推送物流平台。

   => 总的来说，进销存就是用来实时管控供方的**库存情况**。

4. **数据中台主要业务流程**：数据中台，主要承担物料、ASL、寻源品质进销存的一些统计工作。

5. **主要用到的技术组件**：

   | 组件             | 作用                                                         | 部署         |
   | ---------------- | ------------------------------------------------------------ | ------------ |
   | Vue + Element UI | 前端                                                         | 8 Nginx      |
   |                  | 负载均衡                                                     | F5、Nginx    |
   | SpringCloud      | 微服务拆分后，Eureka 做服务注册，Feign 做服务通信，Ribbon 做负载均衡，Sleuth+Skywalking+ELK 做链路追踪，Config 做配置中心，Gateway 做服务网关 | 应用服务器   |
   | SpringBoot       | 微服务拆分前，还是单体应用                                   | 3 应用服务器 |
   | MyBatis          | MyBatis ORM 映射                                             | 3 应用服务器 |
   | MySQL            | 数据库                                                       | 1主          |
   | Redis、Jedis     | 缓存中间件                                                   | 3 主 3从集群 |

6. **表数据量**：

   | 分表                        | 数据量    | 表注释         | 系统 | 分片规则               |
   | --------------------------- | --------- | -------------- | ---- | ---------------------- |
   | qc_oem_order_line           | 1200 w+   | 成品下线表     | QC   | 手工按日期分片备份旧表 |
   | qc_oqc_item_standard_value  | 1500 w+   | 物料抽检标准表 | QC   | 手工按日期分片备份旧表 |
   | oem_external_mes_colorcode  | 722.26 w+ | MES 条码表     | QC   | 无分片                 |
   | psi_base_packing_relation   | 315.28 w+ | 箱包关系表     | PSI  | 无分片                 |
   | psi_prd_pallet_box_relation | 250.03 w+ | 板箱关系表     | PSI  | 无分片                 |
   | psi_base_barcode            | 314.67 w+ | 下线条码表     | PSI  | 无分片                 |
   | psi_sales_stock_out         | 840.58 w+ | 销售出库表     | PSI  | 无分片                 |
   | cloud_gsrm_item             | 38.61 w+  | 物料表         | DC   | 无分片                 |
   | cloud_gsrm_asl              | 31.63 w+  | ASL 物料表     | DC   | 无分片                 |
   | mcc_company_info            | 1321      | 企业统计表     | DC   | 无分片                 |
   | mcc_deliver_receive_sum     | 2.24 w+   | 企业下线出库表 | DC   | 无分片                 |
   | mcc_order_quotation_sum     | 342       | 企业寻源报价表 | DC   | 无分片                 |

7. **服务器（虚拟机）的配置**：

   - **微服务**：基本每个虚机部署 2~3 个模块，平常 CPU 10%，内存 60%，所以一共注册了 14 个服务。
   - **数据库**：监控显示，16 C、64 G、3 T，平均响应时间 15ms，平均 30% CPU 利用率，内存利用率 75%。

   | 模块             | 数量             | CPU          | 内存              | 硬盘  |
   | ---------------- | ---------------- | ------------ | ----------------- | ----- |
   | 前端 Nginx       | 3                | 4 C          | 16 G              | 50 G  |
   | Eureka           | 3                | 1 C          | 1 G               | 10 G  |
   | BASE             | 3                | 8 C          | 16 G              | 100 G |
   | APP              | 3                | 8 C          | 16 G              | 100 G |
   | OEM              | 3                | 8 C          | 16 G              | 100 G |
   | JOBS             | 2                | 8 C          | 16 G              | 100 G |
   | MySQL            | 1 主             | 16 C         | 64 G              | 3 T   |
   | Redis            | 6                | 8 C          | 16 G              | 100 G |
   | 文件存储 FASTDFS | 文件数：1 w / 年 | 数据量：20 G | 增长量：20 G / 年 | -     |

8. **应用的 JVM 参数**：

   ```bash
   -Xmx1000m -Xms1000m -Xss256k -Dgen_data_center_id=0 -Dgen_machine_id=2
   ```

   单体时，翻车更改为：

   ```bash
   -Xmx5000m -Xms5000m -Xmn1000m ...
   ```

###### 3）比亚迪 SRM

1. **微服务模块**：

   | 模块名 | 模块业务       | 备注               |
   | ------ | -------------- | ------------------ |
   | APP    | 前端控制器模块 | Controller，6 台   |
   | MID    | 业务处理模块   | Service，7 台      |
   | RPORT  | 报表模块       | 报表导出，6 台     |
   | NGINX  | 前端模块       | WEX5、Jquery，2 台 |

2. **主要业务流程**：

   1. 定时同步云平台 SAP 需要的基础信息，比如采购订单、采购金额、采购数量等到 SRM，按照履行模板和规则，生成相应的履行计划和付款计划。
   2. 通过履行计划触发，生成交付、验收、结算对应节点的订单与待办信息，通知对应的责任人进行审批或者验收。
   3. 通过迁移线下交付单到线上，完成各类的无纸化交付，交付单审批通过后，会推进下一个节点的履行计划和付款计划。
   4. 通过付款计划触发，生成发货款、到货款、调试款、验收款、上线款、完工款等多种款项的待办，通知对应的责任人进行审批或者结算。

   => 总的来说，就是通过交付单推进履行计划和付款计划，来数字化线下供方结算与付款流程。

3. **主要用到的技术组件**：

   | 组件                | 作用             | 部署          |
   | ------------------- | ---------------- | ------------- |
   | JQuery + Element UI | 前端             | 2 Nginx       |
   |                     | 负载均衡         | 2 Nginx       |
   | Spring MVC + Tomcat | 后端应用         | 19 应用服务器 |
   | MyBatis             | MyBatis ORM 映射 | 19 应用服务器 |
   | Oracle              | 数据库           | 1 主          |
   | Redis、Jedis        | 缓存中间件       | 3 主备 + 哨兵 |
   | RabbitMQ            | 消息中间件       | 3 镜像集群    |

4. **表数据量**：

   | 分表          | 数据量     | 表注释              | 系统   | 分片规则 |
   | ------------- | ---------- | ------------------- | ------ | -------- |
   | SAP_CDHDR     | 6.39 E+    | PO 单增量表         | 云平台 | 无需分片 |
   | SAP_EKPO      | 4019.80 w+ | PO 行表             | 云平台 | 无需分片 |
   | SAP_ZMMSSBM03 | 1544.27 w+ | PO 描述表           | 云平台 | 无需分片 |
   | SAP_EKKO      | 1147.68 w+ | PO 币种表           | 云平台 | 无需分片 |
   | SAP_ZEKKO     | 965.70 w+  | PO 寻源表           | 云平台 | 无需分片 |
   | PR / PO       | 1948.13 w+ | 采购需求 / 采购订单 | SRM    | 无需分片 |

5. **服务器（虚拟机）的配置**：

   - **微服务**：基本每个虚机部署 2~3 个模块，平常 CPU 10%，内存 60%，所以一共注册了 21 个服务。
   - **数据库**：监控显示，32 C、128 G、3 T，平均响应时间 15ms，平均 30% CPU 利用率，内存利用率 75%。

   | 模块             | 数量             | CPU          | 内存              | 硬盘  |
   | ---------------- | ---------------- | ------------ | ----------------- | ----- |
   | 前端 Nginx       | 2                | 4 C          | 16 G              | 50 G  |
   | APP              | 6                | 8 C          | 16 G              | 100 G |
   | MID              | 7                | 8 C          | 16 G              | 100 G |
   | RPORT            | 6                | 8 C          | 16 G              | 100 G |
   | Oracle           | 1                | 32 C         | 128 G             | 3 T   |
   | Redis            | 3                | 8 C          | 16 G              | 100 G |
   | 文件存储 FASTDFS | 文件数：1 w / 年 | 数据量：20 G | 增长量：20 G / 年 | -     |

6. **应用的 JVM 参数**：

   ```bash
   -Xmx1000m -Xms1000m -Xss256k -Dgen_data_center_id=0 -Dgen_machine_id=2
   ```

   单体时，翻车更改为：

   ```bash
   -Xmx5000m -Xms5000m -Xmn1000m ...
   ```

##### 3、项目亮点

| 系统       | 亮点清单          | 关键词                                                       |
| ---------- | ----------------- | ------------------------------------------------------------ |
| GSRMC      | PLM 配套接口      | Around 切面注解、Kafka + MongoDB 、MD5  + AES、多线程 + Future、Redis、Jmeter |
|            | 供应商画像同步    | 缓存型线程池、工厂 + 建造者 + 责任链、EsJob + Kafka + MongoDB、线程池调优 |
|            | 线程池调优        | 线程池源码、调优原则                                         |
|            | 死锁问题排查      | 意向锁、间隙锁、可重复读、并发                               |
|            | 内存溢出排查      | MAT、POI、ArrayList                                          |
| Sass 产品  | 条码销售出库      | Druid 监控、SQL 调优、Redis                                  |
|            | 品质云微服务拆分  | SpringCloud                                                  |
|            | 校验失败问题      | 分布式锁、QMS Client Json 对比、问题排查                     |
| 比亚迪 SRM | 大文件上传        | FastDFS、Redis List                                          |
|            | 邮件发送组件      | 线程池 + RabbitMQ + 定时任务                                 |
|            | 发版平台 CPU 过高 | Tomcat Manager、ELK、CPU 95%                                 |

STAR 法则，是情境（Situation）、任务（Task）、行动（Action）、结果（Result）四项的缩写，是一种讲述自己故事的方式，或者说，是一个清晰、条理的作文模板，合理熟练运用该法则，可以轻松的描述事情的逻辑方式，表现出分析与问题的逻辑性、条理性和逻辑性。

1. Situation：情境，本次事件是在什么情况下发生的。
2. Task：任务，在本次事件中，主要负责什么任务。
3. Action：行动，在本次事件中，针对这些情况的分析，采用了什么样的行动。
4. Result：结果，本次事件最后的结果是怎么样的，以及学到了什么。

| 举例 | 内容（大一辩论比赛获得冠军）                                 |
| ---- | ------------------------------------------------------------ |
| S    | 系里一共有 5 支队伍，实例...，我们小组...                    |
| T    | 熟悉辩论流程，掌握辩论技巧，获得系冠军                       |
| A    | 自己主动整理资料，组织小组学习流程，编制训练题，小组训练，根据每个人的特点，分配任务（要尽量详细，包括当中遇到的困难是什么，怎么解决的） |
| R    | 获得系辩论赛冠军                                             |

###### 1）PLM 配套接口 | 分表、并发、Redis

1. **背景 Situation**：供应链体系管理专员反馈，他们在 PLM 建送样申请单时，由于物料配套没有**自动匹配**，导致经常选错配套人员，然后就要撤回、修改、重新提交，影响业务流程效率。

2. **任务 Task**：所以他们希望在 PLM 上，根据物料就可以**自动匹配** SRM 品类分工中的**配套专员信息**，方便他们走业务流程。

3. **行动 Action**：

   1. 接口的实现逻辑大体是这样的，根据 ITEM_CODE + ORG_CODE 找对应分表的采购分类，找不到就根据 ITEM_CODE 找全部库存组织分表的采购分类，再找不到的话就认为要去查物料试用表了，以当前作为试用 P 编码找到对应转正后的编码，然后重走一遍上面逻辑，找到后就合并按照 BU_CODE + 特征名称 + 特征值 + 研发分类 ID，查找研发分类中的采购分类，然后根据采购分类查找品类分工表中的配套人员信息，去重后返回即可。

      ![1644903196224](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1644903196224.png)

   2. 接口实现比较复杂，其中需要优化的点有：

      - 1）第一，接口日志要做统一的收集。
      - 2）第二，接口要做安全性校验。
      - 3）第三，接口要查询所有的库存组织的采购分类分表。
      - 4）第四，接口需要频繁按照 ORG_CODE 查找 BU_CODE。

4. **结果 Result**：所以我当时优化的手段就是：

   - 1）第一，写了一个 Around 类型的切面注解，对公共的日志收集逻辑进行了提取，然后把收集到的日志投递到 Kafka 中，再由消费者插入到 MongoDB，这种做法的好处是，使得日志收集逻辑解耦了业务代码，同时异步收集的话经过测试，对比同步插入还有一定的性能提升。
   - 2）第二，安全性校验这边统一采用的是 MD5 对整个请求体生成摘要，SRM 需要重新生成一遍，然后与传过来的比对是否一致，一致才认为请求体没被修改过，然后再根据缓存中的 AES 秘钥进行解密，得到真正的请求 JSON 串。
   - 3）第三，使用线程池多线程并发的方式，查询每个采购分类分表，通过 FutureTask 监听汇总所有的采购分类编码，对比串行查询有一定的性能提升。
   - 4）第四，由于 ORG_CODE 与 BU_CODE 是那些不怎么会改变的数据，所以通过使用了 Redis 对这些关系进行了远端缓存（使用 Guava Cache 重启可能会导致缓存雪崩，且数据量过大还会占用应用内存，本身也才 2 G，所以放 Redis 中，虽然增多了一次 I/O 这也是可以接受的），使得多次请求只会获取一次库存事业部关系，对比批量查询时有一定的性能提升。 

###### 2）供应商画像同步 | 架构、设计模式、Kafka

1. **背景 Situation**：
   1. 业务方需要一个页面，可观地展示出供应商各维度的一个画像，包括抬头展示它的**基本信息**，词云浓缩它的**肖像标签**，动画展示它自美的引入以后所走的一些**历程**，对接天眼查展示对应它的一些企业经营风险，柱状图展示它历年的**招投标**、**红黄牌**以及**采购金额**的数量，折线图展示它历年的**考核**和**绩效**的趋势，饼图展示它的**分级**和**供货编码**的占比情况，用来横向、纵向辅助对比找出优质供方。
   2. 允许数据延迟，可以每隔 15 天同步一次。
2. **任务 Task**：这样，要做的东西就有，在 TASK 模块的定时任务触发时，要一一查找每家供应商的 POS 供方生命周期库的基本信息、红黄牌、获奖以及供货编码信息，查找 BID 招标库的历年招标信息，查找 PERF 考核绩效库的历年考核、绩效以及分级信息，查找 PRICE 价格库的采购金额信息，对接天眼查找告警风险，最后再根据规则组装出要展示的肖像标签。
3. **行动 Action**：当时有几种方案，
   - 1）第一种就是，1 个定时器，负责同步所有供应商的上述所有维度的信息，优点是实现简单，不用做其他的架构设计，缺点是如果同步实现的话一个事务完成所有工作，任意一处异常则全局回滚，如果异步实现的话，由于是单点触发定时任务，还是会有单台机器负担过重的问题。
   - 2）第二种就是，10 个定时器，每个定时器只负责同步所有供应商的某个维度的信息，优点是可以分为 10 台机器分别触发定时，避免了单台机器负担过重，缺点是一个定时器一个事务，还是会出现任意一处一处则全局回滚，以及定时器过多，管理麻烦。
   - 3）第三种就是，1 个定时器 + 分布式消费，定时器只负责找出要生成画像的供应商编码，封装成消息，扔到 Kafka 上，自己不负责画像的生成，而是交给消费者去进行处理，消费者每次消费一个消息，相当于生成一个供应商的画像，这样做的好处就是，定时逻辑和画像生成逻辑解耦，1 个定时器即可完成任务，要管理维护的地方少，然后就是，画像生成的效率取决于 Partition 和机器的数量，能够充分利用集群多实例部署的优点，还有就是，即使某次供应商画像生成失败了，可以不进行手工 ACK，下次再从 Kafka 里拿消息出来进行重复消费即可。
4. **结果 Result**：所以，当时就采用了第三种方案，在实现时又有几个优化点：
   - 1）第一就是，还是没能解决由于一个供应商所有维度处于同一个事务，出现异常时的全局回滚问题，解决方案就是，通过线程池的方式异步实现，每个线程一个事务，只完成一个维度的信息同步，这样即使某个回滚了也不影响全局。
   - 2）第二就是，由于一共有 10 个维度的信息要同步，就需要 10 个 Callable 任务给线程去执行，所以就抽象了公共的接口。
   - 3）第三就是，由于任务较多，所以就采用了工厂 + 建造者 + 责任链来串联式地组织任务，而且到了后期，由于是责任链的组织方式，就可以根据业务组合出很多种画像的同步方案出来，体现了灵活性。
   - 4）第四就是，由于任务与任务间，经常有重复的信息要查询，所以就对这个接口分为了同步式实现和异步式实现的抽象类，业务只需要继承抽象类实现对应的业务即可，同步式实现主要是为了在一开始给执行链设置公共上下文，避免重复查询，异步式实现则是交给线程池去执行，在要拿那些”重复“信息时只需要去上下文中获取即可。
   - 5）第五就是，某次消费异常，也就是执行链中某个节点异常，其堆栈信息需要对其进行合理记录，方便后面排查问题，解决方案就是，通过如果有任务异常后，那么就收集放到执行链上下文中，返回时再统一地打包成对象，存放到 MongoDB 上。
   - 6）最后，这样设计和实现，用 Kafka + 线程池，保证了分布式高性能消费，用 Kafka + 异常时不手工 ACK 来重复生成画像，保证可靠性，用 MongoDB 记录异常日志，保证异常排查的便捷性。

###### 3）线程池调优 | 线程池、源码、调优

1. 我们系统某个模块中常常需要使用别的模块的数据源，比如在 POS 模块中需要去查询 PRICE 库，此时，由于主线程默认绑定的是 POS 数据源，要想使用 PRICE 数据源的话，就需要开辟子线程去做切换，所以就为每个模块抽象了数据源切换的工具类。
2. 由于正常都是调用一次切换方法，也就是把 Lambda 表达式提交给线程池，主线程再去做 `FutureTask.get()` 的监听，拿到结果后返回，这是正常情况，因此我们一开始也没怎么留意。
3. 但问题就来了，有一次某段代码对连续进行了两次的切换，就发现接口超时，以及往后的切换都超时了，那时排查了很久，最后在这个切换工具类里看到，生产上跑的参数为 `corePoolSize=0、maximumPoolSize=32、workQueue=LinkedBlockingQueue`，按照原理，本以为是错误的配置，会导致任务都阻塞在队列中直到 OOM 发生的，但改成一次切换又可以了，这就很奇怪了，然后就去研究源码，发现刚才上面所说的那段**补充非核心线程**的逻辑，也就是即使 `corePoolSize=0`，只要最大线程数不为 0，那么还是会保证至少创建一个线程去消费任务队列的，所以切换一次就可以拿到任务的运行结果。
4. 那么为什么切换两次就不行了呢？这是因为，在第一次切换时，是主线程在做 `FutureTask.get()`，阻塞的是主线程，而如果还做一次连续切换，那么就会导致线程池中仅有的 1 个线程也做了 `FutureTask.get()`，从而进入了阻塞，但线程池认为有效线程数还是 0，所以新任务进来也不会去创建新线程消费，会直到 OOM 的发生为止~
5. 最后，更正了合理的构造参数 `corePoolSize=Runtime.getRuntime().availableProcessors() + 1`，`maximumPoolSize=2 * Runtime.getRuntime().availableProcessors()`，`keepAliveTime=120s`，`LinkedBlockingQueue=1024` 保证不会发生内存溢出，`Google#ThreadFactoryBuilder`来设构建线程工厂、在线程创建时设定名称前缀和递增线程号，`RejectedExecutionHandler=CallerRunsPolicy` 保证任务不丢失，同时设置 `allowCoreThreadTimeOut=true` 允许核心线程空闲时也会被回收。

###### 4）死锁问题排查 | 间隙锁、可重复读、并发

1. **背景 Situation**：在现场评审优化了一版上线后，DBA 反馈说，POS 数据库因为有大量死锁，导致数据库不断地重启，并且也把问题的 SQL 给发了出来，是一条删除语句导致的：

   ```sql
   DELETE FROM srm_reiew_audit_evidence WHERE locale_review_id = 3 AND REVIEW_TYPE = 3;
   ```

2. **任务 Task**：然后我们很快就定位到代码，找到了以下逻辑：方法传入一个 `List<DTO>`，先是获取第一行的  locale_review_id `${localeReviewId}`，对满足 locale_review_id= `${localeReviewId}` 的都进行删除，完成删除后，再批量插入 locale_review_id= `${localeReviewId}` 的 `List<DTO>` 数据：

   ```java
   int flag=0;
   Long localeReviewId=new Long("1");
   for (AuditEvidenceDto auditEvidenceDto : list) {
       if (auditEvidenceDto.getEvidenceId()!=null){
           flag=1;
           localeReviewId=auditEvidenceDto.getLocaleReviewId();
       }
   }
   HashMap<String,Object>params =new HashMap<>();
   if (flag==1){
       params.put("localeReviewId",localeReviewId);
       params.put("categoryCode",list.get(0).getCategoryCode());
       params.put("reviewType",list.get(0).getReviewType());
       auditEvidenceService.deleteFlag(params);
   }
   if(!"Y".equals(list.get(0).getAttribute1())){
       list.sort(Comparator.comparing(AuditEvidenceDto::getFileType));
       auditEvidenceService.batchInsertAuditEvidence(list);
   
   }
   ```

3. **行动 Action**：

   1. 对于这种写法，首先，在代码里做业务逻辑操作，肯定是不对的，当时事态紧急，也没有做过多的分析，最直接的解决思路就是，找到对应的同事，让他把 Controller  `delete` 和 `insert` 操作都放入一个 Service 中，用同一个事务管理，做一个紧急发版看下能否解决。

   2. 发了新版后，好像问题是解决了，但是结合 `show engine innodb status`，就分析出在并发场景下，还是会死锁的问题，步骤是：

      1. 先在表中先插入几条数据，然后给 locale_review_id 加普通索引（而在表全新，且没有数据时，有没有索引都会加一个行 X 锁，然后也会出现下面的情况）。
      2. 然后，事务 1 对 locale_review_id=3 的记录进行删除，则会先获取一个表级的 IX 锁，然后由于记录不存在，而对 （负无穷，13）、（13，正无穷）上一个间隙锁。
      3. 同理，事务 2 再对 locale_review_id=3 的记录进行删除，也会先获取一个表级的 IX 锁，然后由于记录不存在，而对 （负无穷，13）、（13，正无穷）上一个间隙锁，由于间隙锁不冲突，所以事务 2 不会等待。
      4. 接着，事务 1 再插入  locale_review_id=3 的记录时，由于已经存在事务 2 的间隙锁，所以需要等待它释放，才能获取到插入意图的间隙锁。
         - 在插入行之前，会设置一种称为**插入意图的间隙锁**，表示插入的意图，即如果插入到同一索引间隙中的多个事务未插入到间隙内的同一位置，则它们无需相互等待，是不冲突的。
         - 而如果发生重复键错误，则会在重复索引记录上设置**共享锁**，并且如果另一个会话已经拥有排他锁，那么如果有多个会话尝试插入同一行，则使用共享锁可能会导致**死锁**，比如说后面分析的那种情况。
      5. 同理，事务 2 再插入  locale_review_id=3 的记录时，由于已经存在事务 1 的间隙锁，所以需要等待它释放，才能获取到插入意图的间隙锁。
      6. 此时，就发生了死锁，即事务 1 持有间隙锁，同时等待事务 2 的间隙锁，事务 2 也持有间隙锁，同时等待事务 1 的间隙锁，如果没有外力作用，它们都将无法推进下去，而在 MySQL 5.6.26 上模拟则是，自动回滚了事务 2 的事务，让事务 1 顺利得推进下去，成功插入 locale_review_id=3 的记录。

      ```sql
      -- 使得 show engine innodb status; 能展示锁的信息
      set GLOBAL innodb_status_output_locks=ON;
      -- 查看事务锁持有情况
      show engine innodb status;
      ```

      | 步骤 | 事务 1                                                       | 事务 2                                                       |
      | ---- | ------------------------------------------------------------ | ------------------------------------------------------------ |
      | 1    | SET AUTOCOMMIT = 0; START TRANSACTION;                       | SET AUTOCOMMIT = 0; START TRANSACTION;                       |
      | 2    | DELETE FROM srm_reiew_audit_evidence WHERE locale_review_id = 3 AND REVIEW_TYPE = 3; |                                                              |
      | 3    |                                                              | DELETE FROM srm_reiew_audit_evidence WHERE locale_review_id = 3 AND REVIEW_TYPE = 3; |
      | 4    | INSERT INTO test.srm_reiew_audit_evidence (EVIDENCE_ID, locale_review_id, FILE_TYPE,...) VALUES (3, 3, '3'...); 等待中 |                                                              |
      | 5    |                                                              | INSERT INTO test.srm_reiew_audit_evidence (EVIDENCE_ID, locale_review_id, FILE_TYPE,...) VALUES (3, 3, '3'...); |
      | 6    |                                                              | DEAD LOCK；Auto Roll Back                                    |
      | 7    | 插入成功                                                     |                                                              |

   3. 所以，这种方案也不是万全之策，然后我们重新考虑新的方案，其思路是打算先把一开始的死锁给复现出来，再做解决方案的分析。

   4. 不久后，场景就被模拟出来了，那是 `delete` + `insert` 在不同事务中导致死锁的发生，具体的步骤是：

      1. 事务 1 执行删除，但未提交，由于 locale_review_id=3 的记录已经存在，所以在 locale_review_id=3 行上了排他锁，此时该行的主键 ID 也是等于 3，即 ID=3。
      2. 然后，事务 2 打算插入 ID=3 的记录，由于 ID=3 的记录已经存在，发生了主键重复错误，所以需要请求 ID=3 行的共享锁，而又由于 ID=3 行上存在排他锁，所以事务 2 进入等待。
      3. 同理，事务 3 打算插入 ID=3 的记录，由于 ID=3 的记录已经存在，发生了主键重复错误，所以需要请求 ID=3 行的共享锁，而又由于 ID=3 行上存在排他锁，所以事务 3 也进入等待。
      4. 接着，事务 1 提交，ID=3 的行记录被标记为删除状态（这些标识为删除状态的记录，会后续由后台的 Purge 操作进行物理删除，但是，此时还是会在索引中存放一段时间），ID=3 上的排他锁被释放。
      5. 然后，事务 2、3 就成功抢到了共享锁，打算执行插入操作，由于已经存在事务 3 的共享锁，所以需要等待它释放，才能获取到插入意图的间隙锁，事务 2 进入等待。
      6. 同理，由于已经存在事务 2 的共享锁，所以需要等待它释放，才能获取到插入意图的间隙锁，事务 3 也进入等待。
      7. 此时，就发生了死锁，即事务 2 持有共享锁，同时等待事务 3 的共享锁，事务 3 也持有共享锁，同时等待事务 2 的共享锁，如果没有外力作用，它们都将无法推进下去，而在 MySQL 5.6.26 上模拟则是，自动回滚了事务 3 的事务，让事务 2 顺利得推进下去，成功插入 locale_review_id=3 的记录。

      | 步骤 | 事务 1                                                       | 事务 2                                                       | 事务 3                                                       |
      | ---- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
      | 1    | SET AUTOCOMMIT = 0; START TRANSACTION;                       | SET AUTOCOMMIT = 0; TRANSACTION;                             | SET AUTOCOMMIT = 0; START TRANSACTION;                       |
      | 2    | DELETE FROM srm_reiew_audit_evidence WHERE locale_review_id = 3 AND REVIEW_TYPE = 3; |                                                              |                                                              |
      | 3    |                                                              | INSERT INTO test.srm_reiew_audit_evidence (EVIDENCE_ID, locale_review_id, FILE_TYPE,...) VALUES (3, 3, '3'...); 等待中 |                                                              |
      | 4    |                                                              |                                                              | INSERT INTO test.srm_reiew_audit_evidence (EVIDENCE_ID, locale_review_id, FILE_TYPE,...) VALUES (3, 3, '3'...); 等待中 |
      | 5    | COMMIT;                                                      |                                                              |                                                              |
      | 6    |                                                              |                                                              | DEAD LOCK；Auto Roll Back                                    |
      | 7    |                                                              | 插入成功                                                     |                                                              |

   5. 面对以上的分析情况，解决方案有两个，第一个方案是，在后面批量插入前，先把 ID 给置为 NULL，利用自增机制去设置 ID，避免两个 `insert` 语句同时插入同一个位置，但问题是，`delete` 在释放排他锁后，两个 `insert` 语句都会执行成功，会比正确结果多出来一条记录，所以此方案放弃。

   6. 第二个方案是，把 `delete` + `insert` 的操作，转换为 `update` 操作，则可以避免上述死锁的发生，通过。

4. **结果 Result**：最后我们就是通过第二个方案，把这段有问题的代码，又重新优化了一遍，解决了这种死锁的问题。

###### 5）内存溢出排查 | MAT、POI、ArrayList

1. **背景 Situation**：生产的一台 POS 服务器宕机了，由于配置了 `-XX:+HeapDumpOnOutOfMemoryError`，所以崩溃时导出了对应的 hprof 文件。

2. **任务 Task**：导入 hprof 到 MAT，分析应用崩溃的原因。 

3. **行动 Action**：

   1. MAT -> Leak Suspects 做内存泄露分析，然后就得到了怀疑报告：说，一个 Tomcat 的线程使用了 78.58% 的堆大小，共 1.2 GB。

      ![1646412309344](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646412309344.png)

      ![1646413204478](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646413204478.png)

   2. 然后，查看 List Objects -> with outgoing references，观察一个占据了 22.68% 内存的 ArrayList 的保留集，发现它引用了 27.69 w 个元素，估计是某个 SQL 查了太多元素导致。

      ![1646413486175](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646413486175.png)

   3. 所以，就继续查看堆栈信息，定位问题代码，是一个 Service 实现类的 `setData()` 方法。

      ![1646413368891](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646413368891.png)

   4. 原来，是调用了 Controller 的 `downloadItemDailyCapacityTemplate()` 方法，看了下业务含义，大概的意思是，在导出日常产能预测模板时，由于把产能表每日信息的查询结果，放入到了上面所说的那个 27.69 w 个元素的 ArrayList 中，然后再遍历这个列表，调用 POI#API 在内存中生成 excel，但由于堆内存只配置了 `-Xmx=2048m`，所以导致了内存溢出！

      ![1646413877943](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646413877943.png)

4. **结果 Result**：因此，解决方案是，持有这么大的 ArrayList 时，不应该把 excel 还写入内存中，而是遍历过程中，先把 excel 一点一点写入到磁盘，释放掉这个 ArrayList 后，再读取磁盘的 excel 文件的字节流，写入到 response 输出流中，或者直接扩大堆内存大小 `-Xmx=4096m `，从而解决堆内存溢出的问题。 

###### 6）自研 @DataSource 注解，ThreadLocal 没及时释放的问题 | 线程池、ThreadLocal、数据源

1. **背景 Situation**：生产 JOBS 模块，采购金额汇总导出接口报错：同一线程不允许切换数据源读写类型，定位到报错的代码位置，发生在 `@DataSource` 自研注解，的管理器 `DataSourceClusterManager` 中的一个段逻辑代码上：

   ```java
   private static final ThreadLocal<DataSourceThreadInfo> dataNodeInfo = new ThreadLocal();
   
   public static void set(String nodeName, boolean readOnly) {
       if (StringUtils.isBlank(nodeName)) {
           nodeName = getDefaultNode();
           if (StringUtils.isBlank(nodeName)) {
               throw new RuntimeException("系统未配置缺省数据节点");
           }
       }
   
       DataSourceThreadInfo nodeInfo = (DataSourceThreadInfo)dataNodeInfo.get();
       if (nodeInfo != null) {
           if (!StringUtils.equals(nodeName, nodeInfo.getNodeName())) {
               throw new RuntimeException("同一线程不允许切换数据源");
           } else if (readOnly != nodeInfo.isReadOnly()) {
               throw new RuntimeException("同一线程不允许切换数据源读写类型");
           }
       } else {
           dataNodeInfo.set(new DataSourceThreadInfo(nodeName, readOnly));
       }
   }
   ```

2. **任务 Task**：分析逻辑代码就可以知道，是因为在设置注解属性 `readOnly` 时，发现和已有的 `readOnly` 不等，比如 `true != false`，然后就想到是存储这些属性的 `ThreadLocal` 变量，在上一次使用完的线程，没清除掉这些变量，就被下一个接口复用了，从而导致的问题发生，经过各种实验，也证明了确实是这个原因。

3. **行动 Action**：

   1. 至于问题的解决，首先还是看发生问题的业务代码，它把一些查询封装成了 Lambda 表达式，然后调用了数据源切换工具类的方法，丢给线程池去执行。

   2. 问题就在于这个 Lambda 表达式中，比如先调用 `DataSourceThreadInfo.set("srmpos"，true)`，然后调用 mapper 接口查询数据库，最后在却没有在释放掉线程本地变量，导致另一个接口复用这个线程时设置 `DataSourceThreadInfo.set("srmpos"，false)` 报错。

   3. 然后，查找了整个系统这些问题代码，发现很多都没作释放的，此时最好的方案要么是 AOP，要么是代理。

   4. AOP 的话，由于没有很好的一个切点表达式，因为如果只找那些打了 `@DataSource` 注解的 service 或者 mapper，那么就会漏掉一些直接 `DataSourceThreadInfo.set(..)` 设置的，这也正是为什么之前配了 `DataSourceAfterAspect` 切面，还是会发生这个问题的原因所在，所以这个方案放弃。

      ```java
      @Aspect
      @Component
      @Order(0)
      public class DataSourceAfterAspect {
          /**
           * 数据源处理完后清理
           * @param joinPoint
           */
         @After("@within(com.midea.mcomponent.mybatisplus.datasource.DataSource) || @annotation(com.midea.mcomponent.mybatisplus.datasource.DataSource)")
          public void after(JoinPoint joinPoint) {
              DataSourceClusterManager.clean();
          }
      }
      ```

   5. 代理的话，可以在切换工具类的 `queryMethod.query(params)` 方法调用后，通过在 `finally` 中实现线程本地变量的释放。

      ```java
      //  baseNode 数据源 线程池
      public static <V> Future<V> executeSrmBaseQuery(QueryMethod<V> queryMethod, Object... params) {
          return executeSrmBase.submit(() -> {
              try {
                  return queryMethod.query(params);
              } finally {
                  DataSourceClusterManager.clean();
              }
          });
      }
      ```

4. **结果 Result**：最终，发版测试，没问题后上线，最终解决问题。

###### 7）条码销售出库 | 索引调优、生产问题

1. **背景 Situation**：OEM 供应商反馈，系统响应扫码速度过慢，专门用手机计时，记录延迟有 4.55 s，扫码枪扫完一整板的货，系统才开始对异常条码报错，导致如果一板出现问题条码的话，就需要这一整板重新扫过，才能找出问题条码，影响了供方出货的效率。

2. **任务 Task**：当时，OEM 供方有 2 个，日班和夜班一共有 2 条产线，根据视频中的操作速度，大概是 4~5 个 / 1s，所以也就是并发数为 5 * 2 = 10 的 QPS，分摊到 3 台服务器的话，每台服务器需要让接口满足 300 ms 以下的延迟，因此，在不增加服务器的情况下，尽量让接口的延迟足够低。

3. **行动 Action**：根据供方提供的截图，找到对应的接口，然后在测试环境模拟跑一遍接口，把接口路过的 SQL 日志都收集起来，统一分析，最终发现有几处索引是没有添加的，分别是：

   1. **psi_base_barcode**：条码表，314.67w+，16 c 64 G MySQL， 优化前 Druid SQL 监控【执行时间】显示 1000 ms 左右，优化后 Druid SQL 监控显示 35 ms 左右。

      ```sql
      ALTER TABLE psi_base_barcode ADD INDEX psi_base_barcode_n2(CUS_COMPANY_CODE, PRODUCT_CODE);
      ```

   2. **psi_base_packing_relation**：箱包关系表，315.28w，16 c 64 G MySQL， 优化前 Druid SQL 监控【执行时间】显示 650 ms 左右，优化后 Druid SQL 监控显示 32 ms 左右。

      ```sql
      -- 4、判断条码是否已出库：增加【CARTON_BARCODE】和【BARCODE】索引有提升
      SELECT COUNT(1)
      FROM psi_sales_stock_out
      WHERE (CARTON_BARCODE = '331100007920401110191W' OR BARCODE = '331100007920401110191W');
      ```

   3. **psi_sales_stock_out**：已出库条码表，840.58w，16 c 64 G MySQL，优化前 Druid SQL 监控【执行时间】显示  2290ms 左右，优化后 Druid SQL 监控显示 40 ms 左右。

      ```sql
      -- Add Index： explain => index_merge，Using union(psi_sales_stock_out_n1,psi_sales_stock_out_n2); Using where
      ALTER TABLE psi_sales_stock_out ADD INDEX psi_sales_stock_out_n1(CARTON_BARCODE);
      ALTER TABLE psi_sales_stock_out ADD INDEX psi_sales_stock_out_n2(BARCODE);
      ```

4. **结果 Result**：最终通过使用 Druid 监控，发现关键 SQL 的耗时已经满足要求了，psi_base_barcode 提效 96.51%，psi_base_packing_relation 提效 95.08%，psi_sales_stock_out 提效 98.25%。

###### 8）品质云微服务拆分 | 微服务、架构、设计

1. **背景 Situation**：

   1. 当时，OEM 供方有 2 个，日班和夜班一共有 2 条产线，不能停线的那种，此时如果品质这块有新需求要发，就需要工厂停线，等我们发版、验证通过了才能继续生产，对于大家来说其实都不好。
   2. 而且，由于是单体， OEM 生产功能与其他零部件供方的 SPC、PQC、OQC 的代码统统在一块，导致系统的负载比较高，常常是能看到 3 台服务器，每台 CPU 和内存都过 60%，有时遇到生产高峰以及定时器触发的话，会出现半夜电话报障说，应用宕机了，需要紧急重启的情况。

2. **任务 Task**：所以出于上面说的诉求，以及加上对未来供方不断上线，当时就规划了对各单体云，进行微服务拆分，共用一个注册中心和配置中心，其中就由我来负责品质云的微服务落地。

3. **行动 Action**：

   1. 我这边拆了 4 个业务模块（2 个月左右），分别是 BASE 基础数据模块，主要是一些字典、用户权限、企业、客户、组织等基础业务。
   2. OEM 代工生产模块，也就是上面所说的主要诉求点，包括成品下线、抽检、出库等业务，把这块生产线相关的业务剥离出去，可以做针对性的优化以及灵活的发版处理。
   3. 而剩余的其他业务，都统统放 APP 综合模块，与单体时的业务基本保持不变，主要是零部件供应商的 SPC 过程检验、PQC 半成品检验、OQC 成品出货检验等业务。
   4. JOBS 定时任务模块，则是后来继续剥离的一个模块，剥离这个模块可以保证生产高峰期时，在夜间也能平滑运行，不会受到大量定时器触发，导致的负载突然增高的影响，主要是一些与外围系统数据同步等业务的处理。

4. **结果 Result**：中间遇到的难点有：

   - 1）**服务的依赖边界问题**：

     1. 在拆分时，一开始我是直接把下线扫码、抽检、出库给拆开，SPC、PQC、OQC 也都拆开，然后就发现拆得粒度过小，出现了很多相互的依赖调用，就又要去考虑，如何代价尽可能小地，去避免带来新的分布式事务和性能问题。
     2. 而最后，是根据业务诉求，以及拆分后的风险与成本的评估，才认为最好的方案应该是，下线扫码、抽检、出库这三个生产线相关的业务合回一个 OEM 模块，其他的 SPC、PQC、OQC 等不是本次诉求的业务又合回一个 APP 模块。

   - 2）**Feign 远程调用时，遇到的响应结果被统一包装的问题**：

     1. 一开始单体时的系统，是有对响应结果做 `Result` 统一包装的，其原理是实现了 `HandlerMethodReturnValueHandler#handleReturnValue()` 方法，这是在 `invocableMethod.invokeForRequest()` 方法处理完 Handler Method 并拿到结果后，调用 `this.returnValueHandlers.handleReturnValue` 对返回结果进行的一个后置处理。

        ```java
        // 统一包装
        public class ResponseBodyWrapperHandler implements HandlerMethodReturnValueHandler {
        	@Override
        	public void handleReturnValue(Object returnValue,
        			MethodParameter returnType, ModelAndViewContainer mavContainer,
        			NativeWebRequest webRequest) throws Exception {
        		_logger.info("return data is " + returnValue);
        		if (returnValue instanceof Void) {
        			returnValue = Result.DefaultSuccessResult;
        		} else {
        			if (!(returnValue instanceof Result)) {
        				returnValue = Result.build(true, ResultCode.SUCCESS_CODE, "", returnValue);
        			}
        		}
        		handler.handleReturnValue(returnValue, returnType, mavContainer, webRequest);
        	}
        }
        ```

     2. 这个包装主要是为了打印原始的响应结果，以及自定义 `retCode` 响应码和 `retMsg` 响应语，但在使用 Feign 调用后，就出现，如果使用那边 Controller 的方法返回值类型，作为这边 Fegin 接口的返回值类型，则收到的就只是一个 null，因为中间做了一层 `Result` 的包装，此时需要对其进行解码，解码的话是实现了 `feign.codec.Decoder#decode()` 方法，通过注入并使用`com.fasterxml.jackson.databind.ObjectMapper` 进行读取 body 并修改，从而实现包装的统一解码工作，避免了在业务代码中，写过多的包装解码代码。

        ```java
        // 自定义Feign解码器
        @Component
        class FeignResultDecoder implements Decoder {
        
            @Autowired
            private ObjectMapper objectMapper;
        
            @Override
            public Object decode(Response response, Type type) throws IOException, DecodeException, FeignException {
                if (response.body() == null)
                    throw new DecodeException(response.status(),  "接口没有返回有效的数据, url: " + response.request().url(), response.request());
        
                // 解析body, 得到统一包装Result实例
                Result result = objectMapper.readValue(Util.toString(response.body().asReader(Util.UTF_8)), Result.class);
                if (ResultCode.ERROR_CODE.equals(result.getRetCode()))
                    throw new DecodeException(response.status(), "接口返回错误: " + result.getRetMsg() + ", url: " + response.request().url(), response.request());
        
                // 重新解析Result#data实例并返回
                return objectMapper.readValue(objectMapper.writeValueAsString(result.getData()), TypeFactory.defaultInstance().constructType(type));
            }
        }
        ```

   - 3）**网关路由问题**：由于主要是后端进行的微服务拆分，前端基本保持不变，所以就需要在收到请求后，对原始 url 进行截断，找到对应的关键字进行服务路由，当时采用的是 Gateway 做了服务网关，其好处就是高性能、对开发友好，只需要在代码中更改路由规则即可 、以及对接注册中心，服务地址不需要经常在网关上配来配去。

   - 4）**过滤器问题**：单体时的过滤器是实现 `javax` 包下的 `Filter` 类来实现的，主要是对一些 token 做校验、给上下文设置当前用户信息等操作，但由于 Feign 调用使用的是 HTTP 请求，也会经过这一层层的 `Filter` 校验，在有了服务网关后，就在网关上做了前置的过滤，主要是实现 `GatewayFilter` 和 `GlobalFilter` 接口来完成对应的过滤，这样，服务间调用就无需走那一层层的 `Filter` 校验了，只需要在网关中被校验一次即可，从而保证远程调用时的性能问题。

###### 9）token 校验失败问题 | 分布式锁、QMS Client Json 对比、问题排查

1. **背景 Situation**：OQC 零部件供方在成品出货报告新建完毕后，需要把报告推送给 QMS，在接口联调时没有问题，但放到生产上就偶尔能推得过去，偶尔推不过去，导致供方有些成品无法继续出货，货物堆在了仓库外面，需要马上解决这个问题。
2. **任务 Task**：
   1. 根据日志记录，推不过去的原因是，品质这边的客户端，收到了 QMS 的一个异常返回结果 `token校验失败`，这就很奇怪了，token 如果校验失败的话，那么为什么有时候能够成功呢，不应该一直都是失败的吗？
   2. 然后就翻了日志，与 QMS 收到的 token 进行比对，发现确实是 token 的问题。
   3. 这个 token 呢，是根据 DTO 对象转成 json 字符串后，使用内存中 QMS 的一个序列号 `appSec` 进行 MD5 加盐，生成摘要而得到的，由于其他参数都是写"死"的，也就是问题出在，用于生成摘要的 DTO 对象与传输过去的 DTO 对象不一致！
3. **行动 Action**：
   1. 然后，我扒出了有问题的推送报文，与 QMS 收到的报文进行对比，用了在线 Json 比对工具 `www.bejson.com`，结果放入后就明显看到，QMS 收到了多余的 `NULL串` ，从而造成两边生成的 token 不同。
   2. 果然，看回代码就发现，发送给 QMS 的 json 串和用于生成摘要的 json 串的生成逻辑不一样，前者是交给 `forest.httpClient` 做 json 转换，而后者是直接用了 `objectMapper#setSerializationInclusion(JsonInclude.Include.NON_NULL)` 和 `ObjectMapper#writeValueAsString()` 做的生成。
   3. 在这个工具类里，ObjectMapper 实例作为成员变量，如果先调 ``ObjectMapper#writeValueAsString()` 填充好了 NULL 序列化器，那么这个 ObjectMapper 每次都会输出 `NULL串` ，而如果先调的是 `objectMapper#setSerializationInclusion(JsonInclude.Include.NON_NULL)` 设置了普通序列化器，那么这个 ObjectMapper 每次都不会输出 `NULL串`，这也是为什么 token 时对时不对的原因所在。
   4. 所以，解决方法就是，在工具类中分开两者的使用，持有一个生成 `NULL串` 的 ObjectMapper，再持有一个生成正常串的 ObjectMapper，在生成 token 时使用后者，从而解决 token 不正确的问题。
4. **结果 Result**：最后，再使用分布式锁做一个单点定时，每隔 3 分钟把推送失败的报告重新推送过去，并设置 3 次的最大重推次数，保证这个推送接口的可靠性，和业务的连续性。

###### 10）大文件上传 | FastDFS、Redis List

1. **背景 Situation**：在建广告类交付单时，要求上传大视频，以让审批人去浏览理解当前交付的广告内容。

2. **任务 Task**：当时系统文件存储用的是 FastDFS，主要是用来存储中小文件，比如用户头像、图片、附件等内容，对文件大小也是做了 200 M 的限制，当时如果要实现大视频的话，一来文件接收上限要放宽，违反了组件用来上传小文件的约定，二来，会有大文件不能续点上传的问题，所以最直接的思路就是，对上传过来的文件进行分片上传，接收到所有分片后再合并成进行大文件存储，具体的做法是：

3. **行动 Action**：

   1. 前端方面，用的是 WebUploader 组件对文件进行分片，其原理是，在获取到文件后，先计算它的一个 MD5 文件摘要，然后对其按照每 200 M 作为一个 chunk 分片，并使用序号顺序标记每个 chunk，再调用后端提供的一个接口进行上传。
   2. 后端这边，接口则接收文件名、文件 MD5 摘要值、有无分片、最大分片号、当前分片号、MultipartFile，以及一些其他辅助信息，接收到后，先是用 MD5 判断当前这个大文件是否曾经上传过，如果上传过就不再处理了，再判断 MD5 + 分片号是否存在了，存在则不再处理了，从而解决即使分片报文重复，或者前一次中断然后本次重新上传的问题，实现续点上传。
   3. 然后，调用框架 API 保存好分片，这个框架主要是对FastDFS 做了封装，把文件存到特定的位置，然后返回文件存储对象的元数据。
   4. 接着，把【分片号 + 文件ID 】顺序放入用的 MD5 值做 key 的 Redis List 中。
   5. 最后，在收到最后一个分片时，则取出 Redis 中的所有【分片号 + 文件ID 】，对分片号做一次排序，从而解决分片不按顺序到达的问题，再顺序合并所有分片成为一个大文件，调用框架 API 保存好这个大文件，拿到它的元数据存储到文件关系表中，这样，下次在下载时，则可以根据文件 ID 找到当时的元数据，再去拿存储服务器上对应路径下的大文件即可，从而完成一次大视频文件的上传和下载。

4. **结果 Result**：总的来说，就是用了 WebUploader 前端组件进行文件分片，后端则是共用同一个接口，先对分片进行缓存，然后再顺序合并成大文件，解决大文件续点上传的问题。

   ![1646294342544](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646294342544.png)

   ![1646294375795](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646294375795.png)

###### 11）邮件发送组件 | 设计、RabbitMQ

1. **背景 Situation**：交付单触发待办提醒时，需要发送邮件通知对应的责任人进行处理，此时需要对接比亚迪的一个邮件服务器，进行邮件发送。
2. **任务 Task**：由于邮件发送不需要非常的及时，所以就采用了异步发送的发送，以提高性能。
3. **行动 Action**：
   1. 当时采用的方案是异步发送，接口把邮件对象丢给线程池，就返回响应客户端其他信息了，再由子线程去把邮件落库。
   2. 然后定时触发，捞取对应未发送和发送失败的邮件，进行批量发送，直连式地把消息发送到一个 `SCC_COMMON_EXCHANGE` 的 `SCC_MAIL_QUEUE` 的队列中，在收到 Broker ACK Confirm 后，则标记为发送成功，否则继续被下轮定时器触发捞取出来重新发送。
   3. 最后，由比亚迪那边的消费者进行消费，发送到邮件服务器中，完成异步邮件发送。
4. **结果 Result**：基于上述的方案，我封装了一个异步邮件发送工具类，在每次遇到需要异步发送邮件时，则调用那个工具类的对应方法即可，既方便，又解耦了业务代码，然后还拥有不错的性能。

###### 12）发版平台 CPU 占用过高 | Tomcat Manager、ELK、CPU 95%

1. **背景 Situation**：我们团队这次的迭代，需要在云平台开发几个接口，然后在和 SRM 联调测试时，为了不用等比亚迪 SAP 测试环境的发版窗口，就需要有我们自己的一个云平台测试环境，方便快速发版快速验证。

2. **任务 Task**：那时由我负责来负责搭建，但由于是在内网环境进行开发，访问不了外网，就拉取不到 Jenkins 的依赖，当然也没有云原生这些基础设施，所以采取了 Tomcat Manager + ELK 部署的方式来发版，前者是 Tomcat 的后台，可以开发完代码后导出 war 包，然后在浏览器上把 war 包进行上传部署，后者是用于在 Kibana 上观察接口的异常日志。

3. **行动 Action**：

   1. 就这样，一个简单易用的发版平台就搭建好了，不过没几天问题就来了，我们发现每一次上传 war 包到启动完成，所花的时间好像都会比上一次的要久，最后一次 war 包上传则是浏览器一直在转了。

   2. 当时为了不阻塞测试进度，就到发版平台的服务器上看，先是使用 `top` 命令看到 118243 的 Tomcat 进程，居然占用了 95.3% 的 `%CPU`：

      ![1646305025603](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646305025603.png)

   3. 然后使用 `top -Hp 118243 ` ，查看 Tomcat 进程的所有线程，发现最高的一个线程 `118332` 占用了 73.3% `%CPU`：

      ![1646305182797](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646305182797.png)

   4. 然后使用 `printf %x 118332`，获取对应的 16 进制线程号，用于后面的 jstack 文件搜索：

      ![1646305395917](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646305395917.png)

   5. 然后就是 `jstack -l 118243 > thread-dump.txt `，导出对应 `118243` 进程的线程快照，打开搜索对应的 `1ef96` 16 进制线程号（这个是当时真实的线程号，上面是后期模拟的） ，就定位到了一堆 `ParallelGC` 线程，原来是 GC 太多而导致的 CPU 飚高，那么为什么一个发版的 Tomcat，会有这么多 GC 线程呢？会不会是项目代码哪里写的有问题啊？

      ![1646305627751](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646305627751.png)

   6. 然后我就使用 `jmap -dump:format=b,file=heap-dump.hproft 118243`，以 `hprof` 格式导出 `118243`  Tomcat 进程的堆内存快照，再导入到 MAT -> Leak Suspects 做内存泄露分析，然后就得到了怀疑报告：说，有 18 个 `ParallelWebappClassLoader`  共占据了 44.61% 堆空间，咦？为什么要 18 个这种类加载器呢？

      ![1646306496272](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646306496272.png)

      ![1646306148994](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646306148994.png)

   7. 然后我就想起 Tomcat 的类加载过程，是打破了双亲委派机制的：

      1. **ext -> bootstarp模型**：保证了 JRE 核心类库不会被重复加载，满足了加载JVM共同类库的需求。
      2. **ext -> webapp模型**：实现了每个 web 应用只加载自己的类库（WEB-INF/classes -> WEB-INF/lib），从而实现了应用间的类库隔离的需求。
      3. **webapp -> share -> common模型**：实现了所有 web 应用之间、web 与 Tomcat 之间，能够加载相同的类库，避免指定的类库不会被重复加载，满足了加载其他共同类库的需求。

      ![1626090700749](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1626090700749.png)

   8. 也就是一个 `ParallelWebappClassLoader` 对应一个 Web App 项目，但用来发版的 Tomcat 应该就只有一个项目呀，所以我就怀疑应该是每次发版没把上一版的项目卸载掉所导致的。

4. **结果 Result**：所以我就让组内的成员，大家在下次发版前，先用 Tomcat Manager 把上一个项目卸载掉，再发新的 war 包上去，这样做了后，就没碰到过这样的情况出现了，应该算是蒙对了吧~哈哈

#### 2.1.1.2. 分片广播式定时的设计及原理？

##### 1、总

除了用 Kafka 去实现分片广播式定时，我还知道以下解决方案：

##### 2、分

1. 基于 **Redis ZSet** 有序集合实现：

   - 1）添加定时任务，把每个画像消息作为一个定时任务元素，定义好任务数据结构（表任务 ID、开始时间戳、循环条件等），然后使用 `ZADD` 命令把定时任务加入有序集合中，分数为开始时间戳。
   - 2）取定时任务，则是通过应用轮训去取，每隔 1 秒去消费有序集合，其中要注意的点是，由于应用是集群部署的，所以消费时要先获取分布式锁，获取到的才能进行消费。
   - 3）获取集合最低的分数，判断分数是否比当前时间戳小，是的话则执行后删除元素，然后根据循环条件来决定是否需要将其重新加入集合中，在下次循环之前，释放分布式锁，否的话则直接释放分布式锁。

   => 优点是，Redis 保证任务顺序执行，但可能会存在一定延迟，缺点是，每次轮训都发生所有应用来抢占分布式锁，网络带宽可能会受到影响。

2. 基于 **DB** 实现：

   - 1）添加定时任务，把每个画像消息作为一条记录，带上开始时间戳、循环条件丢到表里。
   - 2）取定时任务，由应用去查找该表最新的一条记录，通过乐观锁的方式，更新版本号，更新成功的则代表获取到此行的分布式锁，然后去执行此行的任务，执行完毕后删除任务，根据循环条件来决定是否需要将其重新加入表中。

   => 优点是，实现简单，没有多余的组件，缺点是，对单表压力大，相当于基于数据库实现的每个任务作为一个分布式锁。

3. 基于开源组件实现：EsJob、Xxl-job 等。

##### 3、总

以上就是我对除 Kafka 消息方案以外的一些实现分片广播式定时的方案，请指教一下方案是否合理？

#### 2.1.1.3. 目前生产上的机器规模？

##### 1、GSRMC

- **微服务**：基本每个虚机部署 2~3 个模块，平常 CPU 10%，内存 60%，所以一共注册了 126 个服务。
- **数据库**：监控显示，32 C、128 G、3 T，平均响应时间 15ms，平均 30% CPU 利用率，内存利用率 75%。

| 模块           | 数量              | CPU           | 内存               | 硬盘  |
| -------------- | ----------------- | ------------- | ------------------ | ----- |
| 前端 Apache    | 8                 | 4 C           | 16 G               | 50 G  |
| Eureka         | 3                 | 1 C           | 1 G                | 10 G  |
| BASE           | 13                | 8 C           | 16 G               | 100 G |
| POS            | 19                | 8 C           | 16 G               | 100 G |
| PERF           | 7                 | 8 C           | 16 G               | 100 G |
| QUO            | 6                 | 8 C           | 16 G               | 100 G |
| BID            | 3                 | 8 C           | 16 G               | 100 G |
| PRICE          | 13                | 8 C           | 16 G               | 100 G |
| CON            | 6                 | 8 C           | 16 G               | 100 G |
| ESB            | 5                 | 8 C           | 16 G               | 100 G |
| JOBS           | 9                 | 8 C           | 16 G               | 100 G |
| TASK           | 5                 | 8 C           | 16 G               | 100 G |
| MIP            | 4                 | 8 C           | 16 G               | 100 G |
| SYNC           | 5                 | 8 C           | 16 G               | 100 G |
| INDEX          | 3                 | 8 C           | 16 G               | 100 G |
| 其他模块       | 25                | 8 C           | 16 G               | 100 G |
| MySQL          | 16（8 主 8 从）   | 32 C          | 128 G              | 3 T   |
| MongoDB        | 3                 | 8 C           | 16 G               | 500 G |
| Redis          | 3                 | 8 C           | 32 G               | 100 G |
| Kafka          | 3                 | 8 C           | 16 G               | 100 G |
| 报表导出 OSS   | 文件数： 500 / 天 | 数据量：10 G  | 增长量：300 M / 天 | -     |
| 供应商附件 OSS | 文件数：10 w / 年 | 数据量：200 G | 增长量：200 G / 年 | -     |

##### 2、云化项目

- **微服务**：基本每个虚机部署 2~3 个模块，平常 CPU 10%，内存 60%，所以一共注册了 14 个服务。
- **数据库**：监控显示，16 C、64 G、3 T，平均响应时间 15ms，平均 30% CPU 利用率，内存利用率 75%。

| 模块             | 数量             | CPU          | 内存              | 硬盘  |
| ---------------- | ---------------- | ------------ | ----------------- | ----- |
| 前端 Nginx       | 3                | 4 C          | 16 G              | 50 G  |
| Eureka           | 3                | 1 C          | 1 G               | 10 G  |
| BASE             | 3                | 8 C          | 16 G              | 100 G |
| APP              | 3                | 8 C          | 16 G              | 100 G |
| OEM              | 3                | 8 C          | 16 G              | 100 G |
| JOBS             | 2                | 8 C          | 16 G              | 100 G |
| MySQL            | 1 主             | 16 C         | 64 G              | 3 T   |
| Redis            | 6                | 8 C          | 16 G              | 100 G |
| 文件存储 FASTDFS | 文件数：1 w / 年 | 数据量：20 G | 增长量：20 G / 年 | -     |

##### 3、比亚迪 SRM

- **微服务**：基本每个虚机部署 2~3 个模块，平常 CPU 10%，内存 60%，所以一共注册了 21 个服务。
- **数据库**：监控显示，32 C、128 G、3 T，平均响应时间 15ms，平均 30% CPU 利用率，内存利用率 75%。

| 模块             | 数量             | CPU          | 内存              | 硬盘  |
| ---------------- | ---------------- | ------------ | ----------------- | ----- |
| 前端 Nginx       | 2                | 4 C          | 16 G              | 50 G  |
| APP              | 6                | 8 C          | 16 G              | 100 G |
| MID              | 7                | 8 C          | 16 G              | 100 G |
| RPORT            | 6                | 8 C          | 16 G              | 100 G |
| Oracle           | 1                | 32 C         | 128 G             | 3 T   |
| Redis            | 3                | 8 C          | 16 G              | 100 G |
| 文件存储 FASTDFS | 文件数：1 w / 年 | 数据量：20 G | 增长量：20 G / 年 | -     |

#### 2.1.1.4. 生产慢问题修复的例子？

见项目自我介绍里的，线程池调优，以及 CPU 95% 的例子。

##### 1、线程池调优 | 线程池、源码、调优

1. 我们系统某个模块中常常需要使用别的模块的数据源，比如在 POS 模块中需要去查询 PRICE 库，此时，由于主线程默认绑定的是 POS 数据源，要想使用 PRICE 数据源的话，就需要开辟子线程去做切换，所以就为每个模块抽象了数据源切换的工具类。
2. 由于正常都是调用一次切换方法，也就是把 Lambda 表达式提交给线程池，主线程再去做 `FutureTask.get()` 的监听，拿到结果后返回，这是正常情况，因此我们一开始也没怎么留意。
3. 但问题就来了，有一次某段代码对连续进行了两次的切换，就发现接口超时，以及往后的切换都超时了，那时排查了很久，最后在这个切换工具类里看到，生产上跑的参数为 `corePoolSize=0、maximumPoolSize=32、workQueue=LinkedBlockingQueue`，按照原理，本以为是错误的配置，会导致任务都阻塞在队列中直到 OOM 发生的，但改成一次切换又可以了，这就很奇怪了，然后就去研究源码，发现刚才上面所说的那段**补充非核心线程**的逻辑，也就是即使 `corePoolSize=0`，只要最大线程数不为 0，那么还是会保证至少创建一个线程去消费任务队列的，所以切换一次就可以拿到任务的运行结果。
4. 那么为什么切换两次就不行了呢？这是因为，在第一次切换时，是主线程在做 `FutureTask.get()`，阻塞的是主线程，而如果还做一次连续切换，那么就会导致线程池中仅有的 1 个线程也做了 `FutureTask.get()`，从而进入了阻塞，但线程池认为有效线程数还是 0，所以新任务进来也不会去创建新线程消费，会直到 OOM 的发生为止~
5. 最后，更正了合理的构造参数 `corePoolSize=Runtime.getRuntime().availableProcessors() + 1`，`maximumPoolSize=2 * Runtime.getRuntime().availableProcessors()`，`keepAliveTime=120s`，`LinkedBlockingQueue=1024` 保证不会发生内存溢出，`Google#ThreadFactoryBuilder`来设构建线程工厂、在线程创建时设定名称前缀和递增线程号，`RejectedExecutionHandler=CallerRunsPolicy` 保证任务不丢失，同时设置 `allowCoreThreadTimeOut=true` 允许核心线程空闲时也会被回收。

##### 2、发版平台 CPU 占用过高 | Tomcat Manager、ELK、CPU 95%

1. **背景 Situation**：我们团队这次的迭代，需要在云平台开发几个接口，然后在和 SRM 联调测试时，为了不用等比亚迪 SAP 测试环境的发版窗口，就需要有我们自己的一个云平台测试环境，方便快速发版快速验证。

2. **任务 Task**：那时由我负责来负责搭建，但由于是在内网环境进行开发，访问不了外网，就拉取不到 Jenkins 的依赖，当然也没有云原生这些基础设施，所以采取了 Tomcat Manager + ELK 部署的方式来发版，前者是 Tomcat 的后台，可以开发完代码后导出 war 包，然后在浏览器上把 war 包进行上传部署，后者是用于在 Kibana 上观察接口的异常日志。

3. **行动 Action**：

   1. 就这样，一个简单易用的发版平台就搭建好了，不过没几天问题就来了，我们发现每一次上传 war 包到启动完成，所花的时间好像都会比上一次的要久，最后一次 war 包上传则是浏览器一直在转了。

   2. 当时为了不阻塞测试进度，就到发版平台的服务器上看，先是使用 `top` 命令看到 118243 的 Tomcat 进程，居然占用了 95.3% 的 `%CPU`：

      ![1646305025603](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646305025603.png)

   3. 然后使用 `top -Hp 118243 ` ，查看 Tomcat 进程的所有线程，发现最高的一个线程 `118332` 占用了 73.3% `%CPU`：

      ![1646305182797](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646305182797.png)

   4. 然后使用 `printf %x 118332`，获取对应的 16 进制线程号，用于后面的 jstack 文件搜索：

      ![1646305395917](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646305395917.png)

   5. 然后就是 `jstack -l 118243 > thread-dump.txt `，导出对应 `118243` 进程的线程快照，打开搜索对应的 `1ef96` 16 进制线程号（这个是当时真实的线程号，上面是后期模拟的） ，就定位到了一堆 `ParallelGC` 线程，原来是 GC 太多而导致的 CPU 飚高，那么为什么一个发版的 Tomcat，会有这么多 GC 线程呢？会不会是项目代码哪里写的有问题啊？

      ![1646305627751](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646305627751.png)

   6. 然后我就使用 `jmap -dump:format=b,file=heap-dump.hproft 118243`，以 `hprof` 格式导出 `118243`  Tomcat 进程的堆内存快照，再导入到 MAT -> Leak Suspects 做内存泄露分析，然后就得到了怀疑报告：说，有 18 个 `ParallelWebappClassLoader`  共占据了 44.61% 堆空间，咦？为什么要 18 个这种类加载器呢？

      ![1646306496272](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646306496272.png)

      ![1646306148994](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646306148994.png)

   7. 然后我就想起 Tomcat 的类加载过程，是打破了双亲委派机制的：

      1. **ext -> bootstarp模型**：保证了 JRE 核心类库不会被重复加载，满足了加载JVM共同类库的需求。
      2. **ext -> webapp模型**：实现了每个 web 应用只加载自己的类库（WEB-INF/classes -> WEB-INF/lib），从而实现了应用间的类库隔离的需求。
      3. **webapp -> share -> common模型**：实现了所有 web 应用之间、web 与 Tomcat 之间，能够加载相同的类库，避免指定的类库不会被重复加载，满足了加载其他共同类库的需求。

      ![1626090700749](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1626090700749.png)

   8. 也就是一个 `ParallelWebappClassLoader` 对应一个 Web App 项目，但用来发版的 Tomcat 应该就只有一个项目呀，所以我就怀疑应该是每次发版没把上一版的项目卸载掉所导致的。

4. **结果 Result**：所以我就让组内的成员，大家在下次发版前，先用 Tomcat Manager 把上一个项目卸载掉，再发新的 war 包上去，这样做了后，就没碰到过这样的情况出现了，应该算是蒙对了吧~哈哈

#### 2.1.1.5. MySQL MVCC？

##### 1、总

MVCC，Multi-Version Concurrency Control，多版本并发控制，可以实现数据库读写冲突时的无锁并发访问，可以做到在读时不阻塞写，写时不阻塞读，提高了数据库并发的读写性能，同时还可以解决 MySQL 不可重复读、幻读等事务并发问题。

##### 2、分

1. 首先，需要讲一下 MySQL 中的当前读和快照读：

   - 1）**当前读**：非 MVCC 实现，读取的是记录的最新版本，读取时，要保证其他并发事务不能修改当前记录，因此会对读取的记录进行加锁。
     - 举例：select lock in share mode（共享锁）, select for update（排他锁），update、insert、delete（排他锁）、串行化事务隔离级别等。
   - 2）**快照读**：MySQL 实现 MVCC 模型的中一个具体的非阻塞读功能，可以避免读时的加锁操作，从而降低开销、提高并发性能，但快照读可能读到的不一定是数据的最新版本，而是之前的历史版本，但前提是，事务使用非串行化的隔离级别，否则串行化级别下的快照读会退化成当前读。
     - 举例：不加锁的 select。

2. 然后就是实现原理，MySQL 是由 undo log + 版本链 + Read View 来实现 MVCC 的：

3. 第一个是 **undo log**，回滚日志，属于逻辑日志，记录的是 sql 执行相关的信息，是 MySQL 中事务原子性和隔离性实现的基础，其实现原理是：

   - 1）在 MySQL InnoDB 中，当事务对数据库进行修改时，InnoDB 会生成对应的 undo log。
   - 2）如果事务执行失败或调用了 rollback，导致事务需要回滚时，就可以根据 undo log 的内容做与之前相反的工作，把数据回滚到修改之前的样子，以实现事务原子性操作。
   - 3）对于每个 insert，回滚时会执行 delete。对于每个 delete，回滚时会执行 insert。对于每个 update，回滚时会执行一个相反的 update，把数据改回去。
   - 4）而 MVCC 则是采用 undo log 来记录旧版本，链首为最新的旧记录，链尾为最早的旧记录。

4. 第二个是**版本链**，在 InnoDB 中，每次修改版本都会在版本链中记录，通过 undo log + trix_id + roll_pointer 来实现，undo log 原理如上所说，

   ![1630673126935](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630673126935.png)

   - 1）**trx_id**：当前版本的事务ID，用来存储每次对某条记录进行修改时的事务ID，其中事务 ID 则是，当每个事务开启时，都会被分配一个ID，而这个ID是递增的，因此越新的事务其ID值越大。
   - 2）**roll_pointer**：回滚指针，由于每次对记录修改时，都会把老版本写入到 undo 日志中，使用回滚指针 roll_pointer 来指向这条记录上一个版本的位置，通过它来获得上一个版本的记录信息，其中，插入操作的 undo 日志是没有 roll_pointer 的，因为它没有老版本。

5. 第三个是 Read View，它是事务进行快照读操作时产生的读视图，是数据库当前的一个快照，记录了系统当前活跃事务ID 集合（即还没有提交的事务 ID），用来做可见性判断，即当某个事务执行快照读时，会对该记录创建一个Read View 读视图， 并把它作为条件，来判断当前事务能够看到哪个版本的数据，其中可能是当前版本的数据，也有可能是该行记录 undo log 里面某个版本的老数据，具体原理如下：

   ![1630674963257](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630674963257.png)

   1. **trx_id == creator_trx_id**：可以访问这个版本，这个版本的事务 ID 等于当前创建 Read View的事务 ID，即自己能够读到自己的版本。
   2. **trx_id < min_trx_id**：可以访问这个版本，这个版本的事务 ID 小于最小活跃事务 ID，说明这个版本已经被提交过了，对于当前做可见性判断的事务来说，是可以看见的。
   3. **trx_id > max_trx_id**：不可以访问这个版本，这个版本的事务 ID 大于下一个事务 ID，说明这个版本记录是在该 Read View 生成之后产生的，已经超出了版本链范围，而快照读只能读取版本链中的版本，因此该版本对于当前做可见性判断的事务来说，是不应该看见的。
   4. **min_trx_id <= trx_id <= max_trx_id**：
      - 如果这个版本的事务 ID 为 m_ids 中的某个值，则不可以访问这个版本的，因为m_ids 都是活跃的、还没提交的事务，说明该版本记录还没有提交，对于当前做可见性判断的事务来说，是不应该看见的。
      - 如果这个版本的事务 ID 不为 m_ids 中的某个值，则可以访问这个版本，因为没在 m_ids 里，又小于等于 max_trx_id，说明该版本记录已提交了，对于当前做可见性判断的事务来说，是可以看见的。

   => 因此，可见性判断总的来说就是，要当前事务能看到的版本应该是自己创建的或者已经提交了的，这也是实现 MVCC 的原理所在。

##### 3、总

以上，就是我对 MySQL MVCC 的一个理解，请问有什么细节需要补充的吗？

#### 2.1.1.6. Dubbo 配置的注意点有哪些？

##### 1、Provider 多配置 Consumer 参数

1. 作为服务提供⽅，自己⽐消费⽅更清楚服务的这些接口参数的取值，比如超时时间、重试次数、负载均衡策略等。
2. 由于配置覆盖策略存在，在 Provider 端配置后，Consumer 端不配置则会使⽤ Provider 端配置，即 Provider 配置可以作为 Consumer 的缺省值。
3. 而如果 Provider 不配置，Consumer 也不直接配置，则会使⽤ Consumer 端的全局设置，这对于 Provider 是不可控的，并且往往是不合理的。
4. 因此，Provider 端尽量多配置、完善这些接口参数，让 Provider 实现者⼀开始就思考 Provider 端的服
   务特点和服务质量等问题。

![1636882183556](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1636882183556.png)

##### 2、Provider 合理配置性能参数

比如，**threads**（服务线程池⼤⼩）、**executes**（服务提供者并发请求的上限）。

##### 3、服务使用固定端口

使⽤固定端⼝来暴露服务，不要使⽤随机端⼝，这样在注册中⼼推送延迟的情况下，消费端仍然能够通过缓存列表，调⽤到原地址+原端口的服务，保证调⽤成功。

##### 4、推荐使用 XML 进行配置

XML 配置优先级高于 properties 和 yml，且标签的配置方式更加容易阅读和理解。

##### 5、应用配置负责人参数

配置 `dubbo:application#owner` 负责人参数，这些可以在运维平台上看到，以便于在发现问题时，找到对应服务负责⼈。

#### 2.1.1.7. Dubbo 提供者、消费者服务启动原理？

##### 1、总

1. 首先是 Dubbo 与 Spring 的融合原理，基于 dubbo.jar 内的 `META-INF/spring.handlers` 配置，Spring 在遇到 Dubbo XML 配置的名称空间 `http\://dubbo.apache.org/schema/dubbo` 时，会回调 `DubboNamespaceHandler` 。
2. 而 `http\://dubbo.apache.org/schema/dubbo/dubbo.xsd` 则定义了 Dubbo XML 的标签语法，所有 dubbo 的标签，都统一用 `DubboNamespaceHandler#DubboBeanDefinitionParser` 进行解析，基于一对一属性映射，将 XML 标签解析为 Bean 对象。
3. Provider 在 `ApplicationListener<ContextRefreshedEvent>#onApplicationEvent` 回调时，或者 Consumer 在接口注入 `FactoryBean#getObject` 时，将 Bean 对象转换 URL 格式，所有 Bean 属性转成 Dubbo#URL，然后将 Dubbo#URL 传给 Protocol 扩展点，基于扩展点的 SPI 机制，根据 Dubbo#URL 的协议头，进行不同协议的服务暴露或者引用。

##### 2、分

###### 1）Provider 服务发布原理

1. `ServiceConfig` 在 XML 解析后，拿到对外提供实现类 `XML#ref` 配置。
2. 然后，Provider 在 `ApplicationListener<ContextRefreshedEvent>#onApplicationEvent` 回调时，通过 `ProxyFactory#getInvoker（）` 为 `XML#ref` ，生成一个 interfaceClass 的 javasist 动态代理 Wrapper 包装类 Invoker 实例，可以动态代理调用 `XML#ref` 实现类的方法，到这一步就完成了具体实现类到 `Invoker` 的转化。
3. 接下来，就是 `Invoker` 转换到 `Exporter` 的过程，是服务暴露的关键过程，其转换分为两种类型：
   - 1）**暴露本地服务**：
     1. 指服务暴露和引用都在同一个 JVM 里，自己调用自己接口，没必要进行远程通信。
     2. 会调用 `InjvmProtocol#export（）` 把 invoker 转换为 `InjvmExporter`，并存进 exporters 缓存中。
     3. 调用则是获取 exporters 缓存中的 `InjvmExporter`，进行一个普通的动态代理到实现类的方法。
   - 2）**暴露远程服务**：
     1. 指服务暴露给远程客户端IP和端口号，以实现远程通信。
     2. 会调用 `DubboProtocol#export（）`  把 invoker 转换为 `DubboExporter`，然后打开 Netty 服务器暴露服务，并存进 exporters 缓存中。
     3. 然后调用 `RegistryProtocol#register（）`，使用 Curator 客户端建立 ZK 连接，注册 provider 持久化结点、当前服务的非持久化结点、configurators 非持久化结点，并设置监听器，当配置发生变更时，则会回调监听器的 `notify（）` 方法，来修改 invoker 信息。
     4. 调用则是通过 `Netty#channelRead()` 方法，当有数据请求时，则调用 handler 和线程池进行接收、处理，最终获取缓存中的 `DubboExporter`，动态代理到实现类的方法。

![1637067703250](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1637067703250.png)

###### 2）Consumer 服务引用原理

1.  Consumer 在接口注入 `FactoryBean#getObject` 时，调用 `Protocol#refer（）`生成 `Invoker` 实例，是服务消费的关键，其分类 3 种类型：
   - **本地服务引用**：如果为本地暴露服务，则 `InjvmProtocol` 生成本地执行的 `InjvmInvoker` 。
   - **直连服务引用**：如果为直连服务，则 `DubboProtocol` 创建 Netty 客户端，连接 url 服务，构建 `DubboInvoker` 。
   - **远程服务引用**：
     1. 如果为非直连的远程服务引用，则会调用 `RegistryProtocol` 进行 Consumer 注册到 ZK 并订阅，拉取相关 url 和 配置信息，当有发生变更时，则触发监听器的回调函数，调用 `DubboProtocol#refer()` 生成 `DubboInvoker` ，并加入集群，最后默认伪装返回一个 `FailoverClusterInvoker`。
2. 最后利用动态代理，把 `Invoker` 转换为客户端需要的接口实现类。
3. 当应用对代理对象进行方法调用时，则是动态代理到对应 `Invoker#invoke` 方法，调用本地方法或者发起远程请求。

![1637149757299](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1637149757299.png)

###### 3）Consumer 远程调用 Provider 原理

1. 消费方使用的接口动态代理实现类 proxy，调用其对应的 `Invoker`，发起真正的远程服务调用。

2. 而提供方的实现类，则会被封装成为一个 `AbstractProxyInvoker` ，然后生成一个 `Exporter` ，当服务提供方收到一个请求后，则会找到对应的 `Exporter` 实例，并调用它所对应的 `AbstractProxyInvoker` 实例，从而真正调用了服务提供者的代码。

3. 整个**消费方调用提供方**的调用链架构如下：

   ![1637150417647](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1637150417647.png)

   1. **服务引用**：通过 Javassist 反向代理，代理调用 InvokerInvocationHandler#invoke 方法。
   2. **服务本地调用、降级、缓存**。
   3. **集群容错与负载均衡**：非 mock Invoker 筛选，Invoker 目录查找，根据容错策略、负载均衡策略，挑选唯一的 Invoker。
   4. **服务过滤链、监听器、包装类 SPI 扩展**。
   5. **服务协议**：根据协议，使用不同的 Invoker 调用不同的网络传输底层。
   6. **网络传输**：抽象 Netty、Mina 等统一接口，把消息序列化后发送到网络，传输给 Server 端。
   7. **消息接收与异步处理**：Server 端接收到消息后，经过反序列化后，交由线程池异步处理。
   8. **服务协议**：根据协议，选择不同的 Exporter 进行调用。
   9. **服务过滤链、监听器、包装类 SPI 扩展**。
   10. **服务调用**：最后调用真正的接口实现类，得到方法执行结果。

4. 而**提供方响应消费方**的顺序为：

   1. **服务响应**：在得到方法执行结果后，通过网络传输底层，序列化后发送响应报文给客户端。
   2. **结果接收**：客户端接收到响应报文后，交由线程池异步处理程序，经过反序列化后塞回到 Future 对象中，完成一次服务调用。

##### 3、总

以上，就是我对 Dubbo Provider 和 Consumer 启动过程以及远程调用过程的一个理解，请问有什么细节需要补充的吗？

#### 2.1.1.8. SpringBoot 的启动原理？

见《[1.1.1.4. SpringBoot 自动装配和自定义 starter？](#1.1.1.4. SpringBoot 自动装配和自定义 starter？)》。

#### 2.1.1.9. Spring MVC 中过滤器和拦截器的区别？

见《[1.1.1.6. Spring MVC 拦截器和过滤器的区别？](#1.1.1.6. Spring MVC 拦截器和过滤器的区别？)》。

#### 2.1.2.0. 算法题 | 实际问题中树的深度优先遍历

思路就是，把对应的数据结构，转换为树结点的数据结构，然后进行常规的深度优先遍历即可，比如二叉树的**先序遍历**：

##### 1、递归法 | O（n）

- **思路**：递归遍历，中 -> 左 -> 右，因此在一开始来到中时，就把节点的值加入集合中即可。
- **结论**：时间，0ms，100%，36.4mb，92.56%，非常简单。

```java
/**
 * Definition for a binary tree node.
 * public class TreeNode {
 *     int val;
 *     TreeNode left;
 *     TreeNode right;
 *     TreeNode() {}
 *     TreeNode(int val) { this.val = val; }
 *     TreeNode(int val, TreeNode left, TreeNode right) {
 *         this.val = val;
 *         this.left = left;
 *         this.right = right;
 *     }
 * }
 */
class Solution {
    public List<Integer> preorderTraversal(TreeNode root) {
        List<Integer> res = new LinkedList<>();
        f(root, res);
        return res;
    }
    
    private void f(TreeNode root, List<Integer> res) {
        if(root == null) {
            return;
        }

        res.add(root.val);
        f(root.left, res);
        f(root.right, res);
    }
}
```

##### 2、迭代法 | O（n）

- **思路**：迭代法中，先序遍历实现最简单，因为当前使用的 while 循环可以认为是“中”，然后模拟系统栈把右、左一次压栈即可（可以认为是方法倒过来压栈）。
- **结论**：时间，0ms，100%，36.4mb，93.30%，注意添加和使用前都做个判空，防止空指针和节省空间的使用。

```java
/**
 * Definition for a binary tree node.
 * public class TreeNode {
 *     int val;
 *     TreeNode left;
 *     TreeNode right;
 *     TreeNode() {}
 *     TreeNode(int val) { this.val = val; }
 *     TreeNode(int val, TreeNode left, TreeNode right) {
 *         this.val = val;
 *         this.left = left;
 *         this.right = right;
 *     }
 * }
 */
class Solution {
    public List<Integer> preorderTraversal(TreeNode root) {
        LinkedList<TreeNode> stack = new LinkedList<>();
        if(root != null) {
            stack.push(root);
        }
        
        List<Integer> res = new LinkedList<>();
        while(!stack.isEmpty()) {
            root = stack.pop();
            if(root != null) {
                res.add(root.val);
                if(root.right != null) {
                    stack.push(root.right);
                }
                if(root.left != null) {
                    stack.push(root.left);
                }
            }
        }

        return res;
    }
}
```

#### 2.1.2.1. 算法题 | Linux#uniq 指令、Java BufferedReader#ready 和 readLine

这道题的难点在于，Linux#uniq 指令的作用，和 Java BufferedReader 的 API。

##### 1、Linux#uniq 文档编辑命令

Linux#uniq，输入一个文件，检查文本文件中重复出现的行列，并输出到另一个文件，-c 代表在每列旁边显示该行重复出现的次数，-d 代表仅显示重复出现的行列，-u 代表仅显示只出现一次的行列。

##### 2、Java BufferedReader API

BufferedReader 可以从字符输入流中读取文本，并缓冲字符，以便有效地读取字符，使用时可以通过构造函数指定缓冲区大小或者也可以只使用默认大小 8192，同时建议使用 BufferedReader 包装 Reader 实例，以提高效率。

```java
private static void printLinuxCmd(File file) throws IOException {
    // 使用 BufferedReader 包装 Reader 实例
    BufferedReader reader = new BufferedReader(new FileReader(file));
    Set<String> res = new HashSet<>();

    // 允许读取：ready()
    String str;
    while (reader.ready()) {
        // 按行读取：readLine()
        str = reader.readLine();
        if(str.contains("abc")) {
            res.add(str);
        }
    }
    
    // 排序后去重
    List<String> reslist = res.stream().sorted(Comparator.comparingInt(Object::hashCode)).collect(Collectors.toList());
    System.err.println(reslist);
    
    // 底层是去关闭输入流
    reader.close();
}
```

#### 3.1.1.1. sychrozied 锁升级流程？

##### 1、总

sychrozied 锁状态一共有 4 种，级别由低到高分别为：无锁、偏向锁、轻量级锁和重量级锁。

1. 在 JDK 1.6 之前，sychrozied  锁是一个重量级锁，效率比较低下。
2. 在 JDK 1.6 之后，为了提高锁的获取和释放效率，就对 synchronized 的实现进行了优化，引入了偏向锁和轻量级锁。
3. 从此 synchronized 锁就有了以上 4 种状态，并且它们会随着竞争的情况逐渐升级，但不可降级。

对于 sychrozied  优化后的，锁执行过程总结如下：

![1629617805708](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629617805708.png)

![1629617826584](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629617826584.png)

1. **确认是否为可偏向状态**：线程抢锁时，JVM 首先检测内置锁对象 Mark Word 中的 biased_lock（偏向锁标识）是否设置成 1，lock（锁标志位）是否为 01，如果都满足，则确认内置锁对象为**可偏向状态**。
2. **确认线程 ID**：在内置锁对象确认为可偏向状态后， JVM 会检查 Mark Word 中的线程 ID 是否为抢锁线程的 ID。
3. **同线程 ID，直接执行临界区代码**：如果是，则表示抢锁线程处于偏向锁状态，然后抢锁线程快速获得锁，开始执行临界区代码。
4. **竞争锁成功，升级为偏向锁**：如果 Mark Word 中的线程 ID 并未指向抢锁线程，则通过 CAS 操作去竞争锁。
   - 如果竞争成功，则将 Mark Word 中的线程 ID 设置为抢锁线程，偏向标志位设置为 1，锁标志位设置为 01，然后执行临界区代码，此时内置锁对象为**偏向锁状态**。
5. **发生锁竞争失败，偏向锁升级为轻量级锁**：如果 CAS 操作竞争失败，说明发生了竞争，此时 JVM 会去检查原先持有锁的线程是否存活。
   - 如果原线程已经死亡，则设置 Mark Word 为抢到锁的线程 ID，内置锁继续保持为偏向锁。
   - 如果原线程没有死亡，且其堆栈中存在该 sychrozied 锁的所记录，则需要撤销偏向锁，进而升级为**轻量级锁**（继续由原持有锁线程持有）。
6. **其他线程继续 CAS 竞争，以获取轻量级锁**：如果仍然有其他线程继续竞争轻量级锁，则在 JVM 将在替换锁对象 Mark Word 中的 ptr_to_lock_record 过程中，使用 CAS 替换为抢锁线程的锁记录指针。
   - 如果替换成功，则代表抢锁线程获得了**轻量级锁**。
   - 如果替换失败，表明存在其他线程竞争锁，则 JVM 接着尝试使用 CAS + 自旋方式来替换。
     - 如果替换成功，代表抢锁成功，则锁对象继续保持为**轻量级锁状态**。
7. **CAS + 自旋失败，轻量级锁升级为重量级锁**：如果 CAS + 自旋替换失败，即超过了最大自旋次数，轻量级锁则升级为重量级锁，此后等待锁的线程都需要进入**阻塞状态**。

=> 总的来说：

1. **偏向锁**是在没有发生锁争用的情况下使用的。
2. 一旦有了第二个线程争用锁，偏向锁则会升级为**轻量级锁**。
3. 如果锁争用很激烈，轻量级锁的 CAS + 自旋到达自旋阈值后，轻量级锁则会升级为**重量级锁**。

##### 2、分

###### 1）无锁

![1629597556442](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629597556442.png)

无锁，即无线程竞争，在 java 对象刚创建时，还没有任何线程来竞争，对象处于无锁状态，此时，偏向标志位为0，锁状态标志位为 01。

###### 2）偏向锁

![1629597869808](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629597869808.png)

偏向锁，是指一段同步代码，一直被同一个线程所访问，偏向锁状态下的 Mark Word，会记录 synchronized 锁偏爱的线程 ID，从而让 synchronized 锁认定该线程为“熟人”，从而该线程可以自动获取锁，降低获取锁的代价。

- **主要解决无竞争下的锁性能问题**：由于线程 ID 被记录在锁对象的 Mark Word 中（CAS设置），以后该线程获取锁时，只需要判断一下线程 ID 和标志位，就可以直接进入同步块，连 CAS 操作都不需要，省去了大量有关锁申请的操作，消除了无锁竞争情况下的同步原语，从而提高了程序的性能。
- **偏向锁的膨胀**：
  1. 如果偏向锁已经被 A 占据，一旦有第二个线程 B 来争抢这个对象，由于偏向锁不会主动释放，所以 B 看到的 synchronized 锁是偏向状态，表明已经存在了竞争，则 JVM 会去检查原来持有该对象锁的占有线程A 是否依然存活。
  2. 如果发现 A 已经挂了，则将锁对象变为无锁状态，然后重新偏向 B 线程。
  3. 如果发现 A 依然存活，则会进一步检查 A 的调用堆栈是否有锁记录持有该偏向锁。
  4. 如果存在锁记录，表明原来的线程 A 仍然在使用该偏向锁，即 A 和 B 此时发生了锁竞争，则 JVM 会撤销原来的偏向锁，将偏向锁膨胀（INFLATING）为轻量级锁。
- **偏向锁的撤销**：如果锁对象经常被多个线程竞争，那么偏向锁就是多余的，并且撤销偏向锁的过程也会带来一些性能开销。
  1. 在一个安全点停止拥有锁的线程。
  2. 遍历线程的栈帧，检查是否存在锁记录，如果存在，则需要清空锁记录（锁记录是线程私有的，每个线程都有自己的一份锁记录），使其变为无锁状态，并修复锁记录指向的 Mark Word，清除其线程 ID。
  3. 将当前锁升级为轻量级锁。
  4. 唤醒当前线程。
- **偏向锁的撤销条件**：
  - 多个线程竞争偏向锁。
  - 调用偏向锁对象的 `hashCode（）`方法，或者 `System.identityHashCode（）`方法，计算对象的HashCode 之后，将哈希码放置到了 Mark Word 中，synchronized 锁变成无锁状态，偏向锁会被撤销。

=> 经验表明，大部分情况下，一个同步代码块的线程都是同一个线程，总体来说，使用偏向锁带来的好处大于偏向锁撤销和膨胀带来的代价。如果某些临界区存在两个，或者两个以上的线程竞争，偏向锁反而会降低性能，此时可以在启动 JVM 时，把偏向锁的默认功能关闭。

###### 3）轻量级锁

![1629598045639](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629598045639.png)

轻量级锁，也被成为非阻塞同步锁、或者叫乐观锁，是指当锁处于偏向锁，又被另一个线程企图抢占时，偏向锁就会升级为轻量级锁，而企图抢占的线程会通过自旋的形式尝试获取锁，不会阻塞抢锁线程，以提高性能，其中，哪个线程先占有锁对象，锁对象的 Mark Word 就会指向哪个线程栈帧中的锁记录。

- **引入轻量级锁的主要目的**：在多线程竞争不激烈的情况下，通过 CAS 机制竞争锁，减少重量级锁产生的性能损耗，尽可能不动用操作系统层面的互斥锁，在应用层面上通过自旋来解决线程同步的问题。

  - **自旋原理**：如果持有锁的线程能在很短时间内释放锁资源，竞争等待锁的线程则不需要进行内核态和用户态的切换进入阻塞状态，它们只需要 CPU 自旋，等待持有锁的线程释放锁后即可立即获取锁，避免了用户线程和内核切换的消耗，从而提高性能。
  - **自旋锁分类**：
    - **普通自旋锁**：指当有线程来竞争锁时，抢锁线程会在原地循环等待，而不是被阻塞，直到那个占有锁的线程释放锁之后，这个抢锁线程才可以获得锁。
      - JDK 1.6 的轻量级锁使用的是普通自旋锁，需要使用 `-XX：+UseSpinning` 选项手工开启。
      - 默认情况下，自旋次数为 10 次，可以通过 `-XX：PreBlockSpin` 选项来进行更改。
      - 然而，线程自旋需要消耗 CPU，如果一直获取不到锁，那么线程也不能一直占用 CPU 自旋做无用功，因此需要设定一个自旋等待的最大时间。
    - **适应性自旋锁**：
      - JVM 对于自旋周期的选择，JDK 1.7 引入了适应性自旋锁（自动开启），指的是自旋的时间不是固定式的，而是由前一次在同一个锁上的自旋时间，以及锁的拥有者状态来决定的，解决的是锁竞争时间不确定的问题，使得竞争程度趋于稳定。
      - **自旋成功了则下次自旋的次数就会更多**：如果抢锁线程在同一个锁对象上之前成功获得过锁（竞争力强，适合竞争），JVM 则会认为这次自旋很可能再次成功，此次允许自旋等待持续相对更长的时间。
      - **自旋失败了则下次自旋的次数就会减少**：如果对于某个锁，抢锁线程很少成功获得过（竞争力弱，不适合竞争），JVM 则减少自旋时间甚至省略自旋过程，以避免浪费 CPU 资源。

- **轻量级锁的抢占过程**：

  1. 在抢锁线程进入临界区之前，如果该 synchronized 锁没有被锁定，JVM 首先将在抢锁线程的栈帧中建立一个锁记录（Lock Record），用于存储对象目前 Mark Word 的拷贝。

     ![1629601733727](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629601733727.png)

  2. 然后，抢锁线程将使用 CAS 自旋操作，尝试将内置锁对象头的 Mark Word 的 ptr_to_lock_record（锁记录指针），更新为抢锁线程栈帧中锁记录的地址，如果这个更新执行成功了，这个线程就拥有了对象锁。

  3. 接着，JVM 将 Mark Word 中的 lock 标记位改为 00，代表该内置锁对象处于轻量级锁状态。

  4. 抢锁成功之后，JVM 会将 Mark Word 中原来的锁对象信息（如哈希码等），保存在抢锁线程锁记录中的Displaced Mark Word 字段中，再将抢锁线程中锁记录的 owner 指针指向锁对象。

     ![1629602512221](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629602512221.png)

- **轻量级锁的膨胀**：

  - 如果临界区代码执行耗时较长，在其执行期间，其他线程都在原地 CAS + 自旋等待替换ptr_to_lock_record，导致一直空耗 CPU，带来很大的性能损耗。
  - 而轻量级锁的本意，是为了减少多线程进入操作系统层面互斥锁的概率，并不是要替代其互斥锁。
  - 因此，在内置锁争用激烈的场景下，轻量级锁会膨胀为基于操作系统内核互斥锁实现的重量级锁。

###### 4）重量级锁

![1629599089661](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629599089661.png)

重量级锁，也称为同步锁，是指当锁处于轻量级锁，如果持有锁的线程执行的时间，超过了自旋等待的最大时间仍然没有释放锁，自旋等待锁的线程不会一直自旋下去，而是会进入阻塞状态，该锁也膨胀为重量级锁。

- 锁对象的 Mark Word 会再次发生变化，指向一个监视器对象，该监视器对象会用集合的形式来登记和管理排队的线程。

  - **监视器**：是一个同步工具，相当于一个许可证，拿到许可证的线程即可进入临界区进行操作，没有拿到的则需要阻塞等待。在 JVM 中，每个对象都关联一个监视器，监视器和对象一起创建、销毁，保证同一时间只有一个线程可以访问被保护的临界区代码。监视器也可以说是同步机制，主要特点是：
    - **同步**：监视器所保护的临界区代码互斥地执行，一个监视器是一个运行许可，任一线程进入临界区代码都需要获得这个许可，离开时会把许可归还。
    - **协作**：监视器提供 Signal 机制，允许正持有许可的线程，暂时放弃许可进入阻塞等待状态，等待其他线程发送 Signal 去唤醒；其他拥有许可的线程可以发送 Signal，唤醒正在阻塞等待的线程，让它可以重新获得许可并启动执行。

- 重量级锁通过监视器的方式，保障了任何时间，只允许一个线程通过受到监视器保护的临界区代码。在Hotspot 虚拟机中，监视器由 C++ 类 ObjectMonitor 实现：

  ![1629615372409](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629615372409.png)

  - **_recursions**：线程的重入次数。
  - **_owner**：标识拥有该 Monitor 的线程，即获得锁的线程。
  - **cxq**：竞争队列，所有请求锁的线程首先会被放入这个竞争队列中。cxq 由 Node 及其 next 指针逻辑构成（单向链表），并不存在一个队列的数据结构，只是一个虚拟队列：
    1. 在线程进入 cxq 前，抢锁线程会先尝试通过 CAS 自旋获取锁，如果获取不到，则会进入 cxq 队列，显然抢锁操作这对于那些已经进入了 cxq 队列的线程是不公平的，因此 synchronized 同步块所使用的重量级锁是**非公平锁**。
    2. 每次新加入的 Node 会在 cxq 的队头进行，通过 CAS 改变第一个结点的指针为新增结点，同时设置新增结点的 next 指向后续结点。
    3. 从 cxq 取出元素时，会从队尾获取。由于只有 owner 线程才能从队尾取出元素，即线程出列操作无争用，因此 cxq 是无锁结构。
  - **_EntryList**： 候选竞争队列，在 owner 线程释放锁时，JVM 会从 cxq 中迁移线程到 EntryList 中，然后指定 EntryList 中的某个线程（一般为 Head），为 OnDeck Thread（Ready Thread），因此 EntryList 中的线程，是作为候选竞争线程而存在的。
    - **OnDeck Thread**：
      - JVM 不直接把锁传递给 Owner Thread，而是还要让 OnDeck Thread 与后来的新抢锁线程竞争抢锁，虽然牺牲了一些公平性，但极大提升了系统的吞吐量。
      - OnDeck Thread 获取到锁资源后，就会变为 Owner Thread，而没获得锁的 Thread 则会依然留在 EntryList 中。
  - **_WaitSet**：等待队列，某个拥有 ObjectMonitor 的线程（owner 线程），在调用 Object.wait（）方法之后将被阻塞，然后该线程将被放置在 _WaitSet 链表中，直到某个时刻通过 Object.notify（）或者Object.notifyAll（）唤醒后，该线程才会重新进入 EntryList 中继续候选竞争锁。

- **重量级锁性能低**：重量级锁使用了操作系统底层的互斥锁（Mutex Lock），会导致线程在用户态和内核态之间的频繁切换，从而带来较大的性能损耗。

  - 处于 cxq、EntryList、WaitSet 中的线程都处于阻塞状态，线程的阻塞或者唤醒，都需要操作系统来帮忙。比如在 Linux 内核中采用 pthread_mutex_lock 系统调用（互斥锁）来实现，因此，进程需要从用户态切换到内核态。
  - 进程从用户态切换到内核态，这种切换需要消耗很多时间，有可能比用户执行代码的时间还要长，而由于轻量级锁使用 CAS 进行自旋抢锁，都处于用户态下，进程不存在用户态和内核态之间的切换，因此轻量级锁开销较小，而重量级锁使用了 Linux 内核态下的互斥锁，会造成较大的性能开销。

##### 3、总

适用场景总结：

| 锁       | 优点                                                         | 缺点                                             | 适用场景                                 |
| -------- | ------------------------------------------------------------ | ------------------------------------------------ | ---------------------------------------- |
| 偏向锁   | 加锁和解锁不需要额外的消耗，和执行非同步方法比，仅存在纳秒级的差距 | 如果线程间存在锁竞争，则会带来额外的锁撤销消耗   | 适用于只有一个线程访问临界区的场景       |
| 轻量级锁 | 竞争的线程不会阻塞，提高程序的响应速度                       | 抢不到锁竞争的线程会使用CAS 自旋等待，会消耗 CPU | 锁占用时间很短，追求响应时间，但吞吐量低 |
| 重量级锁 | 线程竞争不使用自旋，不会消耗 CPU                             | 使用系统互斥锁，线程阻塞，响应时间缓慢           | 吞吐量高，追求吞吐量，但锁占用时间较长   |

=> 以上，就是我对 sychrozied 的一个理解，请问有什么细节需要补充的吗？

#### 3.1.1.2. Java 线程同步方式？

##### 1、总

1. 线程同步，指当有一个线程对内存进行操作时，其他线程都不可以对这个内存进行操作，直到此线程完成操作后，其他线程才能对这个内存进行操作。
2. 在 Java 中，实现线程同步的基础是，操作系统的 mutex 互斥锁、volatile 保证内存可见性和 CAS 比较交换，从上层表现形式来看，可以划分为以下几种同步方式：

##### 2、分

1. **锁式**：指的是，互斥访问同步代码块，拿到互斥量的线程进行同步代码块执行，拿不到互斥量的线程则进入阻塞状态，比如 synchronized 内置锁、Lock 接口的实现类比如 ReentrantLock 重入锁、ReentrantReadWriteLock 读写锁。
2. **事件通知式**：指的是，通过事件通知来协调线程，安全地访问同一块内存，阻塞后的线程需要等待其他线程来唤醒，比如 Object#wait()、notify()、notifyAll()，LockSupport#park()、unpark()，Lock.Condition#await()、signal()。
3. **资源控制式**：指的是，通过控制资源数量，来决定线程是否能够访问临界区或者内存，拿到资源的则可以访问，拿不到的则进入阻塞或者排队等待，比如 Semaphore 信号量。
4. **排队等待式**：指的是，通过队列来实现线程的有序访问，比如 AQS CLH 自旋锁队列、BlockingQueue 阻塞队列。

##### 3、总

以上，就是我对 Java 线程同步方式的一个理解，请问有什么细节需要补充的吗？

#### 3.1.1.3. 你为什么值这么多？

##### 1、总

对于这个问题，我认为我个人对于技术的研究，其广度和深度都是十分符合当前条件。

##### 2、分

1. 首先是广度，
   - 1）开发使用方面，本人有全栈开发经验，对于前后端的实现角度，都有自己一些独特的理解，应用开发上自然也能驾驭得不错。
   - 2）应用架构方面，本人对主流开源框架基本所有了解、掌握，在架构设计时能够有足够多的灵感来源，基本能够覆盖较多的需求实现。
   - 3）系统架构方面，本人对于一些系统部署相关的，做了较多实验，也从中得到了不错的经验和心得，比如 LVS、Haproxy、Nginx、Redis、RabbitMQ、Kafka、ELK、Docker、K8s、Cloud Foundry、Mesos、Marathon，以及一些性能调优方面的等等。
2. 然后是深度，
   - 1）java 语言方面的原理，个人研究过集合和并发框架源码，能够在日常开发中保证需求完成的同时，还能保持代码的高质量。
   - 2）应用框架方面的原理，也是能研究都尽量研究了，比如 java 写的 Spring、Spring MVC、Spring Boot、Spring Cloud、Dubbo、MyBatis 源码等，对于非 java 写的比如 MySQL、Redis、RabbitMQ、Kafka 的一些常见原理也是有所掌握，能够在日常工作中解决掉突发的线上问题。

##### 3、总

综上，我认为这些原因都能很好的回答这个问题，希望能够得到您的认可。

#### 3.2.1.1. epoll 详解，以及与 select 的区别？

##### 1、总

1. `select`、`poll`、`epoll` 都是 Linux 为 I/O 多路复用模型提供的函数。
2. 其中，I/O多路复用，是指可以通过一种机制，让一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。

##### 2、分

###### 1）select

```c++
int select (int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);
```

1. `select` 函数监视的文件描述符分 3 类，分别是 `readfds`、`writefds` 和 `exceptfds`。
2. 调用后，`select` 函数会阻塞，直到有描述副就绪（比如有数据 可读、可写、或者有 except），或者超时（`timeout` 指定等待时间，如果立即返回设为 null 即可），则函数返回。
3. 当 `select` 函数返回后，可以通过遍历 `fdset` ，来找到就绪的描述符。

- **优点**：`select` 目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个优点。
- **缺点**：`select` 的一个缺点是，单个进程能够监视的文件描述符的数量存在最大限制，在 Linux 上一般为1024，可以通过修改宏定义，甚至重新编译内核的方式来提升这一限制，但是这样也会造成效率的降低。

###### 2）poll

```c++
int poll (struct pollfd *fds, unsigned int nfds, int timeout);

struct pollfd {
    int fd; /* file descriptor */
    short events; /* requested events to watch */
    short revents; /* returned events witnessed */
};
```

- **特点**：
  1. 不同与 `select` 使用三个位图来表示三个 `fdset` 的方式，`poll` 只使用一个 `pollfd` 的指针来实现。
  2. `pollfd` 结构包含了要监视的 event，和 发生的 event，不再使用 `select` “参数-值” 的传递方式。
  3. 同时，`pollfd` 并没有最大数量限制，但数量过大后性能也还是会下降。 
  4. 和 `select` 函数一样，`poll` 返回后，需要轮询 `pollfd` 来获取就绪的描述符。
- **缺点**：
  1. 从上面看，`select` 和 `poll` 都需要在返回后，通过遍历文件描述符来获取已经就绪的 socket。
  2. 事实上，同时连接的大量客户端，在一时刻可能只有很少的处于就绪状态，因此随着监视的描述符数量的增长，其效率也会线性下降。

###### 3）epoll

`epoll` 是在 Linux 2.6 内核中提出的，是之前的 `select` 和`poll` 的增强版本。

```c++
int epoll_create(int size)；// 创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；
int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
```

1. 在 `select` / `poll` 中，进程只有在调用一定的方法后，内核才对所有监视的文件描述符进行扫描。
2. 而 `epoll` 事先通过 `epoll_ctl()` 来注册一个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似 `callback` 的回调机制，迅速激活这个文件描述符，当进程调用 `epoll_wait()` 时便得到通知，从而去掉了对文件描述符的遍历操作。

- **优点**：
  1. **监视的描述符数量不受限制**：`epoll` 所支持的 fd 上限，是最大可以打开文件的数目，这个数字一般远大于 2048。比如，在 1GB 内存的机器上大约为10 万左右，具体数目可以 `cat /proc/sys/fs/file-max` 察看，一般来说这个数目和系统内存关系很大。
     - `select` 的最大缺点就是，进程打开的 fd 是有数量限制的，这对于连接数量比较大的服务器来说，根本不能满足。
  2. **I/O 效率不会随着监视 fd 的数量的增长而下降**：`epoll` 不同于 `select` 和 `poll` 轮询的方式，而是通过每个 fd 定义的回调函数来实现的，只有就绪的 fd 才会执行回调函数。
- **局限**：如果没有大量的 idle -connection 或者 dead-connection，`epoll` 的效率并不会比 `select` / `poll` 高很多，但是当遇到大量的 idle- connection，就会发现 `epoll` 的效率大大高于 `select` / `poll`。

##### 3、总

以上，就是我对 select、poll、epoll 的一个理解，请问有什么细节需要补充的吗？

#### 3.2.1.2. Netty 采用了哪种 I/O 多路复用模式？

##### 1、总

1. Netty 采用的是 Reactor 模式，应用于同步 I/O 的操作。
2. 然后， Reactor 根据实现方式的不同，其线层模型又分为 3 种，分别是单 Reactor 单线程模式、单 Reactor 多线程模式、主从 Reactor 多线程模式，其中，Netty 采用的是主从 Reactor 多线程模式。

##### 2、分

首先是 I/O 设计模式，分为 Reactor 模式和 Proactor 模式，

###### 1）Reactor 模式

Reactor 模式，应用于同步 I/O 场景，以 read 操作为例，Reactor 中的具体步骤为：

1. 应用程序注册 `读就绪` 事件和相关联的事件处理器。
2. 事件分离器等待事件的发生。
3. 当发生 `读就绪` 事件时，事件分离器调用第一步注册的事件处理器。
4. 事件处理器首先执行实际的读取操作，然后根据读取到的内容进行进一步的处理。

![1646831826891](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646831826891.png)

**Reactor 优点**：

1. 响应快，可以不必被单个同步时间所阻塞，可以最大程度地避免复杂了多线程及同步问题，以及多线程 / 进程切换的开销。
2. 扩展性好，可以方便的通过增加 Reactor 实例个数，来充分利用 CPU 资源。
3. 复用性好，Reactor 模型本身与具体事件处理逻辑无关，具有很高的复用性。

###### 2）Proactor 模式

Reactor 和 Proactor 模式的主要区别就是，真正的读取和写入操作是由谁来完成的，Reactor 中需要应用程序自己读取或者写入数据，而 Proactor 模式中，应用程序不需要进行实际的读写过程，只需要从缓存区读取或者写入即可，操作系统会写入缓存区或者从缓存区读取并写入到真正的 I/O 设备中。

Proactor 模式 read 操作过程为：

1. 应用程序初始化一个异步读取操作，然后注册相应的事件处理器，此时事件处理器不关注 `读取就绪` 事件，而是关注 `读完成` 事件，这是区别于 Reactor 的关键点 1。
2. 事件分离器等待 `读完成` 事件。
3. 在事件分离器等待 `读完成` 时，操作系统调用内核线程完成读取操作，并将读取的内容放入用户传递过来的缓存区中，这是区别于 Reactor 的关键点 2，在 Proactor 中，应用程序**需要传递缓存区**。
4. 事件分离器捕获到 `读完成` 事件后，激活应用程序注册的事件处理器，事件处理器直接从缓存区读取数据，而不需要进行实际的读取操作。
5. 而 Proactor 中的 write 操作和 read 操作类似，即感兴趣的事件是 `写完成` 事件。

![1646831843451](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646831843451.png)

###### 3）单 Reactor 单线程模式

然后，就是 Reactor 第一种线程模型，单 Reactor 单线程模式，

![1646831937063](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646831937063.png)

1. `selector.select()` 是多路复用模型 NIO#API ，可以实现一个阻塞对象监听多路的连接请求。
2. `Reactor` 对象通过 `selector.select()` 监控客户端请求事件，收到事件后通过 `dispatch()` 进行分发。
3. 如果是建立连接请求事件，则由 `Acceptor` 通过 `serverSocketChannel.accept()` 处理连接请求，然后创建一个 `Handler` 对象，来处理连接完成后的后续业务处理。
4. 如果不是建立连接事件，则 `Reactor` 会分发调用连接对应的 `Handler` 来响应。
5. `Handler` 会完成 `read()` -> `doBusiness()` 业务处理 -> `send()` 一个完整的业务流程。

- **优点**：服务器使用一个线程，通过多路复用搞定所有 I/O 操作（包括连接、读、写等），编码简单，清晰明了，但如果客户端连接数量较多，将无法支撑。
- **缺点**：
  1. **性能问题**：只有一个线程，无法完全发挥多核 CPU 的性能，Handler 在处理某个连接上业务时，整个进程无法处理其它连接事件，容易导致性能瓶颈。
  2. **可靠性问题**：线程意外终止，或者进入死循环，会导致整个系统通信模块不可用，不能接收和处理外部消息，造成节点故障。

###### 4）单 Reactor 多线程模式

![1646889343446](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646889343446.png)

1. `Reactor` 对象通过 `selector.select()` 监控客户端请求事件，收到事件后，通过 `dispatch()` 进行分发。
2. 如果是建立连接请求，则由 `Acceptor` 通过 `serverSocketChannel.accept()` 处理连接请求，然后创建一个 `Handler` 对象，来处理完成连接后的各种事件。
3. 如果不是连接请求，则由 `Reactor` 对象分发调用连接对应的 `Handler` 来处理。
4. 此时，`Handler` 只负责读取和响应事件，不做具体的业务处理，通过 `read()` 读取数据后，会分发给后面的 `Worker` 线程池的某个线程进行处理业务。
5. `Worker` 线程池会分配独立线程完成真正的业务，并将结果返回给 `Handler`。
6. `Handler` 收到线程池处理完的结果后，通过 `send()` 将结果返回给 Client。

- **优点**：可以充分利用多核 CPU 的处理能力。
- **缺点**：
  1. 多线程在需要数据共享时，可能实现比较复杂。
  2. Reactor 单线程处理完所有的监听、连接、读、写事件，在高并发场景下，容易出现性能瓶颈。

###### 5）主从 Reactor 多线程模式

![1646889441630](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646889441630.png)

1. `Reactor` 主线程 `MainReactor` 对象，通过 `selector.select()` 监听连接事件，收到事件后，通过 `Acceptor` 处理连接事件。
2. 当 `Acceptor` 处理连接事件后，`MainReactor` 轮训式地将连接分配给 `SubReactor`，其中，`Reactor` 主线程可以对应多个 `Reactor` 子线程，即 `MainReactor` 可以关联多个 `SubReactor`。
3. `SubReactor` 将连接加入到连接队列进行监听，并创建 `Handler` 进行各种事件处理。
4. 当有新事件发生时，`SubReactor` 就会调用对应的 `Handler` 进行处理。
5. `Handler` 通过 `read()` 读取数据，分发给后面线程池中的 `Worker` 线程进行处理。
6. `Worker` 线程池会分配独立的 worker 线程进行业务处理，并返回结果。
7. `Handler` 收到处理结果后，再通过 `send()` 将结果返回给 Client。

- **优点**：
  1. 父线程与子线程的数据交互简单职责明确，父线程只需要接收新连接，子线程完成后续的业务处理。
  2. 父线程与子线程的数据交互简单，Reactor 主线程只需要把新连接传给子线程，子线程无需返回数据。
- **缺点**：编程复杂度较高。

=> 这种模型在许多项目中广泛使用，包括 Nginx 主从 Reactor 多进程模型，Memcached 主从多线程，Netty 主从多线程模型的支持。

##### 3、总

因此，总结的话，Reactor 模式线程模型可以比喻为，

| Reactor 线程模型    | 比喻                                           | 优点                                                         | 缺点                           |
| ------------------- | ---------------------------------------------- | ------------------------------------------------------------ | ------------------------------ |
| 单 Reactor 单线程   | 前台接待员和服务员都是同一个人，全程为顾客服务 | 主线程负责所有的连接、读写、业务处理，编程简单               | 性能问题、可靠性问题           |
| 单 Reactor 多线程   | 1 个前台接待员，多个服务员，接待员只负责接待   | 主线程负责所有的连接、读写，Worker 负责业务处理              | 高并发场景下，容易出现性能瓶颈 |
| 主从 Reactor 多线程 | 多个前台接待员，多个服务生                     | MainReactor 负责连接、SubReactor 负责读写、Worker 负责业务处理，充分发挥多核 CPU 的优势，满足高并发场景 | 编程复杂                       |

=> 以上，就是我对 Netty 多路复用模式 的一个理解，请问有什么细节需要补充的吗？

#### 3.2.1.3. RocketMQ、Kafka 底层文件原理？

##### 1、总

RocketMQ 消息存储是由 Comsume Queue + Commit Log 配合完成的，Kafka 消息存储是由多 Partition 进行存储的。

##### 2、分

###### RocketMQ

1. 在 RocketMQ 中，所有 Topic 消息都存储在一个称为 Commit Log 的文件中，默认最大为 1GB，超过 1GB 后消息后，就会写到下一个 Commit Log 文件。

2. 通过 CommitLog，RocketMQ 把所有消息存储在一起，以顺序 I/O 的方式写入磁盘，充分利用了磁盘顺序写，减少了 I/O 争用，提高了数据存储的性能。

3. 每个 Consumer 在第一次连接时，都会创建  Consume Queue，一个 Consume Queue 表示 topic#queue，不存储具体的消息，只存储路由到在 CommitLog 中的消息  offset，即具体的消息由 CommitLog 存储。

4. Consumer 在读取消息时，会先读取 Consume Queue，再通过 Consume Queue 中的 offset，读取
   CommitLog，从而得到原始的消息。

   ![1635678882127](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1635678882127.png)

###### Kafka

1. 在 Kafka 日志文件存储中，同一个 Topic 下会有多个不同的 Partition，每个 Partiton 为一个目录，Partition 是实际物理上的概念，而 Topic 则是逻辑上的概念。
2. 同时，一个 Partition 物理上又被分为多个 Log Segment 组成，Segment 不是一个目录，而是由 3 部分组成，分别为 `.index` 文件、`.timeindex` 文件 和 `.log` 文件，分别表示偏移量索引文件、时间戳索引文件和日志数据文件。
3. 然后，通过二分查找来解决查找效率问题，每个 Partition 被平均分配到多个 Segment 文件，也方便 Old Segment 已消费消息的清理，提高磁盘的利用率。

![1634213772578](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1634213772578.png)

**如何查找偏移量为 118 的消息**？根据时间戳查找的方式同理。

1. 首先，Kafka 会用一个 `ConcurrentSkipListMap` 跳跃表，来记录每个日志分段，通过它可以根据偏移量 `118` 定位到 Segment 在 00000000000000000000.index 中。
2. 然后，通过**二分查找**在该 `.index` 文件中，找到**不大于 `offset:118`  的最大索引项**，即 `offset:116` 那栏，得到 `position:9679`。
3. 接着，从 `.log` 文件中，物理位置为 `position:9679` 的位置，开始顺序查找 `offset:118` 的消息。

##### 3、总

因此，RocketMQ 与 Kafka 消息存储上的对比总结为：

|          | RocketMQ                                                     | Kafka                                                        |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 消息存储 | 将所有消息存储在同一个 CommitLog 中，且 Consume Queue 中只存储20个字节每个消息的位置信息 | 将每个 Partition的消息分开存储                               |
| 影响     | 单个 Broker 能支持更多的 Topic 和 Consume Queue，单机支持最高5w个队列，并且 Load 不会发生明显变化 | 单机超过 64 个 Partition，Load 会发生明显的飙高，发送消息的响应时间变长，但对于少 Partition 场景， 由于利用了 Partition 并行处理，使得此时的写性能高于 RocketMQ |
| 原因     | 所有消息都存储在同一个文件中，使得消息存储是磁盘顺序写       | 将消息按 Partition 存储在不同的文件中，使得整体消息存储是随机写，当 Partition 数量非常大时，会出现很多随机 I/O，导致所有 Broker 性能明显下降 |

=> 以上，就是我对 RocketMQ 和 Kafka 底层文件存储原理 的一个理解，请问有什么细节需要补充的吗？

#### 3.2.1.4. ThreadLocal 底层原理？

##### 1、总

1. ThreadLocal，线程本地变量，可以将某个变量放到对象中，使该变量在每个线程中都有专属的引用，不会出现一个线程读取时，被另一个线程修改的现象，从而保证线程安全访问。
2. ThreadLocal，用作**线程隔离**，是解决线程安全问题一个较好的方案，比直接使用同步机制（如 synchronized）解决更简单、更方便、更高效，因为避免了加同步锁带来的性能损失，大大提升了并发性的性能，比如，数据库连接、Session 管理等。
3. 另外，根据 ThreadLocal 的特性，同一线程在某些地方进行设置，在随后的任意地方都可以获取到，从而可以方便地实现**跨函数的数据传递**，避免通过参数传递数据带来的高耦合。

##### 2、分

其**线程隔离原理**为：

1. ThreadLocal 内部定义了一个静态的  `ThreadLocalMap`，在线程第一次调用 `get()/set()` 方法时，会被构造出来，并由 Thread 实例持有该引用。
2. 然后，ThreadLocal 实例作为 `ThreadLocalMap` 的 Key，也就是只要传入所需的 ThreadLocal 变量的引用，就可以获取到保存在 `ThreadLocalMap` 中对应的值。
3. 从而保证每个线程，即使在获取同一个 `ThreadLocal ` 变量的值时，也能保证值是互相隔离的。

TheadLocalMap#Entry 定义为**弱键的好处**：

1. 比如，线程 t 调用一个 `funcA()` 方法，新建了一个 ThreadLocal 实例 local，然后调用 `local.set()` 方法设置一个 100 后，调用 `local.get()` 方法去获取值。

   ```java
   public void funcA() {
       ThreadLocal<Integer> local = new ThreadLocal<>();
       local.set(100);
       local.get();
   }
   ```

2. 此时的内存结构是，local 被线程 t 强引用，在 `set()` 调用后，线程 t#ThreadLocalMap，会新建一个 Entry 实例，其 Key 以弱引用的方式包装，`WeakReference` 指向 ThreadLocal 实例。

3. 当线程 t 执行完 `funcA()` 方法后，该方法的栈帧会被销毁，栈帧中的 `local` 强引用则会被回收，但如果线程 t 还在继续调用其他方式时，则会导致 t#ThreadLocalMap 中对应的 Entry.Key 引用，还指向 ThreadLocal 实例。

4. 如果 Entry#Key 引用是**强引用**的话，那么就会导致 Key 引用指向的 ThreadLocal 实例以及对应的 Value 值，都不能被 GC 回收，造成内存泄漏的发生。

5. 如果 Entry#Key 引用是**弱引用**的话，那么在下次 GC 发生时就会，让使那些没有被其他强引用指向、仅被Entry#Key 所指向的 ThreadLocal 实例被顺利回收，在 Entry#Key 引用被回收之后，其 Entry#Key 被设置 null，在后续调用 `get()/set()/remove()` 时，ThreadLocalMap 的内部代码就会清除这些 Key 为 null 的 Entry，从而释放对应的内存，避免内存泄露的发生。

**内存泄露**发生的场景：

1. 然而，即使 ThreadLocalMap#Entry 的 Key 被设置为弱键，可以避免内存泄露的发生，但是，如果 ThreadLocal  使用不当的话，还是可能会出现内存泄露。
2. 第一种情况是，**Entry.value 导致的内存泄露**：
   - 1）在`set()` 方法调用后， 假设没 调用 ` get()` 方法，就退出这个  `funA()` 方法，`local` 局部变量被回收后，t#ThreadLocalMap 的 Entry#key 被设置 null。
   - 2）然后，线程 t 继续运行，没有被销毁，也没有调用过任何 `get()/set()/remove()` 方法，导致那一整个 key 为 null 的 Entry 都不会被回收， 导致 Entry.value 内存泄露额度发生。
   - 3）这种现象很容易，发生在线程池中的 Thread 实例中，解决方法是， 只要后续存在任何一个 `get()/set()/remove()` 方法被调用，就会触发 Key为 null 的 Entry 的清理工作，释放掉对应的 Entry 内存，解决 Entry.value 内存泄露的问题。
3. 而第二种情况则是，**static、final 修饰 ThreadLocal 导致的内存泄露**：
   - 1）首先，ThreadLocal 实例被 static 修饰，可以保证每个线程操作这个 ThreadLocal，作为 ThreadLocalMap Entry#key 时，都会共享一份地址空间，节省内存的使用。
   - 2）其次，使用 final 修饰，可以确保 ThreadLocal 实例的唯一性，防止使用过程中发生动态变更。
   - 3）此时，由于修饰后的 ThreadLocal 会以单例的形式存在，一直被 GC Root 强引用，导致引用该 ThreadLocal 实例的 ThreadLocalMap，它的 Entry#Key 在 Thread t 实例的生命周期内，始终会保持为非 null。
   - 4）这样，该 Entry 就不能被 ThreadLocal 自动清空掉，导致其 Value 所指向的对象，一直被 Entry 强引用，于是这个对象就在该线程 t 的生命周期内，一直不会被释放掉，导致内存泄漏的发生。
   - 5）所以，在使用完 static、final 修饰的 ThreadLocal 实例后，要及时调用 `remove()` 来进行显式地释放掉。

##### 3、总

1. 综上，虽然使用 ThreadLocal 可以轻量级地保证变量的线程隔离。

2. 但如果使用不当，就容易发生内存泄漏，如果我们在 ThreadLocal 使用完毕后，及时调用 `remove()` 方法，就可简单、有效地避免这些内存泄漏的情况发生。

   => 这里可以讲一下这个实际案例《[2.1.1.1. 项目自我介绍？](#2.1.1.1. 项目自我介绍？)》- 3、项目亮点 - 6）自研 @DataSource 注解，ThreadLocal 没及时释放的问题 | 线程池、ThreadLocal、数据源。

3. 以上，就是我对 ThreadLocal 底层原理 的一个理解，请问有什么细节需要补充的吗？

#### 3.2.1.5. HashMap 底层原理？

##### 1、总

HashMap，是 Map 接口的散列表非线程安全形式的实现，可以允许 null 值和 null 键，但不保证元素的顺序，在散列均匀的情况下，`get()/put()/remove()` 方法时间复杂度都是 O（1），当发生哈希冲突时，是通过拉链法来解决，在 JDK 7 中是通过数组 + 链表来实现，在 JDK 8 中是通过数组 + 链表 + 红黑树来实现。

其中，它有几个**重要的参数**：

1. `table.length`：当前散列表的桶容量，初始时默认为 16。

2. `size`：当前 HashMap 的实际大小，等于 Entry 条目总数。

3. `threshold`：HashMap 的阈值，当实际大小超过阈值时，HashMap 会发生扩容为 2 倍的桶容量和阈值，在 JDK 8 中，初始化时 = 默认负载因子 0.75f * 默认桶容量 16 = 12，扩容后超过 Integer.MAX_VALUE 时，等于扩容后的容量 * 设定的负载因子，在 JDK 7 中，初始化和扩容时，都 = 设定的负载因子 * 桶容量。

4. `loadFactor`：HashMap 的负载因子，由于会涉及 HashMap 阈值的计算，所以它是 HashMap 桶容量满意程度的表现，初始时默认为 0.75f，

   - 如果负载因子大于 0.75f，虽然会减少空间的开销，在 JDK 7 中，会导致阈值变大，扩容次数减少，桶拉链变长，增加查找的成本。
   - 如果负载因子小于 0.75f，则会让阈值变小，增加了扩容次数，增大空间的开销，不过好处在于，可以让哈希冲突减少，桶拉链变短，查找效率提高。

   => 所以，0.75f 的负载因子，是 JDK 对 HashMap 在时间和空间成本之间，做的一个很好的权衡取舍。

##### 2、分

在 JDK 8 中，

1. **散列原理**：

   - 1） Key 需要先传入 `hash（Object）` 扰动函数，将 HashCode 右移 16 位，从而混合 HashCode 的高位和低位，加大低位的随机性，减少哈希碰撞发生的概率，是一种性能、效用和质量的折衷方案。
     - 使用简单的位移与异或操作，减少系统的计算损耗；使用高位异或，可以减少低位冲突的可能性，保证查找效率。
     - HashCode 右移 16 位，使得高位能被利用起来，保证了效用性。
     - 使用高位异或，可以减少低位冲突的可能性，保证散列表的质量。
   - 2）`（n - 1） & hash` 计算哈希索引，相当于 `hash % n` 取模计算，n 指散列表当前的桶容量，hash 指获取 key#hashCode 扰动后的结果。
     1. 之所以**要取模计算**，不能直接使用 HashCode 作为索引的原因是，hashCode 为 int 类型，范围为[-2^32，2^32 - 1]，如果散列表数组与 HashCode 一一对应，那么就需要 40 亿的空间，明显在内存中是放不下的，所以 hashCode 是不能直接作为数组索引的。而如果使用 hashCode 对散列表数组长度取模，那么就可以解决这个问题，从而保证较小的数组，也还能利用上 hashCode。
     2. 1）之所以要 n 散列表的桶容量，要取为 **n 的 2 幂次**，原因之一是为了，让 HashMap 在取模计算时，能够通过 n-1，来获得取低位掩码，然后通过低位掩码与扰动后的 hash 值进行一次与运算，即可得到该 hash 值，在散列表数组中的索引。
     3. 2）而另外一个原因是，为了让 HashMap 在扩容时，可通过旧 hash 值与低位掩码相与，只移动少部分与结果高位为 1 的条目，其他条目无需移动，减少了扩容时要搬运的条目数量，减少扩容时间。

2. **条目获取原理**：2 个常用的顶层 API 方法 `get()/getOrDefault()`，都依赖于 `getNode()` 方法，其步骤为：

   - 1）`hash（key）` 计算出 key 的 hash 值，`(n - 1) & hash` 计算出哈希索引，`tab[i]` 得到哈希索引对应散列表的桶 p。
   - 2）如果桶 p 为 null，则返回 null，代表 HashMap 中不存在对应的结点。
   - 3）如果桶 p 不为 null，则分 4 种情况判断：
     1. 如果桶头结点 p#hash 值相等，且 key 值相等或者 key 对象 equals，说明 p 就是要找的节点，直接返回即可。
     2. 否则，说明在桶头没找到对应的节点，如果 p 没有 next 节点，则返回 null，代表 HashMap 中不存在对应的结点。
     3. 否则，说明 p 节点有 next 节点，如果 p 为红黑树结点，则根据 hash 和 key  调用  `TreeNode#getTreeNode()` 获取对应的节点，直接返回即可，如果找不到，则返回 null，代表 HashMap 中不存在对应的结点。
     4. 如果 p 为普通结点，则遍历 p 链表，如果找到 p#hash 值相等，且 key 值相等或者 key 对象 equals的结点，直接返回即可，如果找不到，则返回 null，代表 HashMap 中不存在对应的结点。

3. **条目添加原理**：3 个常用的顶层 API 方法 `put()/putIfAbsent()/putAll()`，都依赖于 `putVal()` 方法，其步骤为：

   - 1）`hash（key）` 计算出 key 的 hash 值，`(n - 1) & hash` 计算出哈希索引，`tab[i]` 得到哈希索引对应散列表的桶 p。
   - 2）如果 p 桶为 null，则直接 new Node 放到该桶即可。
   - 3）如果 p 桶不为 null，则要分为 4 种情况判断：
     1. 如果桶头结点 p#hash 值相等，且 key 值相等或者 key 对象 equals，说明 p 就是要找的结点，如果此时 onlyIfAbsent 为 false，则发生值替换，直接返回即可。
     2. 否则，说明在桶头没找到对应的节点，如果桶头结点 p 为红黑树结点，则使用 `TreeNode#putTreeVal()` 添加 key：value 条目。
     3. 如果 p 为普通链表结点，则遍历 p 桶链表，遍历过程中，如果找到 p#hash 值相等，且 key 值相等或者 key 对象 equals 的结点，如果此时 onlyIfAbsent 为 false，则发生值替换，直接返回即可。
     4. 如果遍历 p 桶链表，没找到对应的结点，则在链尾 new Node 一个结点，且添加后，判断如果当前链表至少有 8 个结点，则调用 `HashMap#treeBin()` 将当前桶链表，树化成一棵红黑树，其中，`treeBin()` 还会判断散列表容量，是否大于等于最小树化容量 64。
     5. 所以，JDK 8#HashMap 中，树化为红黑树的条件为，链表长度大于等于 8，且桶容量大于等于 64。
   - 4）如果发生的不是值替换，则更新修改模数，以及 HashMap 的实际大小，如果实际大小大于阈值，则还需要调用 `resize（）` 进行扩容并转移结点。
   - 5）最后返回 null，代表条目插入成功。

4. **扩容原理**：

   - 1）在添加条目后，判断到实际大小大于阈值时，则会触发 HashMap 的扩容操作。
   - 2）扩容前，正常情况下是，让容量 * 2，阈值 * 2 作为新容量和新阈值，而如果阈值超出了 Integer.MAX_VALUE，则新阈值会被设置为新容量 * 设定的负载因子。
   - 3）然后，根据新容量，创建新数组作为新的散列表： `(Node<K,V>[])new Node[newCap]` 。
   - 4）接着，从头遍历旧的散列表数组，挨个判断桶头节点，或者桶链表：
     1. 如果桶 j 只有一个元素，则重新计算哈希索引，转移元素到新表即可。
     2. 如果桶 j 为红黑树，则调用红黑树的 `TreeNode#split()` 方法，可通过旧 hash 值与低位掩码相与，高位为 0 的条目构成 lo 链表，为 1 的条目构成 hi 链表，lo 链表桶位置保持不动，hi 链表位置移动 `oldCap` 索引，然后分别判断 lo、hi 链表长度小于等于 6，则退化成普通链表，否则树化为红黑树。这也正是 n 之所以要为 2 幂次的其中一个原因。
     3. 如果桶 j 为普通链表，则通过旧 hash 值与低位掩码相与，高位为 0 的条目构成 lo 链表，为 1 的条目构成 hi 链表，lo 链表桶位置保持不动，hi 链表位置移动 `oldCap` 索引。

5. **条目删除原理**：2 个常用的顶层 API 方法 `remove(Object)/remove(Object, Object)`，都依赖于`removeNode()` 方法，其步骤为：

   - 1）`hash（key）` 计算出 key 的 hash 值，`(n - 1) & hash` 计算出哈希索引，`tab[i]` 得到哈希索引对应散列表的桶 p。
   - 2）如果桶 p 为 null，则返回 null，代表 HashMap 中不存在对应的结点。
   - 3）如果 p 桶不为 null，则分 4 种情况判断：
     1. 如果桶头结点 p#hash 值相等，且 key 值相等或者 key 对象 equals，说明 p 就是要找的结点，则将 p 引用赋值给 node 引用，待后面做删除操作。
     2. 否则，说明在桶头没找到对应的节点，如果桶头结点 p 也没有 next 节点，则将 p 引用赋值给 node 引用，待后面做删除操作。
     3. 否则，说明 p 节点有 next 节点，如果 p 为红黑树结点，则根据 hash 和 key  调用  `TreeNode#getTreeNode()` 获取对应的节点，并将其引用赋值给 node 引用，待后面做删除操作。
     4. 如果 p 为普通结点，则遍历 p 链表，如果找到 p#hash 值相等，且 key 值相等或者 key 对象 equals的结点，并将其引用赋值给 node 引用，待后面做删除操作。
     5. 如果找不到，则设置 node 引用为 null。
   - 4）如果 node 引用为 null，则返回 null，代表 HashMap 中不存在对应的结点。
   - 5）如果 node 引用不为 null，说明找到了对应的结点，如果此时 matchValue 为 true，则还要判断 value 对象 equals，分为 3 种情况判断：
     1. 如果 node 为红黑树结点，则调用 `TreeNode#removeTreeNode()` 删除节点。
     2. 如果 node 为普通结点，且作为桶头，则脱钩 node 节点，并更新 node 后继作为新的桶头。
     3. 如果 node 为普通结点，也不为桶头，则链接它的前驱和后继，脱钩 node 节点。
   - 6）最后，如果脱钩 node 节点成功，则更新修改模数、实际大小，返回 node 节点引用。

6. **红黑树 TreeNode 节点性质**：

   - **性质 1**：红黑树的结点要么是红色，要么是黑色。
   - **性质 2**：红黑树的根结点是黑色的。
   - **性质 3**：红黑树的叶子结点（nil）都是黑色的。
   - **性质 4**：红黑树的红色结点必须有两个黑色结点。
     - 推论：从根结点到每个叶子结点的所有路径上，不可能存在两个连续的红色结点。
   - **性质5**：红黑树是黑色平衡的，即从根结点到每个叶子结点的所有路径中，所经过的黑色结点数都是一样的。
     - 推论：如果一个结点右黑色的子结点，那么该结点一定是有两个孩子结点，因为必须有另一半才能保证该结点黑色平衡。

7. **红黑树旋转原理**：目的是，为了在结点的添加和删除后，避免子树高度发生变化，通过调整树结构，来保证树重新达到平衡。

   - **左旋**：口诀，**左右左右右**，即以 x 结点作为旋转点进行**左**旋，旋转后，x 的**右**结点 p 成为 x 的父结点，p 原本的**左**结点成为 x 结点的**右**结点，p 原本的**右**结点保持不变。
   - **右旋**：口诀，**右左右左左**，即以 x 结点作为旋转点进行**右**旋，旋转后，x 的**左**结点p成为x的父结点，p 原本的**右**结点成为 x 结点的**左**结点，p 原本的**左**结点保持不变。

8. **红黑树节点插入原理**：`putTreeVal（HashMap，Node，int，K，V）`

   - 1）从根结点遍历比较待插入结点 x 的 hash 值，小于等于 0 的，说明 x 应该在左边，大于 0 的说明
     x 应该在右边。
   - 2）找到合适位置后（叶子结点），构建 TreeNode 结点，维护 x 与父结点、prev 前驱结点、next 后继结点的关系。
   - 3）插入后，再平衡红黑树，返回 null，表示插入成功。

   其中，插入后，平衡红黑树的原理，要分析 2-3-4 树，分为 4 种情况：

   - 1）为空结点新增：x 插入后，将成为一个 2 结点，插入前，树为 null，插入后， x 需要变黑色，作为根结点，明显在 HashMap 中，不可能会有这种情况，因为链表长度至少要为 8。
   - 2）合并到 2 结点：x 插入后，将成为一个 3 结点，插入前，2 结点为黑色，插入后无论是（上黑下左红 |  上黑下右红）, 都符合 3 结点要求，因此无需调整。
   - 3）合并到 3 结点中：x 插入后，将成为一个 4 结点，插入前，为 3 结点（上黑下左红 |  上黑下右红），插入后，成为 4 结点（黑红红），其中，根据 x 插入位置的不同，又分为 6 种情况：
     1. x 插入后，为左三(中左 左*) 黑红 红，不符合红黑树定义，需要调整，则中 1 右旋，中 1 变红，左 1 变黑。
     2. x 插入后，为(中左右*)  黑红红，其实就相当于左三，如果对父结点进行左旋，就可得到左三 ，不符合红黑树定义，需要调整，则左 1 左旋（得到左三），中 1 右旋，中 1 变红，新左变黑。
     3. x 插入后，为右三(中 右右*) 黑红红，不符合红黑树定义，需要调整，则中 1 左旋，中 1 变红，右 1 变黑。
     4. x 插入后，为(中 右左*) 黑红红，其实就相当于右三，如果对父结点进行右旋，就可得到右三，不符合红黑树定义，需要调整，则右 1 右旋（得到右三），中 1 左旋，中 1 变红，新右变黑。
     5. x 插入后，为(中左 右*) 黑红 红，符合红黑树定义，因此无需调整。
     6. x 插入后，为(中左* 右) 黑红 红，符合红黑树定义，因此也无需调整。
   - 4）合并到 4 结点中：x 插入后，成为一个裂变状态即升元，插入前，为 4 结点（黑红红），插入后，4 结点需要颜色反转，爷结点成为新的 x 结点，继续下一轮的向上调整，其中，根据 x 插入的位置不同，分为 4 种情况：
     1. x 插入后，为(中左左* 右) 黑红红 红，不符合红黑树定义，需要调整，则中变红，左 1 变黑，左 2 保持为红， 右 1 变黑，中看作为“插入结点”，继续向上调整。
     2. x 插入后，为(中左右* 右) 黑红红 红，不符合红黑树定义，需要调整，则中变红，左 1 变黑，右 1 保持为红，右 2 变黑，中看作为“插入结点”，继续向上调整。
     3. x 插入后，为(中左 右左*) 黑红 红红，不符合红黑树定义，需要调整，则中变红，左 1 变黑，左 2 保持为红，右 1 变黑，中看作为“插入结点”，继续向上调整。
     4. x 插入后，为(中左 右右*) 黑红 红红，不符合红黑树定义，需要调整，则中变红，左 1 变黑，右 1 变黑，右 2 保持为红，中看作为“插入结点”，继续向上调整。

9. **红黑树节点删除原理**：`removeTreeNode（HashMap，Node，boolean）`

   - 1）首选，由于红黑树是一种自平衡的二叉搜索树，而二叉搜索树删除，本质上就是找前驱，或者后继结点来替代删除。

   - 2）如果要删除的结点，是叶子结点，则直接删除即可（肯定是黑色），不过由于是黑色，打破了黑色平衡，所以删除后还需要平衡红黑树。

   - 3）如果要删除的结点，只有 1 个孩子结点，则使用孩子节点进行替代，然后删除"替代"的孩子节点，由于可能打破了黑色平衡，所以删除后还需要平衡红黑树。

   - 4）如果要删除的结点，有 2 个孩子结点，则需要找到后继进行替代，然后删除"替代节点"，其中，"替代节点"删除时又要分为 2 种情况：

     1. 如果替代结点，没有孩子结点，说明为 2-3-4 树的 2 结点，则直接选择为"替代结点"进行删除。
     2. 如果替代结点，有孩子结点，且孩子结点为替代方向（后继方向），说明所在的结点为 2-3-4 树的 3 结点或者 4 结点，则选择孩子结点作为"替代结点"进行删除。

     由于可能打破了黑色平衡，所以删除后还需要平衡红黑树。

   => 其中，删除时，平衡红黑树的原理，要分析 2-3-4 树，分为 4 种情况：`balanceDeletion（TreeNode，TreeNode）` 。

   - 1）x 自己搞得定：

     1. 自己搞得定的意思是，可以在 2-3-4 树节点内部，自己处理完毕，不会影响其他节点的结构。
     2. 对应的情况为，x 要么为 3 结点，要么为 4 结点中的红结点，则直接置黑，返回 x 用作删除即可，因为红节点不会打破黑色平衡。

   - 2）x 自己搞不定，但兄弟搞得定：

     1. 自己搞不定的意思是，自身结点为黑结点，如果直接删除，会导致父结点所在的树黑色不平衡。
     2. 兄弟搞得定的意思是，兄弟结点存在多余的子结点，即兄弟结点为 3 结点或者 4 结点，这样，x 的父结点，就可以借出结点下来合并到 x 结点，兄弟结点再借出结点合并到父结点，就可以顺利删除 x 了，同时 2-3-4 树的结构还可以保持不变。
     3. 但是，前提是 x 的兄弟结点是真正的兄弟结点，即为黑色的结点，如果为红色的结点，说明其只是父结点（3结点）的红结点，此时还需要对父结点进行旋转，以保证 x 有真正的兄弟结点。

     此时，根据情况又分为 3 种情况：比如 x 为左边时，右边情况同理。

     1. 兄弟结点为 3 结点，同时有右节点，x 在左子树一方，x 的兄弟结点 xpr 为右子树，如果 xpr 有右孩子，则可以顺利地对父结点 xp 进行左旋。左旋后，在 2-3-4 树结构看来，xp 作为 xpr 的左孩子（相当于父结点借出去一个结点，合并到 x 结点），xpr 作为 xp 的父亲（相当于兄弟结点借出去一个结点，合并到父结点），xpr 借出去的结点颜色为 xp 借出去的结点颜色，xp 借出去的结点颜色一定要为黑色（相当于3结点），xpr 剩余结点一定要为黑色（相当于叶子结点），然后返回 x 用作删除即可。
     2. 兄弟结点为 3 结点，但没有右节点，x 在左子树一方，x 的兄弟结点 xpr 为右子树，如果 xpr 没右孩子，不能直接对父结点进行左旋，所以，这是一个临时情况，需要对 xpr 进行右旋，转换为有右孩子的情况一，再做一的相同处理即可。
     3. 兄弟结点为 4 结点，x 在左子树一方，x 的兄弟结点 xpr 为右子树，如果 xpr 有右，则可以顺利地对父结点 xp 进行左旋。左旋后，在 2-3-4 树结构看来，xp 作为 xpr 的左孩子（相当于父结点借出去一个结点，合并到 x 结点），xpr 作为 xp 的父亲（相当于兄弟结点借出去一个结点，合并到父结点，而且还多借出左孩子合并到 x 结点中，这里选择借出 2 个结点，可以进一步减少花销），xpr 合并到父结点的颜色为 xp 借出去的结点颜色（而借出去的左孩子本来为红色所以不用变），xp 借出去的结点颜色一定要为黑色（相当于 4 结点），xpr 剩余结点一定要为黑色（相当于叶子结点），然后返回 x 用作删除即可。

   - 3）x 自己搞不定，并且兄弟也搞不定：

     1. 自己搞不定的意思是，自身结点为黑结点，如果直接删除，会导致父结点所在的树黑色不平衡。
     2. 兄弟也搞不定的意思是，兄弟结点也为黑结点，没有多余的子结点，如果直接删除 x，则会导致叔结点所在路径多了一个黑色结点，造成黑色不平衡。

     1. 这种情况是，兄弟结点为 2 结点，为了让 x 能够顺利删除，兄弟结点需要置红（自损），这样 x 在删除后，x 父结点所在树还是黑色平衡的。
     2. 但是，如果 x 父结点为黑色，x 爷结点所在树则不黑色平衡了，因为父结点这边少了一个黑色结点，所以，父结点的叔结点要也要被置红。
     3. 因此，需要一路向上自损，直到碰到任意一个终止条件，即可结束自损：
        - **向上碰到根结点**：经过一路置红叔结点，直到循环到根结点时（因为上面已经没有父节点了），则代表自损完毕，此时整棵树都是黑色平衡的了（都减少了一个黑色结点）。
        - **向上碰到红结点**：如果碰到红色结点，只需要把该结点置黑，无需再置红叔结点了，相当于在父结点这边子树补回了一个黑色结点，不影响这边子树的黑色结点数目，整棵树还是黑色平衡的。

10. **红黑树节点获取原理**：`getTreeNode（int，Object）`

    - 1）根据 hash 值和 key 值，从根结点开始查找红黑树结点，小于等于 0 的，说明 x 应该在左边，大于 0 的，说明 x 应该在右边。
    - 2）直到找到 hash 值相等，且 key 值相等或者 key 对象 equals 的结点，说明该结点就是要找的结点，则直接返回即可。
    - 3）如果找不到，则返回 null，代表没找到对应的结点。

11. **链表树化成红黑树**：`HashMap#treeifyBin()`

    - 1）先判断桶容量，是否大于等于 64，如果不是，则调用 `resize()` 扩容即可。
    - 2）如果确实大于等于 64，则先挨个维护每个桶的链表，为双向无环链表。
    - 3）接着，调用 `TreeNode#treeify` 树化该链表成为红黑树，其步骤为：
      1. 取桶头结点作为根结点，置黑。
      2. 然后遍历桶链表，比较根结点 hash 值与当前遍历结点的 hash 值，小于等于的，则继续遍历左子树，大于的，则遍历右子树，然后插入当前遍历结点到对应的位置，再平衡红黑树。
    - 4）直到散列表所有桶、每个桶的链表结点遍历完毕，再返回。

12. **红黑树退化成普通链表**：`untreeify（HashMap）`

    - 1）遍历桶链表，重新构建为 `next=null` 的 Node 结点。
    - 2）再重新维护每个它的 next 指针。
    - 3）直到遍历结束，最后返回链头指针 hd 即可。

##### 3、总

相对于 JDK 7#HashMap，JDK 8 主要优化了以下 3 点：

1. **引用红黑树**：避免链表过长影响查找效率，同时还可以保证插入性能。
2. **`resize()` 扩容优化** 以及**头插法改成尾插法** ：取消了 rehash 操作，通过旧 hash 值与低位掩码相与，高位为 0 的条目构成 lo 链表，为 1 的条目构成 hi 链表，lo 链表桶位置保持不动，hi 链表位置移动 `oldCap` 索引，以及由于头插法会使得结点转移后节点倒序，改用头插法改成尾插法，可以保证转移后，链表结点的相对顺序不变，从而解决了并发情况下扩容导致的死循环问题。

但 HashMap 仍是非线程安全的，并发下添加结点，可能会造成数据丢失，而线程安全的 Map 使用方式有：

1. **在 HashMap API 外层使用锁，**来保证线程安全。
2. **使用 Collections 内部类 SynchronizedMap**：底层原理是，通过持有传入的 Map 引用，以及 `mutex` 对象锁，使用 synchronized 关键字来修饰 Map 方法，从而包装成线程安全的 Map 实现类。
3. **使用 HashTable**：散列表线程安全的实现类，默认初始容量 11，默认负载因子 0.75f，不允许为 null 键和null 值，底层通过使用 synchronized 关键字修饰方法，来保证线程安全，所以效率低下。
4. **使用 ConcurrentHashMap**：支持更高并发更新与查询的散列表线程安全的实现类，默认初始容量和负载因子与 HashMap 一样，但不允许为 null 键和 null 值，可以在保持并发可读的同时，还能最小化锁争用。

=> 以上，就是我对 HashMap 底层原理 的一个理解，请问有什么细节需要补充的吗？

#### 3.2.1.6. ConcurrentHashMap 底层原理？

##### 1、总

ConcurrentHashMap，支持更高并发更新与查询的散列表线程安全的实现类，默认初始容量和负载因子与 HashMap 一样，但不允许为 null 键和 null 值，可在保持并发可读的同时，还能最小化锁争用。

其中，它有几个**重要的参数**：

1. `table`：当前正在使用的散列表，volatile 修饰，具有并发可见性。
2. `nextTable`：并发扩容时，持有的另外一个散列表，volatile 修饰，具有并发可见性。
3. `sizeCtl`：ConcurrentHashMap 的控制变量，volatile 修饰，具有并发可见性，提供 `SIZECTL` 通过 CAS 进行原子性更新。
   - 1）在构造函数中为（1.5 倍指定容量 + 1，或者指定容量 / 负载因子）的最小 2 幂次。
   - 2）当分布式计数统计结果，大于 `sizeCtl` 时，则触发并发扩容机制，高 16 位为扩容标记，低 16 位为并发扩容线程数（步长为 1，然后再每有一个线程参与协助扩容则 +1）。
   - 3）在扩容完成后，等于 0.75f * 旧表桶容量，作为下一次扩容的阈值判断条件。

几种重要的**结点类型**：

1. `Node`：实现 Map.Entry，是 ConcurrentHashMap 中最普通的链表结点，拥有 hash、key、val、next 成员变量，是其他类型结点的父类，其中 val 和 next 使用 volatile 修饰，保证了并发可见性。
2. `TreeNode`：继承 Node 结点，是 ConcurrentHashMap 中的红黑树结点，在 Node 结点的基础上，还维护了parent、left、right、red 红黑树成员变量。
3. `TreeBins`：
   - 1）继承 Node 结点，是 ConcurrentHashMap 中红黑树的桶头结点，hash 值为-2，持有红黑树根结点root 指针和链头 first 指针，不保存键和值。
   - 2）同时还维护了读写锁，强迫写线程必须等待所有读线程完成后，才能进行红黑树结点操作。
   - 3）当读时不存在并发写线程，使用 root 指针走红黑树遍历方式查找结点，当读时存在并发写线程，使用first 指针走链表遍历方式去查找结点。
4. `ForwardingNode`：
   - 1）继承 Node 结点，是 ConcurrentHashMap 中的转发结点，hash 值为 -1，持有 nextTable 引用，没有键和值。
   - 2）在线程协助转移结点到新表后，会在旧表原位置维护一个 `Forwarding` 结点，以标识旧表正在发生扩容操作，让下一个线程碰到时，可以协助进行转移旧表结点。

##### 2、分

在 JDK 8 中，

1. **散列原理**：原理与 HashMap 类似，但不同的地方在于，HashMap 的扰动函数叫 `hash(Object)`，而 ConcurrentHashMap 的扰动函数叫 `spread（Object）` 。

2. **条目获取原理**：其原理与 HashMap 类似，但不同的地方在于，ConcurrentHashMap 的获取方法是线程安全的，其并发安全原理为：

   - 1）`getOrDefault()`，依赖于 `get()` 方法，其中 val 和 next 使用 volatile 修饰，保证线程可见性，所以，`get()` 方法可以不加锁地照常遍历，而在读红黑树时，由于并发更新需要涉及到旋转，所以有写时会走链表方式去读，在树方式读时，不能并发下对红黑树进行写。
   - 2）因此，可与 `put()` 和 `remove()` 等更新方法同时执行，但反映的只是，最近完成更新的结果，并发检索可能只反映出部分条目的更新。

3. **条目添加原理**：其原理与 HashMap 类似，但不同的地方在于，ConcurrentHashMap 的获取方法是线程安全的，其并发安全原理为：

   - 1）如果根据哈希索引得到的桶 p 为null，new Node 以后是通过 CAS 的方式放入到散列表中的。
   - 2）然后，如果桶 p 不为 null，还要判断是否为 `Forwardding` 节点（p#hash = -2），如果是的话，则当前线程要参与协助扩容。
   - 3）然后，如果桶 p 也不为 `Forwardding` 节点，那么还会对其加 synchronized 锁，再做类似于 HashMap 那样，分为情况判断：
     1. 如果桶 p#hash 值小于 0，说明 p 为红黑树，则使用 `TreeBin#putTreeVal()` 添加 key：value 条目。
     2. 如果桶 p#hash 值大于等于 0，说明为普通链表结点，则遍历 p 链表，如果找到 hash 值相等，且 key 值相等或者 key 对象 equals 的结点，如果此时 onlyIfAbsent为false，则发生值替换，直接返回即可。
     3. 如果没找到对应的结点，则在链尾直接 new Node 一个结点。
     4. 如果添加后，当前链表至少有 8 个结点，则调用 `ConcurrentHashMap#treeBin()`，将当前链表，树化成一棵红黑树，其中，`treeBin()` 还会判断散列表容量，是否大于等于最小树化容量 64。
     5. 所以，JDK 8#ConcurrentHashMap 中，树化为红黑树的条件为，链表长度大于等于 8，且桶容量大于等于 64。
   - 4）最后，如果发生的不是值替换，而是条目插入，还要并发分布式计数，然后判断是否需要扩容，要的话则发起扩容。，返回 null，代表结点插入成功。

4. **条目删除原理**：其原理与 HashMap 类似，但不同的地方在于，ConcurrentHashMap 的获取方法是线程安全的，其并发安全原理为：

   - 1）如果根据哈希索引得到的桶 p，为 `Forwardding` 节点，则当前线程要参与协助扩容。
   - 2）如果 p 不为 `Forwardding` 结点，则会对 p 进行加 synchronized 锁，再做类似于 HashMap 那样，分为情况判断：
     1. 如果 p#hash 小于 0，说明 p 为红黑树，则调用 `TreeNode#getTreeNode()`，来获取 hash 和 key 对应的结点，调用 `TreeNode#removeTreeNode()` 删除该结点。
     2. 如果 p#hash 值大于等于0，说明它为普通链表结点，则遍历 p 链表，如果找到 hash 值相等，且 key 值相等或者 key 对象 equals 的结点，则脱钩该结点。
     3. 如果没找到对应的结点，则返回 null，代表 ConcurrentHashMap 不存在对应的结点。
   - 3）最后，如果脱钩结点成功，则还要并发分布式计数，返回旧值，代表删除成功。

5. **分布式计数相关变量**：

   - 1）`baseCount`：分布式计数基数，volatile 修饰，具有并发可见性，提供 `BASECOUNT` 通过 CAS 进行原子性更新。
   - 2）`counterCells`：分布计数单元格数组，volatile 修饰，具有并发可见性，提供 `CELLVALUE` 通过 CAS 进行原子性更新。
   - 3）`cellsBusy`：分布式计数单元格繁忙标记，volatile 修饰，具有并发可见性，提供 `CELLSBUSY` 通过 CAS 进行原子性更新。

   => 在要获取 ConcurrentHashMap 实际大小 `sumCount()`  时，可以通过 `baseCount` + 累加 `counterCells` 数组所有格子的值，得到一个瞬时的累加和作为实际大小。

6. **并发分布式计数原理**：在 `putVal()` 和 `removeNode()` 方法更新玩后，还需要并发叠加 `x=1/-1`，叠加成功后，获取瞬时的累加和作为实际大小，用于扩容判断，其步骤为：

   - 1）先尝试在 `baseCount` 叠加 x，如果叠加成功，则继续做扩容判断。
   - 2）如果 `baseCount` 叠加 x 失败，则根据当前线程随机值 `h=ThreadLocalRandom.getProbe()`，尝试在 `CounterCell[h * (n-1)]` 叠加 x，如果叠加成功，则做扩容判断。
   - 3）如果 `CountCell` 叠加 x 失败，则调用 `fullAddCount()` 自旋 + CAS 竞争添加 x 到 `CounterCell[] as` 中。

7. **并发扩容原理**：

   - 1）如果 `sumCount()` 获取到的瞬时累加和，大于 `sizeCtl` ，说明需要扩容，则启动扩容机制，调用 `resizeStamp（n）` 生成扩容标记 `rs`，保证扩容过程不会被重复启动。
   - 2）如果 rs 大于等于 0，说明散列表还没被扩容，则 CAS 更新 `sizeCtrl=(rs << RESIZE_STAMP_SHIFT) + 2)`，更新成功后，rs 为一个负数，再第一次调用 `transfer(tab, null)`，开始扩容创建 2 倍旧表容量的 nextTab，并开始转移旧表结点到新表。
   - 3）如果 rs 小于 0，说明散列表正在被其他线程扩容，则 CAS 更新并发扩容线程数 `sizeCtrl=sc+1`，更新成功后调用 `transfer(tab, nt)`，加入扩容一起转移旧表结点到新表。
   - 4）其中，在某个桶结点被转移后，会在旧表中留下一个 `Forwardding` 节点，其他线程在调用`put()/remove()` 时，如果遇到这种节点，则会根据它持有的 `nextTab`，参与协助扩容。
   - 5）最后，`transfer()` 中会根据 `(sc - 2) == resizeStamp(n) << RESIZE_STAMP_SHIFT` 来判断，当前转移线程是否为最后一个扩容线程，如果是的话，则会提交新散列表，正式把 `nextTab` 设置为 `table`，然后设置 `sizeCtl` 为 0.75f * 旧表桶容量（作为下一次扩容的阈值判断条件）才返回，返回后还要自旋判断一次是否需要再次扩容，防止并发情况下桶容量又不够的情况。

8. **红黑树线程安全原理**：

   - 1）ConcurrentHashMap 的红黑树过程与 HashMap 类似，但不同的地方在于，由于并发更新时，红黑树可能会涉及到旋转，TreeBin 维护了一个读写锁，在调用 `TreeBin#putTreeVal()` 或者 `TreeBin#removeTreeNode()` 方法时，即使外层有获取可重入锁 synchronized，在操作红黑树之前，也要调用 `lockRoot()`，调用完成后再 `unlockRoot()`，保证写时走链式读，走树读时不能写。
   - 2）其原理如下：
     1. TreeBin 持有 lockState 属性，值有读、等待写、以及写状态（WRITER=1，WAITER=2，READER=4，读/写状态可与等待状态结合）。
     2. 调用 `lockRoot()` 时，CAS 竞争更新 lockState 为 `WRITER`，竞争成功，说明当前线程持有写锁成功，可以继续做红黑树操作，操作完后 `unlockRoot` 释放写锁，置 lockState 为 0。
     3. 竞争失败，则调用 `contendedLock()` 继续争抢写锁，争抢不到则会进入阻塞状态，直到所有调用`TreeBin#find(int，Object)` 的线程，都调用完毕后，才会被唤醒，然后重新争抢写锁。

##### 3、总

相对于 JDK 7#ConcurrentHashMap，JDK 8 主要优化了以下 3 点：

1. **数据结构**：取消 Segment[] + HashEntry[] + 链表的数据结构，改用 Node[] + 链表 + 红黑树的数据结构，提升查找效率。
2. **线程安全方式**：取消了 Segment#ReentrantLock 分段锁保证线程安全的方式，改用 Node + CAS 自旋锁+ synchronized 锁定桶结点 + TreeBin 读写锁的方式，来保证并发安全，进一步提高并发量。
3. **扩容方式**：取消了获取锁再做计数 + 扩容的方式，改用释放 synchronized 同步锁，利用 CAS 自旋锁 + 并发分布式计数 + 并发扩容的方式，支持更高的并发更新与查询。

=> 以上，就是我对 ConcurrentHashMap 底层原理 的一个理解，请问有什么细节需要补充的吗？

#### 3.2.1.7. 红黑树性能稳定吗？

经过大量实验证明，红黑树可以保证，在最好甚至最坏情况下，所有操作（插入/删除/查找等）的时间复杂度，都是对数级别 O（logN），无论插入顺序如何，红黑树都是接近完美平衡的，其操作成本（包括旋转和变色），比 BST 降低 40% 左右。

![1647876146340](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647876146340.png)

=> 因此，红黑树性能是稳定的，任何时间复杂度都为 O（logn）级别。

#### 3.2.1.8. 设计模式有了解哪些？线程池这种是什么设计模式？

##### 1、总

我了解过单例、工厂、建造者、观察者、装饰者、代理、责任链这几个常用的设计模式。

##### 2、分

1. **单例**：某个类只能生成一个实例，该实例被全局访问，比如 Spring 容器一级缓存里的单例池。

   - 实现方式：饿汉式、懒汉式、**双重检查锁、静态内部类、枚举类**等多种实现方式。

2. **工厂**：工厂中最简单实用的模式，简单工厂模式，可以理解为是不同工厂模式的一个特殊实现，比如 Spring 中的 BeanFactory、ApplicationContext。

   实现方式：

   - 简单工厂，是由一个工厂同一个方法来创建多个产品实例。
   - 工厂方法，则是对工厂做了抽象，做到一个产品一个工厂。
   - 抽象工厂，则是一个工厂提供多个创建接口，分别创建不同的产品实例。

3. **建造者**：可以将一个复杂对象的构造与他的表示分离，使同样的构建过程可以创建不同的表示，比如 SpringCloud 中的 FeignClientBuilder。

   - 建造者关注的是组装过程，类似于组装车间，工厂关注的是创建过程，类似于生产车间。

4. **观察者**：在被观察者本身的状态改变时，主动发出通知，呼叫各观察者所提供的方法走对应的实现，比如 Spring 中的各种 ApplicationContextEvent 和 ApplicationListener，Dubbo 中的 InvokerListener、ExporterListener。

5. **装饰者**：通过创建一个包装对象，也就是用包裹真实的对象，在不必改变原类文件和使用继承的情况下，动态地扩展一个对象的功能，比如 JDK#I/O 中的处理流和节点流 。

6. **代理**：为其他对象提供一种代理，在前后织入业务代理，以增强原始对象，比如 JDK 动态代理，Spring AOP 中的 CGLIB 动态代理，Dubbo 中的 Javassist 动态代理。

   - 静态代理：静态代理，是由程序员创建或者工具生成代理类的源码，再在编译生成代理类。所谓静态，也就是程序运行前就已经存在代理类的字节码文件，这时代理类和委托了类的关系在运行前就确定了。
   - 动态代理：动态代理在实现阶段不用关心代理类，而在运行阶段才指定哪一个对象。

7. **责任链**：通过持有下一个实现类的引用，进行链式调用，比如 Web 中的 Filter、Spring Cloud 中的 GatewayFilter、Dubbo 中的 ProtocolFilterWrapper。

8. **适配器**：定义一个包装类，用来包装不兼容接口的对象，把一个类的接口，转换成所期待的另一种接口，从而使原本接口不匹配、无法工作的两个类能够一起工作。

##### 3、总

1. 线程池这种属于享元模式。
2. 享元模式，Flyweight Pattern，会尝试重用现有的同类对象，如果未找到匹配的对象，则创建新对象，主要用于减少创建对象的数量，以减少内存占用和提高性能。
3. 比如还有数据库连接池、Http 连接池等。

=> 以上，就是我对 设计模式 的一些理解，请问有什么细节需要补充的吗？

#### 3.2.1.9. 动态代理实现原理，如果给你实现的话你会怎么做？

##### 1、总

对于我阅读过源码的动态代理有，JDK 动态代理和 CGLIB 动态代理。

##### 2、分

###### 1）JDK 动态代理

1. **条件**：JDK 动态代理，需要提供接口。
2. **使用方式**：
   - 1）实现 `InvocationHandler` 接口，并重写 `invoke()`，在反射调用前后，编写需要代理的业务逻辑。
   - 2）通过 `Proxy.newProxyInstance(..)` 方法，来获取动态代理对象，其**方法参数**有：
     - `ClassLoader loader`：动态代理类的类加载器，一般取原委托类的类加载器。
     - `Class<?>[] interfaces`：原委托类实现的接口 Class 对象数组。
     - `InvocationHandler`：动态代理处理程序，在调用动态代理类的增强方法时，会触发回调到它的 `invoker()` 方法。
3. **实现原理**：
   - 1）`newProxyInstance()` 通过反射，生成含有接口方法的 `$Proxy0` 代理类，`$Proxy0` 继承了 `Proxy` 类。
   - 2）在 `$Proxy0` 的构造方法中，会先调用父类的构造方法，让 `Proxy` 父类持有 `InvocationHandler` 实现类的引用。
   - 3）最后，该代理对象的所有方法，都会转发调用到，父类持有的 `InvocationHandler#invoke()` 方法。
   - 4）`InvocationHandler#invoke()`：该方法会传入原始方法的 Method 对象，用于反射调用原始方法。
   - 5）这样，只需要在反射调用前后，编写需要代理的业务逻辑，即可增强原始方法，实现动态代理。

```java
public interface MessageService { 
    void sendMessage(); 
}

public class MessageServiceImpl implements MessageService {
    public void sendMessage() {
        System.out.println("MessageServiceImpl.sendMessage"); 
    } 
}

public class JdkProxy<T> implements InvocationHandler {

    T target;

    public T getProxy(T target) {
        this.target = target;

        return (T) Proxy.newProxyInstance(target.getClass().getClassLoader(), target.getClass().getInterfaces(), this);
    }

    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
        System.out.println("JDK动态代理拦截开始！");
        Object result =  method.invoke(target, args);
        System.out.println("JDK动态代理拦截结束！");
        return result;
    }
}

public class Executer {
    public static void main(String[] args) {
        JdkProxy<MessageService> jdkProxy = new JdkProxy();
        MessageService messageService = jdkProxy.getProxy(new MessageServiceImpl());
        messageService.sendMessage();
    }
}
```

###### 2）CGLIB 动态代理

1. **条件**：CGLIB 动态代理，无需提供接口，即可实现，当然，也可以代理有接口的原委托类。
2. **使用方式**：
   - 1）实现一个 `MethodInterceptor`：代理方法调用时，会被转发到该类的 `intercept()` 方法。
   - 2）构建 `net.sf.cglib.proxy.Enhacner`：通过 `setSuperClass(Class)` 指定原委托类，`setCallback(Class)` 指定当前 `MethodInterceptor` 实现类 。
   - 3）最后，通过调用 `(T) enhancer.create()` 方法，获取代理对象。
3. **实现原理**：
   - 1）利用 ASM 开源包，通过修改原委托类 Class 文件的字节码，生成子类，覆盖原委托类的方法，并在覆盖方法中实现了增强。
   - 2）在调用代理类方法时，通过调用 `MethodInterceptor#invoke()` ，然后根据方法签名索引，去代理类的 FastClass 中，找到对应的方法，在该方法调用时，又会去调用父类（原委托类）的原始方法，从而实现动态代理。

```java
public class PlayService {
    public void play() {
        System.out.println("PlayService.play");
    }
}

public class CglibProxy<T> implements MethodInterceptor {

    T target;

    public T getProxy(T target) {
        this.target = target;
        Enhancer enhancer = new Enhancer();
        enhancer.setSuperclass(target.getClass());
        enhancer.setCallback(this);
        return (T) enhancer.create();
    }

    public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable {
        System.out.println("CGLIB动态代理拦截开始!");
        Object result = methodProxy.invokeSuper(o, objects);
        System.out.println("CGLIB动态代理拦截结束!");
        return result;
    }
}

public class Executer {
    public static void main(String[] args) {
        CglibProxy<PlayService> cglibProxy = new CglibProxy();
        
        // 测试代理无接口服务类
        PlayService playService = cglibProxy.getProxy(new PlayService());
        playService.play();
		
        // 测试代理有接口服务类
        CglibProxy<MessageService> cglibProxy1 = new CglibProxy(); 
        MessageService messageService = cglibProxy1.getProxy(new MessageServiceImpl());
        messageService.sendMessage();
    }
}
```

##### 3、总

因此，总的来说，

1. JDK 动态代理，是在调用代理类方法时，通过调用 `InvocationHandler#invoke()`，再反射调用原始方法，从而实现动态代理，属于反射调用，会存在一定的性能花销。
2. CGLIB 动态代理，是在调用代理类方法时，通过调用 `MethodInterceptor#invoke()` ，然后根据方法签名索引，去代理类的 FastClass 中，找到对应的方法，在该方法调用时，又会去调用父类（原委托类）的原始方法，从而实现动态代理，属于父类方法调用，在初次调用时，由于需要生成 FastClass，效率会比较低，但在 FastClass 生成后，往后访问由于不用涉及反射调用，此时性能花销非常小。

=> 以上，就是我对 动态代理 的一些理解，CGLIB 动态代理类生成的具体细节也不太记得了~

#### 3.2.2.0. 线上 CPU 100% 如何解决？

见《[2.1.1.1. 项目自我介绍？](#2.1.1.1. 项目自我介绍？)》- 3、项目亮点 - 12）发版平台 CPU 占用过高 | Tomcat Manager、ELK、CPU 95%。

#### 3.2.2.1. Redis 哨兵模式和集群模式的区别？

##### 1、总

Redis 支持三种集群架构，分别是主从模式、哨兵模式、集群模式。

##### 2、分

###### 1）主从模式 | 无高可用、简单

将哨兵模式之前，就需要先介绍一下主从模式~

![1632299571516](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1632299571516.png)

1. **是什么**：

   - 1）只是简单地使用了主从复制，一个 Master 可以有多个 Slave，Slave 也可以有自己的 Slave。
   - 2）默认使用异步复制， Slave 会以每秒 1 次的频率，向 Master 报告当前复制流的处理进度。
   - 3）复制时，不会阻塞 Master，即使是正在进行初次同步， Master 也可以继续处理命令请求，也不会阻塞 Slave，即使 Slave 正在进行初次同步， 也可以使用旧版本的数据集来处理命令查询。
   - 4）同时，还支持读写分离模式，Master 负责读写，Slave 只负责读。

2. **怎么配置**：

   ```shell
   # Slave方参数
   # 1、配置主从复制Master的IP+端口
   slaveof <masterip> <masterport>
   # 2、如果Master通过requirepass配置密码，则Slave也需要进行相应的配置
   masterauth <master-password>
   # 3、默认允许，Slave初次同步未完成时，继续使用旧数据来响应客户端，配置no会阻塞初次同步期间的所有请求
   slave-serve-stale-data yes
   # 4、默认开启读写分离，Slave只能读取数据，不能写入数据
   slave-read-only yes
   
   # Master方参数
   # 5、默认关闭无磁盘化复制，Master磁盘不生成RDB文件，直接通过网络同步给Slave
   repl-diskless-sync no
   # 6、默认为3和10，如果至少有3个从服务器，并且这3服务器的延迟值都少于10秒，Master才会执行客户端请求的写操作
   # min-slaves-to-write 3
   # min-slaves-max-lag 10
   ```

3. **实现原理**：

   - 1）当建立一个 Slave 时， Slave 会向 Master 发送一个 `PSYNC master_run_id offset` 命令。
   - 2）如果 Slave 是首次连接，由于 Master 中不存在该 Slave 的复制偏移量，所以会触发一次完整重同步操作， 此时，Master 开始执行 `BGSAVE`， 并在保存操作执行期间， 将所有新执行的写入命令都保存到一个缓冲区里面。
   - 3）当 Master  `BGSAVE` 执行完毕后， Master 将执行保存操作所得的 .rdb 文件发送给 Slave， Slave 接收这个 .rdb 文件， 并将文件中的数据载入到内存中。
   - 4）之后，Master 会以 Redis 命令协议的格式， 将写命令缓冲区中积累的所有内容都发送给 Slave，Slave 则实时同步这些数据。
   - 5）如果主从复制期间 Slave 断开连接，在自动重连后，会使用 `PSYNC master_run_id offset` 命令来进行同步，Master 会以增量复制的形式，向 Slave 发送断线时缺失的那部分数据， 然后复制工作可以继续执行。

   重连后的**部分重同步原理**：

   - 1）Master 会为被发送的复制流创建一个缓冲区 `in-memory backlog`， 并且 Master 和所有 Slave 之间都记录一个复制偏移量 `replication offset` 和一个主服务器 ID `master run id`。
   - 2）当出现网络连接断开时， Slave 重新连接后，会向 Master 请求继续执行原来的复制进程。
   - 3）如果 Slave 记录的主服务器 ID `master run id` ，和当前要连接的主服务器 ID `master run id` 相同， 并且 Slave 记录的偏移量 `replication offset` 所指定的数据，仍然保存在 Master 的复制流缓冲区 `in-memory backlog` 里面， 那么 Master 会向 Slave 发送断线时缺失的那部分数据， 然后复制工作可以继续执行。
   - 4）否则，Slave 就要执行完整重同步操作。

4. **优点**：

   - 1）**部署简单**：仅使用两个节点，即可构成主从模式。
   - 2）**可以通过读写分离**：来避免读和写同时不可用的情况。

5. **缺点**：

   - 1）**无高可用保证**：一旦 Master 节点出现故障，主从节点就无法自动切换，直接导致 SLA 服务等级下降。
     - **解决方案**：添加哨兵监控。
   - 2）**Master 压力大**：所有的 Slave 节点数据的复制和同步，都由 Master 节点来处理，会造成 Master 节点压力过大。
     - **解决方案**：可以使用主从从的结构，通过引入从从同步，可以减少主从同步的次数。

6. **适用场景**：一般用于业务发展初期、并发量低、运维成本低的情况。

###### 2）哨兵模式 | 高可用、读多

![1632320226337](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1632320226337.png)

1. **是什么**：Sentinel，哨兵，是 Redis 高可用的一种解决方案，可以监视一个或者多个 Redis Master 服务，以及其所有的从服务，当某个 Master 服务宕机后，会自动把这个 Master 下的某个从服务升级为 Master，从而代替已宕机的 Master，保证继续工作。

   Sentinel 的作用有：

   - 1）**监控**：Monitoring，Sentinel 会不断地检查 Master 和 Slave 是否运作正常。
   - 2）**提醒**：Notification，当被监控的某个 Redis 服务器出现问题时， Sentinel 可以通过 API 向管理员或者其他应用程序发送通知。
   - 3）**自动故障迁移**：Automatic failover。当一个 Master 不能正常工作时， Sentinel 会开始一次自动故障迁移操作， 它会将失效 Master 的其中一个 Slave 升级为新的 Master， 并且让失效 Master 的其他 Slave 改为复制新的 Master。当客户端试图连接失效的 Master 时， 集群也会向客户端返回新 Master 的地址， 使得集群可以使用新 Master 代替失效 Master。

2. **怎么配置**：

   ```shell
   # 1、配置监控 127.0.0.1：6379 的 mymaster 服务器，并且客观下线需要取得2个(quorum)哨兵的同意，但是无论该值配置了多少，都需要多数哨兵的选举后，才能发起一次自动故障转移
   sentinel monitor mymaster 127.0.0.1 6379 2
   # 2、配置Master服务器密码（如果Master有配置的话）
   sentinel auth-pass <master-name> <password>
   # 3、配置判定mymaster主观下线的毫秒数
   sentinel down-after-milliseconds mymaster 60000
   # 4、配置主从切换的超时时间，需要自动故障转移时，如果当前哨兵没有去执行，那么在超过这个时间后，会由其他的哨兵来进行处理
   sentinel failover-timeout mymaster 180000
   # 5、配置在执行故障转移时，同步新mymaster的最大Slave并行数，如果全部Slave一起对新Master进行同步，由于在Slave载入RDB时会阻塞客户端请求，所以可能会造成所有Slave在短时间内全部不可用的情况出现
   sentinel parallel-syncs mymaster 1
   
   # 启动命令
   # 6、运行纯Sentinel服务器
   redis-sentinel /path/to/sentinel.conf
   # 7、在Redis Server上运行哨兵
   redis-server /path/to/sentinel.conf --sentinel
   
   # 运维命令
   # 8、查看imooc-master下的master节点信息
   sentinel master imooc-master
   # 9、查看imooc-master下的slaves节点信息
   sentinel slaves imooc-master
   # 10、查看imooc-master下的哨兵节点信息
   sentinel sentinels imooc-master
   ```

3. **Sentinel 自动发现原理**：

   - 1）一个 Sentinel 可以与其他多个 Sentinel 进行连接， 各个 Sentinel 之间可以互相检查对方的可用性， 并进行信息交换，通过发布与订阅功能，向频道 `__sentinel__:hello` 发送信息，来自动发现正在监视相同 Master 的其他 Sentinel ，无须为运行的每个 Sentinel 分别设置其他 Sentinel 的地址。
   - 2）**发布**：每个 Sentinel 会以 2 次 / s 的频率， 通过发布与订阅功能， 向被它监视的所有 Master 和 Slave 的 `__sentinel__:hello` 频道发送一条信息， 信息中包含了 Sentinel 的 IP、端口和运行 ID `runid`，还包括完整的 Master 当前配置。如果一个 Sentinel 包含的 Master 配置，比另一个 Sentinel 发送的配置要旧， 那么这个 Sentinel 会立即升级到新配置上。
   - 3）**订阅**：每个 Sentinel 都订阅了被它监视的所有 Master 和 Slave 的 `__sentinel__:hello` 频道， 查找之前未出现过的 Sentinel，当一个 Sentinel 发现一个新的 Sentinel 时， 它会将新的 Sentinel 添加到一个列表中， 这个列表保存了已知的、正在监视同一个 Master 的所有其他 Sentinel 。

4. **故障判定原理**：

   - 1）Sentinel 会以 1 次 / s的频率，向它所知的 Master、Slave、以及其他 Sentinel 实例，发送一个 `PING` 命令。
   - 2）如果某个实例距离最后一次有效回复 `PING` 命令的时间，超过了 `down-after-milliseconds` 的值， 那么这个实例会被 Sentinel 标记为**主观下线**。 
   - 3）如果一个 Master 被标记为主观下线， 那么正在监视这个 Master 的所有 Sentinel，都要以 1 次 / s 的频率，确认Master 是否真的进入了主观下线状态。
   - 4）当 Master 重新向 Sentinel 的 `PING` 命令，返回有效回复时，Master 的主观下线状态就会被移除。
   - 5）否则，如果有足够数量（>= `quorum` ）的 Sentinel，在指定的时间范围内，同意这一判断， 那么这个主服务器被标记为**客观下线**。
   - 6）而当没有足够数量（>= `quorum` ）的 Sentinel 同意主服务器已经下线， Master 的客观下线状态就会被移除。

   **主观下线**：

   - 1）主观下线，Subjectively Down， 简称 SDOWN，指的是单个 Sentinel 实例，对超时服务实例做出的判断。
   - 2）只有一个 Sentinel 认为是主观下线时，并不一定会引起服务实例的故障迁移，只有在足够数量（>= `quorum` ）的 Sentinel 都将一个服务实例标记为主观下线之后， 服务器才会被标记为**客观下线**， 此时自动故障迁移才会执行。

   **客观下线**：

   - 1）客观下线，Objectively Down， 简称 ODOWN，指的是多个 Sentinel 实例在对同一个服务实例做出 SDOWN 判断， 并且在经过通过  `SENTINEL is-master-down-by-addr` 命令互相交流之后， 得出的服务实例下线判断。
   - 2）客观下线条件只适用于 Master，对于任何其他类型的 Redis 实例， Sentinel 在将它们判断为下线前不需要进行协商， 所以  Slave 或者其他 Sentinel 永远不会达到客观下线的条件。
   - 3）只要 Sentinel 发现某个 Master 进入了客观下线状态，某个 Sentinel 会被其他 Sentinel 推选出， 并对失效的 Master 执行**自动故障迁移**操作。

5. **自动故障转移原理**：

   - 1）**Leader Sentinel 选举**：当一个 Master 被判定为**客观下线**后，监视这个 Master 的所有Sentinel会通过 `Raft` 选举算法，选出一个 Leader Sentinel 去执行故障转移 `failover` 操作。
   - 2）**Master 重新选择**：当选举出 Leader Sentinel 后，Leader Sentinel 会根据以下规则，在失效 Master 属下的 Slave 当中，选择出新的 Master：
     1. 先淘汰主观下线 | 已断线 | 最后一次回复 `PING` 命令时间大于5秒钟 | 与失效 Master 断开连接时长超过 10 倍主观判断时长 `down-after-milliseconds` 的 Slave 节点。
     2. 选择配置 `slave-priority` 最高的 Slave 节点，如果没有，则继续选择。
     3. 选择复制偏移量 `replication offset` 最大的 Slave 节点，复制偏移量越大，说明数据复制的越完整。
     4. 如果复制偏移量不可用，或者 Slave 的复制偏移量相同， 则选择运行 ID `run_id` 最小的 Slave 节点，`run_id` 越小，说明重启次数越少。
   - 3）**故障转移流程**：
     1. Leader Sentinel  则根据 Master 选择规则，选出一个 Slave，向其发送 `SLAVEOF NO ONE` 命令，让它转变为 Master。
     2. 然后， Leader Sentinel 通过发布与订阅功能， 将更新后的配置传播给所有其他 Sentinel，让其他 Sentinel 对它们自己的配置进行更新。
     3. 接着， Leader Sentinel 会向已下线 Master 的其他 Slave，发送 `SLAVEOF host port` 命令， 让它们去复制新的 Master。
     4. 最后，当所有 Slave 都已经开始复制新的 Master 时， Leader Sentinel 则结束这次故障迁移操作。

6. **优点**：监控、提醒、自动故障转移，从而实现 Redis 的高可用。

7. **缺点**：如果写请求较多，当集群 Slave 节点数量多了后，Master 节点同步数据的压力会非常大。

8. **适用场景**：适合读远多于写的业务场景，比如在秒杀系统中，用来缓存活动信息。

###### 3）集群模式 | 高可用、写多

![1632464692288](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1632464692288.png)

1. **是什么**：

   - 1）Redis Cluster，是 Redis 3.0 版本之后推出的高可用实现。
   - 2）采用哈希虚拟槽的数据分区方案，把 key 分布到各个 Master 节点上，每个 Master 还可以跟若干个 Slave 做主从切换。
   - 3）由于做了数据分区，使用的功能，也就只是普通单机 Redis 所有功能的一个子集，比如不支持同时使用多个键的 Redis 命令、不支持多数据库只能适用 0 号数据库。
   - 4）路由方面，客户端可以连接任意 Master 节点，集群内部会按照不同的 key，告诉客户端，把请求转发到其他 Master 节点，同时让请求成功的客户端，还会缓存对应的映射关系，以提高下次集群访问的查询效率。

2. **怎么配置**：

   ```shell
   # 1、开启集群模式
   cluster-enabled yes
   # 2、每一个节点都需要有这么一个配置文件，3主3从则一共需要6份，用于存储集群模式下的集群状态等信息，由Redis自己维护，而这些信息会相互告知其他所有节点。如果要重新创建集群，只需要把这个文件删除掉就行。
   cluster-config-file nodes-201.conf
   # 3、节点超时时限，如果发生超时则会被认定为PFAIL，如果超过半数其他Master认定为PFAIL，则会被集群认定为FIAL，然后会进行主从切换
   cluster-node-timeout 5000
   # 4、开启AOF
   appendonly yes
   
   # 5、Redis3.x旧版集群构建方式，需要使用redis-trib.rb来构建集群，最新版使用C语言来构建了，这个要注意
   # ./redis-trib.rb create --replicas 1 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005
   
   # 新版Redis集群构建方式
   # 6、创建集群，主节点和从节点比例为1，1-3为主，4-6为从，1和4，2和5，3和6分别对应为主从关系，这也是最经典用的最多的集群模式
   redis-cli --cluster create ip1:port1 ip2:port2 ip3:port3 ip4:port4 ip5:port5 ip6:port6 --cluster-replicas 1
   # 7、集群客户端
   redis-cli -c -p 7000
   # 8、检查集群信息
   redis-cli --cluster check 127.0.0.1:6379
   ```

3. **分区实现原理**：Redis 集群的键空间被分割为 `16384` 个槽（slot）， 集群的最大节点数量也是 `16384` 个，但推荐的最大节点数量为 1000 个左右，每个主节点都负责处理 `16384` 个哈希槽的其中一部分，其键的映射算法为 `HASH_SLOT = CRC16(key) mod 16384`。

   为什么哈希槽数为 16384 个？

   `CRC16` 算法产生的 hash 值有 16 bit，即可产生 2 ^ 16 = 65536 个值，换句话说，值是分布在 0 ~ 65535 之间，那 Redis 在做 `mod` 运算的时候，为什么不 `mod` 65536，而是选择 `mod` 16384 呢？

   - 1）槽位数不宜过大：如果槽位为 65536，那么节点发送 `PING/PONG` 心跳包消息头所占用的空间会达到 8k （65536 / 8），过于庞大，传输时浪费带宽，而如果使用 16384 个槽位，心跳包消息头所占用的空间仅为 2k（16384 / 8），则大小还能接受。
   - 2）槽位数不宜过小：Redis Master 的哈希槽配置，是通过一张 bitmap 的形式来保存的，其填充率为 slots / N，N为节点数目，在 N 固定的情况下，如果槽位数越小，bitmap 填充率就越小，导致 bitmap 在传输过程中的压缩率就越高，就越消耗 CPU 资源。
   - 3）因此，16384 个插槽，是综合了心跳包大小、网络带宽、压缩率等方面考虑的结果，同时还能满足各种业务需求。

4. **路由转向原理**：

   - 1）一个 Redis 客户端可以向集群中的任意节点（包括从节点）发送 `GET key` 命令请求。

   - 2）集群节点会对命令请求进行分析， 如果该命令是集群可以执行的命令， 那么节点会查找这个命令所要处理的键所在的哈希槽。

   - 3）如果要查找的哈希槽正好就处于当前节点中，则接收到的命令由当前节点负责处理。

   - 4）如果所查找的哈希槽不处于当前节点中，则当前节点会先查看自身内部所保存的，哈希槽到节点 ID 的映射记录，然后向客户端回复一个 `-MOVED` 转向错误。

     ```shell
     GET x
     # GET命令收到一个 -MOVED 转向错误
     # x真正所在的目标哈希槽，目标节点IP，目标节点端口号
     -MOVED 3999 127.0.0.1:6381
     ```

   - 5）客户端收到 `-MOVED` 转向错误后，根据目标节点 IP 与端口号，会再向目标节点重新发送一次 `GET key`命令请求。

   - 6）如果客户端在重新发送 `GET key` 命令时，集群刚好又更改了 key 的 slot 配置， 此时客户端请求目标哈希槽，会再次收到 `-MOVED` 转向错误， 需要再次向新的目标节点重新发送一次 `GET key`命令请求。

   - 7）客户端会循环以上操作，直到该命令请求成功，然后记录下该成功请求的哈希槽的目标节点信息，在下次执行相同 key 命令时，加快正确节点的寻找速度。

5. **高可用原理**：

   - 1）**节点失效检测**：当一个节点 `PING` 不通另一个节点时，则会把它标记为 `PFAIL`；当一个节点要把另一个节点从 `PFAIL` 标记为 `FAIL` 时， 则必须得到大部分 Master 的同意才行。
   - 2）**从节点选举**：一旦某个 Master 进入 `FAIL` 状态， 如果这个 Master 有一个或多个 Slave 存在， 那么其中一个 Slave 会被升级为新的 Master， 而其他 Slave 则会开始对这个新的 Master 进行复制。

6. **优点**：

   - 1）高可用：当集群中的一部分节点失效或者无法进行通讯时， 集群仍然可以通过主从切换，继续处理命令请求。
   - 2）高性能：操作某个 key 时，Redis 不会先找到节点再处理，而是直接让客户端重定向，到目标 Redis 实例进行请求，这相较代理分片少了 proxy 的连接损耗。
   - 3）高拓展：不存在中心节点或者代理节点，同时最大支持线性拓展 1000 个节点，把新节点加入集群后，可以通过命令平均分配已有节点的哈希槽。

7. **缺点**：

   - 只能使用普通单机 Redis 所有功能的一个子集：不支持同时使用多个键的 Redis 命令，不支持多数据库功能， 只能使用默认的 0 号数据库。
   - 占用带宽：虽然避免了 Master 单节点的问题，但集群内的数据同步、节点通信会占用一定的带宽。
   - 数据弱一致性：与其他高可用方案一样，Redis 集群属于 AP 模型，不保证数据的强一致性，在特定条件下， Redis 集群可能会丢失已经被执行过的写命令。

8. **适用场景**：在写操作比较多的情况下，集群模式才更有优势，相对于其他大多数情况，使用哨兵模式就能满足需求了。

##### 3、总

Redis AKF 拆分：

![1632542544732](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1632542544732.png)

###### 1）X 轴扩展 | 主从复制

- **特点**：
  1. 按照主从设计，Master 负责读写， Slave 负责读。
  2. 再结合哨兵集群，在 Master 故障时，使用 Slave 进行切换，从而实现高可用。
- **优点**：
  1. 主从，解决了读并发压力大的问题。
  2. 哨兵，解决了单点故障问题。
- **缺点**：单机容量会有限制，并且会出现写并发压力大的问题。

###### 2）Y 轴扩展 | 业务拆分

- **特点**：
  1. 把 Redis 所有键，按照业务进行拆分，拆分到不同的 Redis 实例上。
  2. 可以在 Y 轴的基础上，再进行 X 轴的主从复制的扩展，形成不同业务的 Redis 集群。
- **优点**：从分离不同业务数据的角度，暂时解决了单机容量受限，以及写并发压力大的问题。
- **缺点**：当某个业务的集群达到一定规模后，如果数据量过大，仍然会出现单机容量受限，以及写并发压力大的问题。

###### 3）Z 轴扩展 | 数据分区

- **特点**：
  1. 将全部 Key 数据根据分区规则分成多个子集，并存储到 Redis 实例中。
  2. 可以在 Z 轴的基础上，叠加 X 轴的主从复制，集群内进行数据分片，比如 Redis Cluster。
  3. 可以再叠加 Y 轴的业务拆分，把整个 Redis 系统划分成多个不同业务的、数据分片过的 Redis Cluster。
- **优点**：增加了整个集群的算力、带宽和内存，从根本上解决了单机容量受限，以及写并发压力大的问题。
- **缺点**：
  1. 数据分区后，不支持跨实例的命令与事务。
  2. 备份管理要复杂得多。
  3. 扩缩容时可能需要对数据再平衡。

综上，Redis 哨兵模式与 Redis Cluster 的主要区别为：

1. 原理上，Redis 哨兵模式是通过主从复制 + 故障时 Sentinel 做主从切换实现的，而 Redis Cluster 则是通过哈希虚拟槽分区 + 重定向客户端请求 + 主从复制 + 故障时 从节点选举 做主从切换实现的。
2. 使用上，由于 Redis Cluster 对数据做了分区，所以只能使用普通单机 Redis 所有功能的一个子集，比如不能使用多键命令，只能使用 0 号数据库，而 Redis 哨兵模式本质上是主从模式，依然可以使用所有命令。

=> 以上，就是我对 Redis 高可用架构 的一些理解，请问有什么细节需要补充的吗？

#### 3.2.2.2. 算法题 | 100 个数实现随机取两次，不能拿重复？

最优解见方案二~

```java
/**
 * 随机从连续的100个不重复数中, 取出100个不重复的随机数
 */
public class Random100 {

    public static void main(String[] args) {
        Random100 random100 = new Random100();

        // 验证方案一: 全Right!
        System.err.println("方案一开始~");
        for(int i = 0; i < 50000; i++) {
            random100.fun1();
        }

        // 验证方案二: 全Right!
        System.err.println("方案二开始~");
        for (int i = 0; i < 50000; i++) {
            random100.fun2();
        }
    }

    /**
     * 方案一思路：100个顺序int数组, 随机取出, 碰到相同的则重新取, 直到取完
     * <p>
     * 时间复杂度：最坏情况下, 第一次要取1次, 第二次要取2次,...,第100次要去100次, 时间复杂度为O(n^2)
     * 额外空间复杂度: 一开始的100长度数组, 以及返回的100长度数组, 都是是题目需要的, 所以额外空间复杂读为O(1)
     */
    private void fun1() {
        int len = 100;

        int[] nums = getInitNums(len);
        List<Integer> res = new ArrayList<>();

        int i = 0, index;
        Random random = new Random();
        while (i < len) {
            index = random.nextInt(len);
            if (nums[index] != -1) {
                res.add(nums[index]);
                nums[index] = -1;
                i++;
            }
        }

        printIfError(res);
    }

    /**
     * 方案二思路：100个顺序int数组, 随机取出, 取出后替换到末尾位置, 直到取完
     * <p>
     * 时间复杂度：由于每次只需要取1次, 共取100次, 所以时间复杂度为O(n)
     * 额外空间复杂度: 一开始的100长度数组, 以及返回的100长度数组, 都是是题目需要的, 所以额外空间复杂读为O(1)
     */
    private void fun2() {
        int len = 100;

        int[] nums = getInitNums(len);
        List<Integer> res = new ArrayList<>();

        int i = 0, index, tmp;
        Random random = new Random();
        while (i < len) {
            index = random.nextInt(len);
            res.add(nums[index]);

            // 交换index到末尾
            tmp = nums[index];
            nums[index] = nums[len - 1];
            nums[len - 1] = tmp;

            // 数组末尾前移
            len--;

            // 继续下一轮随机取
            i++;
        }

        printIfError(res);
    }
    
    /**
     * 获取连续len个不重复数数组
     *
     * @param len
     * @return
     */
    private int[] getInitNums(int len) {
        int[] nums = new int[len];
        for (int i = 0; i < len; i++) {
            nums[i] = i;
        }
        return nums;
    }

    /**
     * 判断是否有重复, 有则打印Error!
     *
     * @param res
     */
    private void printIfError(List<Integer> res) {
        Set<Integer> set = new HashSet<>(res);
        if (res.size() != set.size()) {
            System.err.println("Error!");
        }
    }
}
```

#### 4.1.1.1. 项目介绍，分表规则、服务器配置、JVM 配置？

见《[2.1.1.1. 项目自我介绍？](#2.1.1.1. 项目自我介绍？)》。

#### 4.1.1.2. 线程池参数？

见《[1.2.1.3. 线程池核心参数，以及选取原则？](#1.2.1.3. 线程池核心参数，以及选取原则？)》。

#### 4.1.1.3. Spring MVC 的处理过程？

见《[1.1.1.6. Spring MVC 拦截器和过滤器的区别？](#1.1.1.6. Spring MVC 拦截器和过滤器的区别？)》。

#### 4.1.1.4. Spring MVC 拦截器和过滤器的区别？

见《[1.1.1.6. Spring MVC 拦截器和过滤器的区别？](#1.1.1.6. Spring MVC 拦截器和过滤器的区别？)》。

#### 4.1.1.5. Spring AOP 的原理？

##### 1、总

AOP，Aspect-Oriented Programming，⾯向切⾯编程，为解耦而生，能够将那些与业务⽆关，却为业务模块所共同调⽤的逻辑，或者责任封装起来（比如事务管理、⽇志管理、权限控制等），以便于减少系统的重复代码，降低模块间的耦合度，有利于未来的可拓展性和可维护性。

##### 2、分

1. AOP 有 **7 个核心概念**：

   - 1）**切面**：Aspect，指对哪些方法进行拦截处理的横切关注点，可能会横切多个对象，比如 Spring 的事务管理， 在 Spring AOP 中，切面可以在普通类中以 `@Aspect` 注解来实现。
   - 2）**连接点**：Join point，指在程序执行过程中某个特定的点，比如某个方法调用的时间点，或者处理异常的时间点，在 Spring AOP 中，一个连接点代表一个方法的执行。
   - 3）**通知**：Advice，在切面的某个特定的连接点上*执行的动作，通知有多种类型，包括 `around`， `before`， 和 `after` 等等。
   - 4）**切点**：Pointcut，匹配连接点的断言，通知会在满足这个切点的连接点上运行，比如 AOP 去执行某个特定名称的方法。
   - 5）**目标对象**：Target object，被一个或者多个切面所通知的对象，也被称作被通知的业务对象。
   - 6）**织入**：Weaving，把切面连接到目标对象上，并创建一个代理对象的过程。
   - 7）**AOP 代理**：AOP proxy，AOP 框架创建的对象，用来实现切面契约，包括通知方法执行等功能，在Spring 中，AOP代理可以是 JDK 动态代理，或者是 CGLIB 动态代理。

   => 比如日志管理的公共代码，可以抽象出一个**切面**，然后注入到**目标对象**中（具体的业务对象），通过**动态代理**，将对目标对象进行代理，在进行调用时，代理对象会根据**通知**类型，在对应的时间点，执行切面中增强的方法，从而实现日志统一管理，避免了代码的冗余。

2. **实现原理**：

   - 1）Spring AOP 是基于动态代理实现的，是 IoC 的一个扩展功能，是在 IoC 整个流程中新增的一个 BeanPostProcessor（`AbstractAutoProxyCreator`）扩展点而已。
   - 2）`AbstractAutoProxyCreator` 实现了 `postProcessAfterInitialization(bean,beanName)` 方法，底层调用动态代理过程。
   - 3）如果要代理的对象实现了某个接⼝，那么 Spring AOP 会使⽤ JDK Proxy，去创建代理对象。
   - 4）⽽对于没有实现接⼝的对象，就⽆法使⽤ JDK Proxy 去进⾏代理了，此时 Spring AOP 则会使⽤基于 asm框架字节流的 Cglib 动态代理 ，⽣成⼀个被代理对象的⼦类来作为代理。

##### 3、总

=> 以上，就是我对 Spring AOP 的一些理解，请问有什么细节需要补充的吗？

#### 4.1.1.6. Spring @Transactional 原理？

##### 1、总

`@Transactional`，是 Spring 中事务传播配置的注解，可通过 `@EnableTransactionManagement` 启用事务传播特性，其中，Spring 事务传播属性分为以下几种：

| 属性                     | 含义                                                         |
| ------------------------ | ------------------------------------------------------------ |
| REQUIRED（默认属性）     | 如果存在一个事务，则支持**当前事务**，如果没有事务，则开启一个**新的事务** |
| MANDATORY                | 支持**当前事务**，如果当前没有事务，就**抛出异常**           |
| NEVER                    | 以**非事务**方式执行，如果当前存在事务，则**抛出异常**       |
| NOT_SUPPORTED            | 以**非事务**方式执行操作，如果当前存在事务，就把当前事务**挂起** |
| REQUIRES_NEW（相互独立） | **新建事务**，如果当前存在事务，把当前**事务挂起**，内外两个事务相互独立，互不影响，当外层事务失败时，并不会回滚内层事务所做的动作，而内层事务操作失败时，也不会引起外层事务的回滚 |
| SUPPORTS                 | 支持**当前事务**，如果当前没有事务，就以**非事务**方式执行   |
| NESTED（局部回滚）       | 支持**当前事务**，新增 Savepoint 点，与当前事务**同步提交或回滚**。嵌套事务一个非常重要的概念，就是内层事务依赖于外层事务，当外层事务失败时，会回滚内层事务所做的动作，而内层事务操作失败时，并不会引起外层事务的回滚 |

##### 2、分

其**实现原理**为：

1. Spring 的事务是由 AOP 来实现的，首先按照 AOP 的整套流程来执行具体的操作逻辑，使用 `InfrastructureAdvisorAutoProxyCreator` （AbstractAutoProxyCreator 的子类）来生成具体的代理对象。
2. 然后通过一个 `TransactionInterceptor` 来实现，在调用 `JdkDynamicAopProxy#invoke(proxy, method, args)` 方法时，则代理到 `TransactionInterceptor`  实现的具体逻辑中。
3. 在调用代理方法时，会先做准备工作，解析各个方法上事务相关的属性，根据具体的属性来判断是否开始新事务。
4. 当需要开启事务时，则获取数据库连接，关闭自动提交功能，开启事务。
5. 然后执行原始业务逻辑。
6. 如果在执行过程中发生异常，那么会通过 `completeTransactionAfterThrowing` 来完成事务的回滚操作，回滚的具体逻辑是通过 `doRollBack` 方法来实现的，实现的时候也是要先获取链接对象，再通过连接对象来回滚。
7. 而如果执行过程中，没有任何意外情况的发生，那么通过 `commitTransactionAfterReturning` 来完成事务的提交操作，提交的具体逻辑是通过 `doCommit` 方法来实现的，实现的时候也要获取链接，通过链接对象来提交。
8. 当事务执行完毕之后，需要通过 `cleanupTransactionInfo` 来清除相关的事务信息。 

##### 3、总

因此，在平常使用过程中，应该留心以下**注意事项**：

1. 事务函数中不要处理耗时任务，会导致长期占有数据库连接。
2. 事务函数中不要处理无关业务，防止产生异常导致事务回滚。
3. 一些 Spring 事务传播**失效场景**有：
   - 1）Bean 对象没有被 Spring 容器所管理。
   - 2）调用方法的访问修饰符不是 public。
   - 3）数据源没有配置事务管理器。
   - 4）数据库不支持事务。
   - 5）异常被捕获，所以没有回滚。

=> 以上，就是我对 Spring @Transactional 的一些理解，请问有什么细节需要补充的吗？

#### 4.1.1.7. JDK 8 的内存分布情况，年轻代和老年代默认的比值，以及 Eden 区的比值，以及为什么？

##### 1、总

![1647941457503](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647941457503.png)

JVM 在执行 Java 程序的过程中，会把它所管理的内存区域，划分为同 JVM 生命周期的线程共享区域，有方法区和堆区，以及生命周期随着线程启动和结束，而建立和销毁的非线程共享区域，有程序计数器、虚拟机栈和本地方法栈。

##### 2、分

###### 1）程序计数器

1. Program Counter Register，非线程共享，是 JVM 当前线程所执行的字节码的行号指示器，字节码解析器的工作是通过改变这个计数器的值，来选取下一条需要执行的字节码指令，从而实现分支、循环、跳转、异常处理、线程恢复等基础功能。
2. 出现的原因是：
   - 1）由于 JVM 多线程是通过线程轮流切换，并分配处理器执行时间的方式来实现的，线程是最小的执行单位，没有“记忆”功能，只负责去干。
   - 2）为了在线程切换后，能恢复到正确的执行位置，需要记住原线程下一条要执行的指令的位置，此时，每条线程就需要有一个独立存储、互不影响、内存不共享的程序计数器。

###### 2）虚拟机栈

![1647941408114](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647941408114.png)

1. 每个线程都会有自己的虚拟机栈，其生命周期与线程生命周期一致，其内存空间只包含基础数据类型的局部变量、以及对象引用。

2. 虚拟机栈的单位是栈帧，在每个方法执行时，都会创建一个栈帧压入虚拟机栈中，在方法执行完毕或者发生异常，对应的栈帧则会从虚拟机栈中出栈。

3. 其中，每个栈帧都存放着局部变量表、操作数栈、动态链接和方法返回地址，还有一些附加信息。

   - 1）**局部变量表**：

     1. 是一组变量值的存储空间，用于存放方法参数，和方法内部定义的局部变量。
     2. 存放了编译期可知的各种基本数据类型 `boolean、byte、char、short、int、float、long、double`（对包装类型则在栈中保存其引用地址，在堆中保存值）、以及对象引用。
     3. 其内存空间，在编译期间完成分配，所以，在方法在运行之前，其内存空间是固定的，运行期间也不会发生改变。

   - 2）**操作数栈**：

     1. 用于保存计算过程中的中间结果，同时作为计算过程中变量临时的存储空间。
     2. 操作数栈在方法执行过程中，根据字节码指令，往操作数栈中写入数据或提取数据，即入栈和出栈操作。

     ```java
     public class Test {
         public void add() {
             int a = 15;
             int b = 1;
             int c = a + b;
         }
     }
     ```

     => 以上过程，其操作数栈与局部变量表的**交互顺序**为：

     1. 15 入栈：操作数栈写入数据。
     2. 15 出栈：操作数栈提取数据到局部变量表。
     3. 1 入栈：操作数栈写入数据。
     4. 1 出栈：操作数栈提取数据到局部变量表。
     5. 15 入栈：加载局部变量表变量 15。
     6. 1入栈：加载局部变量表变量 1。
     7. `iadd`：执行相加 15 + 1 指令。
     8. 16 出栈：操作数栈提取结果到局部变量表。
     9. `return`：如果返回值为void，则当前栈帧出栈即可；如果带有返回值，则局部变量表中的结果16，还会入栈操作数栈中。

   - 3）**动态链接**：

     1. 每个栈帧都包含一个，指向运行时常量池中，当前栈帧所属的方法引用，持有这个引用是为了支持方法在调用过程中的动态链接（Dynamic Linking）。
     2. Class 文件的常量池中，存在大量的符号引用，这些符号引用一部分会在类加载阶段，或者第一次使用时转化为直接引用，这种转化成为静态连接。
     3. 而另一部分则在每一次运行期间都转化为直接引用，这部分称为动态连接。

   - 4）**方法返回地址**：

     1. 指向下一条字节码指令的地址，方法正常退出时，PC 计数器会记录其返回地址，保存在栈帧中。
     2. 而在方法异常退出时，返回地址，则要通过异常处理器表来确定，栈帧中不会保存这部分的信息。

   - 5）**附加信息**：虚拟机规范还允许一些的实现，可以增加一些规范里没有描述的信息到栈帧中，比如，与调试相关的信息等。

###### 3）本地方法栈

1. 本地方法栈，Native Method Stack，非线程共享，与虚拟机栈的作用是一样，只不过虚拟机栈是用来服务 Java 方法的，而本地方法栈，则是用来服务虚拟机调用 Native 方法的。

###### 4）堆区

![1626349372632](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1626349372632.png)

1. Java Heap，线程共享，在 JVM 启动时创建，是 Java 虚拟机中内存最大的一块，专门用来保存对象，几乎所有对象、以及数组的内存都在堆上分配。

2. 对象在堆中分配好以后，会在栈中保存一个 4 字节的实例，用来指向对象堆内存地址，定位对应的对象在堆中的位置，便于找到该对象，因此，操作实例，实际上是通过实例指针间接操作对象。

   - 另外，`ClassA a = new ClassA()`，a 叫做实例，而不能说成 a 对象，因为实例在栈上，对象在堆中。

   - 以及，在开启逃逸分析后，某些未逃逸的对象，也可以通过标量替换的方式在栈上分配。

3. 栈中的数据，和堆中的数据销毁并不是同步的，方法一旦结束，栈中的局部变量会立即销毁，而堆中的对象不一定会销毁，可能有其他变量也指向了该对象，一直等到没有变量指向该对象，它才有可能被垃圾回收掉。

4. 同时，堆是垃圾回收 GC 的主要场所，从内存回收角度来看，可以分为新生代和老年代，默认比值为 `1：2 `，原因是，老年代需要占用更多的内存空间。

5. 对于新生代又可以分为 Eden区（伊甸园）、Survivor 区（存活区），默认比值为 `8：2`，原因是，大部分对象生命周期活不到存活区。

6. Survivor 区又分为 Surviver0（From Survivor）和 Survivor1（To Survivor），默认比值为 `1：1`，原因是，存活区使用的是复制算法。

7. 堆中还有个概念，叫做 TLAB：Thread Local Allocation Buffer，线程私有分配缓存区，是一块线程专用的内存分配区域，JVM 会为每个线程分配一块 TLAB 区域，实质占用的是 Eden 区的空间（分配独享、使用共享），用于给每个线程往自己的 TLAB 中分配小对象，这样可以避免堆分配对象时的线程冲突，从而提升分配对象的效率。

   - 1）优点 - 加速对象分配：
     1. 当多个线程同时在堆上分配对象时，由于堆是线程共享的，为了保证线程同步，JVM 底层采用 CAS + 失败重试的方式来做同步处理。
     2. 如果多线程竞争非常激烈，那么在堆中分配对象性能是非常差的。
     3. 因此，JVM 设计了 TLAB，来避免堆分配对象时的线程冲突，从而提升分配对象的效率。
   - 2）缺点 - 大的对象无法分配：由于 TLAB 空间比较小，所以大的对象无法在 TLAB 分配，此时只能直接分配到其他堆空间中。

###### 5）方法区

![1647944132277](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647944132277.png)

1. Methed Area，别名 Non-Heap 非堆，线程共享，是 JVM 规范中定义的一个**逻辑概念**，用于存储已被虚拟机加载的类信息、常量、静态变量、以及即时编译后的代码，具体放在哪里，不同的实现可能会放在不同的地方。
2. 在 JDK 8 HotSpot 中，方法区存储空间分为堆和元空间，类信息（类版本、字段、方法、接口、父类等描述信息、静态常量池）、运行时常量池放在元空间中，静态变量、字符串常量池放在了堆中。
3. 讲到元空间，就要先讲一下永久代了：
   - 1）永久代，是 Hotspot 虚拟机特有的概念，是方法区的一种实现，主要存放类信息、常量等方法区内容。
   - 2）在 JDK 6 中，方法区中包含的数据，除了 JIT 编译生成的代码外（存放在native memory的CodeCache区域），其他都存放在永久代中。
   - 3）而移除永久代的工作，是从 JDK 7 开始的，但并没完全移除，比如符号引用转移到了本地内存中，字面量（见字符串常量池）、类的静态变量（class statics）转移到了堆中。
   - 4）在 Java 8 中，永久代就彻底被移除，取而代之的是，另一块与堆不相连的本地内存，叫做元空间（Metaspace）中，此时 `‑XX:MaxPermSize` 失去了意义，取而代之的是 `-XX：MaxMetaspaceSize `。
4. 再回到元空间：它是在 JDK 8 以后，用于替代永久代，存储类的元数据信息，存放的地方并不在 JVM 虚拟机中，而是在本地内存中，其大小仅受本地内存的限制，从而解决了永久代容易溢出的问题。
5. 再回到方法区，在 JDK8 以后，元空间替代了永久代，使得方法区与堆存在了交集，静态变量和字符串常量池存放在堆中，类信息和运行时常量池放在元空间中，而静态常量池是 class 文件里的常量池，未加载前并不占用内存。
   - 1）**静态常量池**：也叫 class 文件常量池，即 class 文件中的常量池，占用 class 文件绝大部分空间，主要存放：
     1. 字面量，相当于 Java 语言层面常量的概念，如文本字符串、final 修饰的变量。
     2. 符号引用，属于编译原理方面的概念，包括类和接口的全限定名、字段的名称和描述符、方法的名称和描述符。
   - 2）**运行时常量池**：当 class 文件加载到内存后，JVM 会将静态常量池中的内容，存放到运行时常量池中，这就是我们常说的常量池，主要存放：
     1. 编译期间产生的字面量和符号引用。
     2. 运行时常量池具有动态性，并非只能通过 class 文件常量池进入，运行期间也可以将新的常量放入池中。
   - 3）**字符串常量池**：可以理解为，运行时常量池中分出来的字符串部分，当类加载到内存时，字符串会存到字符串常量池里。
     1. `String#intern()` 方法，native方法，返回规范的字符串，底层调用 `equals()` 会先判断常量池是否有存在的字符串，如果没有，则会将字符串加入常量池。
     2. 程序运行时，除非手动向常量池中添加常量，比如调用 `String#intern()` 方法，否则 JVM 不会自动添加常量到字符串常量池中。
     3. 至于程序启动时，哪些字符串或常量、变量会加入常量池，取决于本身的编译性质，如果本身是字面量，则会加入常量池，如果是变量，由于地址不能确定，所以在不调用 `String#intern()` 时，是并不会加入常量池的。

##### 3、总

=> 以上，就是我对 JDK 8 运行时数据区 的一些理解，请问有什么细节需要补充的吗？

#### 4.1.1.8. 项目中的垃圾收集器用了哪些？

##### 1、总

1. 由于 JVM 启动参数没配置具体的垃圾收集器，所以使用的是默认的垃圾收集器。

   ```shell
   nohup java -Duser.timezone=Asia/Shanghai 
   -Xms2048m -Xmx2048m 
   -XX:OnOutOfMemoryError="sh kkp.sh" 
   -XX:+HeapDumpOnOutOfMemoryError
   -XX:+PrintGCDetails -Xloggc:/apps/svr/srmpos/logs/gc.log
   -Dsun.net.inetaddr.ttl=30 -Dsun.net.inetaddr.negative.ttl=10   
   -jar srm-pos-2.0.0-SNAPSHOT.jar 
   --spring.cloud.bootstrap.location=/apps/svr/srmpos/bootstrap.yml 
   -Xbootclasspath/a:/apps/svr/srmpos/: > /apps/svr/srmpos/logs/srmpos_$(date +%Y-%m-%d).log &
   
   # kkp.sh
   pids=`ps -ef|grep srm-pos|grep -v grep|awk '{print $2}'`
   if [ "$pids" != "" ]; then
    echo "kill -9 $pids"
    kill -9 $pids
   fi
   ```

2. 这时可以去看 gc.log 中的抬头，打印出来的是 `-XX:+UseParallelGC` ，即 `Parallel Scavenge` + `Parallel Old` 的组合。

   ```shell
   OpenJDK 64-Bit Server VM (25.232-b09) for linux-amd64 JRE (1.8.0_232-b09), built on Oct 22 2019 16:14:06 by "mockbuild" with gcc 4.8.5 20150623 (Red Hat 4.8.5-39)
   
   Memory: 4k page, physical 16268352k(6193040k free), swap 8388604k(8089872k free)
   
   CommandLine flags: -XX:+HeapDumpOnOutOfMemoryError -XX:InitialHeapSize=2147483648 -XX:MaxHeapSize=2147483648 -XX:OnOutOfMemoryError=sh kkp.sh -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseParallelGC 
   
   1.626: [GC (Metadata GC Threshold) [PSYoungGen: 241412K->16509K(611840K)] 241412K->16517K(2010112K), 0.0255457 secs] [Times: user=0.03 sys=0.01, real=0.03 secs] 
   
   # PSYoungGen：指 Parallel Scavenge，ParOldGen：指 Parallel Old
   1.652: [Full GC (Metadata GC Threshold) [PSYoungGen: 16509K->0K(611840K)] [ParOldGen: 8K->15950K(1398272K)] 16517K->15950K(2010112K), [Metaspace: 20697K->20697K(1069056K)], 0.0707538 secs] [Times: user=0.09 sys=0.01, real=0.07 secs] 
   ```

##### 2、分

JDK 中有如下的垃圾收集器（挑着讲）：

![1648022290125](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1648022290125.png)

###### 1）新生代 - Serial 收集器 | 单线程

![1648022517445](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1648022517445.png)

1. 最基本的、发展历史最悠久的收集器，采用的是复制算法。
2. 其特点是，单线程，可以专心做垃圾回收，不存在与其他线程的交互开销，效率较高，但收集过程全程 Stop The World。
3. 只适用于客户端程序、或者嵌入式低性能的单核机器。
4. 执行过程为，用户线程到保存点暂停，然后 GC 线程采用复制算法，执行新生代垃圾回收，回收完毕后，用户线程继续运行。

###### 2）新生代 - ParNew 收集器 | 多线程并行

![1648022570986](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1648022570986.png)

1. 相当于 Serial 收集器的多线程版本，采用的也是复制算法，收集过程全程也是 Stop The World。
2. 可使用 `-XX：ParallelGCThreads`，来设置并行线程数，一般设置为 CPU 核心数。
3. 主要用来和 CMS 收集器配合使用。
4. 执行过程为，用户线程到保存点暂停，然后多个 GC 线程采用复制算法，并行执行新生代垃圾回收，回收完毕后，用户线程继续运行。

###### 3）新生代 - Parallel  Scavenge 收集器 | 多线程并行

![1648022746240](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1648022746240.png)

1. 也叫吞吐量优先收集器，也是多线程并行的垃圾收集器，采用的也是复制算法。
2. 其特点是：
   - 1）可以通过配置，以达到一个可控制的吞吐量：
     1. `-XX：MaxGCPauseMillis`：设置后，JVM 将尽力控制垃圾收集时的最大的停顿时间。
     2. `-XX：GCTimeRatio`：用于设置吞吐量的大小，取值为 0~100，设置后，JVM 将花费不超过 `1 + / （1+n）` 的时间，去做垃圾收集。
   - 2）支持自适应 GC：
     1. 可使用 `-XX：+UseAdptiveSizePolicy` 启用，启用后，将无需再手动设置 `-Xmn、-XX：SurvivorRatio` 等参数，虚拟机会根据系统的运行状况，收集性能监控信息，动态地调整这些参数，从而达到最优的停顿时间以及吞吐量。
     2. 可见，Parallel  Scavenge 收集器存在着一定的智能性。
3. 适用于比较注重吞吐量的场景。
4. 执行过程为，用户线程到保存点暂停，然后多个 GC 线程采用复制算法，并行执行新生代垃圾回收，同时支持自适应 GC，回收完毕后，用户线程继续运行。

###### 4）老年代 - Serial Old 收集器 | 单线程

![1648023100553](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1648023100553.png)

1. 也叫串行的老年代收集器，相当于 Serial 收集器的老年代版本，采用的算法是标记整理算法。
2. 和 Serial、ParNew、Parallel Scavenge 三个新生代收集器，都可以形成配合。
3. 在 CMS 收集器出现故障时，则会使用 Serial Old 收集器作为备用。
4. 执行过程为，用户线程到保存点暂停，然后 GC 线程采用标记整理算法，执行老年代垃圾回收，回收完毕后，用户线程继续运行。

###### 5）老年代 - Parallel Old 收集器 | 多线程并行

![1648023323242](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1648023323242.png)

1. 相当于 Parallel Scavenge 的老年代版本，采用的是标记整理算法，只能和 Parallel Scavenge 新生代收集器配合使用。
2. 与 Parallel Scavenge 一样，适用于关注吞吐量的场景。
3. 执行过程为，用户线程到保存点暂停，然后多个 GC 线程采用标记整理算法，并行执行老年代垃圾回收，同时支持自适应 GC，回收完毕后，用户线程继续运行。

###### 6）老年代 - CMS 收集器 | 多线程并发

![1626507231972](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1626507231972.png)

1. CMS，Concurrent Mark Sweep，并发标记清除收集器，可以与用户线程同时工作，采用的是标记清除算法，而 Serial Old、Parallel Scavenge 采用的都是标记整理算法。

2. 优点是，CMS 的 Stop The World 时间比较短，大多过程都是并发执行的，只有初始标记和重新标记，存在者 Stop The World，其他阶段都是并发执行的。

3. 缺点是：

   - 1）**会存在内存碎片**：这也是最令人诟病的地方，这是因为 CMS 采用的是标记清除算法，会导致内存碎片的产生。
     1. 可使用 `UseCMSCompactAtFullCollection`（默认打开），在每次 Full GC 完成后，进行内存碎片的整理。
     2. 也可使用 `CMSFullGCsBeforeCompaction`（默认为0），在进行几次 Full GC 后，就进行一次内存碎片的整理。
   - 2）无法处理浮动垃圾：由于并发清除阶段，用户线程在并发执行，可能会产生新的垃圾，这部分垃圾称为浮动垃圾，而 CMS 是无法在本次 GC 清理掉这些浮动垃圾的，需要留到下次 GC 才能清理掉。
   - 3）不能等到老年代几乎满了才开始收集：
     1. 如果 CMS 执行期间，预留的老年代内存，不能满足用户程序的需要，则会出现一次 Concurrent Mode  Failure 异常，会导致 JVM 改用备用的 Serial Old 收集器，去收集老年代的垃圾，从而导致更长的 Stop The World。
     2. 因此，必须为老年代预留足够的内存给用户线程使用，可使用 `CMSInitiatingOccupancyFraction`（默认68%），来设置老年代占比达到多少后，才会触发 CMS 垃圾回收。
   - 4）CPU 资源比较敏感，并发执行阶段会导致应用吞吐量的降低：由于垃圾收集线程也需要占用一定的 CPU 资源，与业务线程一起去争抢 CPU 时间片，影响业务线程的执行效率，降低应用的吞吐量。

4. 适用于希望系统停顿时间短，响应速度快的场景，比如各种服务端应用。

5. 执行过程为：

   - 1）**初始标记**：initial  mark，标记那些 GC Roots 能直接关联到的对象，此时能够标记到的对象会比较少m。执行期间，存在 Stop The World，但由于标记的对象比较少，所以 STW 的时间也是比较短的。

   - 2）**并发标记**：concurrent mark，找出所有 GC Roots 能够关联到的对象。执行期间，垃圾收集线程，和用户线程并发执行，不存在 Stop The World。

   - 3）**并发预清理**：concurrent-preclean，是一个不一定会执行的阶段，可通过 `-XX：-CMSPrecleaningEnabled`（默认打开），关闭并发预清理阶段，本阶段，JVM 会重新标记那些在并发标记阶段期间，引用被更新了的对象，比如某些新晋升到老年代的对象，从而减少后面重新标记阶段的工作量。执行期间，垃圾收集线程和用户线程并发执行，也不存在 Stop The World。

   - 4）**并发可中止的预清理阶段**：concurrent-abortable-preclean，也是不一定会执行的阶段，使用该阶段的前提条件是，当 Eden 使用量大于 `CMSScheduleRemarkEdenSizeThreshold` 阈值时（默认2M）才会执行，工作内容与并发预清理阶段是一样的。执行期间，垃圾收集线程和用户线程并发执行，也不存在 Stop The World。

     1. 本阶段的主要作用是，允许用户能够控制预清理阶段的结束时机。
     2. 比如，可用 `CMSMaxAbortablePrecleanTime`（默认 5 秒）进行设置扫描时长。
     3. 再如，可用 `CMSScheduleRemarkEdenPenetration` 设置（默认 50%），当 Eden 使用占比达到多大，就结束本阶段。

   - 5）**重新标记**：remark，用于修正并发标记期间，由于用户线程并发运行，导致标记发生变动的那些对象的标记。如果不做修正，那么在并发标记期间，错误地把已经死亡了的对象，标记为了存活，则会导致部分垃圾不被回收，而如果把存活的对象，错误地标记成为了死亡，则可能会导致用户程序之后无法继续执行。执行期间，存在 Stop The World，一般来说，本阶段所花费的时间，会比初始标记阶段的要长一些，但会比并发标记阶段的短一些。

   - 6）**并发清除**：concurrent sweep，会基于标记结果，清除掉要前面标记出来、需要清除的垃圾，不做整理，所以会存在内存碎片。执行期间，垃圾收集线程和用户线程并发执行，不存在 Stop The World。

     那么为什么是并发清除，而不是并发整理？

     1. 因为本阶段是并发执行的，如果还要整理对象的话，则还需要移动对象的位置。
     2. 如果是并发整理，则既要回收垃圾，又要整理移动对象的位置，还要与用户线程并发执行，实现起来会变得非常困难，还容易出错，而采用并发清除，就变得容易了许多。
     3. 因此，这里是并发清除，而不是并发整理。

   - 7）**并发重置**：concurrent reset，清理本次 GC 的上下文信息，为下一次 GC 做准备。执行期间，垃圾收集线程和用户线程并发执行，不存在 Stop The World。

###### 7）新生代&老年代 - G1 收集器 | 多线程并发

![1648024834453](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1648024834453.png)

1. Garbge First，是一款价值优先、既可以用在新生代、又可以用在老年代（即整个堆内存）的垃圾收集器，由于采用的是复制算法，所以无内存碎片的问题。

2. 而其革命性的变化在于：

   - 1）它将整个堆，划分成了若干个大小相等的区域，每个区域叫一个 Region，这个 Region 可通过 `-XX：G1HeapRegionSize` 来指定，取值范围为 1M~32M ，且必须为 2 的幂次。
   - 2）在 G1 中，一共分为 4 类 Region，分别为 Eden Region 伊甸园、Survivor Region 存活区、Old Region 老年代、Humongous Region 存储大对象，同一代对象可能是不连续的，而超过 Humongous Region 大小一半的特大对象，则会被分配到连续的 Humongous Region 里面。
   - 3）然后，G1 会去跟踪每个 Region 里面的垃圾堆积的价值大小，即回收一个 Region 能够获取到多大的剩余空间。
   - 4）最后，构建一个优先列表，根据允许的收集时间，优先回收价值高的 Region，即回收后能够得到更大空间的 Region，以获得更高的垃圾收集效率。

3. 主要用来替换 CMS，适用于占用内存较大（6G 以上）的应用。

   - 1）> JDK 8 都可以使用 G1（如果内存 <= 6G，建议使用 CMS，如果内存 > 6G，可以考虑使用 G1），而 CMS 则从 JDK 8 开始就被废弃了。

4. 执行过程为：

   1、**Young GC**：与之前的 Minor GC 差不多，采用的都是复制算法，只不过回收的单位是 Region 而已。

   - 1）当所有 Eden Region 都满了时，会触发 Young GC，Eden Region 里面所有的存活对象，都会转移到Survivor Region 里面去。
   - 2）而原先在 Survivor Region 中存活的对象，则会转移到新的 Survivor Region 中，或者晋升到 Old Region 中。
   - 3）然后，空闲的 Region 则会被放入空闲的列表中，等待下次被使用。

   2、**Mixed GC**：

   ![1626513840821](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1626513840821.png)

   - 1）最能体现 G1 的设计思想，与 CMS 过程类似，但采用的是复制算法。
   - 2）在老年代大小，占整个堆的百分比达到 `-XX：InitiatingHeapOccupancyPercent` 时（默认 45%），则会触发 Mixed GC。

   - 3）Mixed GC 会回收所有的 Young Region，同时根据收集时间与回收价值回收部分的 Old Region。

   对于**执行过程**，除了并发标记是并发执行外，其他阶段都需要 Stop The World，但由于每次只回收部分的 Region，所以整体 Stop The World 的时间是可控的。

   - 1）**初始标记**：Initial Marking，与 CMS 初始标记类似，都是标记 GC Roots 能直接关联到的对象。执行期间，存在 Stop The World，不过由于标记的对象比较少，所以 STW 的时间比较短。
   - 2）**并发标记**：Concurrent Marking，也与 CMS 并发标记类似，都是找出所有 GC Roots 能够关联到的对象。执行期间，垃圾收集线程和用户线程并发执行，不存在 Stop The World。
   - 3）**最终标记**：Final Marking，也与 CMS 重新标记类似，用于修正并发标记期间，由于用户线程并发运行，导致标记发生变动的那些对象的标记。如果不做修正，那么在并发标记期间，错误地把已经死亡了的对象，标记为了存活，则会导致部分垃圾不被回收，而如果把存活的对象错误地标记成为了死亡，可能会导致用户程序之后无法继续执行。执行期间，存在 Stop The World。
   - 4）**筛选回收**：Live Data Counting and Evaluation，会对各个 Region 的回收价值与成本，进行排序，根据用户所期望的停顿时间 `MaxGCPauseMillis`，来制定回收计划，以选择回收某些 Region。执行期间，存在 Stop The World。
     1. 回收过程，采用的是复制算法，因此无内存碎片产生。
     2. 首先，G1 会选择一系列 Region 构成一个回收集。
     3. 接着，G1 会把决定要回收的 Region 中的存活对象，复制一个空的 Region 中。
     4. 最后，再删除掉需要回收的 Region。

   3、**Full  GC**：

   - 1）当 G1 在复制对象时，发现内存不够，或者无法分配足够内存，比如特大对象没有足够连续的 Humongous Region 可分配时，则会触发 Full GC。
   - 2）一旦触发 Full GC，在 Full GC 模式下，使用的是类似于 Serial Old 的垃圾回收，将出现长时间的 Stop The World。

5. 调优原则 => 因此，使用 G1 时，应该尽量减少 Full GC 的发生，让其只停留在 Young GC，或者 Mixed GC 的模式上进行垃圾回收。

   - 1）**增加预留的内存**：可加大 `-XX：G1ReserveRercent` ，默认为堆的 10%。
   - 2）**更早地进行回收垃圾**：可降低 `-XX：InitiatingHeapOccupancyPercent`（默认 45%），降低老年代大小，占整个堆的百分比的阈值，提早触发 Mixed GC。
   - 3）**增加并发阶段使用的线程数**：可增大 `-XX：ConcGCThreads`，让更多的垃圾回收线程去工作，但会降低业务应用的吞吐量。

##### 3、总

最后，选择垃圾收集器，不能纸上谈兵，要根据实际情况选择。

1. 桌面端应用、是单核的，可以使用 `Serial + Serial Old`。
2. Web 应用，追求响应快、吞吐量高的，可以使用 `Parallel Scavenge + Parallel Old`。
3. Web 应用，追求低延迟的，可以使用 `ParNew + CMS`、或者 G1。

=> 以上，就是我对 JVM 垃圾收集器 的一些理解，请问有什么细节需要补充的吗？

#### 4.1.1.9. 线上怎么确定是老年代还是年轻代的 GC 时间过长？

##### 1、总

查看方式有：

1. 看 Skywalking：

   ![1648027659855](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1648027659855.png)

2. 用 `jstat` 监控统计命令：

   ```shell
   # 查看remote.domain机器上，40496进程，垃圾收集相关的统计信息摘要（每隔1秒采样1次）
   jstat -gcutil 40496@remote.domain 1000
   
   # option参数解释：
   -class    :：显示类加载器的统计信息
   -gcutil    ：垃圾回收统计概述
   -gc        ：垃圾回收堆的行为统计
   -gcnew     ：新生代行为统计
   -gcold     ：年老代和永生代行为统计
   -gccapacity：各个垃圾回收代容量(young,old,perm)和他们相应的空间统计
   ```

3. 看 gc.log，手动计算所花时间：

   ```shell
   # PSYoungGen：指 Parallel Scavenge，ParOldGen：指 Parallel Old
   1.652: [Full GC (Metadata GC Threshold) [PSYoungGen: 16509K->0K(611840K)] [ParOldGen: 8K->15950K(1398272K)] 16517K->15950K(2010112K), [Metaspace: 20697K->20697K(1069056K)], 0.0707538 secs] [Times: user=0.09 sys=0.01, real=0.07 secs] 
   ```

##### 2、分

Minor/Young GC 时间过长原因：

1. **存活对象的标注时间过长**：
   - 1）比如重载了 `Object#finalize()` 方法，会导致标记 Final Reference 耗时过长。
   - 2）或者 `String#intern()` 方法使用不当，导致 GC 扫描 StringTable 时间过长。
   - 3）可以通过配置 `-XX:+PrintReferenceGC` ，显示 GC 处理在 Reference 时的耗时。
2. **对象生命周期变长**：
   - 1）比如本地缓存使用不当，积累了太多存活对象，其中，它们还可能晋升到老年代，增长 FullGC 时间。
   - 2）或者锁竞争严重，导致线程阻塞，引起局部变量的生命周期变长。

Major/Full GC 时间过长原因：

1. **长生命周期的对象多**：过多的全局变量或者静态变量等，会导致标记和复制过程的耗时增加。

##### 3、总

=> 以上，就是我对 GC 时间过长调优 的一些理解，请问有什么细节需要补充的吗？

#### 4.1.2.0. 线上出现慢 SQL 是怎么解决的？

1. Druid 监控拿去 SQL。
2. 然后进行 Explain 本地调优。

=> 见《[2.1.1.1. 项目自我介绍？](#2.1.1.1. 项目自我介绍？)》- 3、项目亮点 - 7）条码销售出库 | 索引调优、生产问题。

#### 4.1.2.1. 讲一下 SQL Explain？

##### 1、总

1. 使用 EXPLAIN 关键字，可以模拟优化器执行 SQL 语句，分析查询 SQL 语句的性能瓶颈。
2. 在 select 语句之前增加 explain 关键字，MySQL 就会在查询上设置一个标记，执行查询会返回执行计划的信息，而不是真正去执行 SQL。

##### 2、分

Explain 结果字段有如下几个：

| Explain 结果字段 | json 名称     | 含义                                                         |
| ---------------- | ------------- | ------------------------------------------------------------ |
| **id**           | select_id     | 该语句的唯一标识，id 越大，越先执行，相同 id 的，从上到下执行 |
| select_type      | 无            | 查询类型                                                     |
| table            | table_name    | 表名                                                         |
| partitions       | partitions    | 匹配的分区                                                   |
| **type**         | access_tpye   | 联接类型                                                     |
| possible_keys    | possible_keys | 可能的索引选择                                               |
| key              | key           | 实际选择的索引                                               |
| **key_len**      | key_length    | 索引的长度                                                   |
| ref              | ref           | 索引的哪一列被引用了                                         |
| **rows**         | rows          | 估计要扫描的行                                               |
| filtered         | filtered      | 表示符合查询条件的数据百分比                                 |
| **extra**        | 没有          | 附件信息                                                     |

其中，影响性能的字段主要有：

###### 1）type | 连接类型

type 有以下几种取值，性能从好到坏：

| 连接类型        | 含义                                                         | 备注                                                         |
| --------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| system          | 该表只有一行，相当于系统表                                   | system是const的特例                                          |
| const           | 针对主键或者唯一索引的等值查询，最多返回一行数据             | 查询速度非常快                                               |
| eq_ref          | 当使用了索引的全部组成部分，并且索引是**主键或者非空唯一索引**才会发生 | 性能仅次于system和const                                      |
| ref             | 当满足索引的最左前缀规则，或者索引**不是主键也不是唯一索引**时才会发生 | 如果索引只匹配到少量的行，则性能也是不错的                   |
| fulltext        | 全文索引                                                     | 使用MyISAM存储引擎才有                                       |
| ref_or_null     | 该类型类似于ref，但MySQL会额外搜索哪些行包含NULL，           | SELECT * FROM ref_table WHERE key_col = expr OR key_col IS NULL； |
| index_merge     | 该类型表示使用了索引合并优化，表示一个查询里面用到了多个索引 | -                                                            |
| unique_subquery | 该类型和eq_ref类型，但使用了**IN查询**，且**子查询是主键或者唯一索引** | value IN (SLECT id FROM single_talbe WHERE expr)             |
| index_subquery  | 和unique_subquery类型，只是**子查询使用的是非唯一索引**      | value IN (SELECT key_col FROM single_table WHERE other_expr) |
| range           | 范围扫描，表示检索了指定范围的行，主要用于有限制的索引扫描   | 常见的有，BETWEEN、>、>=、<、<=、IS NULL、<=>、LIKE、IN      |
| index           | 全索引扫描，和ALL类型，只不过index是全盘扫描了索引的数据     | 当查询仅使用索引中的一部分列时，会使用该类型，有两种触发场景：1）覆盖索引，比ALL快，此时只扫描索引数，Extra列为Using Index；2）全表扫描，同ALL，此时会回表查询数据，Extra列不会出现Using Index； |
| ALL             | 全表扫描                                                     | 性能最差                                                     |

###### 2）key_len | 索引的字段长度

1. 索引字段长度，当字段允许为 NULL 时，key_len 会比不允许为 NULL 的大 1 个字节。
2. 长度越短，一页就能装更多的 B+ 树节点，磁盘 I/O 次数就越少，性能就越高。

###### 3）rows | 估算的扫描行数

MySQL 估算会扫描的行数，数值越小，性能越高。

###### 4）extra | 查询的附加信息

用于展示有关本次查询的附件信息，重要的取值有：

| 附件信息                                                     | 含义                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| **Using filesort**                                           | 当Query中包含ORDER BY操作，而且无法利用索引完成排序操作时，MySQL Query Optimizer不得不选择相应的排序算法来实现。在数据较少时，从内存排序，否则从磁盘排序。 其中，Explain不会显式地告诉客户端用哪种排序。 |
| **Using Index**                                              | 仅使用索引树中的检索列信息，不必进行其他查找以读取实际行，当查询仅使用属于单个索引的列时，会使用此策略 |
| **Using index condition**                                    | 使用索引下推时出现，表示先按条件过滤索引，过滤完索引后，找到符合索引条件的数据行，随后用WHERE子句中的其他非索引条件，去过滤这些数据行。 |
| **Using index for group-by**                                 | 数据访问和Using Index一样，所需数据只需要读取索引，当Query中使用GROUP BY或者DISTINCT子句时，如果所有分组字段也在索引中，该信息就会出现 |
| Using join buffer（Block Nested Loop），Using join Buffer（Batched Key  Access） | 使用Block Nested Loop或者Batched Key  Access算法来提高join的性能 |
| Using MRR                                                    | 使用了Muti-Range Read优化策略                                |
| Using sort_union（..），Using union（..），Using intersect（..） | 这些提示索引扫描如何合并为index_merge连接类型                |
| **Using temporary**                                          | 为了解决该查询，MySQL创建了一个临时表来保存结果，如果查询包含不同列的GROUP BY和ORDER BY子句，通常会发生这种情况。 |
| **Using Where**                                              | 如果不是读取表的所有数据，或者不仅仅通过索引就可以获取所有需要的数据时，则会出现该值 |

##### 3、总

=> 以上，就是我对 SQL Explain 的一些理解，请问有什么细节需要补充的吗？

#### 4.1.2.2. 讲一下为什么要用 Https？

##### 1、总

1. Https，Hyper Text  Transfer Protocol over SecureSocket Layer，是以安全为目标的 Http 通道，在 Http 的基础上，通过传输加密、以及身份认证，保证了传输过程的安全性。
2. Https 在证书验证阶段，使用的是安全性高的非对称加密。在内容传输阶段，使用的是速度快的对称加密，
   - 对称加密：双方持有相同的密钥，加密速度快。典型的对称加密算法有，AES 和 DES。
   - 非对称加密：密钥成对出现（私钥和公钥），加密速度慢。私钥只有自己知道，不在网络中传输。公钥可以公开，A 使用 B 公钥加密后传输给 B，B 可以使用 B 的私钥解密得到原始内容。典型的非对称加密算法有，RSA 和 DSA。

##### 2、分

其执行原理为，

1. **发起请求阶段**：首先，客户端将它所支持的算法列表，和一个用作产生密钥的随机数 1，发送给服务器。
2. **返回证书阶段**：然后，服务器从算法列表中，选择一种算法，并将它和一份包含服务器公钥的证书（即数字签名）、以及随机数 2 ，返回给客户端。
3. **证书验证阶段**：
   - 1）客户端对服务器的证书进行验证，抽取服务器给的公钥，生成一个 `pre_master_secret` 随机密码串 + 服务器公钥，使用非对称加密，将加密后的信息（预主密钥）发送给服务器。
   - 2）以及根据预主密钥、随机数 1、随机数 2，独立计算出 MAC 密钥，作为接下来的会话密钥。
4. **服务器解密阶段**：服务器通过服务器私钥，对客户端发送过来的加密信息进行解密，得到预主密钥，与随机数1、随机数2，独立计算出 MAC 密钥，也作为接下来的会话密钥。
5. **客户端发起测试阶段**：客户端将握手消息，使用对称加密得到 MAC 值，发送给服务端，验证服务器能否正常接受客户端对称加密后的信息。
6. **服务器响应测试阶段**：同理，服务器收到后，，使用对称加密得到 MAC 值，返回给客户端。
7. **连接完成阶段**：如果客户端能够接受，并返回确认报文，则 SSL 层建立完成，开始 Https 对称加密传输。

##### 3、总

最后，总结一下，Https 与 Http 的主要区别为：

| HTTP                           | HTTPS                                         |
| ------------------------------ | --------------------------------------------- |
| 默认端口 80                    | 默认端口 443                                  |
| URL 以 http:// 开头            | URL 以 https:// 开头                          |
| 明文传输、数据未加密、安全性差 | 传输过程 SSL 加密、安全性好、需要用到 CA 证书 |
| 消耗资源少、响应速度快         | 消耗资源多、响应速度慢                        |

=> 以上，就是我对 Https 的一些理解，请问有什么细节需要补充的吗？

#### 4.1.2.3. Docker 和 K8S 有了解过吗？

##### 1、总

1. Docker，由 Go 语言开发，思想如 logo 一样，即集装箱 Container 容器思想，负责容器的运行和管理，通过隔离机制，使得每个容器间互不影响，以及通过限制每个容器的 CPU、内存和 I/O 资源，最大程度地压榨服务器资源源。
2. K8S，指 Kubernetes，是 Google#Omega 的开源版本，是目前市场占有率最高的容器编排产品，可以自动化部署、管理、扩展容器以及容器网络通信处理，为应用提供了理想的部署单元，和独立的执行环境，使得微服务部署更加简单，同时还支持集成到 CI/CD 工作流，提效 DevOps 团队工作。

##### 2、分

###### Docker 架构原理

![1647495796223](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647495796223.png)

1. Client：Docker 的客户端模块，用于运行 Docker 命令以及 Restful API。
2. Docker host：Docker 的服务器模块，存在一个 Docker Daemon 后台进程，负责整个 Docker 容器的生命周期管理，连接 Registry 下载镜像，以及提供 Client 的 API 端口来处理发送过来的命令。
3. Registry：Docker 的镜像仓库，分为 Docker Hub 官方公有的镜像仓库、Docker Datacenter 企业信任仓库、以及 Docker 内网私有仓库。
4. Images：Docker 镜像，可本地制作，也可来源于镜像仓库，是容器运行前代码、配置、环境变量、操作系统资源的打包，运行起来了之后叫做容器，类似于 VM 的配置模板，通过 UnionFS 联合文件系统，支持镜像的一层层叠加。
5. Containers：Docker 容器，每个容器共享主机的操作系统，通过 namespace 对 pid 进程、net 网络、ipc 信号量、mnt 文件系统、uts 用户组等进行**隔离**，通过 cgroup 对每个容器的 cpu、mem、io 等资源进行**限制**。

###### K8S 架构原理

![1647516641430](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1647516641430.png)

1. **Kubernetes Cluster**：K8S 集群，是计算、存储和网络资源的集合，是掌握所有计算、存储、网络资源，进行统计管理、调度的节点群。
2. **Namespace**：虚拟 K8S 集群，解决在同一个 Cluster 集群中，如何区分开 Controller 和 Pod 的问题，默认两个虚拟集群，`kube-system` 用于自身管理的集群，`kube-default` 用于应用部署的默认集群（不指定集群名称时）。

k8s 集群，分为一个 Master 节点和多个 Node 节点，

1. **Kubernets Maseter**：K8S 大脑节点，决定将应用放在哪里运行，它相比图中，该节点还隐藏了许多容器，比如：
   - **1）API Server**：API 服务端，通过控制台、网络，接收传过来的命令，判断是否符合语法标准，并根据实际环境进行处理。
   - **2）Scheduler**：调度执行器，对所有资源按照应用资源，执行统一调度，以及任务发布，负责决定将 Pod 放到哪⼀个 Node 上运⾏。
   - **3）Controller Manager**：控制器管理器，负责 Cluster 各种资源的统筹管理。
   - **4）ETCD**：类似于 ZK，对 K8S 集群的配置进行统一管理，保存了 K8S 的相关配置和状态信息，如果 POD 有发⽣变化，那么它会迅速通知相关的组件进⾏处理。
2. **Kubernets Node**：K8S 手脚节点，负责运行容器应用。
   - **1）Kubelet**：核心工作单元，是 K8S 里唯一一个没有以容器形式运行的组件，负责根据 Scheduler 发过来的信息，去创建和运⾏容器，并向 Master 报告运⾏状态，是直接跟 Docker 容器进行沟通。 
   - **2）Docker daemon**：Docker 后台容器。
   - **3）Kube-proxy**：网络通讯、服务发现负载模块，是网络代理的概念，当 service 接收到请求后，转发到某个 node 时，会由该 node 的 kube-proxy 模块负责接收，然后转发到后端的容器中，如果有多个副本，还能提供负载均衡的能⼒。
   - **4）Pod**：是 K8S 的最小工作单元（所以，K8S 最小工作单元不是容器！），是运行在 Node 手脚节点上的一堆容器集合，相当于是比容器更大的"集装箱"，对网络共享、存储共享的一堆 Docker 容器，封装成一个 Pod，作为一个小单元进行管理，从而扩大管理粒度，降低管理复杂度，实现统一部署，共享网络和存储。
   - **5）Controller**：负责对 Pod 进行统一管理。
     1. **Deployment**：一个应用资源，基于 ReplicaSet，会产生一个部署请求，形成一堆 Pod。
     2. **ReplicaSet**：一个会在多个点节点部署多个的 Pod，以完成更多的功能。
     3. **DaemonSet**：一个在同一节点只会运行一份的 Pod。
     4. **StatefulSet**：一个有状态的服务，对外提供的 Pod 名称永远不变。
     5. **Job**：一个短时、定时作业的 Pod。
   - **6）Service**：
     1. 为 Pod 提供了负载均衡：在当 Pod 之间需要相互访问时，会去 DNS Server 找到对应的 IP 地址，通过 Kube-proxy 服务发现功能，进行网络数据包转发，实现网络服务功能，以用于描述 Pod 和 Pod 之间的应用访问。
     2. 为客户端提供固定的 IP + 端口：当 Pod 的 IP 发生变化时，Service 可以保证，客户端面对的 Pod 还是固定 IP 和端口。
3. 当需要执行部署，并指定两个副本时，其**执行流程**为：
   - 1）Kuberctl 发送部署请求到 API Server。
   - 2）API Server 通知 Controller Manager，去创建一个 Deployment 资源。
   - 3）Scheduler 执行调度任务，将两个副本 Pod，分发到 Node1 和 Node2 上。
   - 4）Node1 和 Node2 上的 Kubelet 在各自的节点上，创建并运行 Pod。

##### 3、总

=> 以上，就是我对 Docker 和 K8S 的一些理解，请问有什么细节需要补充的吗？

#### 4.1.2.4. 算法题 | n 的二进制 1 的个数

leetcode -《汉明距离》。

##### 1）内置函数法 | O（1）

- **思路**：通过异或操作，得到不同 1 的位置后，再利用 《比特位计数 - 内置函数》的思路解决即可。
- **结论**：时间，0ms，100%，空间，38.1mb，7.67%，时间上，由于内置函数也是位移操作，所以时间复杂度为 O（1），空间上，由于只使用有限几个变量，所以空间复杂度也是 O（1）。

```java
class Solution {
    public int hammingDistance(int x, int y) {
        return Integer.bitCount(x ^ y);
    }
}
```

##### 2）右位移法 | O（logn）

- **思路**：通过异或操作，得到不同 1 的位置后，再利用 《比特位计数 - 右位移法》的思路解决即可。
- **结论**：时间，0ms，100%，空间，38.6mb，5.04%，时间上，由于最多遍历 logn 位，所以时间复杂度为 O（logn），空间上，由于只使用有限几个变量，所以空间复杂度是 O（1）。

```java
class Solution {
    public int hammingDistance(int x, int y) {
        int res = (x ^ y), count = 0;
        do {
            if((res & 1) == 1) {
                count++;
            }
        } while((res >>>= 1) > 0);
        return count;
    }
}
```

##### 3）Brian Kernighan  算法 | O（logn）

- **思路**：
  - 通过异或操作，得到不同 1 的位置后，再利用 《比特位计数 - Brian Kernighan  算法 》的思路解决即可。
  - Brian Kernighan （布莱恩·克尼汉）算法给出，对于任意整数 x，令 x = x &（x-1），可将 x 的二进制表示的最后一个 1 变为 0，因此，如果不断循环执行这个过程则可以获得 x 为 1的比特位个数。
- **结论**：时间，0ms，100%，38.5mb，5.04%，时间上，由于最多遍历 logn 位，所以时间复杂度为 O（logn），空间上，由于只使用有限几个变量，所以空间复杂度是 O（1）。

```java
class Solution {
    public int hammingDistance(int x, int y) {
        int res = (x ^ y), count = 0;
        while(res > 0) {
            res &= res - 1;
            count++;
        }
        return count;
    }
}
```

#### 4.1.2.5. 算法题 | 1T 的字符串数字 ID，怎么在一台 2C4G 的机器上去重？

按位所在的值，去生成文件夹，每个文件夹只能存一个 ID，最后再收集（如果顺序收集，则还可以排序）所有文件夹的 ID，输出到一个文件里，那个文件就是最终去重后了的答案了。

#### 4.1.2.6. 是否有参加过开源项目，或者 ACM 比赛获奖什么的？

回答了没有，但有提交过 BUG 验证给 Oracle 官网的 JDK 组织，讲的是提交红黑树在删除后调整，判断红黑树是否为合法的红黑树时，没有判断两个连续红节点的情况，最后 Oracle 反馈说需要提供详细的例子证明，但我没有一个很好的数据状态，事件就不了了之了。

=> 结果不太好，下次别说了~

#### 5.1.1.1. List 与 Set 的区别？

**相同点**：存储元素、接口继承自 Collection 接口、支持泛型。

**不同点**：

1. **List**，有序列表，使用此接口，用户可以通过索引，精确控制列表中每个元素的插入位置，以及访问对应位置的元素，典型实现有 ArrayList、LinkedList、CopyOnWriteArrayList、Vector 和 以及继承自 Vector 的 Stack。
2. **Set**，不包含重复元素的集合，最多包含一个 NULL 元素，典型实现有 HashSet 和 LinkedHashSet，底层都是基于散列表实现的。

#### 5.1.1.2. ArrayList 扩容机制？

##### 1、总

1. ArrayList 是一个基于数组的、非线程安全的 List 接口实现类，它允许添加 NULL 元素，支持对元素进行快速随机访问，适合随机查找和遍历，不适合大量插入和删除。
2. 默认初始容量为 10，当数组容量不够时，会触发扩容机制，扩大到原来容量的 1.5 倍，然后将原来数组的数据，通过 `System.arraycopy(..)` 复制到新数组中。
3. 当从 ArrayList 的中间位置，进行插入或者删除元素时，需要对数组进行复制、移动，代价较高。

##### 2、分

1. 元素获取原理：直接根据索引，获取数组中对应的元素，时间复杂度为 O（1）。
2. 元素添加原理：模拟实际大小 + 1，判断是否需要扩容，然后在末尾添加元素，或者先通过 `System.arraycopy(..)` 把索引及其右边的元素，复制到索引右边去，以腾出索引位置添加新元素。
3. ArrayList 扩容机制：模拟实际大小 + 1，通过与当前容量判断，大于的，则需要触发扩容机制，扩容为原来容量的 1.5 倍，然后通过 `System.arraycopy(..)` 复制到新数组中。
4. 元素删除原理：直接根据索引，或者从头找到第一个 equals 的元素，然后删除，删除之后，需要扣减实际大小，以及通过 `System.arraycopy(..)` 把索引右边的元素，复制到索引这个位置来，以避免出现空位置浪费存储空间。

##### 3、总

=> 以上，就是我对 ArrayList 的一些理解，请问有什么细节需要补充的吗？

#### 5.1.1.3. ArrayList 与 LinkedList 的区别？

##### 1、总

1. LinkedList，是一个基于双向链表的、非线程安全的 List 接口实现类，它允许添加 NULL 元素，适合数据的动态插入和删除。
2. 使用它，可以直接操作表头、表尾元素，由于实现了 List、Queue、Deque 接口，以及提供 Stack#API，可以把它，当作有序列表、队列、双端队列、以及栈来使用，非常多功能。

##### 2、分

1. 元素获取原理：由于是基于双向链表实现，所以不能直接根据索引进行查找，而是要从头、或者从尾开始挨个遍历查找，时间复杂度为 O（n），由于还实现了双端队列和栈的 API，所以还可以直接从链头和链尾获取元素，时间复杂度为 O（1）。
2. 元素添加原理：同理，由于是基于双向链表实现，所以不能直接根据索引进行添加，而是要从头、或者从尾开始挨个遍历，找到对应的位置后，才能进行元素添加，时间复杂度为 O（n），由于还实现了双端队列和栈的 API，所以还可以直接从链头和链尾添加元素，时间复杂度为 O（1），添加元素时，处理好前后指针关系即可。
3. Linked 扩容机制：由于是基于双向链表实现，理论上不存在达到容器边界这种情况，所以也无需扩容，只需要连接前后指针即可。
4. 元素删除原理：同理，由于是基于双向链表实现，所以不能直接根据索引进行删除，而是要从头、或者从尾开始挨个遍历，找到对应的元素后，才能进行元素删除，时间复杂度为 O（n），由于还实现了双端队列和栈的 API，所以还可以直接从链头和链尾删除元素，时间复杂度为 O（1），删除元素时，处理好前后指针关系即可。

##### 3、总

=> 以上，就是我对 LinkedList 的一些理解，请问有什么细节需要补充的吗？

总的来说，ArrayList 和 LinkedList 的**区别**有：

|                    | ArrayList                                          | LinkedList                                                   |
| ------------------ | -------------------------------------------------- | ------------------------------------------------------------ |
| 1、提供的 API 不同 | 只提供了 List#API                                  | 除了提供 List#API 外，还提供 Queue、Deque、以及 Stack#API    |
| 2、数据结构不同    | 是基于数组实现的                                   | 是基于双向链表实现的                                         |
| 3、扩容机制不同    | 默认初始容量为 10，每次扩容到原来容量的 1.5 倍     | 无需初始容量，也无需扩容机制，只需要处理好前后的指针关系即可 |
| 4、适用场景不同    | 适合随机查找和遍历元素，不适合大量元素的插入和删除 | 适合动态插入和删除元素，不适合元素的随机访问，因为每次都需要从头、或者从尾遍历链表 |

#### 5.1.1.4. 如何保证 List 的线程安全？

1. 在 List#API 外，使用同步锁，来保证线程安全。

2. 直接使用 `Collections#SynchronizedList`，底层原理是，通过持有传入的 List 引用，以及 `mutex` 对象锁，使用 synchronized 关键字来修饰 List 方法，从而包装成线程安全的 List 实现类。
3. 使用 Vector 实现类，它是一个可增长的数组，默认初始容量为 10，每次扩容为原来容量的 2 倍，底层原理是，通过每个 API 方法都使用 synchronized 关键字来修饰，从而保证线程安全。
4. 使用 CopyOnWriteArrayList，它是一个写时复制、读操作无需加锁、写时加锁复制、只保证数据最终一致性，无法保证实时性的、线程安全的 List 实现类。

#### 5.1.1.5. HashMap 的数据结构？

见《[3.2.1.5. HashMap 底层原理？](#3.2.1.5. HashMap 底层原理？)》。

#### 5.1.1.6. HashMap 的扩容机制，以及为什么是 2^n？

见《[3.2.1.5. HashMap 底层原理？](#3.2.1.5. HashMap 底层原理？)》。

#### 5.1.1.7. JDK 7 与 JDK 8 中 HashMap 的区别？

见《[3.2.1.5. HashMap 底层原理？](#3.2.1.5. HashMap 底层原理？)》。

#### 5.1.1.8. HashMap 如何保证线程安全？

见《[3.2.1.5. HashMap 底层原理？](#3.2.1.5. HashMap 底层原理？)》。

#### 5.1.1.9. JDK 7 与 JDK 8 中 ConcurrentHashMap 的区别？

见《[3.2.1.6. ConcurrentHashMap 底层原理？](#3.2.1.6. ConcurrentHashMap 底层原理？)》。

#### 5.1.2.0. CAS 的缺点，以及什么是总线风暴？

##### 1、总

1. CAS，CompareAndSwap，比较并交换，一种乐观锁的实现方式，是一种无锁算法，该算法关键依赖两个值，分别是期望值和新值，底层 CPU 利用原子操作，判断期望值是否还等于旧值，只有等于才会赋新值，否则就不做更新操作，而是继续自旋、继续比较。
2. 在 JDK 中，常常用到的地方有，ConcurrentHashMap、Atomic 原子类、FutureTask、synchronized 轻量级锁、ReentrantLock 显式锁、以及 AQS 等。

##### 2、分

1. 实现原理为，在操作系统层面，CAS 是一条 CPU `cmpxchg` 的原子指令，由于该指令具备原子性，所以，在使用 CAS 操作数据时，并不会造成数据不一致的问题。

2. 使用 CAS 进行无锁编程的步骤大致为：
   - 1）获得字段的期望值（oldValue）。
   - 2）计算出需要替换的新值（newValue）。
   - 3）然后，通过 CAS，尝试将新值（newValue）放入对应的内存地址中。
   - 4）如果 CAS 失败，就重复第（1）步到第（2）步，一直到CAS成功，这种重复就叫做 CAS 自旋。

3. 其**优点**是，性能开销小，
   - 1）由于 CAS 是处于用户态下 CPU 指令级的原子操作，所以，在用于线程同步时，线程也无需进入阻塞态，没有用户态与内核态之间的切换，性能开销较小。
   - 2）而 synchronized 重量级锁，涉及操作系统内核态下互斥锁的使用，线程的阻塞和唤醒，都需要在用户态和内核态间的切换，导致开销大、性能低下。
   - 3）然后就有了 synchronized  轻量级锁，使用 CAS 进行自旋抢锁优化，由于一直处于用户态下，所以，轻量级锁的开销是比较小。

4. 而其**缺点**则是，
   - 1）**只能保证单个变量的原子性**，当对一个变量执行操作时，可以使用 CAS 自旋的方式，来保证其原子性，但是，如果面对多个变量的操作时，CAS 就无法保证操作的原子性了。
     1. 解决方案是，把多个变量，合并成一个变量来操作。
   - 2）**空耗CPU资源**，在高并发场景下，大量的 CAS 做空自旋，会浪费很多 CPU 资源。
     1. 优化思路是，当并发修改的线程很少，冲突出现的机会很少时，自旋的次数很少，CAS 的性能会很高，而当并发修改的线程很多，冲突出现的机会很多时，自旋的次数很多，CAS 的性能则大大降低，所以，提升 CAS 无锁编程效率，关键在于减少冲突的机会。
     2. 一种解决方案是，**分散操作热点**，
        - 1）比如 LongAdder，其核心思想是热点分离，将 value 分离成一个数组，在多线程访问时，通过 Hash 算法，将线程映射到数组的某个元素进行操作，避免都访问同一个点。
        - 2）在最终结果获取时，则对数组中所有元素进行求和即可。
        - 3）可见，LongAdder 是通过以空间换时间的方式，将原来的一个 value 值，拆分为分布式的多个 value 数组元素，从而减少了 CAS 时线程之间的高冲突，以提升性能。
     3. 另一种解决方案是，**使用队列削峰**，将发生 CAS 争用的线程，加入一个队列中排队，降低 CAS 争用的激烈程度，比如 JUC#AQS 抽象队列同步器。
   - 3）**会有 ABA 问题**，是指线程在进行 CAS 操作时，虽然发现某个数据仍然等于期望值，CAS 也能操作成功，但这个期望值，可能已经不是之前意思了，存在被别的线程修改过、然后又修改回来的风险，这就是 ABA 问题。举个例子，在操作某个链表尾结点时，虽然看到的还是那个尾结点，但 CAS 操作成功之前，很可能就已经被别的线程，在前面增加了几个其他结点，此时的 CAS 面对的链表，实际上已经不是当时认为的那个链表了，是有问题的。
     1. 解决方案是，使用增加版本号或者时间戳的比较，比如，每次在执行数据的修改操作时，都带上一个版本号，版本号一致，才可以执行修改操作，并对版本号执行加 1 操作，否则，执行失败。由于每次操作的版本号都会增加，不会减少，因此，不会再出现 ABA 的问题。
   - 4）**会有发生总线风暴的风险**，
     1. CPU 会通过 MESI 缓存一致性协议，保障变量的缓存一致性，不同的内核需要通过总线来回通信，产生缓存一致性流量，由于总线被设计为固定的通信能力，如果缓存一致性流量过大，总线将会成为瓶颈，导致总线风暴的发生。
     2. 不过，总线风暴与 CPU 架构有关，并不是所有的 CPU 都会产生总线风暴，在 SMP CPU 架构上，当有很多线程同时执行 `lockcmpxchg` lock 前缀指令操作时，可能会产生总线风暴。
     3. 而 Java#CAS，恰恰会在发现 CPU 是多核 CPU 时，为 CAS 底层的 `cmpxchg` CPU 原子指令，添加上 `lockcmpxchg` lock 前缀，导致存在总线风暴的风险。
     4. 解决方案还是那些，分散热点、使用队列削峰，最大程度地减少了同时 CAS 的操作数量。

   => 另，其他辅助资料有（选讲），

   - 1）Symmetric Multi-Processor，SMP，对称多处理器，服务器中多个 CPU 对称工作，每个 CPU 访
     问内存地址所需时间相同，主要特征是共享，包含对 CPU、内存、I/O 等进行共享。所有的 CPU 会共享一条总线，靠此总线连接主存，每个核都有自己的高速缓存，各核相对于总线，呈对称分布。因此，被称为对称多处理器。
   - 2）缓存一致性协议，指在多 CPU 的系统中，为了保证各个 CPU 的高速缓存中数据的一致性，会实现缓存一致性协议，每个 CPU 通过嗅探在总线上传播的数据，来检查自己的高速缓存中的值是否过期，当 CPU 发现自己缓存行对应的主存地址被修改时，就会将当前 CPU 的缓存行设置成无效状态，当其他 CPU 对这个数据执行修改操作时，本 CPU 会重新从系统主存中，把数据读到自己的高速缓存中。最常见的实现就是 MESI 协议。
   - 3）MESI 协议，是 MSI 写入失效协议的扩展，要求在每个缓存行（64字节，高速缓存操作的基本单位），维护两个状态位（2bit），使得每个缓存行可能处于 M 被修改的、E 独占的、S 共享的和 I 无效的 4 种状态的其中之一，是一种基于过期机制的高速缓存一致性保障协议。

##### 3、总

=> 以上，就是我对 CAS 的一些理解，请问有什么细节需要补充的吗？

#### 5.1.2.1. volatile 关键字的实现原理？

##### 1、总

1. 使用 volatile 关键字修饰变量，能够保证该变量的内存可见性，以及禁止该变量相关的指令重排序，但不能保证该变量做复合操作的原子性。
2. 不过，讲清楚 volatile 原理之前，还要讲一下内存屏障和 JMM Java 内存模型。

##### 2、分

###### 1）硬件层面的内存屏障

1. 内存屏障，Memory Barrier，又称内存栅栏，Memory Fences，是一系列的 CPU 指令，是一项让 CPU 高速缓存内存可见的技术，也是一项保障跨 CPU 内核有序执行指令的技术。
2. 硬件层内存屏障，共分为三种：读屏障、写屏障和全屏障。
   - 1）读屏障：Load Barrier，在指令前插入读屏障，可以在指令执行时，让高速缓存中的数据失效，强制重新从主存加载数据，同时，会告诉 CPU 和编译器，先于这个屏障的指令必须先执行。
   - 2）写屏障：Store Barrier，在指令后插入写屏障，可以在指令执行时，让高速缓存中的最新数据更新到主存，让其他线程可见，同时，会告诉CPU和编译器，后于这个屏障的指令必须后执行。
   - 3）全屏障：Full Barrier，又称为 StoreLoad Barriers，是一种全能型的屏障，具备读屏障和写屏障的能力。
3. 所以，硬件层内存屏障的作用：
   - 1）强制让高速缓存的数据失效：它会强制把高速缓存中的最新数据写回主存，让高速缓存中相应的脏数据失效，一旦完成写入，任何访问这个变量的线程将会得到最新的值。
   - 2）阻止屏障两侧的指令重排序：编译器和 CPU，可能为了使性能得到优化，而对指令重排序，但是插入一个硬件层的内存屏障，相当于告诉 CPU 和编译器，先于这个屏障的指令必须先执行，后于这个屏障的指令必须后执行。

=> 由于不同的物理 CPU 硬件，所提供的内存屏障指令的差异非常大，因此，JMM 定义了自己的一套相对独立的内存屏障指令，用于屏蔽不同硬件的差异性，JMM 会要求，JVM 要为不同的平台，生成相应的硬件层的内存屏障指令。

所以，为了解释 volatile 的禁止指令重排序，还需要讲一下 JMM 的内存屏障~

###### 2）JMM Java 内存模型

![1630043244494](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630043244494.png)

1. JMM，Java Memory Model，Java 内存模型，它不像 JVM 内存结构那样，为真实存在的运行实体，而是体现为一种规范和规则，这些规范定义了一个线程对共享变量写入时，是如何确保对另一个线程是可见的。

   - 1）为此，JMM 提供合理的禁用缓存、以及禁止重排序的方法，以解决可见性和有序性。
   - 2）同时，它屏蔽了各种硬件和操作系统的访问差异，保证 Java 程序在各种平台下，对内存的访问最终都是一致的。

2. JMM 规定，所有变量都存储在主存中（类似于物理内存），每个线程都有自己的工作内存（类似于CPU中的高速缓存）。工作内存保存了线程使用到的变量的拷贝副本，线程对变量的所有操作（读取、赋值等）必须在该线程的工作内存中进行，不同线程之间，无法直接访问彼此工作内存中的变量，要想访问，就只能通过主存来进行传递。

3. 在 JMM 中，还定义了一套，JMM 主存与工作内存之间交互的协议，即一个变量是如何从主存拷贝到工作内存的，又是如何从工作内存写入主存的，该协议包含 8 种操作，并且要求 JVM 的具体实现，必须保证其中每一种操作都是原子的、不可分割的。

   ![1630045226983](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630045226983.png)

   | JMM 操作 | 作用对象 | 说明                                                         |
   | -------- | -------- | ------------------------------------------------------------ |
   | Read     | 主存     | 读取。把一个变量的值从主存传输到工作内存中，以便随后的Load操作使用。 |
   | Load     | 工作内存 | 载入。把Read操作从主存中得到的变量值，载入到工作内存的变量副本中（可以简单理解为CPU的高速缓存）。 |
   | Use      | 工作内存 | 使用。每当JVM遇到一个需要使用变量值的字节码指令时，都会执行Use操作，会把工作内存中的一个变量值传递给执行引擎。 |
   | Assign   | 工作内存 | 赋值。每当JVM遇到一个给变量赋值的字节码指令时，都会执行Assign操作，操作引擎通过Assign操作给工作内存的变量赋值。 |
   | Store    | 工作内存 | 存储。把工作内存的一个变量值传递到主存中，以便随后的Write操作使用。 |
   | Write    | 主存     | 写入。把Store操作从工作内存中得到的变量值，写入到主存的变量中。 |

4. 关于 JMM 内存屏障，由于不同 CPU 硬件实现内存屏障的方式不同，JMM 屏蔽了这种底层 CPU 硬件平台的差异，定义了不对应任何 CPU 的 JMM 逻辑层内存屏障，提供了自己的内存屏障指令，要求 JVM 编译器实现这些指令，由 JVM 在不同的硬件平台，生成对应的内存屏障机器码，禁止特定类型的编译器（不是所有的编译器重排序都要禁止）和 CPU 重排序，从而解决有序性问题。

5. JMM 内存屏障，主要有 Load 和 Store 两类：

   - Load Barrier：读屏障，在读指令前插入读屏障，可以在指令执行时，让高速缓存中的数据失效，重新从主存加载数据。
   - Store Barrier：写屏障，在写指令后插入写屏障，可以在指令执行时，让写入缓存的最新数据写回主存。

6. 然后，在实际使用时，JMM 会对 Load Barrier 和 Store Barrier 两类屏障进行组合：

   - 1）LoadLoad：LL 屏障，在 Load2 要读取的数据被访问前，使用 LoadLoad 屏障，能保证 Load1 要读取的数据被读取完毕。

     ```java
     // LoadLoad屏障伪代码
     Load1；LoadLoad；Load2
     ```

   - 2）StoreStore：SS 屏障，在 Store2 以及后续写入操作执行前，使用 StoreStore 屏障，能保证 Store1 的写入结果对其他 CPU 可见。

     ```java
     // StoreStore屏障伪代码
     Store1；StoreStore；Store2
     ```

   - 3）LoadStore：LS 屏障，在 Store2 以及后续写入操作执行前，使用 LoadStore 屏障，能保证 Load1 要读取的数据被读取完毕。

     ```java
     // LoadStore屏障伪代码
     Load1；LoadStore；Store2
     ```

   - 4）StoreLoad：SL 屏障，该屏障是一个全能型屏障，由于兼具其他3个屏障的效果，因此开销是 4 种屏障中最大的。在 Load2 以及后续所有读取操作执行前，使用 StoreLoad 屏障，能保证 Store1 的写入对所有 CPU 可见。

     ```java
     // StoreLoad屏障伪代码
     Store1；StoreLoad；Load2
     ```

###### 3）volatile 底层原理

1. 再说回 volatile，使用 volatile 关键字修饰变量，能够保证该变量的内存可见性，以及禁止该变量相关的指令重排序，但不能保证该变量做复合操作的原子性。

2. 保证内存可见性的意思是，使用 volatile 修饰的变量，在变量值发生改变时，会立刻同步到主存，并使其他线程的变量副本失效，即一个线程修改了某个 volatile 变量的值，该值对其他线程立即可见，其原理为：

   - 1）使用 volatile 修饰的变量，它的 JMM read、load、use 操作都是连续出现的，每次使用变量时，都要从主存读取最新的变量值，替换私有内存的变量副本值。
   - 2）同时，assign、store、write 操作也都是连续出现的，每次对变量的改变，都会立马同步到主存中，从而保证它的读写的内存可见性。

3. 另外，在硬件层面，volatile 变量操作的汇编指令前，会多出一个 lock 前缀指令 `lock ADD` ，lock 前缀指令具有以下功能：

   - 1）将当前 CPU 缓存行数据，立即写回主存，在执行指令期间。
   - 2）失效其他 CPU 中相同地址的缓存行，修改回写操作要经过总线传播数据，其他 CPU 通过嗅探在总线上传播的数据，来检查自己缓存的值是否过期，一旦发现自己缓存行对应的数据被修改，就会将自己的缓存行设置为无效状态，强制重新从主存中把数据读到自己的缓存中。

   => 以上两点，可以说是从硬件层面实现了 volatile 的内存可见性。

   - 3）lock 前缀指令，还可以作为内存屏障，禁止指令重排序，避免多线程环境下，程序出现乱序执行的现象。

4. 禁止指令重排序的意思是，用 volatile 修饰的变量，JVM 在生成字节码时，会在指令序列中插入内存屏障，

   - 1）在每个 volatile 读操作后，插入一个 LoadLoad 屏障，以及一个 LoadStore 屏障，禁止后面的普通读、普通写、和前面的 volatile 读操作之间发生重排序。

     ![1630053036307](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630053036307.png)

   - 2）在每个 volatile 写操作前，插入一个 StoreStore 屏障，在写操作后，插入一个 StoreLoad 屏障，禁止前面的普通写、和后面的 volatile 写操作之间发生重排序，同时，禁止后面的普通读、和前面的 volatile 写操作之间发生重排序。

     ![1630052862174](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630052862174.png)

   => 以上两点，实现了 volatile 的有序性。

5. 虽然 volatile 修饰的变量，其要求对变量的（read、load、use）以及（assign、store、write）必须是连续出现的，每次读取的变量可以是最新值，且可以强制刷新回主存，但是在不同 CPU 内核上并发执行的线程，还是有可能出现读取脏数据的。比如：

   ![1630118559462](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630118559462.png)

   - 1）假设有两个线程 A、B，分别运行在 Core1、Core2 上，此时有个 value，值为 0，线程 A、B 也都读取了 value 值到他们自己的工作内存中。
   - 2）现在，线程 A 将 value 改成了 1 之后，完成了 assign、store 操作，但在执行 write 指令之前，线程 A 的 CPU 时间片用完了，导致 write 操作没有执行，value=1 没有到达主存。
   - 3）由于线程 A 的 store 指令触发了写信号，导致线程 B 的缓存行过期，B 则重新从主存读取了 value 值，此时线程 A 的 write 操作还没有完成，所以，线程 B 读到的 value 值还是 0。
   - 4）在线程 B 执行完成所有操作之后，则将 value 改成 1，并写入主存中。
   - 5）然后，线程 A 的时间片重新拿到，重新执行 store 操作，将过期了的 1 写入主存。
   - 6）此时，A、B 两线程执行了 2 次 +1 操作，但最终结果只增加了 1，而不是 2。
   - 7）因此，对 volatile 变量的复合操作不具备原子性，本案例中的复合操作是读取 value，value + 1，写回 value。
   - 8）对于这种复合操作，其原子性解决方案是，使用锁，保证临界区互斥执行。

##### 3、总

=> 以上，就是我对 volatile 的一些理解，请问有什么细节需要补充的吗？

#### 5.1.2.2. 线程池的数据结构，实现原理，以及线程数如何选择？

见《[1.2.1.3. 线程池核心参数，以及选取原则？](#1.2.1.3. 线程池核心参数，以及选取原则？)》。

#### 5.1.2.3. MySQL 主键索引、二级索引、联合索引查找数据的原理？

##### 1）主键索引数据查找原理

1. InnoDB#主键索引，使用的是聚蔟索引，如果创建的表没有主键，那么 InnoDB 会隐式定义一个主键，来作为聚蔟索引。
2. 聚蔟索引，叶子结点就是数据结点，将表数据和主键一起存储，数据的物理存放顺序与索引顺序一致，一张表只有一个聚蔟索引。
3. 聚蔟索引在查找数据时，只要根据搜索树的查找原则，小的查左边，大的查右边，定位到叶子节点即是数据，这个操作叫做回表。

##### 2）二级索引数据查找原理

1. 聚蔟索引的二级索引，其叶子结点不保存引用数据，而是保存行的主键值。
2. 然后，根据叶子节点的主键值，回表查找到数据。
3. 这种二级索引在 InnoDB 中比如有，普通索引 和 联合索引。

##### 3）联合索引数据查找原理

1. 联合索引，指在多个字段上创建的索引，数据查找原理需要遵循最左前缀原则。

2. 最左前缀原则，指的是索引按照最左优先的方式匹配索引，主要使用在联合索引中，联合索引的数据结构
   在 InnoDB 中是 B+Tree，它会按照第一个关键字、第二个关键字...顺序进行索引排列。

3. 如果查询的条件遵循了最左前缀原则，那么 MySQL 会：

   ![1648112848858](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1648112848858.png)

   - 1）先根据第 1 个关键字，查找联合索引树。
   - 2）定位到索引树的叶子结点后，找出所有满足第 1 个关键字的叶子结点。
   - 3）第 1 个关键字的叶子结点确定后，如果这些叶子结点，对于第 2 个关键字是存在且是顺序的，那么 MySQL 则会使用二分查找的方式，查找这些叶子结点，相当于该索引继续匹配第 2 个关键字。
   - 4）如果这些叶子结点，对于第 2 个关键字来说，不存在或者是乱序的，那么就出现了索引失效，导致后面的条件，都无法继续走上该联合索引，而是根据之前选择的叶子结点的主键，进行回表查找。

#### 5.1.2.4. MySQL 联合索引失效原理？

联合索引失效场景，其原理都是因为，匹配到某个关键字的叶子结点时，不满足存在性或者顺序性：

1. 如果不是从索引的最左列开始查找，则无法使用该联合索引。
2. 不能在中间跳过索引中的某个列，这样的查询只能使用到联合索引的前几列。
3. 如果查询中有某个列的范围查询，那么该列右边的所有列，都无法使用该联合索引进行查找。

#### 5.1.2.5. MySQL 3 个二级索引好还是 1 个 3 字段的联合索引好？

1. 在应有的查询条件下，MySQL 面对 3 个二级索引，如果是 `and` 操作，则会使用性能最好的 1 个二级索引进行匹配，其他 2 个条件则会走索引下推优化（extra 会显示 `Using index condition`），或者走普通的 where 查询（extra 会显示 `Using Where`），而不是直接走它们的二级索引。
2. 如果是 `or` 或者 `union` 的操作，则有可能会走索引合并，type 会显示 `index_merge`，表示合并了 `or` 两侧字段的索引，性能来得比 `range` 好些。
3. 而如果面对相同的条件，建立起一个 3 字段的联合索引的话，那么有可能会根据最左前缀原则去匹配。
4. 因此，这两者到底谁性能更好，要看具体情况才能定夺，比如这些条件到底是什么，会不会引起索引失效，表的数据量有多大，以及 SQL 的结构是什么等。

#### 5.1.2.6. MySQL 二级索引和联合索引是不是建的越多越好？

1. 在 MySQL 中，创建索引的目的是，让索引过滤更多的行，快速定位记录，或者利用索引的有序性，对于那些不符合目的的，无需创建多余的索引。
2. 对于那些用作 where 条件的、参与分组的、参与排序的、参与去重的、参与联表的、或者是唯一的字段，一般会去创建索引。
3. 而对于那些更新多、查询少、不频繁 where 的、或者表数据少、字段列重复数据多的字段，一般都不用创建多余的索引，因为要么是字段用不到，要么是用了但不划算。
4. 无用的索引过多，会额外占据存储空间，所以，索引并不是建的越多越好，满足要求、够用就行。

#### 5.1.2.7. 介绍一下 B+ 树，以及 MySQL 为什么采用 B+ 树？

##### 1、总

MySQL 之所以采用 B+ 树，作为索引底层的数据结构，我认为是为了追求性能，以及满足范围查询。

##### 2、分

1. **如果采用 BST**，二叉查找树，虽然可以在 log（n）内查找到目标数据，但在最坏情况下，可能会退化成链表，查询的时间复杂度也退化成了 O（n），性能太慢，故放弃。

   ![1648116903919](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1648116903919.png)

2. **如果采用 AVL**，平衡二叉搜索树，虽然能保证，每个结点的左子树和右子树的高度差不会超过 1，优化了 BST 退化成链表的问题，但在大规模数据存储的时候，AVL 会由于树高度过大，会导致每次查找只能找出 2 个结点装入内存，造成磁盘 I/O 读写过于频繁，导致效率低下，故也放弃。

   ![1648116960901](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1648116960901.png)

3. **如果采用 HashMap**，哈希表，虽然可以实现在 O（1）内，查找到目标数据，但在数据量过大、散列算法不好时，会出现大量的哈希冲突，查询效率大幅度降低，且由于哈希的特性，不支持范围查找，只能做等值匹配，故也放弃。

   ![1648117079088](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1648117079088.png)

4. **如果采用 SkipListMap**，跳跃表，虽然可以以二分查找的方式，在 O（logn）内找到目标数据，但每次二分，都需要把一半的节点加载到内存，而在数据量大时，一半节点 MySQL 一页装不下，需要多次 I/O，效率也不理想，故也放弃。而 Redis 本身就是内存数据库，所以采用跳跃表是没什么问题的。

   ![1648117132681](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1648117132681.png)

5. **如果采用 B-Tree**，平衡多路搜索树，m 阶 B-Tree 每个节点都包含 data 数据、m - 1 个关键字、以及 m 个子节点引用，可以有效地降低数树的高度，大大减少查找的次数，性能得到了大幅提升。但由于 B-Tree 每个节点包含 data 数据，会使得一页内容装不下太多结点，磁盘 I/O 次数比较多，且每次范围查询时，都需要多次从头遍历 B-Tree，性能低下，故也放弃。

   ![1648117001612](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1648117001612.png)

6. **如果采用 B+Tree**，B+ 树，m 阶 B+ 树 非叶子节点包含 m 个关键字、m 个子节点引用，不包含 data 数据，可以使得每次 I/O 加载更多的节点到内存中，减少了 I/O 次数，对比 B-Tree 性能有提升，同时，叶子节点会冗余父结点的关键字，包含了全部的关键字，以及按照关键字大小， 顺序链接起来，构成一个有序双向链表，在做范围查找时，只需要在链表上顺序查找，而无需从头遍历整颗树，对比 B-Tree 性能得到了很大的提升，故 B+Tree 在做大量的磁盘数据查找，是非常适合的。

   ![1648117033116](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1648117033116.png)

##### 3、总

因此，MySQL 采用的 B+ 树，来作为索引底层的数据结构，而不是其他。

#### 5.1.2.8. 分布式 ID 的实现方案？

##### 1、总

1. 在分库分表环境中，由于表中数据同时存在不同数据库中，平时使用的自增主键 ID 将无用武之地，因为某个分区数据库自生成的 ID 无法保证全局唯一。
2. 因此，需要单独设计全局主键，来避免跨库主键重复的问题，我了解到的方案有：UUID、MyISAM ID 表、高可用 ID 服务器、Snowflake 分布式自增 ID 算法、以及美团的 Leaf 分布式 ID 生成系统。

##### 2、分

###### 1）UUID

UUID 标准形式包含 32 个 16 进制数字，分为 5 段，形式为 8­4­4­4­12 的 36 个字符，比如：550e8400­e29b­41d4­a716­446655440000。

- **优点**：方案最简单，且本地生成，性能高，没有网络耗时。
- **缺点**：
  1. 由于 UUID 非常长，会占用大量的存储空间。
  2. UUID 作为主键，建立索引和基于索引进行查询时，都会存在性能问题，在 InnoDB 下，UUID 的无序性会引起数据位置频繁变动，导致页分裂。

###### 2）MyISAM ID 表

```sql
-- 使用MyISAM存储引擎建立ID表
CREATE TABLE `sequence` (  
  ìd` bigint(20) unsigned NOT NULL auto_increment,  
  `stub` char(1) NOT NULL default '', 
  PRIMARY KEY  (ìd`),  
  UNIQUE KEY `stub` (`stub`)  
) ENGINE=MyISAM;

-- 先删除再获取自增ID
REPLACE INTO sequence (stub) VALUES ('a');  
SELECT LAST_INSERT_ID();
```

- **概念**：
  1. `stub` 字段（存根）设置为**唯一索引**，同一 `stub` 值在 `sequence` 表中只有一条记录，支持同时给多张表生成全局 ID。
  2. 使用 MyISAM 存储引擎而不是 InnoDB，可以获取更高的性能，因为 MyISAM 使用的是表级锁，对表的读写是串行的，不用担心在并发时两次读取同一个 ID 值的问题。
  3. 使用 `REPLACE INTO + SELECT` 来获取自增 ID，保证两操作在同一事务内，它会先删除旧数据，再生成新数据，从而实现主键自增。
- **优点**：实现简单。
- **缺点**：
  1. 存在单点问题，且强依赖 DB，当 DB 异常时，会导致整个分布式系统都不可用。
  2. 虽然可以配置主从来增加可用性，但当主库挂了，主从切换时，数据一致性难以得到保证。
  3. 因此，整个系统的性能瓶颈，被限制在单台 MySQL 的读写性能上。

###### 3）高可用 ID 服务器

![1631524708834](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631524708834.png)

- **背景**：Flickr（弗里克，雅虎的一个图片分享网站）团队使用的一种主键生成策略，与上面的 `sequence` 表方案类似，但可以更好地解决了单点故障和性能瓶颈的问题。

- **思想**：

  1. 建立 2 个以上的全局 ID 生成的服务器，每个服务器上只部署一个数据库，每个库有一张 `sequence` 表用于记录当前全局 ID。
  2. 表中 ID 增长的**步长相同，等于库的数量，起始值依次错开**，这样能将 ID 的生成散列到各个数据库上，比如第一台为（1，3，5，7，...）以及第二台为（2，4，6，8，...）等等。

- **优点**：生成 ID 的压力，能够均匀分布在多台机器上，同时提高了系统的容错能力，当第一台出现了错误，可以自动切换到第二台机器，来获取 ID。

- **缺点**：

  1. 系统添加机器水平扩展时，需要停止原本正在运行的 ID 服务器，以**修改步长**。
  2. 每次获取 ID 都要读写一次 DB，DB 的压力还是很大，只能靠堆机器来提升性能。

- **优化方案 **：**批量获取 ID**。

  - 使用批量获取的方式，可以降低数据库的写压力，每次获取**一段**区间的 ID 号段，用完之后再去数据库获取，可以大大减轻数据库的压力。
    - 1）比如，还是使用 2 台 DB 保证可用性，数据库中只存储当前的最大 ID。
    - 2）ID 生成服务，每次批量获取 6 个ID，可以先将 max_id 修改为 5，当应用访问 ID 生成服务时，就不需要访问数据库，从**号段缓存**中依次派发 0~5 的 ID。
    - 3）当这些 ID 发完后，再将 max_id 修改为 11，下次就能派发 6~11 的ID。
    - 4）这样，数据库的压力降低为原来的 1/6。
  - **缺点**：ID 生成服务需要维护最大 ID 值，再下次生成 ID 时，需要告诉 DB M1、DB M2 各自的初始值。

  ![1631525249951](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631525249951.png)

###### 4）Snowflake 分布式自增 ID 算法

Twitter 的 snowflake 算法，解决了分布式系统生成全局 ID 的需求，可以生成 64 位的 long 类型的数值。

![1631525530303](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631525530303.png)

- **概念**：1 + 41 + 10 + 12  = 64位。
  1. 首先是，第 1 位不使用。
  2. 接下来是， 41 位的毫秒级时间戳，最大可以表示 **69年** 的时间。
  3. 然后是，5 位的 `datacenterId`，5位的 `workerId`，这 10 位长度，最多支持部署**1024 个节点**。
  4. 最后是，12 位是毫秒内的计数值，最大支持每个节点、每毫秒产生**4096 个 ID 序列**。
- **优点**：
  1. 毫秒数在高位，生成的 ID 整体上按时间趋势是**递增**的，还可以根据自身业务灵活分配 bit 位。
  2. 不依赖第三方系统，稳定、效率高，理论上 QPS 约为 409.6 w/s（2^12 * 1000 ms），且整个分布式系统中不会产生 ID 碰撞。
- **缺点**：强依赖于机器时钟，如果时钟回拨，则可能导致生成的 ID 重复。

###### 5）美团点评分布式ID生成系统 - Leaf

Leaf，服务美团点评公司内部产品，包含金融、支付交易、餐饮、外卖、酒店旅游、猫眼电影等众多业务线，其性能在 4 C 8 G 的机器上，QPS 能压测到近 5 w/s，TP 999 1ms，能够满足大部分的业务需求。

![1631532240042](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631532240042.png)

**1、Leaf - segment ID 服务器方案：**

- **思想**：
  1. 获取 ID 时，向 proxy server 代理服务器批量获取 ID，每次获取一个 segment 号段（由 `step` 决定大小）的值。
  2. 用完之后，再去获取新的号段，可以大大减轻数据库的压力。
- **实现**：
  1. `biz_tag` 用来区分业务，`max_id` 表示该 `biz_tag` 目前所被分配的 ID 号段的最大值，`step` 表示每次分配的号段长度。
  2. 比如，`test_tag` 在第 1 台 Leaf 机器上是 1~1000 的号段，当这个号段用完时，会去加载另一个长度为step=1000 的号段，而如果另外机器的号段都没有更新的话，此时第 1 台机器会重新加载 3001~4000 的号段，同时，数据库对应的 `biz_tag` 这条数据的 `max_id` 会从 3000 被更新成 4000。
  3. 这样，各业务不同的发号需求用 `biz_tag` 字段来区分，每个 `biz-tag` 的 ID 相互隔离，互不影响，如果以后有性能要求，需要对数据库进行扩容时，则不用复杂的扩容操作，只需要对 `biz_tag` 分库分表即可。
  4. 而且，对比原来获取 ID 每次都需要写数据库，现在只需要把 `step` 设置得足够大，比如 1000，那么只有当 1000 个号被消耗完了之后，才会去重新读写一次数据库，此时读写数据库的频率从1  减小到了 **1 / step** 。
- **优点**：
  1. Leaf 服务可以很方便的进行**线性扩展**，性能完全能够支撑大多数业务场景。
  2. 生成的 ID 是**趋势递增**的 8 byte 的数字，满足上述数据库存储的主键要求。
  3. 容灾性高，Leaf 服务内部有**号段缓存**，即使 DB 宕机，短时间内，Leaf 仍能正常对外提供服务。
  4. 可以自定义 `max_id` 的大小，非常方便业务从原有的 ID 方式上**迁移**过来。
- **缺点**：
  1. **TP 999 数据波动大**：当号段使用完之后，还是会 hang 在更新数据库的 I/O 上，TP 999 数据会出现偶尔的尖刺。
  2. **高可用得不到保障**：DB 宕机会造成整个系统不可用。
  3. **生成的 ID 不够随机**：会泄露发号数量的信息，不太安全。

**2、双 buffer 优化方案：**

目的是，优化第 1 个缺点，当号段使用完、线程取号时阻塞的问题。

![1631532711485](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631532711485.png)

- **背景**：
  1. Leaf 取号段的时机，是在号段消耗完时进行的，意味着号段临界点的 ID 下发时间，取决于下一次从 DB 取回号段的时间，并且在这期间，进来的请求也会因为 DB 号段没有取回来，导致线程阻塞。
  2. 假如 Leaf 服务取 DB 时，网络发生抖动，或者 DB 发生慢查询，就会导致整个系统的响应时间变慢。
- **思想**：
  1. 为了让 DB 取号段的过程能够做到无阻塞，不会在 DB 取号段时阻塞请求线程，可以让发号段消费到某个点时，就**异步**的把下一个号段加载到内存中，而不需要等到号段用尽时，才去更新号段。
- **实现**：
  1. 采用双 buffer 的方式，Leaf 服务内部有两个号段缓存区 segment。
  2. 当前号段已下发 10% 时，如果下一个号段未更新，则**异步**另启一个更新线程去更新下一个号段。
  3. 当前号段全部下发完后，如果下个号段准备好了，则切换到下个号段为当前 segment 接着下发，循环往复。

**3、高可用容灾方案：**

目的是，解决第 2 个缺点，DB 宕机会造成整个系统不可用的问题。

![1631543929139](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631543929139.png)

**DB 高可用方案**：

1. 采用 1 主 2 从的方式，同时分机房部署，Master 和 Slave 之间采用半同步复制的方式，进行数据同步，同时使用公司的 DBProxy（原 Atlas）数据库中间件，做主从切换。
2. 当然，这种方案在一些情况会退化成**异步模式**，甚至在非常极端情况下仍然会造成**数据不一致**的情况，但是出现的概率非常小。
3. 如果系统确实要保证 100% 的数据强一致，可以选择使用**类Paxos算法**，实现强一致 MySQL 的方案，但这样运维成本和精力都会相应的增加，应该需要根据实际情况进行选型。

**应用高可用方案**：

1. Leaf 服务分 IDC 部署，内部的服务化框架是 `MTthrift RPC`。
2. 服务调用时，根据负载均衡算法，优先调用同机房的 Leaf 服务。
3. 如果该 IDC 内，Leaf 服务不可用，则会选择其他机房的 Leaf 服务。
4. 同时，服务治理平台 `OCTO` ，还提供了针对服务的过载保护、一键截流、动态流量分配等服务保护措施。

**4、Leaf-snowflake 方案：**

目的是，优化第 3 个缺点，生成的 ID不够随机，会泄露发号数量信息，不太安全的问题。

![1631544317384](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631544317384.png)

Leaf-snowflake 方案，完全沿用了 snowflake 方案的 bit 位设计，即是 `1+41+10+12` 的方式组装 ID 号。

1. 对于 `workerID` 的分配，当服务集群数量较小时，完全可以手动配置。
2. 但当 Leaf 服务规模较大时，动手配置成本太高，此时可以使用 ZK 持久顺序节点的特性，自动对 snowflake 节点配置 `wokerID`。

**对接 ZK 的步骤**：

1. 启动 Leaf-snowflake 服务时，会去连接 ZK，在 `leaf_forever` 父节点下，检查自己是否已经注册过，是否有该节点的持久化顺序子节点。
2. 如果有注册过，则直接取回自己的 `workerID`，启动服务。
3. 如果没有注册过，则在该节点下，创建一个持久化顺序节点，创建成功后，取回顺序号当做自己的 `workerID` 号，启动服务。
4. 除了每次会去 ZK 拿数据以外，也会在本机文件系统上，缓存一个 `workerID` 文件，当 ZK 出现问题且恰好 Leaf-snowflake 服务机器也需要重启时，还能保证服务正常启动，做到了对 ZK 的**弱依赖**，一定程度上提高了 SLA 。

![1631544564754](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631544564754.png)

**解决 snowflake 时钟回退问题**：

由于 snowflake 强依赖机器时间，如果其发生了回拨，则可能会生成重复的 ID，解决方案为：

1. 首先，服务启动时，先检查自己是否写过 `ZK#leaf_forever` 节点。
   - 1）如果写过，则用自身系统时间与 `leaf_forever/#{self}` 节点记录时间做比较，且小于 `leaf_forever/​#{self}` 的时间，则认为当前机器时间发生了回拨，服务启动失败并报警。
   - 2）如果没写过，则证明是新服务节点，此时需要创建持久顺序节点 `leaf_forever/#{self}` ，并写入自身系统的时间。
2. 接下来，综合对比其余 Leaf 节点的系统时间，来判断自身系统时间是否准确，具体做法是：
   - 1）取 `leaf_temporary` 下的所有临时节点（**所有运行中的** Leaf-snowflake节点）的服务 IP+端口。
3. 然后，通过 RPC 请求，得到它们各自的系统时间，计算 `at = sum(time) / nodeSize`。
4. 如果计算结果 `at < 阈值`，认为当前系统时间准确，可以正常启动服务，同时写临时节点 `leaf_temporary/#{self}` ，每隔一段时间（3s），上报自身系统时间，并写入到 `leaf_forever/#{self}` 中，以维持租约。
5. 否则，则认为本机系统时间发生大步长的偏移，启动失败并报警。

##### 3、总

=> 以上，就是我对 分布式全局 ID 实现方案的一些理解，请问有什么细节需要补充的吗？

#### 5.1.2.9. MySQL 隐藏主键生成原理？

##### 1、总

1. 如果没有主动设置主键，就会选一个不允许为 NULL 的第一个唯一索引列作为主键列，并把它用作聚集索引。
2. 如果没有这样的索引，就会使用自增的、占 6 个字节的行号，来生成一个聚集索引，把它当做主键，这个行号可以使用 `select _rowid from ${table_name}` 进行查询。

##### 2、分

主键自增 / `AUTO_INCREMENT ` 原理：

1. 如果字段被定义为 `AUTO_INCREMENT`， 那么在插入一行数据时，该字段被指定为 0、null、或者没指定值，那么，就先获取自增锁，把这个表存好的 `AUTO_INCREMENT` 值（初始从 `auto_increment_offset` 开始）填到自增字段中。
   - 自增锁，不是一个事务锁，而是要看 `innodb_autoinc_lock_mode` 策略的配置（默认为 1），0 表示语句执行完才释放，1 表示普通语句申请完马上释放，批量插入语句则等到语句结束才释放，2 表示所有语句都会申请完马上释放。
2. 如果插入数据时，该字段指定了具体的值，则会直接使用语句里指定的值。
3. 如果这个值比当前 `AUTO_INCREMENT` 值小，那么自增值不变，否则，就需要修改 `AUTO_INCREMENT` 为该字段插入的值 + 1。注意，如果此时插入的主键值重复、或者其他原因，导致了事务回滚，被修改的 `AUTO_INCREMENT` 是不会重新减少回去的，所以，会出现自增值不连续的情况。
4. 接着往后，则会以 `auto_increment_increment` 为步长，持续叠加，或者像上面那样碰到大于 `AUTO_INCREMENT` 的、新插入的值然后 +1 并更新。

##### 3、总

=> 以上，就是我对 MySQL 隐藏主键生成原理 的一些理解，请问有什么细节需要补充的吗？

#### 5.1.3.0. MySQL 事务 ACID 特性，以及实现原理？

##### 1、总

1. 事务，Transaction，是指访问、更新数据库数据一个程序执行单元，是逻辑上的一组操作，要么都执行，要么都不执行，其执行的结果，必须使数据库从一种一致性状态，变到另一种一致性状态。
2. 而 ACID 则是衡量事务的四个维度，分别是 A 原子性、C 一致性、I 隔离性、D 持久性。

##### 2、分

1. **原⼦性**： Atomicity， 指事务是最小的执行单位，不可再分割，整个事务中所有的操作，要么全部提交成功，要么全部失败回滚，强调的是事务操作的原子不可分割。
   - 1）在 MySQL 中，其实现原理是，当事务对数据库进行修改时，InnoDB 会生成对应的 undo log，如果事务执行失败或调用了 `rollback`，导致事务进行了回滚，就可以根据 undo log 的内容，做与之前相反的操作，把 Buffer Pool 中修改了的数据，回滚到修改之前的样子，从而实现事务原子性。
2. **一致性**：Consistency， 指事务执行结束后，数据库的完整性约束不会被破坏，都是合法的数据状态，强调的是数据状态事务前后的一致性。
   - 1）一致性是事务追求的最终目标，原子性、持久性和隔离性都是为了保证数据库的一致性。
   - 2）其中，在 MySQL 中，实现一致性的措施包括：
     1. 数据库层面的保障：原子性、持久性和隔离性的保证，以及其他一些完整性约束，比如不允许向整型列插入字符串值，或者字符串长度不能超过列的限制等等。
     2. 应用层面的保障：比如，如果转账操作只扣除转账者的余额，而没有增加接收者的余额，无论数据库实现的多么完美，也无法保证状态的一致，此时需要保证应用逻辑是符合一致性的。
3. **隔离性**： Isolation，指并发访问数据库时，⼀个⽤户的事务不被其他事务所⼲扰，各并发事务之间的数据库是独⽴的，一个事务所做的修改，在最终提交之前，对其他事务是不可见的，强调的是事务间的数据操作互不影响。
   - 1）在 MySQL 中，其实现原理是，写写隔离，通过使用锁机制来保证，写读隔离，快照读使用 MVCC 来保证，当前读则使用锁机制l来保证。
4. **持久性**：Durability， 指事务被提交之后，它对数据库中的数据改变是持久的，即使数据库发⽣故障，也不应该对其有任何影响，强调的是事务后的数据会被永久保存。
   - 1）在 MySQL 中，其实现原理是，Buffer Pool 和 redo log。
   - 2）首先是 Buffer Pool，
     1. 为减少每次读写数据的磁盘 I/O，InnoDB 提供了 Buffer Pool 缓存，作为访问数据库的缓冲。
     2. 当从数据库读取数据时，InnoDB 会先从 Buffer Pool 中读取，如果 Buffer Pool 中没有，才会从磁盘读取，然后放入 Buffer Pool 中。
     3. 当向数据库写入数据时，同样 InnoDB 也会先写入 Buffer Pool，Buffer Pool 中修改的数据，会定期刷新到磁盘中，这一过程称为刷脏。
     4. 优点是，大大提高了读写数据的效率。而缺点则是，如果 MySQL 宕机，Buffer Pool 中修改的数据还没有刷新到磁盘中，那么就会导致数据的丢失，无法保证事务的持久性。
   - 3）所以，InnoDB 引入了 redo log，来解决 Buffer Pool 的问题， 以保证事务的持久性：
     1. 这样，在修改 Buffer Pool 中的数据后，会再把数据写到 redo log 缓冲区中。
     2. 然后，根据 `innodb_flush_log_at_trx_commit` 来决定 redo log 的刷盘机制：
        - 1）0：表示当事务提交时，不会把 redo log 缓冲区的日志，同步到 redo log 磁盘文件中，而是要等待线程每秒的刷新，所以，不能保证全部写入成功。
        - 2）1：表示当事务提交时，会主动把 redo log 缓存区的日志，同步到 redo log 磁盘文件中，能够保证全部写入成功。
        - 3）2：表示当事务提交时，会异步把 redo log 缓存区的日志，同步到 redo log 磁盘文件中，也是不能保证全部写入成功的。
     3. 由于写 redo log 是追加操作，属于顺序 I/O，且刷回磁盘的只是被修改的部分，对比 Buffer Pool 刷脏的随机 I/O 以及全量刷脏，性能来得更高。
     4. 有了 redo log 磁盘文件后，当 MySQL 宕机了，重启时，就可以读取 redo log 中的数据，对数据库进行恢复，保证了事务的持久性。

##### 3、总

总结一下，undo log、redo log 和 bin log：

| 不同点   | undo log                                    | redo log                       | bin log                      |
| -------- | ------------------------------------------- | ------------------------------ | ---------------------------- |
| 名称     | 回滚日志                                    | 重做日志                       | 二进制日志                   |
| 存储内容 | 属于逻辑日志，内容为版本链需要的相关信息    | 属于物理日志，内容为数据页     | 逻辑日志，内容为一条条SQL    |
| 作用     | 是 MySQL 实现事务原子性和 MVCC 隔离性的基础 | 用于崩溃恢复，保证事务的持久性 | 用于时间点恢复，保证主从复制 |
| 实现层面 | 由 InnoDB 存储引擎实现                      | 由 InnoDB 存储引擎实现         | 由 MySQL 服务器实现          |
| 写入时机 | 事务提交前，当做数据合并到 redo log 中      | 事务提交前刷盘                 | 事务提交前刷盘               |

最后，再总结一下，事务读写数据到提交的整体流程为：

1. 事务读时，则加载缓存数据到 Buffer Pool 缓冲池。
2. 事务写时，先是记录数据旧值到 undo log 缓冲区中。
3. 然后，更新 Buffer Pool 中的内存数据。
4. 接着，把更新后的数据，写入到 redo log 缓冲区中。
5. 事务准备提交，则把 undo log 缓冲当做数据，一起写入 redo log 缓冲区中，然后 redo log 日志刷入磁盘。
6. 也是在此时，bin log 也写入磁盘。
7. bin log 写入磁盘后，还会写入 commit 标记到 redo log 磁盘文件中。
8. 最后，I/O 线程把 Buffer Pool 中修改的数据，定期刷脏到磁盘中，事务提交。
9. 如果要重启恢复，则先回放 redo log，生成数据到 Buffer Pool 内存中，然后从 redo log 中构造 undo page，做相反的操作，回滚掉被修改了的内存数据，以及使用 undo page 实现 MVCC。

![1648126170770](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1648126170770.png)

=> 以上，就是我对 MySQL 事务 ACID 实现原理的一些理解，请问有什么细节需要补充的吗？

#### 5.1.3.1. MySQL Buffer Pool 满了以后会怎么样？

##### 1、总

首先，要讲一下 Buffer Pool 是什么，

1. 实际上，MySQL 中的数据，最终都是要存放在磁盘文件上的。
2. 但是，在对 MySQL 执行增删改时，不可能直接更新磁盘上的数据，因为如果对磁盘进行随机读写操作，那速度相当的慢，随便一个大磁盘文件的随机读写操作，可能都要几百毫秒、数据库每秒也就只能处理几百个请求而已。
3.  所以，在对 MySQL 执行增删改时，主要都是针对内存里的 Buffer Pool 中的数据进行的，也就是实际上主要是对 MySQL 内存里的数据结构进行增删改，再配合 redo log、undo log、刷脏机制，确保事务的持久性和原子性。
4. 可见，Buffer Pool 是 MySQL 的一个内存组件，里面缓存了磁盘上的真实数据，在对 MySQL 执行的增删改时，其实都是对这个内存数据结构中的缓存数据进行的。

##### 2、分

然后就是，Buffer Pool 里存放了什么东西，

1. 实际上，MySQL 对数据抽象出了一个数据页的概念，把很多行数据放在了一个数据页里，也就是磁盘文件中会有很多数据页，每一页数据里存放了很多行数据。

2. 在要更新一行数据时，MySQL 会找到这行数据所在的数据页，然后从磁盘文件里，把这行数据所在的数据页，加载到 Buffer Pool 里，也就是说，Buffer Pool 中存放的是一个一个的数据页。

3. 默认情况下，磁盘中存放的一页数据页大小为 16 KB，而 Buffer Pool 中存放的一个一个的数据页，叫做缓存页，一个缓存页的大小，也是和磁盘上的一个数据页大小一一对应起来的，都是16KB。 

4. 对于每个缓存页，实际上都会有一个描述信息，这个描述信息大体可以认为是用来描述这个缓存页的，比如包含：这个数据页所属的表空间、数据页的编号、这个缓存页 在 Buffer Pool 中的地址等别的信息。每个缓存页的描述数据放在最前面，然后各个缓存页放在后面。这种描述数据，相当于缓存页大小的 5% 左右，所以，Buffer Pool 实际上真正的最终大小会比设置的超出一些。因此，Buffer Pool 实际长如下这个样子：

   ![1648173791945](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1648173791945.png)

接着就是，Buffer Pool 的工作流程，

1. 在执行 curd 时，MySQL 会不停地加载磁盘上的数据页到 Buffer Pool 的缓存页里来，再查询和更新缓存页里的数据，同时维护一系列的链表结构。
2. 然后，后台线程定时根据 lru 链表和 flush 链表，去把一批缓存页刷入磁盘释放掉这些缓存页，同时更新 free 链表。
3. 而如果执行 curd 时发现缓存页都满了，无法加载自己需要的数据页进缓存时，则会把 lru 链表冷数据区域的缓存页刷入磁盘，然后加载自己需要的数据页进来。

##### 3、总

最后就是，设置合理的 Buffer Pool 大小了，

1. 所以，设置合理的 Buffer Pool 大小，可以尽可能地保证 MySQL 高性能和高并发能力。
2. 通常来说，一般建议把 Buffer Pool 设置为机器内存的 50%~60% 左右，剩余的留给操作系统以及其他地方使用。比如，我们生产环境中的 MySQL 总内存是 128 GB，通过 `SELECT @@innodb_buffer_pool_size/1024/1024/1024;` 查到的是了 64 GB，大小是合理的。
3. 而如果要修改的话，可通过 `set global innodb_buffer_pool_size = ${字节数};` 来进行设置。

=> 以上，就是我对 MySQL Buffer Pool 的一些理解，请问有什么细节需要补充的吗？

#### 5.1.3.2. Redis 内存淘汰策略？

内存淘汰策略是指，当达到指定的 `maxmemory` 内存量时（默认为 0，代表没有限制），会根据不同的策略，选择不同的旧数据进行淘汰，可通过 `maxmemory-policy` 进行设置。

1. LRU，Least Recently Used，最近最少使用淘汰算法，用于淘汰最长时间没有被访问的旧数据。
2. LFU：Least Frequently Used，最不经常使用淘汰算法，用于淘汰在一段时间内访问次数最少的旧数据。

| 策略                      | 作用对象   | 客户端请求发现内存不足时                                     | 适用场景                                            |
| ------------------------- | ---------- | ------------------------------------------------------------ | --------------------------------------------------- |
| noeviction                | 全局 key   | 会返回错误                                                   | 常量字典，不能淘汰任何 key 时                       |
| allkeys-lru               | 全局 key   | 会先尝试删除 LRU key                                         | 热点缓存，需要淘汰非热点 key 时                     |
| volatile-lru              | 可过期 key | 会先尝试删除 LRU key，但仅限于可过期 key，如果没有任何可过期 key，则会返回错误 | 热点缓存，需要淘汰非热点 key，又需要保护永久 key 时 |
| allkeys-random            | 全局 key   | 会随机淘汰 key                                               | 需要以相同概率去淘汰 key 时                         |
| volatile-random           | 可过期 key | 会随机淘汰 key，但仅限于可过期 key，如果没有任何可过期 key，则会返回错误 | 需要以相同概率去淘汰 key ，又需要保护永久 key 时    |
| volatile-ttl              | 可过期 key | 会先尝试删除剩余 TTL 最短的 key，但仅限于可过期 key，如果没有任何可过期 key，则会返回错误 | 需要根据过期时间去淘汰 key 时                       |
| allkeys-lfu（Redis 4.0）  | 全局  key  | 会先尝试删除 LFU key                                         | 需要淘汰访问次数少的 key 时                         |
| volatile-lfu（Redis 4.0） | 可过期 key | 会先尝试删除 LFU key，但仅限于可过期 key，如果没有任何可过期 key，则会返回错误 | 需要淘汰访问次数少的 key，又需要保护永久 key 时     |

#### 5.1.3.3. Redis 过期策略？

##### 1、总

在 Redis 中，可以设置缓存 key 的过期时间，过期策略就是指，当 Redis 中缓存 key 过期了，Redis 是如何处理的。

##### 2、分

常见的过期策略，通常有以下三种：

1. 定时过期：每个设置过期时间的 key，都需要创建一个定时器，到过期时间就会立即清除。优点是，该策略可以立即清除过期的数据，对内存很友好。缺点是，会占用大量的 CPU 资源，去处理过期的数据，从而影响缓存的响应时间和吞吐量。
2. 惰性过期：只有当访问一个 key 时，才会判断该 key 是否已过期，过期则清除。优点是，该策略可以最大化地节省 CPU 资源。缺点是，对内存非常不友好，在极端情况下，可能出现大量的过期 key 没有再次被访问，从而不会被清除，占用大量内存。
3. 定期过期：每隔一定的时间，会扫描 expires 字典中一定数量的 key，并清除其中已过期的 key。该策略是前两者的一个折中方案，通过调整定时扫描的时间间隔、以及每次扫描的限定耗时，在不同情况下，使得 CPU 和内存资源达到最优的平衡效果。 
   - expires 字典，会保存所有可过期 key 的过期时间数据，该字典 key 指向 Redis 集群键空间中的某个可过期 key 的指针，字典 value 是该可过期 key 用 UNIX 毫秒时间戳表示的过期时间。

##### 3、总

而 Redis 中，则是同时使用了后两种，即惰性过期 + 定期过期，这两种过期策略。

#### 5.1.3.4. Redis 常用数据结构，以及 ZSet 底层原理？

见《[1.2.1.5. Redis 数据结构？](#1.2.1.5. Redis 数据结构？)》。

#### 5.1.3.5. Spring AOP 的原理？

见《[4.1.1.5. Spring AOP 的原理？](#4.1.1.5. Spring AOP 的原理？)》。

#### 5.1.3.6. Spring @Transactional 原理？

见《[4.1.1.6. Spring @Transactional 原理？](#4.1.1.6. Spring @Transactional 原理？)》。

#### 5.2.1.1. 自我介绍，以及项目亮点？

见《[2.1.1.1. 项目自我介绍？](#2.1.1.1. 项目自我介绍？)》。

#### 5.2.1.2.  线程池调优？

见《[1.2.1.3. 线程池核心参数，以及选取原则？](#1.2.1.3. 线程池核心参数，以及选取原则？)》。

#### 5.2.1.3. Kafka 高可靠保证？

见《[1.1.1.7. Kafka 的高可靠保证？](#1.1.1.7. Kafka 的高可靠保证？)》。

#### 6.1.1.1. Kafka 高可靠保证？

见《[1.1.1.7. Kafka 的高可靠保证？](#1.1.1.7. Kafka 的高可靠保证？)》。

#### 6.1.1.2. 如果一个 Broker 宕机了，还能保证高可靠吗？

还能保证高可靠，原因是，

1. 如果挂的是 Leader：
   - 1）当 Leader 宕机时，ZK 会向 Controller 发送选举新 Leader 的通知，Controller 会向所
     有 Broker 发送一个命令，通知 Leader 即将发生变化，Leader Replica 重新再平衡。
   - 2）然后，由于生产者上一次消息写入失败，会被定时器捞取出来，重新投递到新 Leader 上，即消息最终会被写入成功。
2. 如果挂的是 Follower：
   - 1）当 Follower 宕机时，如果在 `replica.lag.time.max.ms` 时间内，都没有重启完毕，没发出 fetch 请求，那么 Leader 会认为该 Follower 是死副本，会把它从 ISR 中剔除，然后消息照常写入到 Leader 上。
3. 因此，即使一个 Broker 宕机后，使用上一题的方案，还是能保证消息的可靠性投递的。

#### 6.1.1.3. 算法题 | 1T 字符串数字 ID，怎么在 2C4G 的机器上做去重并且排序？

见《[4.1.2.5. 算法题 | 1T 的字符串数字 ID，怎么在一台 2C4G 的机器上去重？](#4.1.2.5. 算法题 | 1T 的字符串数字 ID，怎么在一台 2C4G 的机器上去重？)》。

#### 6.1.1.4. Netty 有了解过吗？

##### 1、总

1. Netty 是一个基于 JAVA NIO 实现的高性能的、异步事件驱动的 NIO 框架，提供了对 TCP、UDP 和文件传输的支持。

2. 使用 Netty 实现通信的服务端步骤大致为：
   1. 先创建 2 个 NIO 线程组，一个专门用于网络事件的处理，接受客户端的连接，另一个则进行网络通信读写。
   2. 然后，创建一个 ServerBootStrap 对象，配置 Netty 的一些列参数，比如接受传出数据的缓存大小等等。
   3. 接着，创建一个实际处理数据的 ChannelInitializer 类，进行初始化的准备工作，比如设置接受传出数据的字符集、格式、实际处理数据的 ChannelHandler 等。
   4. 最后，绑定接口，执行同步阻塞方法，等待服务端启动即可。

   （客户端与服务端基本一致）。

3. Netty 之所以高性能，是因为采用了许多优秀的性能方案，比如 I/O 多路复用模型、NIO 模型、主从 Reactor 线程模型、以及零拷贝机制。

##### 2、分

###### 1）Linux I/O 多路复用模型

I/O multiplexing 就是我们说的 `select`、`poll` 和 `epoll`，有些地方也称这种 I/O 方式为事件驱动型 I/O event driven IO。

1. `select` / `epoll` 的好处就在于，单个 process 就可以同时处理**多个**网络连接的 I/O，其基本原理是，`select`、`poll` 、`epoll` 这些 function 会不断的轮询所负责的所有 socket，当某个 socket 有数据到达了，就通知用户进程。
2. 当用户进程调用了 `select`，那么整个进程会被 block，同时，kernel 会“监视”所有 `select` 负责的 socket，当任何一个 socket 中的数据准备好了，select 就会返回。这个时候用户进程再调用 read 操作，将数据从 kernel 拷贝到用户进程。
3. 所以，I/O 多路复用的特点是，通过一种机制，让一个进程能同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入读就绪状态，`select` 函数就可以返回了。

![1646817566451](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646817566451.png)

###### 2）Java NIO 模型

NIO，同步非阻塞 I/O：

1. 服务器实现一个连接一个线程，即客户端发送的连接请求都会注册到多路复用器上。
2. 多路复用器轮询到连接有 I/O 请求时，才会启动一个线程进行处理。
3. 适用于连接数目多、且连接比较短（轻操作）的架构，比如聊天服务器，但编程比较复杂，所以才有了 Netty 框架来简化编程。

NIO vs BIO：

1. BIO 是阻塞的，NIO 是非阻塞的。
2. BIO 是面向流的，只能单向读写，NIO 是面向缓冲的, 可以双向读写。
3. 使用 BIO 做 Socket 连接时，由于单向读写，当没有数据时，会挂起当前线程，阻塞等待，为防止影响其它连接，需要为每个连接新建线程处理，然而系统资源是有限的，不能过多的新建线程，线程过多带来线程上下文的切换，从来带来更大的性能损耗，因此，需要使用 NIO 进行 BIO 多路复用，使用一个线程来监听所有 Socket 连接，使用本线程或者其他线程处理连接。这就要讲到 Reactor 模式了。

###### 3）主从 Reactor 模式

1. 在高性能 I/O 设计中，有两个比较著名的模式，Reactor 和 Proactor 模式，Reactor 模式用于同步 I/O，而 Proactor 运用于异步 I/O 操作。
2. Reactor，又称之为响应器模式，在线程模型上，又分为单 Reactor 单线程模式、单 Reactor 多线程模式、以及主从 Reactor 多线程模式。

![1646889441630](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646889441630.png)

1. `Reactor` 主线程 `MainReactor` 对象，通过 `selector.select()` 监听连接事件，收到事件后，通过 `Acceptor` 处理连接事件。
2. 当 `Acceptor` 处理连接事件后，`MainReactor` 轮训式地将连接分配给 `SubReactor`，其中，`Reactor` 主线程可以对应多个 `Reactor` 子线程，即 `MainReactor` 可以关联多个 `SubReactor`。
3. `SubReactor` 将连接加入到连接队列进行监听，并创建 `Handler` 进行各种事件处理。
4. 当有新事件发生时，`SubReactor` 就会调用对应的 `Handler` 进行处理。
5. `Handler` 通过 `read()` 读取数据，分发给后面线程池中的 `Worker` 线程进行处理。
6. `Worker` 线程池会分配独立的 worker 线程进行业务处理，并返回结果。
7. `Handler` 收到处理结果后，再通过 `send()` 将结果返回给 Client。

- **优点**：
  1. 父线程与子线程的数据交互简单职责明确，父线程只需要接收新连接，子线程完成后续的业务处理。
  2. 父线程与子线程的数据交互简单，Reactor 主线程只需要把新连接传给子线程，子线程无需返回数据。
- **缺点**：编程复杂度较高。

=> 这种模型在许多项目中广泛使用，包括 Nginx 主从 Reactor 多进程模型，Memcached 主从多线程，而 Netty 也正式采用了主从多线程模型。

###### 4）零拷贝机制

1. Netty 接收和发送的 ByteBuffer，底层采用 `DirectBuffers` 使用堆外内存进行 Socket 读写，无需进行字节缓冲区的二次拷贝。
2. 如果使用传统的堆内存 `HEAP BUFFERS` 进行 Socket 读写，JVM 会将堆内存 Buffer 拷贝一份到直接内存中，然后才写入 Socket 中，此时相比于堆外直接内存，消息在发送过程中多了一次缓冲区的内存拷贝。
3. Netty 提供了组合 Buffer 对象，可以聚合多个 ByteBuffer 对象，用户可以像操作一个 Buffer 那样
   方便，对组合 Buffer 进行操作，避免了传统通过内存拷贝的方式，将几个小 Buffer 合并成一个大的
   Buffer。 
4. Netty 文件传输采用了 `transferTo()` 方法，它可以直接将文件缓冲区的数据发送到目标 Channel，
   避免了传统通过循环 `write()` 方式导致的内存拷贝问题。

##### 3、总

Netty 正是主要基于以上几点的优化，大大简化了 NIO 编程，以及提高了 I/O 通信的性能。

=> 以上，就是我对 Nettty 的一些理解，请问有什么细节需要补充的吗？

#### 6.1.1.5. 自动打包有了解过吗？

有了解过，用的 Jenkis#Pipline 脚本，大致步骤如下：

```shell
node {
   # 1、先指定打包服务器的jdk、maven、git启动类，以及要拉取的代码分支、拉取到的代码的存放地址、打好包后的代码包地址、打包后要发送的目标服务器地址、存放路径等
   def mvnHome
   env.JAVA_HOME="/apps/soft/jdk/jdk1.8.0_201"
   def gitHome="/apps/soft/git"
   def codeHome="/apps/soft/code/cloud_qc/uat/cloud_qc/cloud-qc/"
   def targetHome="/apps/soft/code/cloud_qc/uat/cloud_qc/cloud-qc/domain/cloud-qc-app/"
   def deployHost="10.18.12.148";
   def deployPath="/apps/svr/cloud-qc/uat/backend/cloud-qc-app";
   def branch="cloud-back-dev";
   def artifactId="cloud-qc-app";
   
   dir("${codeHome}"){
   	   # 2、然后，进入第1阶段，使用git，拉取gitlab仓库中指定分支的代码
       stage('Pull Code') {
           // Run the maven build
           sh "'${gitHome}/bin/git' checkout ${branch}"
           sh "'${gitHome}/bin/git' pull"
           mvnHome="/apps/soft/Maven339"
           sh "${mvnHome}/bin/mvn clean"
       }
       # 3、接着，进入第2阶段，并使用maven，对拉取到的代码重新clean、install
       stage('Build Install') {
           // Run the maven build
           if (isUnix()) {
              sh "cd ${codeHome}"
              sh "'${mvnHome}/bin/mvn' -Dmaven.test.skip=true clean install"
           } else {
              echo "none build"
           }
       }
       # 4、再然后，进入第3阶段，使用scp命令，把打好的包发送到目标服务器中的指定地址
       stage('Transfer Jar') {
           // remote transfer
           if (isUnix()) {
              sh "ssh -o StrictHostKeyChecking=no -l apps ${deployHost} mkdir -p ${deployPath}"
              sh "scp ${targetHome}/target/${artifactId}.jar apps@${deployHost}:${deployPath}"
              echo "Transfer Jar Success"
           } else {
              echo "none transfer"
           }
       }
       # 5、最后，使用ssh命令，登录目标服务器，取到指定地址，启动已经写好的应用启动脚本，从而完成一次CI/CD流程（持续集成/持续交付）
       stage('Execute Remote Bash'){
            // sh "ssh -o StrictHostKeyChecking=no -l apps ${deployHost} 'export PATH=/apps/svr/java/jdk1.8.0_191/bin:$PATH; /apps/svr/cloud-qc/uat/backend/cloud-qc-app/server.sh restart' "
            sh "ssh -o StrictHostKeyChecking=no -l apps ${deployHost} 'export PATH=/apps/svr/java/jdk1.8.0_191/bin:$PATH; /apps/svr/cloud-qc/uat/backend/cloud-qc-app/server_skywalking.sh restart' "
            echo "Execute Remote Bash Success"
       }
   }
}
```

#### 6.1.1.6. 低代码平台，或者说代码生成器，有了解过吗，给你设计你怎么设计？

1. 低代码平台，顾名思义就是，把重复的框架代码，交给程序来实现，我们只需要编写核心的业务代码就好。
2. 实现的话，我认为分为 2 个方向。
3. 第一个方向是后端，先根据业务，设计好表，根据表生成对应的实体、mapper、service、controller，以及一些定制化的通用业务代码，这个可以使用 Spring#Freemarker 占位符的方式实现。
4. 第二个方向是前端，简单的页面，也可以采用后端的那种，根据表生成固有的样式，但复杂一些的前端的 UI 以及样式自定义代码，可以通过在界面拖拽的方式，以及样式调配，最终生成与界面视觉一样的代码，JS 代码则可以通过生成重复的框架代码，比如页面加载事件、按钮触发事件、业务通用事件等等。

=> 以上，就是我对实现低代码平台的一些理解，请问有什么细节需要补充的吗？

#### 6.1.1.7. 一张表每月一条记录实现员工日常打卡记录？

1. 先设计表，字段有：主键，员工号，年，月，打卡信息字段。
2. 其中，打卡信息字段是一个整数，其中每一位二进制位，都代表每一天的打卡信息，一个整型为 4 字节，32 bit，刚好可以存放一个月的打卡信息。
3. 假设从低到高为记录，如果一个员工 1 号没来，那么就要使用字段 或上 2^0，以让第 0 位标记为 1，如果一个员工 31 号没来，那么就要使用字段 或上 2^30，以让第 30 位标记为 1，这样，就可以实现用一个字段来记录下该员工一个月的打卡记录了。

#### 6.1.1.8. 怎么用程序模拟浏览器上的手工业务操作，且会经常掉线？

1. 先抓包获取参数，分析业务需要的内容，建立其业务模型。
2. 然后使用程序模拟人工发起连接，并进行相应的业务处理。
3. 如果还要防止时间长被剔除来，则可以通过捕捉异常后，更换 ip 地址或者 token，重新发起连接。

#### 7.1.1.1. CountDownLatch 和 AQS 讲一下？

##### 1、总

1. AbstractQueuedSychronizer，简称 AQS，抽象队列同步器，使用它可以简单、⾼效地构造出，依赖单个原子值表示同步状态的、以及先进先出（FIFO）等待队列的阻塞锁和相关同步器。
2. 支持独占模式（默认）和共享模式，在不同模式下，等待线程同享一个 FIFO 队列。当以独占模式获取时，其他线程尝试获取不会成功。当以共享模式获取时， 其他线程尝试获取可能会成功。
3. 同时，还定义了一个内部的 ConditionObject 类，用作 Lock#Condition 的实现。

##### 2、分

###### 1）AQS 核心思想

![img](file://D:/MyData/yaocs2/AppData/Roaming/Typora/typora-user-images/1630209724753.png?lastModify=1648205707)

1. 如果请求的共享资源空闲，则将当前请求资源的线程，设置为有效的⼯作线程，并且将共享资源设置为锁定状态。
2. 如果请求的共享资源被占⽤，那么就需要⼀套线程阻塞等待、以及被唤醒时进行锁分配的机制，而这个机制 AQS 是通过主等待队列来实现的，即将暂时获取不到锁的线程，加⼊到队列中。
3. 每当线程通过 AQS 获取锁失败时，线程将被封装成一个 Node 节点，通过 CAS 操作，插入到队列尾部。
4. 当有线程释放锁时，AQS 会尝试让队头的后继节点占用锁。

###### 2）AQS 主等待队列

![img](file://D:/MyData/yaocs2/AppData/Roaming/Typora/typora-user-images/1630238528789.png?lastModify=1648205707)

1. AQS 主等待队列，是一个自旋等待队列，基于双向链表实现，每个节点都充当一个特定的通知式的监视器。
2. 要入列，只需要将 Node 结点，通过 CAS 拼接到队列尾部即可。
3. 要出列，则需要设置该结点为 head 结点，在其 Release 同步状态后，后继结点将会收到信号，会尝试获取同步状态，成为在队列中的 head 结点，但并不能保证成功，因为还可能需要与新来的结点，进行非公平竞争。

###### 3）Node 结点的数据结构

![img](file://D:/MyData/yaocs2/AppData/Roaming/Typora/typora-user-images/1630208598316.png?lastModify=1648205707)

1. 持有一个 waitStatus 数值，
   - 1）`初始时 waitStatus = 0`：表示当前节点处于初始状态。
   - 2）`int CANCELLED = 1`：表示线程因为中断或者等待超时，需要从等待队列中取消等待。
   - 3）`int SIGNAL= -1`：表示后继节点处于等待状态，当前节点如果释放了同步资源或者被取消，需要通知唤醒后继节点，让后继节点的线程得以重新运行。
   - 4）`int CONDITION = -2`：表示线程在条件队列中阻塞，当其他线程调用了 `ConditionObject#signal()` 方法后，被阻塞的线程会从条件队列，转移到 AQS 主等待队列上，去参与排队竞争。
   - 5）`int PROPAGATE = -3`：表示共享状态会被无条件地传播下去，让后面的 N 个节点都进行工作，从而实现共享锁被 N 个线程同时持有。
2. 一个 thread 指针：表示当前结点的线程引用。
3. 一个 nextWaiter 指针：
   - 1）如果当前结点为条件等待队列的结点，那么该值应该指向条件队列的后继结点。
   - 2）如果当前结点为 AQS 主等待队列的结点，那么该值只是作为独占模式的标记 `Node EXCLUSIVE = null`，或者共享模式的标记 `Node SHARED = new Node()` 。
4. 一个 prev 指针：表示前驱结点，用来处理取消结点，如果一个结点被取消，那么它的后继，会往前找到一个未取消的前驱，并重新链接、以及判断前驱是否为头结点，以让后继结点执行一次尝试获取同步资源的操作。
5. 以及一个 next 指针：表示后继结点，用来实现阻塞与通知机制，在前驱结点释放同步资源后，通过 next 指针，来确定唤醒哪个线程。

###### 4）AQS 扩展原理

1. 在 AQS 中，维持了一个 `private volatile int state`，表示同步资源状态，作为同步器子类实现的共享资源状态。
2. 以及提供几个钩子方法，让同步器子类去扩展，默认抛出 UnsupportedOperationException 异常。
   - 1）`tryAcquire(int)`：独占锁钩子，尝试获取独占资源。若成功则返回 true，若失败则返回 false 。
   - 2）`tryRelease(int)`：独占锁钩子，尝试释放独占资源。若成功则返回 true，若失败则返回 false 。
   - 3）`tryAcquireShared(int)`：共享锁钩子，尝试获取共享资源。负数，表示获取失败；正数，表示成功，且有剩余资源；0，也表示成功，但代表没有剩余可用资源。
   - 4）`tryReleaseShared(int)`：共享锁钩子，尝试释放共享资源。若成功则返回 true，若失败则返回 false 。
   - 5）`isHeldExclusively()`：独占锁钩子，判断当前线程，是否正在独占资源。
3. 而 AQS 其他方法，则都是 final 类型，无法再被子类重写，从而抽象队列同步框架的封装与扩展。

###### 5）独占模式实现原理

![img](file://D:/MyData/yaocs2/AppData/Roaming/Typora/typora-user-images/1630209980487.png?lastModify=1648205707)

![img](file://D:/MyData/yaocs2/AppData/Roaming/Typora/typora-user-images/1630210001133.png?lastModify=1648205707)

1. 独占模式下，`tryAcquire(int)` 钩子返回的是 boolean 值，其含义取决于实现类具体的语义。主要思路是，如果要实现独占，就要保证，在获取同步资源时，如果已经存在了独占线程，那么就要返回 false，如果没存在独占线程，则可以返回 true。

2. 而在获取同步资源失败的，则要生成 Node 结点，加入 AQS 主队列中，参与排队竞争，竞争失败的，则将前驱结点设置为 `int SIGNAL= -1` 状态，然后阻塞。

3. 在前驱结点释放同步资源后，由于被后继设置为了  `int SIGNAL= -1` 状态，所以会唤醒后继结点，重新参与AQS 竞争。

4. 源码解析 - 普通式获取原理：`acquire(int arg)`，

   - 1）调用钩子方法 `tryAcquire(int)`，尝试以独占模式获取同步资源，如果获取成功，则直接返回即可。
   - 2）如果获取失败，则把当前线程，构建成独占模式的 Node 结点，通过 CAS+自旋的方式入队。
   - 3）入队成功后，则调用 `acquireQueued(final Node node, int arg)` 方法，自旋判断前驱是否为 head 头结点，如果是，则执行一次抢占同步资源的操作，抢占成功，则更新为 head 头结点（此时旧 head 结点脱钩将来会被 GC 掉），并返回 interrupted 中断标记（该方法唯一返回入口）。
   - 4）如果发现前驱不是 head 结点，或者抢占同步资源失败，则调用 `shouldParkAfterFailedAcquire(Node pred, Node node)` 方法，更新前驱为 `int SIGNAL= -1`，代表在该前驱释放同步资源成功后，需要唤醒当前线程。
   - 5）接着，使用 `LockSupport.park(this)` 来阻塞当前线程。
   - 6）最后，被 `LockSupport.unpark(当前线程的实例)` 唤醒后，则检查线程中断状态，重新自旋判断前驱、抢占锁、或者继续阻塞。

5. 源码解析 - 可中断式获取原理：`acquireInterruptibly(int arg)`，对比普通式获取独占资源的方法，该方法主要不同的地方在于，

   - 1）第一是，调用钩子方法 `tryAcquire(int)`，尝试以独占模式获取同步资源前，会先检查线程是否被中断过，如果是，则会提前抛出中断异常，阻止继续往下执行。
   - 2）第二是，在抢占同步资源失败后，调用的是  `doAcquireInterruptibly()` 方法，来对结点进行入队。
   - 3）第三是，在自旋判断到前驱是否为 head 头结点，并成功抢占同步资源后，并不会返回任何值。
   - 4）最后是，在被 `LockSupport.unpark(当前线程的实例)` 唤醒后，会先检查线程的中断状态，如果发现线程已被中断了，则会立马抛出中断异常。

   => 可见，普通式获取返回的是中断标记，只能在方法调用返回后，才能做线程中断响应，而可中断式获取无任何返回值，是在方法调用过程中，做的线程中断响应，即抛出中断异常。这也正是两者之间最大的区别。

6. 源码解析 - 定时式获取原理：`tryAcquireNanos(int arg, long nanosTimeout)`，对比普通式获取独占资源的方法，该方法主要不同的地方在于，

   - 1）第一是，调用钩子方法 `tryAcquire(int)`，尝试以独占模式获取同步资源前，会先检查线程是否被中断过，如果是，则会提前抛出中断异常，阻止继续往下执行。
   - 2）第二是，在抢占同步资源失败后，调用 `doAcquireNanos(int arg, long nanosTimeout)` 方法来入队结点，并且根据当前系统纳秒时间，来计算剩余的过期时间。
   - 3）第三是接着，自旋判断到前驱为 head 头结点，并成功抢占同步资源后，会返回 true，代表在规定时间内，成功获取到了同步资源。
   - 4）第四是，如果前驱不为 head 头结点，或者抢锁同步资源失败，则会先根据当前系统纳秒时间，来计算剩余的过期时间，并且如果发现超过了定时时间，则会返回 false，代表未能在规定时间内，获取到同步资源。
   - 5）第五是，如果还没超过定时时间，则再调用 `LockSupport.parkNanos(this, nanosTimeout)` 来定时阻塞当前线程。
   - 6）最后是，被 `LockSupport.unpark(当前线程的实例)` 唤醒后，会先检查线程的中断状态，如果发现线程已被中断，则会立马抛出中断异常。

   => 可见，定时式获取与可中断式获取，最大的不同在于，有设置剩余等待时间，并且返回的 boolean 值，代表是否能够在规定时间内，获取到同步资源。

7. 源码解析 - 独占资源释放原理：`release(int arg)`，

   - 1）调用钩子方法 `tryRelease(int)`，尝试释放独占资源，如果释放失败，则直接返回 false 即可，代表释放失败。
   - 2）如果释放成功，则判断 head 头结点，如果 head 头结点为空，或者为独占脱钩，说明队列没有其他结点，则返回 true，代表释放成功。
   - 3）如果头结点为正常的业务结点，则还需要调用 `unparkSuccessor(head)` 方法，来唤醒后续排队的结点，唤醒后则返回 true，代表释放成功。
     1. 其中， `unparkSuccessor(head)` 方法，并不会更新后继为 head 头结点，而仅仅是更新头结点的waitStatus = 0、以及唤醒后继结点。
     2. 而更新 head 头结点、以及脱钩当前 head 头结点，是在后继获取同步资源成功后，才进行更新，这样可以保证由后继线程自己来确保成为头结点，比放在释放方法里更新要严谨一些。

###### 6）共享模式实现原理

1. 共享模式下，`tryAcquireShared(int)` 返回的是 int 值，其含义取决于实现类定义的语义。主要思路是，使用 `int PROPAGATE = -3`，来表示共享状态会被无条件地传播下去，让后面的 N 个节点都进行工作，从而实现共享锁被 N 个线程同时持有。

2. 其中，`int PROPAGATE = -3` 相当于一个占位的作用：
  - 1）如果结点不是最后一个获取到同步资源的，则状态为 `int PROPAGATE = -3` 。
  - 2）而如果结点是最后一个获取到同步资源的，则状态一开始为 0，不过很快就会在下一个结点进入阻塞前，设置为 `int SIGNAL= -1` 表示后继节点处于等待状态，当前节点如果释放了同步资源或者被取消，需要通知唤醒后继节点，让后继节点的线程得以重新运行。
  - 3）而当释放共享锁时，如果是 `int PROPAGATE = -3` 结点释放的，则无任何唤醒后继结点，如果是`int SIGNAL= -1` 结点释放的，则会唤醒后继结点，接着又会重复，一片非最后的 `int PROPAGATE = -3` 结点以及最后一个为  `int SIGNAL= -1` 的结点，然后前面的操作。

3. 源码解析 - 普通式获取原理：`acquireShared(int arg)`，对比普通独占获取资源方法，该方法主要不同的地方在于，

   - 1）第一是，调用 `tryAcquireShared(int)` 钩子方法，尝试获取共享资源，如果获取到的数量大于等于 0，说明获取成功，直接返回即可。
   - 2）第二是，如果获取到的数量小于 0 时，说明获取失败，则需要调用 `doAcquireShared(int arg)` 方法，来入队结点，但此时结点的 nextWaiter 不再是为 null，而是 SHARED，代表为共享模式的结点。
   - 3）第三是，接着，自旋判断到前驱为 head 头结点，是的话则进行一次尝试获取共享资源操作。如果获取的数量大于等于 0，说明获取成功，则调用 `setHeadAndPropagate(Node node, int propagate)` 方法，更新当前结点为 head 头结点。再调用 `doReleaseShared()` 方法，设置当前结点为 `int PROPAGATE = -3`，表示下一个线程获取共享资源后，当前的共享状态需要被无条件地传播下去，以通知其他等待的线程尽快获取共享资源。共享状态传播完毕后，返回之前，需要判断当前线程是否有被中断，是的话则设置线程中断标志再返回。
   - 4）最后是，被 `LockSupport.unpark(当前线程的实例)` 唤醒后，会先检查线程的中断状态，如果发现线程已被中断，不会抛出中断异常，而是更新代码局部变量中的中断标志位为 true，待自旋获取同步资源成功后，设置线程中断标志位处理。

   => 可见，共享式获取与独占式获取，最大的不同在于，入队获取共享资源成功后，是先设置当前结点为 `int PROPAGATE = -3`，然后再释放掉共享资源，进行一个共享状态的传播。

4. 源码解析 - 可中断式获取原理：`acquireSharedInterruptibly(int arg)`，对比普通式获取共享资源的方法，该方法主要不同的地方在于，

   - 1）第一是，调用 `tryAcquireShared(int)` 钩子方法，尝试获取共享资源前，会先检查线程是否被中断过，如果是则会提前抛出中断异常，阻止继续往下执行。
   - 2）第二是，在抢占失败一次后，调用的是 `doAcquireSharedInterruptibly()` 方法，来入队结点。
   - 3）第三是，接着自旋判断到前驱为头结点，在尝试获取共享资源成功，并传播完毕后，返回之前不会判断当前线程是否有被中断，而是直接返回，没有任何返回值。
   - 4）第四是，被 `LockSupport.unpark(当前线程的实例)` 唤醒后，会先检查线程中断状态，如果发现线程已被中断，则会立马抛出中断异常。

   => 可见，普通式获取返回的是中断标记，只能在方法调用返回后，才能做线程中断响应，而可中断式获取无任何返回值，是在方法调用过程中，做的线程中断响应，即抛出中断异常。这也正是两者之间最大的区别。

5. 源码解析 - 定时式获取原理：`tryAcquireSharedNanos(int arg, long nanosTimeout)`，对比普通式获取共享资源的方法，该方法主要不同的地方在于，

   - 1）第一是，调用 `tryAcquireShared(int)` 钩子方法，尝试获取同步资源前，会先检查线程是否被中断过，如果是则会提前抛出中断异常，阻止继续往下执行。
   - 2）第二是，在抢占失败一次后，调用的是 `doAcquireSharedNanos(int arg, long nanosTimeout)` 方法，来入队结点，并且根据当前系统纳秒时间，来计算剩余过期时间，如果定时时间小于等于 0，则返回 false，代表未能在规定时间内获取到同步资源。
   - 3）第三是，自旋判断到前驱为 head 头结点，再尝试获取同步资源，获取成功并传播完毕后，返回之前并不会判断当前线程是否有被中断，而是返回 true，代表在规定时间内成功获取到了同步资源。
   - 4）第四是，如果前驱不为 head 头结点，或者抢占同步资源失败，则会继续根据当前系统纳秒时间，计算剩余过期时间，如果发现超过了定时时间，则会返回 false，代表未能在规定时间内获取到同步资源。
   - 5）第五是，如果最后前没超过定时时间，则调用的是 `LockSupport.parkNanos(this, nanosTimeout)` 来定时阻塞当前线程。
   - 6）最后是，被 `LockSupport.unpark(当前线程的实例)` 被唤醒后，会先检查线程中断状态，如果发现线程已被中断，则会立马抛出中断异常。

   => 可见，定时式获取与可中断式获取，最大的不同在于，有设置剩余等待时间，并且返回的 boolean 值，代表是否能够在规定时间内，获取到同步资源。

6. 源码解析 - 共享资源释放原理：`releaseShared(int arg)`，对比独占资源释放方法，该方法主要不同的地方在于，

   - 1）第一是，调用钩子方法 `tryAcquireShared(int)`，尝试释放共享资源，如果释放失败，则直接返回 false 即可，代表释放失败。
   - 2）第二是，如果释放成功，则判断 head 头结点，如果 head 头结点为 `int SIGNAL= -1` 结点，则需要调用 `unparkSuccessor(head)` 方法，来唤醒后续排队的结点，唤醒后则返回 true，代表释放成功。
   - 3）最后是，如果头结点不为 `int SIGNAL= -1` 结点，而是脱钩独占结点，则通过 CAS 更新它为 `int PROPAGATE = -3`，表示当前走的是共享释放，要做共享状态的传播，然后返回 true，代表释放成功。

   => 可见，共享式释放与独占式释放，最大的不同在于，如果做的不是唤醒后继，则要把当前结点标识为 `int PROPAGATE = -3`，表示走共享释放，做共享状态传播。

###### 7）AQS 条件队列

![img](file://D:/MyData/yaocs2/AppData/Roaming/Typora/typora-user-images/1630238750935.png?lastModify=1648205707)

1. 将条件队列之前，还要讲一下 Condition 接口，
   - 1）Condition，是 JUC 用来替代传统 `Object#wait()/notify()` 线程间通信与协作机制的新组件，其作用与 `Object#wait()/notify()` 相似，都会让一个线程等待某个条件，只有当该条件具备 `signal()/signalAll()` 方法被调用时，阻塞等待的线程才会被唤醒，重新去争夺锁。
   - 2）但不同的是，`Object#wait()/notify()` 由 JVM 底层实现，而 Condition 实现类，完全使用 Java 代码来实现。当需要进行线程间通信时，相比调用 `Object#wait()/notify()`，调用 `ConditionObject#await()/signal()` 这种方式实现起来更加高效。
2. 而 AQS 内部类的 ConditionObject 类，正是实现 Condition 接口的关键，
   - 1）每个 ConditionObject 对象，都维护了一个单独的 AQS 条件队列。
   - 2）AQS 条件队列是单向的，而 AQS 主等待队列是双向的，他们是聚合的关系。
   - 3）因此，在一个显式锁上，可以创建多个 AQS 条件队列，而 Java 内置锁，只有唯一的一个等待队列。
   - 4）AQS 条件队列，也使用了相同的 Node 结点，但额外维护了一个 nextWaiter 指针，在调用`ConditionObject#await()` 时，会把一个 ConditionObject 结点，插入到条件队列中，然后等到 `ConditionObject#signal()` 信号，该结点会被转移到 AQS 主等待队列中，去参与同步资源竞争，竞争失败的会被阻塞，然后依赖 AQS 排队与通知机制实现唤醒。

###### 8）条件等待与唤醒原理

![img](file://D:/MyData/yaocs2/AppData/Roaming/Typora/typora-user-images/1630210049637.png?lastModify=1648205707)

![img](file://D:/MyData/yaocs2/AppData/Roaming/Typora/typora-user-images/1630210030025.png?lastModify=1648205707)

1. ConditionObject 数据结构：
   - 1）firstWaiter：条件队列的头节点。
   - 2）lastWaiter：条件队列的尾节点。

2. 源码解析 - 条件等待原理：（类比于 `Object#wait()` 与 `Object#notify()/notifyAll()`）

   ![1629615372409](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629615372409.png)

   - 1）首先，会创建一个 Condition 类型的 Node 结点，即 Condition 结点，并放入条件队列的队尾。
   - 2）然后，调用 `release(int arg)` 方法释放所有独占资源。
     - 相当于 `Object#wait()` 释放内置锁资源。
   - 3）接着，判断 Condition 结点是否已经在主等待队列中，如果不是，则将当前线程阻塞，直到被调用 `doSignal()` 方法唤醒。
     - 相当于 `Object#wait()` 调用后，进入到 _WaitSet 等待队列中。
   - 4）如果被 `doSignal()` 放入了主等待队列（waitStatus 会被置为 0），并唤醒当前线程，则调用 `acquireQueued(final Node node, int arg)` 方法，获取独占资源，抢到则返回，没抢到则被 `LockSupport.part(this)` 给阻塞住，直到被 AQS 主等待队列的前驱唤醒。
     - 相当于 `Object#notify()/notifyAll()` 随机唤醒 _WaitSet 中的线程，并将它放入到 EntryList 候选竞争队列中。
   - 5）如果抢占到独占资源，  `acquireQueued(final Node node, int arg)` 方法返回，则先检查返回值，如果为 true，说明争抢期间发生了中断，则响应中断。
     - 相当于线程在 EntryList 中，竞争到锁资源，成为了 Owner Thread。

3. 源码解析 - 定时式条件等待原理：对比普通条件等待方法，该方法主要不同的地方在于：

   - 1）第一是，在创建 Condition 结点前，会先校验线程是否已中断，如果是，则会提前抛出中断异常，阻止继续往下执行。
   - 2）第二是，接着，会先根据系统纳秒时间，计算出超时时间，如果没有超时，则调用 `LockSupport.parkNanos(this, nanosTimeout)` 方法，来定时阻塞当前线程。
   - 3）第三是，如果发生了超时，则会将 Condition 结点，转移到 AQS 主等待队列中（waitStatus 会被置为 0），并退出 while 循环，调用 `acquireQueued(final Node node, int arg)` 去争抢同步资源，抢到则返回，没抢到则被 `LockSupport.part(this)` 给阻塞住，直到被 AQS 主等待队列的前驱唤醒。
   - 4）最后是，如果抢占到独占资源，  `acquireQueued(final Node node, int arg)` 方法返回，则先检查返回值，如果为 true，说明争抢期间发生了中断，则响应中断，要计算剩余的超时时间，并返回。

   => 可见，定时式条件等待与普通等待，最大的不同在于，定时等待会定时阻塞当前线程，发生超时或者被 `doSignal()` 唤醒，都会将 Condition 结点转移到 AQS 主等待队列、并退出 while 循环，还有的不同就是，定时等待会有返回值，表示剩余超时时间，而普通等待方法，则只会响应中断，没有任何返回值。

4. 源码解析 - 条件唤醒原理：

   - 1）先通过 CAS 的方式，将条件队列中的第一个 Condition 结点的 waitStatus 更新 0。
   - 2）然后，调用 `enq()` 方法，将结点转移到 AQS 主等待队列中，并设置前驱为 `int SIGNAL= -1`。
   - 3）最后，调用 `LockSupport.unpark(node.thread)` 唤醒 Condition 结点的线程，并返回 true，从而实现通知唤醒 Condition 结点的作用。
      - 相当于 `Object#notify()/notifyAll()` 随机唤醒 _WaitSet 中的线程，并将它放入到 EntryList 候选竞争队列中。

##### 3、总

基于 AQS 以上原理，JUC 中的典型实现有，

![img](file://D:/MyData/yaocs2/AppData/Roaming/Typora/typora-user-images/1630201659838.png?lastModify=1648205707)

###### 1）CountDownLatch

1. CountDownLatch，门闩，它允许一个或多个线程等待，直到一组操作执行完成后，才唤醒这些线程。
2. 它使用一个给定的计数值，进行初始化，调用 `await()` 方法可以使线程阻塞，直到计数被 `countDown()` 方法调用至零后，所有阻塞等待线程才会被唤醒，并且门闩使用一次后，任何 `await()` 再被调用时，会立即返回。
3. 因此，门闩计数无法被重置，是一个一次性的同步器，如果需要重置计数的实现，可以使用 CyclicBarrier。

它有几个核心方法，

1. 第一个是，构造方法，构造方法中，传入一个整数 n，作为 AQS 共享资源。
2. 第二个是， `await()` 方法，一般由主线程调用、并阻塞等待各工作线程签到完成，其实现原理为：
   - 1）底层调用了 AQS 的 `doAcquireSharedInterruptibly(arg)` 可中断式获取共享资源，该方法会先去调用钩子方法。
   - 2）由于 CountDownLatch#Sync 实现了 `tryAcquireShared(int)` 共享资源获取钩子，如果计数为 0，则返回 1，代表共享资源获取成功，则会传播式地唤醒一个又一个的线程，释放全部等待的线程。
   - 3）如果计数不为 0，则返回 -1，代表共享资源获取失败，则会阻塞所有调用的线程。
3. 第三个是，`countDown()` 方法，一般由子线程调用，表示当前线程工作已完成并签到一下，其实现原理为：
   - 1）底层调用了 AQS 的 `releaseShared(int arg)` 释放共享资源方法，该方法会先去调用钩子方法。
   - 2）由于 CountDownLatch#Sync 实现了 `tryReleaseShared(int)` 共享资源释放钩子，如果计数为 0，则返回 false，代表门闩已用完，不能重复使用。
   - 3）如果计数不为 0，则计数 -1，如果还没减到 0，则返回 false，代表当前线程还不是最后一个到达的线程，此时不会做任何唤醒操作。
   - 4）如果减到了 0，则返回 true，代表当前线程为最后一个到达的线程，然后回调 AQS 的 `doReleaseShared()` 方法，传播式唤醒所有等待线程。

###### 2）CyclicBarrier

1. CyclicBarrier，栅栏，它允许一组线程全部到达公共栅栏点后，执行一次指定栅栏任务，相对于 CountDownLatch，CyclicBarrier 的计数是可以被重置的，可以被重复使用。
2. 核心方法有 1 个，为 `await()` 方法，调用该方法的线程，在没符合条件时会阻塞住，直到栅栏条件达成后被唤醒。其实现原理为：
   - 1）先阻塞式获取 ReentrantLock 显示锁，获取到后，则获取分代实例 Generation，如果发现Generation 已经损坏，则抛出 BrokenBarrierException 异常。
   - 2）如果 Generation 还没损坏，则再检验当前线程的中断状态，如果发生了中断，则破坏当前 Generation，重置栅栏的参与线程数，并唤醒所有阻塞等待的线程。
   - 3）如果 Generation 还没损坏，且当前线程没被中断，说明当前线程达到了栅栏，因此栅栏的线程参与数 -1。
   - 4）如果计数减少到了 0，说明当前线程为最后一个到达栅栏的线程，如果有指定栅栏任务，则运行一次栅栏任务，然后生成下一代 Generation，重置栅栏的参与线程数，并唤醒所有阻塞等待的线程，并返回 0，代表当前线程到达栅栏的最后索引。
   - 5）如果计数还没减少到 0，则开始自旋，如果不需要超时，则调用 `Condiction#await()` 方法，阻塞当前线程，如果需要计时，且还没发生超时，则调用 `Condiction#awaitNanos()` 定时阻塞当前线程，因为这俩方法会释放显示锁的所有独占资源，以让其他线程能够顺利抢到显示锁。
   - 6）而在定时阻塞等待期间，任意一个线程发生了中断，则会破坏当前 Generation，唤醒所有阻塞等待的线程，并抛出异常。
   - 7）如果期间没发生中断，当前线程唤醒后，如果发现 Generation 已被破坏了，则抛出 BrokenBarrierException 异常，如果 Generation 没被破坏，但已被更新，说明最后一个线程确实到达了栅栏，则返回当前线程的到达索引。如果 Generation 没被破坏也没被更新，说明当前线程发生了等待超时，则破坏当前 Generation，唤醒所有阻塞等待线程，并抛出超时异常 TimeoutException。
   - 8）最后，释放 ReentrantLock 显示锁。

###### 3）ReentrantLock

1. ReentrantLock，Lock 接口的可重入、互斥锁实现，被成为显式锁，与使用 synchronized 具有基本相同的行为和语义，但功能更丰富，比如支持公平锁、定时式获取、锁持有检查等等。
2. 支持公平锁的意思是，在 ReentrantLock 构造时，传入 true，在锁争用时，锁会等待时间最长的线程获取到，可以避免线程饥饿问题，但会降低整体的吞吐量。

它的核心方法主要有 3 个。

1. 首先说下，非公平、阻塞式的抢锁原理：`NonfairSync#lock()`，
   - 1）先使用 CAS，尝试更新同步资源（这里代表的是锁次数） 为 1，只有锁次数为 0，才会更新成功，说明当前线程非公平地提前抢到了锁。
   - 2）如果没抢到，则调用 AQS 的 `acquire(int arg)` 方法，老老实实地走 AQS 排队流程。
   - 3）由于 ReentrantLock#NonfairSync 实现了 `tryAcquire(int acquires)` 钩子方法，该方法底层调用 `Sync#nonfairTryAcquire(int acquires)` 方法。
   - 4）`Sync#nonfairTryAcquire(int acquires)` 方法，可以为 ReentrantLock 提供公共的、非公平的、非阻塞的尝试获取独占资源功能，
     1. 首先，它获取锁次数，如果锁次数为 0，说明当前线程第一次抢锁，则 CAS 更新锁次数为 1，代表第一次就抢锁成功，并设置当前线程为独占线程，然后返回 true，代表获取独占资源成功，AQS `acquire(int arg)` 方法回调，则什么也不做。
     2. 如果锁次数不为 0，但独占线程为当前线程，说明发生了锁重入，则累加锁次数 + 1，然后返回true，代表获取独占资源成功，AQS `acquire(int arg)` 方法回调，则什么也不做。
     3. 如果锁次数不为 0，且独占线程也不为当前线程，则返回 false，代表独占资源获取失败，需要等待独占线程释放资源，AQS `acquire(int arg)` 方法回调，阻塞当前线程，从而实现 ReentrantLock 非公平、阻塞式地抢锁。
2. 然后就是，公平、阻塞式的抢锁原理：`FairSync#lock()`，对比非公平、阻塞式获取锁的方法，该方法不同的地方在于，
   - 1）第一是，它一开始没有使用 CAS 尝试获取锁次数，而是直接调用了 AQS 的 `acquire(int arg)` 方法，老老实实地走 AQS 排队流程。
   - 2）第二是，由于 ReentrantLock#FairSync 实现了 `tryAcquire(int acquires)` 钩子方法，该方法先获取锁次数，如果锁次数为 0，说明当前线程第一次抢锁，不同的是，它会先判断 AQS 主等待队列中，有没有存在其他排队的线程，如果不存在，才 CAS 更新锁次数为 1，代表第一次就抢锁成功，并设置当前线程为独占线程，然后返回 true，代表获取独占资源成功，AQS `acquire(int arg)` 方法回调，则什么也不做，从而实现公平按排队、顺序抢锁的功能。
3. 最后就是，释放锁的原理：`ReentrantLock#unlock()`。
   - 1）该方法底层调用了 AQS 的 `release(int arg)` 方法，由 ReentrantLock#Sync 实现了 `tryRelease(int acquires)` 钩子方法，为 NonfairSync 和 FairSync，提供释放独占资源的公共方法。
   - 2）该方法先通过当前锁次数 - 1，计算剩余锁次数。
   - 3）然后判断当前线程是否为独占线程，如果不是，则抛出 IllegalMonitorStateException 异常。
   - 4）如果是独占线程，则判断剩余锁次数是否为 0，是的话，则清空独占线程、重置锁次数，并返回 true，代表锁释放成功。
   - 5）如果剩余锁次数不是 0，则更新锁次数为剩余锁次数，即 -1，然后返回 false，代表锁仍被当前线程持有。

###### 4）ReentrantReadWriteLock

1. ReentrantReadWriteLock，读写锁，支持与 ReentrantLock 类似的语义，同样也支持公平和非公平、定时式获取、可重入等功能。
2. 但重入规则不同的是，只支持锁降级，不支持锁升级，即支持从写锁降为读锁，但不支持读锁升级为写锁。
3. 并且，只有写锁提供了 Condition 的支持，读锁并不支持。
4. 以及能够支持更高的读并发，适用于预期集合很大、读线程比写入线程多、读取时需要的开销大于写同步开销的场景。

它的核心方法主要有如下几个，分别是阻塞式获取非公平、公平写锁、非公平、公平读锁、释放写锁、释放读锁。

1. 首先，读写状态只使用了一个 int 整数来存储，高 16 位作为读次数，低 16 位作为写次数。
2. 然后就是，阻塞式获取非公平、公平写锁的原理：`WriteLock#lock()`，
   - 1）该方法底层调用了 AQS 的 `acquire(int arg)` 方法，由 ReentrantReadWriteLock#Sync 实现了 `tryAcquire(int acquires)` 钩子方法，为 NonfairSync 和 FairSync，提供获取独占资源的公共方法。
   - 2）`tryAcquire(int acquires)` 先获取整体的锁状态，以及写锁次数，如果写锁次数为 0，如果锁状态不为 0，且写锁次数为 0，或者独占线程不是当前线程，说明存在读锁（读写要互斥），或者存在别的写锁（写写互斥），则返回false，代表写锁获取失败。
   - 3）如果锁状态不为 0，但写锁次数为 0，且独占线程就是当前线程，说明为当前线程重入获取写锁，则低 16 位写锁次数 +1，然后更新锁状态，返回 true，代表写锁重入成功。
   - 4）如果锁状态为 0，说明不存在任何读锁和写锁，则要看 `writerShouldBlock()` 钩子怎么实现，这里用于控制非公平和公平的逻辑：
     1. 如果实现的是非公平写锁，则要看 `NonfairSync#writerShouldBlock()`，该方法的实现只是直接返回 false，认为写锁获取时，无需任何阻塞等待，因为实现的是非公平锁。
     2. 如果实现的是公平写锁，则要看 `FairSyncwriterShouldBlock()`，该方法的实现是，看 AQS 主等待队列 head 头结点是否存在后继线程，如果存在，则返回 true，代表公平地排队获取写锁，否则返回 false，代表没有线程在排队，可以作为第一个线程，获取到写锁。
   - 5）因此，上述方法回调后会这样走，通过 CAS，更新写锁次数 + 1，设置独占线程为当前线程，然后返回 true，代表写锁获取成功。
3. 接着就是，阻塞式获取非公平、公平读锁的原理：`ReadLock#lock()`，
   - 1）该方法底层调用了 AQS 的 `acquireShared(int arg)` 方法，由 ReentrantReadWriteLock#Sync 实现了 `tryAcquireShared(int unused)` 钩子方法，为 NonfairSync 和 FairSync，提供获取共享资源的公共方法。
   - 2）`tryAcquireShared(int unused)` 先获取整体的锁状态，以及读锁次数、写锁次数，如果写锁次数不为 0，且独占线程不是当前线程，则返回 -1，说明存在写锁，且写锁不是自己锁持有，代表读锁获取失败（读写互斥）。
   - 3）如果写锁次数为 0，或者独占线程就是当前线程，说明不存在写锁，或者当前线程正在持有写锁，可以获取读锁（锁降级），此时要看 `readerShouldBlock` 钩子怎么实现，这里用于控制非公平和公平的逻辑：
     1. 如果实现的是非公平锁，则要看 `NonfairSync#readerShouldBlock()`，该方法的实现是，看第一个后继结点是否为独占模式，如果是的话，则返回 true，认为如果后一位在申请写锁，下次就要先轮到写锁，即使当前是非公平获取读锁，也去 AQS 主等待队列中排队等待，如果否的话，则返回 false，认为后一个没有在申请写锁，就可以先非公平获取读锁，无需进入 AQS 主等待队列排队。
     2. 如果实现的是公平锁，则要看 `FairSync#readerShouldBlock()`，该方法的实现是，看 AQS 主等待队列 head 头结点是否存在后继线程，如果存在，则返回 true，代表公平地排队获取写锁，否则返回 false，代表没有线程在排队，可以作为第一个线程，获取到读锁。
   - 4）因此，上述方法回调后会这样走，如果获取读锁不需要被阻塞，则 CAS 更新高 16 位读锁次数 +1，然后看之前的读锁次数是否为 0，如果是的话，则认为当前线程是第一次获取读锁，则用 `firstReader` 标记好，如果是当前线程重入获取读锁，则更新一级缓存中的读锁次数 + 1，否则说明当前线程是在重入别人的读锁，则更新二级缓存中的读锁次数 + 1，然后返回 1，代表读锁获取成功，实现非公平获取读锁。
   - 5）如果获取读锁需要被阻塞，或者 CAS 更新高 16 位读锁次数 + 1失败，则调用 `fullTryAcquireShared(Thread current)` 开始走自旋更新，大体逻辑与不被阻塞的类似，都是为了更新 `firstReader` 第一次获取读锁标记、一级缓存、二级缓存中的读锁次数，不过不一样的是，如果自旋过程中发现存在别线程获取到了写锁，或者当前线程是在第一次重入别人的读锁，那么就需要返回 -1，代表读锁获取失败，需要进入到 AQS 主等待队列中排队，实现公平获取读锁。
4. 再然后就是，释放写锁的原理：`WriteLock#unlock()`，
   - 1）该方法底层调用了 AQS 的 `release(int arg)` 方法，由 ReentrantReadWriteLock#Sync 实现了 `tryRelease(int releases)` 钩子方法，为 NonfairSync 和 FairSync，提供释放独占资源的公共方法。
   - 2）`tryRelease(int releases)` 先判断独占线程是否为当前线程，如果不是的话，则抛出IllegalMonitorStateException 异常，代表不能释放别人的写锁。
   - 3）如果独占线程就是当前线程，则更新低 16 位的写锁次数 - 1，如果更新后的写锁次数为 0，说明写锁释放了，则清空独占线程、更新整体锁状态，然后返回 0，代表写锁释放成功，最后回调到 AQS 中，通知唤醒 AQS 主等待队列中的后继结点。
   - 4）如果更新后的写锁次数不为 0，说明写锁还被当前线程持有，则更新整体锁状态，然后返回剩余写锁次数，代表写锁仍未释放，最后回调到 AQS 中，但不通知唤醒 AQS 主等待队列中的后继结点，什么也不做。
5. 最后就是，释放读锁的原理：`ReadLock#unlock()`，
   - 1）该方法底层调用了 AQS 的 `releaseShared(int arg)` 方法，由 ReentrantReadWriteLock#Sync 实现了 `tryReleaseShared(int unused)` 钩子方法，为 NonfairSync 和 FairSync，提供释放共享资源的公共方法。
   - 2）`tryReleaseShared(int unused)` 方法，先判断当前线程是否在释放自己的读锁，如果是第一次释放，则清空 `firstReader` 第一次读锁标记，如果是其他次释放，则更新一级缓存中的读锁次数 - 1，而如果不是在释放自己的读锁，则更新二级缓存中的读锁次数 - 1。
   - 3）然后开始自旋，先更新高 16 位的读锁次数 - 1，再 CAS 更新整体锁状态，更新失败的，则继续自旋，更新成功的，则返回剩余读锁次数是否为 0。
   - 4）如果是的话，则返回 true，代表读锁释放成功，最后回调到 AQS 中，传播式通知唤醒 AQS 主等待队列中的后继结点。
   - 5）如果不是的话，则返回 false，代表读锁释放失败，最后回调到 AQS 中，但不通知唤醒 AQS 主等待队列中的后继结点，什么也不做。

###### 5）Semaphore

1. Semaphore，计数信号量，它允许维护一组信号量许可，调用 `acquire()` 阻塞获取信号量许可，调用 `release()` 归还一个信号量许可，也支持非公平和公平模式，通常用于限制访问某些资源的线程数。
2. 当初始化为 1 个许可时，可用作互斥锁（二元信号量），但这种互斥锁与 ReentrantLock 显示锁不同，因为这种锁的许可，可以被其他线程释放掉，可以用作死锁的恢复。

它有几个主要的方法，

1. 首先是，构造方法，允许传入一个 int 整数，作为同步资源，在这里指的是信号量许可。
2. 然后就是，阻塞、非公平获取 n 个信号量许可的实现原理：`acquire(int permits)`，
   - 1）该方法底层调用了 AQS 的 `acquireSharedInterruptibly(int arg)` 方法，由 Semaphore#NonfairSync 实现了 `tryAcquireShared(int acquires)` 钩子方法，调用 Semaphore#Sync 的 `nonfairTryAcquireShared(int acquires)` 方法。
   - 2）`nonfairTryAcquireShared(int acquires)` 方法，会进行自旋，先获取剩余的信号量许可，然后扣减掉要获取的 n 个许可，判断扣减后的剩余许可是否小于 0，如果是的话，说明信号量许可不够了，回调到 AQS 中时，则需要去 AQS 主等待队列中排队。
   - 3）如果否的话，说明信号量许可还够，则尝试 CAS 更新信号量许可 - n，成功的话，则返回许可剩余数，代表许可获取成功，回调到 AQS 中时，无需进入 AQS 中排队，实现非公平获取信号量许可，如果 CAS 更新失败，则继续自旋。
3. 接着就是，阻塞、公平获取 n 个信号量许可的实现原理：`acquire(int permits)`，
   - 1）该方法底层调用了 AQS 的 `acquireSharedInterruptibly(int arg)` 方法，由 Semaphore#FairSync 实现了 `tryAcquireShared(int acquires)` 钩子方法。
   - 2） `tryAcquireShared(int acquires)` 方法，会进行自旋，与非公平获取不同的是，会先判断 AQS 主队列 head 头结点是否存在后继线程，如果存在，则返回-1，代表获取信号量许可失败，需要去 AQS 主等待队列中排队，实现公平式获取。
   - 3）如果不存在后继线程，则走与公平式获取相同的自旋逻辑。
4. 最后就是，释放 n 个信号量许可的实现原理：`release(int permits)`，
   - 1）该方法底层调用了 AQS 的 `releaseShared(int arg)` 方法，由 Semaphore#FairSync 实现了 `tryReleaseShared(int releases)` 钩子方法，为 NonfairSync 和 FairSync，提供释放共享资源的公共方法。
   - 2）`tryReleaseShared(int releases)` 方法，会进行自旋，先获取剩余的信号量许可，然后累加回这 n 个许可，判断累加后的剩余许可是否还小了，如果是则说明发生异常了，因为不能归还负的许可数，则抛出异常。
   - 3）如果剩余许可增加了，则 CAS 更新剩余信号量许可，更新失败的，继续自旋，更新成功的，则返回 true，回调到 AQS 中时，需要传播式通知唤醒后续结点，实现共享式传播信号量许可。

###### 6）ThreadPoolExecutor

1. Worker，线程池中的线程工人类，继承 AQS，实现了 Runnable 接口，实现了一个简单的、不可重入的、互斥的任务锁，在任务执行前，需要先获取独占锁，才能运行，运行完毕后释放，保证并发安全。

它有几个重要的方法，

1. 首先是，阻塞式获取独占锁原理：`ThreadPoolExecutor#Worker#lock()`，
   - 1）该方法底层调用了 AQS 的 `acquire(int arg)` 方法，由 ThreadPoolExecutor#Worker 实现了 `tryAcquire(int unused)` 钩子方法。
   - 2）`tryAcquire(int unused)` 方法，会尝试通过 CAS 更新独占资源，如果更新成功，则设置独占线程为当前线程，然后返回 true，代表独占锁获取成功，回调到 AQS 中，无需进入主等待队列中排队。
   - 3）如果更新失败，则返回 false，代表独占锁获取失败，回调到 AQS 中，需要进入主等待队列中排队。
2. 最后就是，释放独占锁原理：`ThreadPoolExecutor#Worker#unlock()`，
   - 1）该方法底层调用了 AQS 的 `release(int arg)` 方法，由 ThreadPoolExecutor#Worker 实现了 `tryRelease(int unused)` 钩子方法。
   - `tryRelease(int unused)` 方法，会清空独占线程、重置独占资源，并返回 true，代表独占锁获取成功（100% 成功），回调到 AQS 中，需要通知唤醒后继结点。

=> 以上，就是我对 AQS 的一些理解，请问有什么细节需要补充的吗？

#### 7.1.1.2. 分布式锁的实现方案？

见《[1.2.1.4. 分布式锁的实现方案，以及红锁的缺点？](#1.2.1.4. 分布式锁的实现方案，以及红锁的缺点？)》。

#### 7.1.1.3. AOP 的实现原理？

见《[4.1.1.5. Spring AOP 的原理？](#4.1.1.5. Spring AOP 的原理？)》。

#### 7.1.1.4. 责任链模式的特点，以及与适配器模式的区别？

见《[3.2.1.8. 设计模式有了解哪些？线程池这种是什么设计模式？](#3.2.1.8. 设计模式有了解哪些？线程池这种是什么设计模式？)》。

#### 7.2.1.1. 项目亮点讲一下？

见《[2.1.1.1. 项目自我介绍？](#2.1.1.1. 项目自我介绍？)》。

#### 7.2.1.2. Kafka 分区日志存储原理？

见《[3.2.1.3. RocketMQ、Kafka 底层文件原理？](#3.2.1.3. RocketMQ、Kafka 底层文件原理？)》。

#### 7.2.1.3. Kafka 有进行过调优吗？

##### 1）Server.properties 重要配置

| 属性                               | 释义                                                         |
| ---------------------------------- | ------------------------------------------------------------ |
| zookeeper.connect                  | 指明 Broker 要连接的 ZK 集群，多个节点用逗号分隔开，ZK 用于管理 Kafka集群的元数据，比如 Topic、Partition、Leader Partition、副本 Replicas 等 |
| listeners                          | 与客户端进行交互的端口，比如消息投递、消息创建，可结合 `listener.security.protocol.map` 指定具体传输的协议类型，比如有 PLAINTEXT 明文传输、SSL 加密传输等 |
| log.dirs                           | 日志存储路径，建议配置多个路径，因为多个不同磁盘的路径，Kafka 会在含有分区目录最少的文件夹下创建新的分区目录，一来可提高吞吐量，二来可提高磁盘的容错性 |
| log.retention.{hours\|minutes\|ms} | Broker 级别的日志留存寿命，默认为 hours=168                  |
| log.retenion.bytes                 | Broker 级别的日志留存大小，默认为 -1，表示没有限制           |
| message.max.bytes                  | Broker 级别的最大消息大小，默认为 976 KB                     |
| retention.ms                       | Topic 级别的日志留存寿命                                     |
| retention.bytes                    | Topic 级别的日志留存大小                                     |
| max.message.bytes                  | 消息级别的最大消息大小                                       |
| auto.create.topics.enable          | 是否允许自动创建 Topic，建议为 false                         |
| unclean.leader.election.enable     | 是否允许选举未完全同步的副本作为 Leader                      |
| auto.leader.rebalance.enable       | 是否允许一段时间后进行 Leader 重选举，重新更换 Leader，建议为 false |

##### 2）Producer 客户端重要配置

在请求完成之前，Producer 要求 Broker 返回 ACK 确认的策略， 同时也控制着所发送消息的持久性：

- **acks=0**：如果设置为 0，那么 Producer 不会 Leader Broker 的任何 ACK 确认，该消息记录会被将立即添加到 socket 缓冲区并视为已发送。
  - 在这种情况下，不能保证 Broker 已经收到记录，并且 Producer 配置的重试机制也会生效，为每个记录返回的偏移量将始终设置为 -1。
- **acks=1**：默认为 1，意味着 Leader Broker 会把记录写入其本地日志，然后做出 ACK 响应给 Producer，并不会等待所有 Follower 确认完。
  - 在这种情况下，如果 Leader Broker 在返回 ACK 确认后发生失败，Follower 被选举为新 Leader 时，由于该记录 Follower 还没完成同步，所以将导致丢失。
- **acks=all**：相当于 `ack=-1`，意味着 Leader Broker 将等待 ISR 中所有的 Broker 确认后才返回 ACK 响应给 Producer。
  - 这是最高的可用保证，只要至少有一个同步副本保持活动状态，该记录就不会丢失。 

| 属性                    | 释义                                                         |
| ----------------------- | ------------------------------------------------------------ |
| acks                    | Broker ACK 确认策略，默认为 1                                |
| max.request.size        | 用于限制 Producer 发送消息的最大值                           |
| retries                 | 重试次数，默认为 0                                           |
| retry.backoff.msretries | 重试间隔，默认为 100                                         |
| compression.type        | 消息的压缩方式，默认为 none，支持 gzip、snappy、lz4 压缩格式 |
| connections.max.idle.ms | 用于指定在多久之后关闭限制的连接，默认为 540000 ms（9 分钟） |
| linger.ms               | 用于指定 Producer 批发送之前，等待消息加入 ProducerBatch Deque 时间，默认为 0 |
| batch.size              | 指定累加多少条消息，才进行一次批发送                         |
| buffer.memeory          | Producer 缓冲待批发送消息的大小，默认为 32 MB                |
| receive.buffer.bytes    | 用于设置 Socket 接收消息缓冲区 SO_RECBUF 的大小，默认为 32 KB |
| send.buffer.bytes       | 用于设置 Socket 发送消息缓冲区 SO_SNDBUF 的大小，默认为 128 KB |
| request.timeout.ms      | 用于设置 Producer 等待请求响应的最长时间，默认为 3000 ms     |

##### 3）Consumer 客户端重要配置

- 当 Kafka 中没有初始偏移量时，比如偏移量数据已被删除，可以根据以下策略进行设置：
  1. **earliest**：自动将偏移量重置为最早的偏移量。
  2. **latest**：自动将偏移量重置为最新的偏移量。
  3. **none**：如果没有找到之前的偏移量，则向 Consumer 抛出异常。

| 属性                      | 释义                                             |
| ------------------------- | ------------------------------------------------ |
| fetch.min.bytes           | 一次拉取的最小数据量，默认为 1 B                 |
| fetch.max.bytes           | 一次拉取的最大数据量，默认为 50 MB               |
| max.partition.fetch.bytes | 一次拉取一个 Partition 的最大数据量，默认为 1 MB |
| fetch.max.wait.ms         | 拉取请求的最大延迟等待时间，默认为 500 ms        |
| max.poll.records          | 每次拉取的消息最大条数                           |
| auto.offset.reset         | 偏移量丢失处理策略，默认为 latest                |

#### 7.2.1.4. ZK 与 Redis 分布式锁的区别？

见《[1.2.1.4. 分布式锁的实现方案，以及红锁的缺点？](#1.2.1.4. 分布式锁的实现方案，以及红锁的缺点？)》。

#### 7.2.1.5. 设计一个注解，实现对返回值（比如手机号码）进行脱敏，基于 Spring、Spring MVC 如何实现？

见《[1.1.1.5. SpringMVC 请求前改写 @RequestBody 参数？](#1.1.1.5. SpringMVC 请求前改写 @RequestBody 参数？)》- Controller AOP | 实现简单强大。

#### 7.2.1.6.  设置一个配置中心，以满足配置动态刷新的需求？

##### 1）消息中间件实现

1. 假设，配置是放在 GitHub 上，由于要满足动态刷新，所以配置中心服务端需要提供一个服务，来接收 GitHub 的钩子回调。
2. 回调时，调用配置中心服务端的业务逻辑，生成一条消息，丢到消息队列中，消息体存放的是，本次拉取最新配置的路由地址。
3. 这时，会被各个配置中心客户端进行消费，根据消息体中的路由，拉取最新的消息，替换掉本地缓存，并且销毁所有 RefreshScope 自定义作用域的 Bean。
4. @RefreshScope 是懒加载模式，是一个 FactoryBean 对象，在下次用到时，会先去本地缓存中获取，获取不到再重新去 Config Server 中加载，从而实现配置的动态刷新。

##### 2）ZK 实现

1. 假设，配置是放在 GitHub 上，由于要满足动态刷新，所以配置中心服务端需要提供一个服务，来接收 GitHub 的钩子回调。
2. 回调时，调用配置中心服务端的业务逻辑，修改 ZK 某一配置文件结点。
3. 此时，会被各个配置中心客户端监听到发生变化，从而拉取最新的配置，替换掉本地缓存，并且销毁所有 RefreshScope 自定义作用域的 Bean。
4. @RefreshScope 是懒加载模式，是一个 FactoryBean 对象，在下次用到时，会先去本地缓存中获取，获取不到再重新去 Config Server 中加载，从而实现配置的动态刷新。

#### 7.2.1.7. k8s 架构图？

见《[4.1.2.3. Docker 和 K8S 有了解过吗？](#4.1.2.3. Docker 和 K8S 有了解过吗？)》。

#### 7.2.1.8. 阻塞队列有哪些？

##### 1、总

1. BlockingQueue，阻塞队列，继承自 Queue 接口，除了 Queue 接口方法外，还提供了以下操作：在添加元素时，队列满了则阻塞等待队列非满，在删除元素时，队列为空则阻塞等待队列非空。

2. BlockingQueue，是线程安全的，所有排队方法，使用了内部锁、或者其他并发控制的方式，来原子地实现其操作，它的方法一共 4 种形式：

   - 1）抛出异常。
   - 2）返回一个特殊值，{@code null} 或 {@code false}，具体取决于操作。
   - 3）无限期地阻塞当前线程，直到操作成功为止。
   - 4）阻塞等待指定的时间。

   |             | *Throws exception*         | *Special value*         | *Blocks*             | *Times out*                                                 |
   | ----------- | -------------------------- | ----------------------- | -------------------- | ----------------------------------------------------------- |
   | **Insert**  | {@link #add add(e)}        | {@link #offer offer(e)} | {@link #put put(e)}  | {@link #offer(Object, long, TimeUnit) offer(e, time, unit)} |
   | **Remove**  | {@link #remove remove()}   | {@link #poll poll()}    | {@link #take take()} | {@link #poll(long, TimeUnit) poll(time, unit)}              |
   | **Examine** | {@link #element element()} | {@link #peek peek()}    | *not applicable*     | *not applicable*                                            |

3. JDK 8 中，典型实现有：

   - 1）ArrayBlockingQueue，数组有界阻塞队列。
   - 2）LinkedBlockingQueue，链表有界阻塞队列。
   - 3）PriorityBlockingQueue，优先级无界阻塞队列。
   - 4）SynchronousQueue，无容量的同步阻塞队列。

##### 2、分

下面打算讲下几个重要实现的原理，

###### 1）ArrayBlockingQueue

1. ArrayBlockingQueue，数组有界阻塞队列，保存固定大小的数组，数组创建后，其容量无法被更改。
2. 元素从队尾进、队头出，在添加元素时，队列满了则阻塞等待队列非满，在删除元素时，队列为空则阻塞等待队列非空。
3. 同时支持公平性设置，设置为 true，则代表使用公平策略，队列则严格按 FIFO 顺序添加、获取元素，可避免线程饥饿，但会降低吞吐量。

它的实现原理是，

1. 数据结构为，
   - 1）持有一个 `Object[] items` 数组来存储元素。
   - 2）持有一个 `int takeIndex` 、以及一个 `int putIndex`，表示最近获取、添加索引，每次获取、添加完元素之后，都会向前、向后更新它的值，由于是在显示锁控制的临界区内操作，所以是线程安全的。
   - 3）持有一个 `int count`，表示当前队列中的实际大小，每次获取、添加完元素之后，都会  -1 或者 +1，由于是在显示锁控制的临界区内操作，所以是线程安全的。
   - 4）持有一个 `ReentrantLock lock` 显式锁来控制独占访问以及公平策略。
   - 5）持有一个 `Condition notEmpty` 队列非满条件、以及一个 `Condition notFull` 队列非空条件，两条件都是由 lock 显式锁构造，遵守与 ReentrantLock 相同的公平性策略。
2. 然后就是，阻塞式添加元素方法的实现原理：`put(E e)`，
   - 1）首先，会调用 `lock.lockInterruptibly()`，可中断、阻塞式获取独占锁，获取失败，当前线程会进入阻塞状态。
   - 2）获取成功，则再去判断实际大小，是否已经等于数组容量大小，是的话，说明队列已经满了，不能再添加元素了，则调用 `notFull.await()`，释放当前线程独占的锁资源，阻塞等待非满条件的发生，即发生一次元素获取事件。
   - 3）如果队列还没满，或者非满条件已发生，当前线程重新抢到锁资源，则调用 `enqueue(E x)`，往数组中追加此元素，并更新实际大小 + 1，然后调用 `notEmpty.signal()`，通知非空事件已发生，唤醒正在阻塞的 `take()` 方法线程。
   - 4）最后，释放显式锁。
3. 最后就是，阻塞式删除元素方法的实现原理：`take()`，
   - 1）与 `put(E e)` 方法相反，首先，会调用 `lock.lockInterruptibly()`，可中断、阻塞式获取独占锁，获取失败，当前线程会进入阻塞状态。
   - 2）获取成功，则再去判断实际大小，是否已经等于 0，是的话，说明队列已经空了，不能再获取元素了，则调用 `notEmpty.await()`，释放当前线程独占的锁资源，阻塞等待非空条件的发生，即发生一次元素添加事件。
   - 3）如果队列还没空，或者非空条件已发生，当前线程重新抢到锁资源，则调用 `dequeue()`，获取数组最近索引的元素，获取后再清空该位置，并更新实际大小 - 1，然后调用 `notFull.signal()`，通知非满事件已发生，唤醒正在阻塞的 `put(E e)` 方法线程。
   - 4）最后，释放显式锁，返回清空之前该位置的值。

###### 2）LinkedBlockingQueue

1. LinkedBlockingQueue，链表有界阻塞队列，容量未指定时，默认使用 `Integer＃MAX_VALUE`，可能会导致内存溢出的发生，所以一般在构造时都指定实际容量，防止队列存放过多的元素。
2. 元素从队尾进、队头出，在添加元素时，队列满了则阻塞等待队列非满，在删除元素时，队列为空则阻塞等待队列非空。
3. 不支持公平性设置，只能使用非公平策略，同时，阻塞方法的实现原理也不同，理论上会比 ArrayBlockingQueue 拥有更高的吞吐量。

它的实现原理是，

1. 数据结构为，
   - 1）持有一个 `Node<E> head`、一个 `Node<E> last`  链表的头尾结点，以支持从尾添加从头获取元素。
   - 2）持有一个 `int capacity`，表示队列的容量边界，超过此容量后，再添加元素，则线程会阻塞等待非满条件的发生。
   - 3）持有一个 `AtomicInteger count = new AtomicInteger()`，表示当前队列的实际大小，通过 Atomic 原子类控制线程同步。
   - 4）持有一个 `ReentrantLock takeLock` take 锁、一个 `ReentrantLock putLock`，以分别控制排队的线程，提高吞吐量。
   - 5）持有一个 `Condition notEmpty = takeLock.newCondition()` 队列非满条件，以及一个 `Condition notFull = putLock.newCondition()` 队列非空条件，分别由 take 锁和 put 锁构造，同样也是非公平的策略。
2. 然后就是，阻塞式添加元素方法的实现原理：`put(E e)`，
   - 1）首先，会调用 `lock.lockInterruptibly()`，可中断、阻塞式获取独占 put 锁，获取失败，当前线程会进入阻塞状态。
   - 2）获取成功，则再去判断实际大小，是否已经等于 `capacity` 容量边界，是的话，说明队列已经满了，不能再添加元素了，则调用 `notFull.await()`，释放当前线程独占的 put 锁资源，阻塞等待非满条件的发生。
   - 3）如果队列还没满，或者非满条件已发生，当前线程重新抢到 put 锁资源，则调用 `enqueue(E x)`，往数组中追加此元素，并更新实际大小 + 1，然后如果添加之后，容量还没满，则调用 `notFull.signal()`，通知非满事件已发生，唤醒正在阻塞的 `put()` 方法线程。
     - 可见，这里非满条件通知，除了 take 线程会发生外，连其他的 put 线程也会发生，来发出通知，使得阻塞的 `put()` 方法线程，其等待时间更少，吞吐量来得更高。
   - 4）最后，释放 put 锁，再判断添加前容量是否为 0，是的话，说明添加之后队列不为空了，则获取独占 take 锁，获取失败，则当前线程会进入阻塞状态，获取成功，则会调用 `notEmpty.signal()`，通知非空事件已发生，唤醒正在阻塞的 `take()` 方法线程。
3. 最后就是，阻塞式删除元素方法的实现原理：`take()`，
   - 1）与 `put(E e)` 方法相反，首先，会调用 `lock.lockInterruptibly()`，可中断、阻塞式获取独占 take 锁，获取失败，当前线程会进入阻塞状态。
   - 2）获取成功，则再去判断实际大小，是否已经等于 0，是的话，说明队列已经空了，不能再获取元素了，则调用 `notEmpty.await()`，释放当前线程独占的 take 锁资源，阻塞等待非空条件的发生。
   - 3）如果队列没空，或者非空条件已发生，当前线程重新抢到 take 锁资源，则调用 `dequeue(E x)`，脱钩链头结点，并更新实际大小 - 1，然后如果删除之后，容量还没空，则调用 `notEmpty.signal()`，通知非空事件已发生，唤醒正在阻塞的 `take()` 方法线程。
     - 可见，这里非空条件通知，除了 put 线程会发生外，连其他的 take 线程也会发生，来发出通知，使得阻塞的 `take()` 方法线程，其等待时间更少，吞吐量来得更高。
   - 4）最后，释放 take 锁，再判断添加前容量是否为 `capacity` 队列容量边界，是的话，说明删除之后队列不满了，则获取独占 put 锁，获取失败，则当前线程会进入阻塞状态，获取成功，则会调用 `notFull.signal()`，通知非满事件已发生，唤醒正在阻塞的 `put()` 方法线程，再返回脱钩出来的头结点的值。

###### 3）PriorityBlockingQueue

1. PriorityBlockingQueue，优先级无界阻塞队列，基于小顶堆实现，能够每次都按传入的 Comparator 比较器或者自然顺序，对元素以 O（logn）进行排序，因此，队头中的元素是比较器给出的最小结果元素，队尾则是最大的元素，相同的元素之间是不稳定的。

它的实现原理是，

1. 数据结构为，

   - 1）持有一个 `Object[] queue` 数组来存储元素。
   - 2）持有一个 `Comparator<? super E> comparator`，表示按指定的比较器，来进行小顶堆排序。
   - 3）持有一个 `int size`，表示当前队列中的实际大小。
   - 4）持有一个 `ReentrantLock lock` 显式锁来控制独占访问，采用的是非公平性策略。
   - 5）持有一个 `Condition notEmpty` 队列非满条件，由 lock 显式锁构造，采用的是非公平性策略。

2. 然后就是，小顶堆原理，

   - 1）最后一个非叶子结点，在数组中的序号 i = n / 2 - 1，n 为数组长度。
   - 2） 对于序号 i 的结点，其父结点序号 f = （i - 1）/ 2，左孩子序号 l = 2 * i + 1，右孩子序号 r = 2 * i + 2，如果有的话。
   - 3）小顶堆堆化：`heapify()`，
     1. 从最后一个非叶子结点 K 位置开始，从下到上，从右到左，每次遍历都对结点进行向下调整，一直遍历到根结点 queue[0] 后，queue[0] 就是数组中的最小值，即完成了数组的小顶堆堆化。
   - 4）小顶堆向下调整：`siftDownComparable(int k, T x, Object[] array, int n)`， k 为要调整的结点序号，x 为要调整的结点的值，array 为数组，n 为数组长度。
     1. 从 k 位置开始循环，比较 x、左、右孩子的值，交换最小者到堆顶，然后继续向下调整，直到调整到最后一个非叶子结点为止。
   - 5）小顶堆向上调整：`siftUpComparable(int k, T x, Object[] array)`，k 为要调整的结点序号，x 为要调整的结点的值，array 为数组。
     1. 从 k 的父结点 f = （k  - 1）/ 2 开始，比较 x、f 的值，交换大者到 k 位置，然后继续向上调整，直到调整到根结点位置。

   ![1625386699340](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1625386699340.png)

3. 接着就是，阻塞式添加元素方法的实现原理：`put(E e)`，

   - 1）首先，会调用 `lock.lock()`，阻塞式获取独占锁，获取失败，当前线程会进入阻塞状态。
   - 2）获取成功，则判断当前队列中的实际大小，是否大于等于数组容量，是的话，说明队列满了，则调用 `tryGrow(Object[] array, int oldCap)`，进行数组扩容。
   - 3）如果没小于数组容量，说明队列还没满，则调用 `siftUpComparable(int k, T x, Object[] array)`，插入元素并向上调整小顶堆（大的放后面），然后实际大小 + 1，调用 `notEmpty.signal()`，通知非空事件已发生，唤醒正在阻塞的 `take()` 方法线程。
   - 4）最后，释放独占锁。

4. 再然后就是，数组扩容原理：`tryGrow(Object[] array, int oldCap)`，

   - 1）首先，释放独占锁，获取自选锁 `allocationSpinLockOffset`，判断旧容量（当前数组大小），是否小于 64，如果是，则扩容为旧容量 * 2 倍 + 2，否则扩容为旧容量 * 1.5 倍。
   - 2）然后做容器最大值 `Integer.MAX_VALUE - 8` 校验，校验不通过则抛出异常，校验通过，则重新构建长度为新容量的数组，给中间变量 `Object[] newArray`。
   - 3）最后，释放自选锁，重新获取独占锁，赋值中间变量 newArray 给 `Object[] queue` 队列数组，然后调用 `System.arraycopy(array, 0, newArray, 0, oldCap)`，拷贝并移动旧数据到新数组中，再返回。

5. 最后就是，阻塞式删除元素方法的实现原理：`take()`，

   - 1）首先，会调用 `lock.lockInterruptibly()`，可中断、阻塞式获取独占锁，获取失败，当前线程会进入阻塞状态。
   - 2）获取成功，则去判断实际大小，是否已经等于 0，是的话，说明队列已经空了，不能再获取元素了，则调用 `notEmpty.await()`，释放当前线程独占锁资源，阻塞等待非空条件的发生。
   - 3）然后，再获取 queue[0] 数组的第一个元素，调用 `siftDownComparable(int k, T x, Object[] array, int n)` 对其进行向下调整，更新实际大小 - 1。
   - 4）最后，更新实际大小 - 1，释放独占锁，并返回第一个元素的值。

###### 4）SynchronousQueue

1. SynchronousQueue，无容量的同步阻塞队列，每个插入操作，需要等待另一个删除操作，以及每个删除操作，需要等待另一个插入操作。
2. 它支持可选的公平策略，默认为非公平，设置为 true，则代表使用公平策略，队列则按 FIFO 顺序添加、获取元素，可避免线程饥饿，但会降低吞吐量。

它的实现原理是，

1. 数据结构为，持有一个 `volatile Transferer<E> transferer`，内部类的实例引用，它有两个实现类，如果为非公平策略，则为栈结构，如果为公平策略，则为队列结构。

2. 栈结构的实现是，使用自旋锁提高吞吐量，如果只出现添加、或者删除其中一个操作，则会调用 `LockSupport.park(this)` 让其阻塞等待到另一个操作的出现，实现添加和删除方法两两配对、两两同步。由于栈是后进先出，也就是新来的，会比栈底更久的，更先得到配对响应，所以实现了非公平策略。

   ![1621954377695](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1621954377695.png)

3. 队列结构的实现是，使用自旋锁提高吞吐量，如果只出现添加、或者删除其中一个操作，则会调用 `LockSupport.park(this)` 让其阻塞等待到另一个操作的出现，实现添加和删除方法两两配对、两两同步。由于队列是先进先出的，所以另一个操作到来时，会去队头做匹配，从而让最久的先得到响应，所以实现了公平策略。

   ![1622033076546](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1622033076546.png)

##### 3、总

=> 以上，就是我对 BlockingQueue 的一些理解，请问有什么细节需要补充的吗？

#### 7.2.1.9. ArrayBlockingQueue 与 LinkedBlockingQueue 的异同，比如在锁的角度呢？

1. 相同点：都实现了 BlockingQueue 接口，都是有界的阻塞队列，在添加元素时，队列满了则会阻塞等待队列非满，在删除元素时，队列为空则会阻塞等待队列非空。
2. 不同点：
   - 1）**底层实现的数据结构不一样**，ArrayBlockingQueue 是基于数组实现的，而 LinkedBlockingQueue 是基于单向链表实现的。
   - 2）**构造方法不一样**，ArrayBlockingQueue 构造时必须设定容量边界，同时支持公平和非公平的排队策略，而 LinkedBlockingQueue 构造时可以不设定容量边界，默认为 `Integer.MAX_VALUE`，且仅支持非公平的排队策略。
   - 3）**条件通知机制不一样**，ArrayBlockingQueue 中的队列非满和非空条件，基于的都是同一把显式锁，非满只能被删除方法唤醒，非空只能被添加方法唤醒，而 LinkedBlockingQueue 中的队列非满条件，基于 put 锁实现，可以被添加、删除两个方法唤醒，队列非空条件，基于 take 锁实现，也可以被添加、删除两个方法唤醒，理论上吞吐量来得要比 ArrayBlockingQueue 的高。
   - 4）**队列实际大小统计不一样**，ArrayBlockingQueue 的实际大小，是通过持有一个 `int count` 成员变量，在独占锁的临界区内进行更新，而 LinkedBlockingQueue 的实际大小，是通过 `AtomicInteger count = new AtomicInteger()` 成员变量，利用 CAS 进行更新。

#### 7.2.2.0. 有参与过 JVM 调优吗，只有过 MAT 吗，OGLIB 有听过没？

见《[2.1.1.1. 项目自我介绍？](#2.1.1.1. 项目自我介绍？)》- 3、项目亮点 - 5）内存溢出排查 | MAT、POI、ArrayList、12）发版平台 CPU 占用过高 | Tomcat Manager、ELK、CPU 95%。

#### 7.2.2.1. CMS 和 G1 有什么差别？

见《[4.1.1.8. 项目中的垃圾收集器用了哪些？](#4.1.1.8. 项目中的垃圾收集器用了哪些？)》。

#### 7.2.2.2. 反射的 API 里有哪些类？

![1648430396798](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1648430396798.png)

| 类名                       | 接口 / 实现类 | 作用                                                         |
| -------------------------- | ------------- | ------------------------------------------------------------ |
| Type                       | 接口          | Java 所有类型的通用超接口，包括 GenericArrayType、ParameterizedType 和 TypeVariable 和 WildcardType |
| GenericArrayType           | 接口          | 表示一个数组类型，包括 ParameterizedType 和 TypeVariable     |
| ParameterizedType          | 接口          | 表示一个泛型类型                                             |
| TypeVariable               | 接口          | 表示各种类型变量的通用超接口                                 |
| WildcardType               | 接口          | 表示通配符表达式类型                                         |
| AnnotatedElement           | 接口          | 表示在类上的所有注解元素                                     |
| AnnotatedType              | 接口          | 表示注解类型，包括 AnnotatedArrayType、AnnotatedParameterizedType、AnnotatedTypeVariable 和 AnnotatedWildcardType |
| AnnotatedArrayType         | 接口          | 表示注解数组类型，包括 AnnotatedParameterizedType 和 AnnotatedTypeVariable |
| AnnotatedParameterizedType | 接口          | 表示注解泛型类型                                             |
| AnnotatedTypeVariable      | 接口          | 表示注解变量类型                                             |
| AnnotatedWildcardType      | 接口          | 表示注解通配符表达式类型                                     |
| GenericDeclaration         | 接口          | 表示声明类型变量的所有实体的通用接口，包括 Class、Constructor、Method 和 Method |
| Member                     | 接口          | 表示一个字段、方法、或者构造函数的标识信息                   |
| AccessibleObject           | 实现类        | 是 Field、Method 和 Constructor 对象的基类，提供了反射对象在使用时的访问保护 |
| Parameter                  | 实现类        | 表示方法参数，提供有关方法参数信息的获取方法，包括名称、修饰符、以及参数属性 |
| Executable                 | 抽象类        | Method 和 Constructor 对象的通用抽象基类                     |
| Field                      | 实现类        | 提供关于类或接口的某个字段的信息和动态访问，可以是一个静态字段或实例字段 |
| Constructor                | 实现类        | 提供关于类的某个构造函数的信息和访问权限                     |
| Method                     | 实现类        | 提供有关类或接口上的某个方法的信息和访问权限，可以是静态方法，也可以是实例方法或者抽象方法 |
| Modifier                   | 实现类        | 提供类和成员访问修饰符的获取方法                             |
| Array                      | 实现类        | 提供动态创建和访问 Java 数组的方法                           |
| InvocationHandler          | 接口          | 表示代理实例实现的处理程序接口                               |
| Proxy                      | 实现类        | 是由创建的所有动态代理类的超类，提供了创建动态代理类和实例的方法 |
| ReflectPermission          | 实现类        | 反射操作的权限类                                             |

#### 7.3.1.1. 自我介绍？

见《[2.1.1.1. 项目自我介绍？](#2.1.1.1. 项目自我介绍？)》。

#### 7.3.1.2. 为什么要离职？

1. 主要原因是，个人发展受限，想在市场上看看有没有更好的机会。
2. 比如说，这些年积累的很多技术经验，包括深度和广度上，拓展得不错，想要找到一个更好突出自己价值的平台。

#### 7.3.1.3. 根据已有的经验，谈谈你对未来 G 系统迭代方向的一些看法？

1. 项目管理方面：目前采用的 Git + Jenkins 做的 CI / CD 持续集成、持续交付，但容器化技术也越来越成熟，更适合微服务的部署和管理，更节省机器成本，所以，项目管理上可以从这角度进行迭代。
2. 平常开发方面：目前是一个大版本迭代后，才做的一次 Code Review，项目上也还是有挺多设计不够合理、不太符合高内聚的理念，所以，平常开发方面，可以改成一个小版本适当地进行一次 Code Review，以及适当地抽取通用接口、工具类，来让系统代码趋向高内聚，可以减少测试开发成本，避免重复造轮子。
3. 业务迭代方面：目前采购方 + 供方都是基于电脑端使用，由于采购方是企业用户，所以电脑端使用无可厚非，而对于供方，G 系统未来可以抽取出更简单的业务，向移动端靠拢，让系统更加便捷、容易使用。
4. 以上，就是我对未来迭代方向的一些看法~

#### 8.1.1.1. 项目亮点？

见《[2.1.1.1. 项目自我介绍？](#2.1.1.1. 项目自我介绍？)》。

#### 8.1.1.2. 谈一下你对微服务的一个理解？

见《[1.1.1.1. 项目上用到了 SpringCloud 哪些组件？](#1.1.1.1. 项目上用到了 SpringCloud 哪些组件？)》。

#### 8.1.1.3. G 系统分库分表方案是怎么设计的？

见《[2.1.1.1. 项目自我介绍？](#2.1.1.1. 项目自我介绍？)》。

#### 8.1.1.4. 分布式锁的实现方案？

见《[1.2.1.4. 分布式锁的实现方案，以及红锁的缺点？](#1.2.1.4. 分布式锁的实现方案，以及红锁的缺点？)》。

#### 8.2.1.1. 讲一下 Eureka 源码？

##### 1、总

![1638234417008](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1638234417008.png)

1. 在传统应用组件间调用，是通过接口规范约束来实现的，从而实现不同模块间良好协作。
2. 但是在被拆分成微服务后，每个微服务实例的网络地址和数量都可能动态变化，导致使用原来硬编码地址的方式极不方便，因此，需要一个中心化的组件来进行服务的登记和管理。
3. Eureka，是 Spring Cloud 的一个基于 AP 模型的注册中心，支持服务注册、服务发现、服务调用、服务续约、服务剔除、服务自保、服务下线、集群高可用等功能。
4. 其实现包括 Server 端和 Client 端，Server 端可以作为一个公共的注册中心服务，为 Client 提供服务注册和服务发现的功能，维护自身的服务列表信息，Client 端又分为服务提供者和服务消费者，服务提供者通过注册自身的信息到 Server 端，方便消费者发现自己，服务消费者，通过服务发现，从 Server 端拉取服务列表，然后使用内置的负载均衡器，实现服务调用。

##### 2、分

###### 1）Eureka Server 启动原理

1. 首先，@EnableEurekaServer 会导入一个 `EurekaServerMarkerConfiguration`，这个配置类会加载 `Marker` 标志类。
2. 然后，Spring Boot 会自动装配 `EurekaServerAutoConfiguration`，只有存在 `Marker`  标志类时，Spring 才会去解析这个配置 Bean。由于`Marker`  标志类已被 @EnableEurekaServer 导入了，所以，该配置 Bean 会继续读取 `classpath:/eureka/server.properties`，往 IOC 注入注册表等 Bean 对象。
3. 接着，`DefaultLifecycleProcessor` 实现了 `LifecycleProcessor#onRefresh()` 生命周期回调接口，在 Spring 执行到 `AbstractApplicationContext#finishRefresh()` 时进行回调，该方法会启动一个异步线程，去初始化 Eureka Server 环境、上下文，以及读取 `eureka.server.evictionIntervalTimerInMs` （默认 60 s）， 启动一个定时任务 `EvictionTask`，执行**服务剔除**逻辑。

###### 2）Eureka Client 启动原理

1. 首先，Spring Boot 启动时，会自动装配 `EurekaClientAutoConfiguration` ，该配置 Bean 会装配一个 `EurekaClient`，并指定其 `shutdown()` 方法，实现**服务下线**。
2. 在构造 `EurekaClient` 时，会：
   - 1）读取 `eureka.instance.lease.renewalInterval` 续约周期（默认 30 s），创建 ` heartbeatExecutor` 线程池，启动 `HeartbeatThread` 线程任务，用于定时**服务续约**。
   - 2）读取 `eureka.client.refresh.interval` 服务列表拉取周期（默认 30 s），创建 ` cacheRefreshExecutor` 线程池，启动 `CacheRefreshThread` 线程任务，用于定时**服务发现**，刷新服务列表缓存。
   - 3）读取 `eureka.appinfo.initial.replicate.time` 注册延迟时间（默认 40 s），启动 `InstanceInfoReplicator` 线程任务，用于**服务注册**。

###### 3）服务注册原理

1. Eureka Client `InstanceInfoReplicator` 线程任务，延迟 ``eureka.appinfo.initial.replicate.time` （默认 40 s）后，调用 `discoveryClient.register()` 做服务注册。
2. 它会经过一些列的装饰者调用，包括 `SessionedEurekaHttpClient#execute ` 更新 session 时间、`RetryableEurekaHttpClient#execute ` 默认最大重试 3 次、`RedirectingEurekaHttpClient#execute` 重定向配置、`MetricsCollectingEurekaHttpClient#execute` 指标记录。
3. 最终，调用到 `AbstractJerseyEurekaHttpClient#register(InstanceInfo)` 方法，该方法正式发起 post请求 Eureka Server，进行服务注册。
4. Eureka Server `addInstance` 接口，收到 `apps/${服务名}` 后，先进行一系列校验，然后调用 `register(final InstanceInfo info, final boolean isReplication)` 方法，写入新实例到 `ConcurrentHashMap<String, Map<String, Lease<InstanceInfo>>> registry` 注册表中，更新该实例的最后更新时间、清空读写二级缓存后，返回 204 给 Eureka Client，代表服务注册成功。

###### 4）服务发现原理

1. Eureka Client `CacheRefreshThread` 线程任务，会调用 `DiscoveryClient#refreshRegistry()` 方法，以 `eureka.client.refresh.interval` （默认 30）作为周期， 做定时服务发现。
2. 每次服务发现，分为增量拉取和全量拉取，如果强制配置、或者第一次拉取，则做全量拉取，否则走增量拉取，拉取完毕后更新服务列表缓存。
3. 全量拉取时，同样经历注册时的装饰者链调用，最终由 `AbstractJerseyEurekaHttpClient#getApplications()`，发起 `apps/` 请求到 Eureka Server。增量拉取时，同样也经历装饰者链调用，最终由 `AbstractJerseyEurekaHttpClient#getApplicationsInternal()`，发起 `apps/delta` 请求到 Eureka Server。
4. Eureka Server 先一级只读缓存中获取，获取不到则从二级读写缓存中获取，还获取不到则触发 `Guava Cache#load()` 方法，从 `ConcurrentHashMap<String, Map<String, Lease<InstanceInfo>>> registry` 注册表中读取出来，并加载到二级缓存中，最后返回 200，表示服务列表获取成功。

###### 5）Eureka Server 服务列表缓存原理

![1648467823898](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1648467823898.png)

1. 服务注册到注册中⼼后，服务实例信息存储在 `ConcurrentHashMap<String, Map<String, Lease<InstanceInfo>>> registry` 注册表中。

2. Eureka 为了提⾼响应速度，做了两层的缓存结构优化，将 Client 需要的实例信息，直接缓
  存起来，在获取时直接从缓存中拿数据，然后响应给 Client。
3. 第⼀层缓存是， `ResponseCacheImpl#readOnlyCacheMap`，采⽤ ConcurrentHashMap 来存储数据，定时
  `eureka.server.responseCacheUpdateIntervalMs`（默认 30 s 一次） 与 `ResponseCacheImpl#readWriteCacheMap` 进⾏数据同步。
4. 第⼆层缓存是， `ResponseCacheImpl#readWriteCacheMap`，采⽤ Guava 来实现缓存，缓存过期时间 `eureka.server.responseCacheAutoExpirationInSeconds` （默认为 180 s），当服务下线、过期、注册、状态变更等操作，都会清除该缓存中的数据。
5. 如果两级缓存都无法查询，则会触发 Guava 缓存的加载 `CacheLoader#load()`，从 `ConcurrentHashMap<String, Map<String, Lease<InstanceInfo>>> registry` 注册表，读取数据到二级缓存中，再返回给 Client。
6. 对于 Eureka Client 的本地缓存，是通过 `CacheRefreshThread` 线程任务，以 `eureka.client.refresh.interval` （默认 30 s）作为周期，做定时服务发现时更新的。
7. 对于 Ribbon 缓存，是通过 `PollingServerListUpdater` 启动一个线程任务，以 `${服务名}.ribbon.serverListRefreshInterval`（默认 30 s）为周期，调用 `ServerListUpdater.UpdateAction#doUpdate()` 方法，从 Eureka Client 本地缓存中获取再更新。

###### 6）服务调用原理

1. Eureka 依赖与 Ribbon，在 Spring Boot 启动时，会启动装配一个 `RibbonEurekaAutoConfiguration`，该配置 Bean 会读取 Ribbon 配置，向 IoC 容器注入一个 `LoadBalancerClient` 对象。
2. 在调用 `loadBalancerClient.choose("eureka-client")` 发起初次服务调用时，会使用 `SpringClientFactory#getLoadBalancer("eureka-client"）` ，构造 `ILoadBalancer` 对象，触发 `RibbonClientConfiguration` 的注入，通过 `PollingServerListUpdater` 启动一个线程任务，以 `${服务名}.ribbon.serverListRefreshInterval`（默认 30 s）为周期，调用 `ServerListUpdater.UpdateAction#doUpdate()` 方法，从 Eureka Client 本地缓存中获取再更新。
3. 然后再选择从中服务列表缓存中，选择一个具体的实例，进行服务调用。

###### 7）服务续约原理

1. Eureka Client `HeartbeatThread` 线程任务，会调用 `DiscoveryClient#renew()`，以 `eureka.instance.lease.renewalInterval` 为续约周期（默认 30 s），做服务续约。
2. 它会调用 `registrationClient.sendHeartBeat()` 发送心跳包，经过一些列的装饰者链调用，最终调用到 `AbstractJerseyEurekaHttpClient#sendHeartBeat()` 方法，发起 `apps/service-name/host:service-name:ip` 请求、以及 `lastDirtyTimestamp` 到 Eureka Server，进行续约。
3. Eureka Server `renewLease` 接口，收到请求后，会执行服务续约，更新过去一分钟进行服务续约的数量、最后更新时间等信息，续约失败的，则返回 404，让客户端重新注册。
4. 续约成功的，先根据服务名获取 `registry` 服务列表中对应的租约实例，然后判断租约实例中的脏时间戳与传过来的 `lastDirtyTimestamp` 大小，如果传过来的大，说明客户端重启过，Eureka Server 的租约落后了，则返回 404，让客户端重新注册。
5. 如果传过来的小，说明收到的是集群同步请求，则返回当前租约实例的复制。

###### 8）服务剔除、服务自保原理

1. 在 Eureka Server 启动时，`AbstractApplicationContext#finishRefresh()` 进行回调，会启动一个异步线程，去初始化 Eureka Server 环境、上下文，以及读取 `eureka.server.evictionIntervalTimerInMs` （默认 60 s）， 启动一个定时任务 `EvictionTask`，调用 `evict(long additionalLeaseMs)` 方法，执行服务剔除逻辑。

2. 该方法会先判断能否执行服务剔除，如果 `eureka.server.enableSelfPreservation` 服务自保开关已关闭（默认打开），则直接返回 true，代表不启用服务自保，要做服务剔除操作，如果服务自保开关已打开，则要根据 `numberOfRenewsPerMinThreshold > 0 && getNumOfRenewsInLastMin() > 
   numberOfRenewsPerMinThreshold` ，即过去一分钟进行服务续约的数量，是否大于每分钟最少续约阈值，是的话则返回 true，代表不启用服务自保，要做服务剔除操作。

   ```java
   public abstract class AbstractInstanceRegistry implements InstanceRegistry {
       protected void updateRenewsPerMinThreshold() {
           // 每分钟最少收到租约阈值=实例数 * （60/续租周期）* 0.85
           this.numberOfRenewsPerMinThreshold = (int) 
   (this.expectedNumberOfClientsSendingRenews
                   * (60.0 / serverConfig.getExpectedClientRenewalIntervalSeconds())
                   * serverConfig.getRenewalPercentThreshold());
       }
   }
   ```

3. 如果确定要做服务剔除了，则通过租约期望时间 =  `lastUpdateTimestamp + duration + additionalLeaseMs`（租约最后更新时间 + 租约持续时间默认 90 s + 补偿时间），如果租约期望时间小于当前时间，则认为租约已过期，然后将其收集起来。

4. 然后，随机抽取一个租约实例，从 `registry` 服务列表中删除，并清除 Guava Cache 二级读写缓存，随机剔除，可以使得该租约实例在整个 Eureka Server 集群中，剔除得比较离散，防止服务雪崩。

###### 9）服务下线原理

1. Spring Boot 启动时，会自动装配 `EurekaClientAutoConfiguration` ，该配置 Bean 会装配一个 `EurekaClient`，并指定其 `shutdown()` 方法，实现服务下线。
2. 该方法会销毁 `instanceInfoReplicator` 服务注册线程、`heartbeatExecutor` 服务续约线程、`cacheRefreshExecutor` 服务发现线程、以及 `scheduler` 调度定时器，调用 `unregister()`  发起取消服务注册请求。
3. 最终由 `AbstractJerseyEurekaHttpClient#cancel()` ，发起 `apps/service-name/host:service-name:port` 请求到 Eureka Server。
4. Eureka Server 的 `cancelLease()` 接口，收到服务取消注册请求后，调用 `PeerAwareInstanceRegistryImpl.cancel()` 方法，从 `registry` 服务列表中删除对应租约实例，并清除 Guava Cache 二级读写缓存，减少过去一分钟进行服务续约的数量，同步操作到 Eureka Server 集群的其他节点，最后返回 true，代表服务下线成功。

###### 10）Eureka Server 集群同步原理

1. Eureka Server 集群同步，有几个关键时机，
   - 1）第一是，Eureka Server `addInstance` 接口，在接收到客户端的**服务注册**请求时，当前 Eureka Server 会发送相同的请求，到其他 Eureka Server `addInstance` 接口。
   - 2）第二是，Eureka Server  `renewLease` 接口，在接收到客户端的**服务续约**请求时，当前 Eureka Server 会发送相同的请求，到其他 Eureka Server `renewLease` 接口。
   - 3）第二是，Eureka Server  `cancelLease` 接口，在接收到客户端的**服务下线**请求时，当前 Eureka Server 会发送相同的请求，到其他 Eureka Server `cancelLease` 接口。

##### 3、总

=> 以上，就是我对 Eureka 实现原理的一些理解，请问有什么细节需要补充的吗？

#### 8.2.1.2. 服务下线后，客户端会立马感应到吗？

不会立马感应到。

1. 首先，对于 Eureka 这种 AP 模型，数据是非强一致的，在服务提供者主动下线，或者被服务剔除后，Eureka Server 会把其从 `registry` 注册表中移除，以及作废 Guava Cache 二级缓存，但此时一级缓存需要等到 `eureka.server.responseCacheUpdateIntervalMs` （默认 30s）后，才能从二级缓存中获取，由于二级缓存没有，则触发 Guava Cache#load 方法，读取 `registry` 注册表，获取到最新的服务注册信息。
2. 同时，客户端的 Ribbbon 缓存，也需要等到 `xxxService.ribbon.serverListRefreshInterval` （默认 30s）后，才会定期更新。
3. 所以，Eureka AP 模型是无法让客户端实时感应到的。
4. 其次，如果对于 ZK 这种 CP 模型，即使服务端数据强一致，在服务注册信息发生变更时，触发 Watcher 机制，通知客户端，由于 TCP 传输需要花费一定的时间，在这传输到客户端缓存更新的情况下，很可能导致客户端去请求了旧的已下线的服务，也无法保证实时感知。
5. 可见，基于客户端实现的负载均衡算法，由于本地暂存了旧的服务列表，在更新期间导致的不一致性是不可避免的，因此，是无法实现实时感知的。
6. 而基于 CP 模型的服务端负载均衡，虽然可以解决这个问题，但又出现了新的问题，由于设计反向代理的转发，以及代理服务器在更新服务列表时，会出现一端时间不可用，这问题更严重，也不是个好方案。
7. 所以，服务下线，要保证客户端 100% 实时感知，在目前实现是比较困难的，因此，需要提供一种高可用的负载均衡算法，使得在服务下线时，客户端能够尽量不去调用它们，而是去调用还在线的服务，以提高系统可用性。

#### 8.2.1.3. 设计一个负载均衡算法，实现服务下线时，客户端能够实时感知？

1. 首先，注册中心采用 CP 模型，保证服务端层面的注册列表数据一致性。
2. 然后，客户端在第一次拉取到服务列表信息后，会注册监听器，在服务端的注册列表发生变更时，由注册中心主动通知客户端进行更新，这都与上面一样。
3. 不同的来了，客户端会对历史用过的服务实例，按照平均响应时间进行质量评分，时间越长，评分越低，发生过超时的会被评为 60 分以下，平均响应时间在 200 ms 以上会被评为 60 分 ~ 90 分，响应时间在 0 ms ~ 200 ms 的会被评为 90 分以上。质量评分在每次调用后，会进行更新，以保证 90 分以上的服务实例，能够一直维持高质量。
4. 客户端优先从高质量服务实例列表中（如果质量高的没有，则选择质量次高的），随机抽取一个实例进行调用，一来认为质量高的会一直质量高，发生服务下线的可能性不大，二来随机抽取，可以防止某个实例被过热调用。
5. 当然，即使是从质量好的服务，也会有宕机的时候，所以，在上面调用失败时，会将该实例打标为 60 分以下、从高质量实例列表中剔除，然后进行降级，降级时会重新选取一个高质量服务实例，重新发起调用，重试 n 次，当重试次数用完后，如果还没能调用成功，则根据配置的策略，走默认降级逻辑、或者抛出异常。
6. 从而，满足高可用负载均衡算法，在服务下线时，客户端能够尽量不去调用它们，而是去调用还在线的服务，以提高系统可用性。

