十三、庞大系统介绍

| 考察维度（2022-02-26 更新） | 精讲部分（必须掌握、背熟）                                   |
| --------------------------- | ------------------------------------------------------------ |
| Redis                       | 分布式锁、Redis 常用数据结构                                 |
| 消息队列                    | 可靠性保证、Kafka 日志存储原理                               |
| MySQL                       | MVCC、Explain、SQL 语句调优、分布式 ID 实现方案              |
| JVM                         | 运行时数据区结构、垃圾收集器、垃圾回收算法                   |
| Spring                      | IOC 原理、AOP 原理、Spring 事务原理、Spring 启动原理、SpringBoot  自动装配、MVC 原理 |
| Dubbo                       | 架构图、Provider 服务注册原理、Consumer 服务引用原理、调用原理 |
| Java                        | HashMap、ConcurrentHashMap、Synchronized、Volatile、ThreadPoolExecutor、AQS 原理、Queue |

#### 1.1.1.1. 项目上用到了 SpringCloud 哪些组件？

##### 1、总

用到了 Eureka 做**服务注册**，Feign 做**服务通信**，Ribbon 做**负载均衡**，Sleuth+Zipkin 做**链路追踪**，Config 做**配置中心**、Stream+Bus 做**消息驱动**。

##### 2、分

1. 服务注册的意思就是，在微服务中，由于服务可能会比较多，其 IP 也经常会发生变化，如果**手动维护**每个服务的 IP+端口 的话，那么就太耗时耗力了，此时可以通过服务启动时，**上报**包括自己当前的 IP 地址、端口、健康状态等信息，到一个注册中心中，由注册中心来统一收集并管理它们，从而减轻人工维护的成本，且信息也比较及时和准确。
2. 服务通信的意思就是，在微服务中，由于模块与模块之间，常常需要相互获取数据，那么就需要进行通信，此时可以到注册中心那里，获取一份想要访问的服务器的**注册表**，得到它的 IP+端口，然后发起比如 HTTP 的远程调用，从而获取到某个参数下远程服务器的执行结果。
3. 负载均衡的意思就是，在微服务中，由于服务实例常常不止一个，如果没有一个合适的访问策略，那么很可能会导致某个实例**访问过热**，此时就需要一种策略来**打散这些流量**，这就是负载均衡，在 SpringCloud 中的实现是 Ribbon 组件，其原理是服务实例启动时，通过获取注册中心中所有的服务列表，然后缓存到机器内存中，在要发起服务调用时，会经过 Ribbon 配置的负载算法，来保证调用的均衡性。
4. 链路追踪的意思就是，在微服务中，由于一个前端请求，可能会拉起多个服务间的调用，导致排查问题可能需要跨机器的挨个挨个排查，浪费人力精力，此时可以通过使用一个**全局 ID** 来标记当前请求，在需要跨机器请求时，全局 ID 不变，但生成一个新的**局部 ID**，以及**时间戳**，传递到下一台机器，这样全局 ID 标记也就跟着过去，同时，还可以通过使用当前时间戳减去传过去的时间戳，从而计算出本次跨机器请求所花费的时间，完成链路追踪。
5. 配置中心的意思就是，在微服务中，由于服务实例可能会有很多，且每个模块的业务、参数等配置可能会不一样，导致配置可能会有多个版本，如果以人工的方式去登记每个模块配置的不同之处，识别哪个版本是哪个服务的配置的话，很容易出错，且难以维护和管理，此时可以抽象出一个配置中心，通过**按路径、按标签**地存放所有服务模块的配置信息，在每个服务启动时，再去配置中心**拉取**对应自己的配置，填入自己配置的**占位符**中，然后再去加载容器，从而实现**远端配置**的统一管理与控制。
6. 消息驱动的意思就是，在微服务中，常常需要用到消息中间件，但不同中间件的 API 使用方式不同，比如业务代码基于 RabbitMQ 做了实现，如果某一天想要更换底层为 Kafka，就需要对原有的业务代码进行修改，此时，可以抽取出一个统一的消息驱动组件，来**屏蔽底层的实现差异**，只负责输入输出消息即可，从而实现一套从上层来看是以**消息作为事件驱动**的架构。

##### 3、总

综上，就是我对 Spring Cloud 微服务的一个理解，请问有什么细节需要补充的吗？

#### 1.1.1.2. 如何在 Feign 调用时增强一下 Header？

关键字：RequestInterceptor#apply()、RequestTemplate、ThreadLocal。

##### 1、总

可以实现 `RequestInterceptor#apply()` 方法，该方法会传入一个 `RequestTemplate`，然后从上下文，比如 `ThreadLocal` 中取出需要增强的属性，然后设置到 `RequestTemplate#header` 中即可，比如这样：

```java
@Component
@Slf4j
public class FeignRequestInterceptor implements RequestInterceptor {
    @Override
    public void apply(RequestTemplate requestTemplate) {
        // Member User信息
        MemberUser memberUser = GlobalMemberContext.getMemberUser();
        if(memberUser != null){
            try {
                requestTemplate.header(ProjectConstant.MEMBER_USER_INFO, URLEncoder.encode(JSON.toJSONString(memberUser), Charsets.UTF_8.name()));
            } catch (UnsupportedEncodingException e) {
                log.info("member user编码失败");
            }
        }
    }
}
```

##### 2、分

其中，Feign 的底层原理是这样的，

- **注入原理**：
  1. 由于 SpringBootApplication 上打了 @EnableFeignClients 注解，所以会回调注解导入的 **ImportBeanDefinitionRegistrar**#registerBeanDefinitions 方法，该方法可以扫描并注册 FeignClient。
  2. 其中注册 FeignClient 时， 会在 BeanDefinitionRegistry 中添加 FeignClientFactoryBean#beanDefinition。
  3. 由于 FeignClientFactoryBean 实现了 FactoryBean 接口，所以在对应的 FeignClient 被注入时，则会调用 **FactoryBean#getObject** 方法。
  4. 然后该 FeignClientFactoryBean#getObject 方法在构造 Feign.Builder 时，获取到容器中所有打了 @Component 注解，且实现 **RequestInterceptors** 接口的拦截器，然后注入到 Feign.Builder#requestInterceptors 属性中。
  5. 接着就是使用 Feign.Builder 来构建 LoadBalancer 的动态代理，返回对应的动态代理对象。

```java
Map<String, RequestInterceptor> requestInterceptors = context
				.getInstances(this.contextId, RequestInterceptor.class);
if (requestInterceptors != null) {
    builder.requestInterceptors(requestInterceptors.values());
}
```

- **执行原理**：
  1. 当业务方法调用注入的 FeignClient 实例对应的接口方法时，则会触发 JDK 动态代理，回调到 **InvocationHandler** 的 invoke 方法。
  2. 在 invoke 方法中，会去遍历所有的 **requestInterceptors**，并执行他们的 **apply** 方法，从而实现 Header 的增强。

```java
final class SynchronousMethodHandler implements MethodHandler {
    @Override
    public Object invoke(Object[] argv) throws Throwable {
        ...
        return executeAndDecode(template, options);
    }
    
    Object executeAndDecode(RequestTemplate template, Options options) throws Throwable {
    	Request request = targetRequest(template);
        ...
    }
    
  	Request targetRequest(RequestTemplate template) {
    	for (RequestInterceptor interceptor : requestInterceptors) {
      		interceptor.apply(template);
    	}
    	return target.apply(template);
  	}
}
```

##### 3、总

以上，就是我对 Feign Client 的一个理解，请问有什么细节需要补充的吗？

#### 1.1.1.3. Feign 的调优参数有哪些，以及HTTP 连接池优化？ 

| 参数解释                               | 具体配置                                                     |
| -------------------------------------- | ------------------------------------------------------------ |
| 具体服务的连接超时时间（ms）           | xxx.ribbon.ConnectionTimeout                                 |
| 具体服务的读取超时时间（ms）           | xxx.ribbon.ReadTimeout                                       |
| 具体服务的重试开关（true / false）     | xxx.ribbon.okToRetryOnAllOperations                          |
| 具体服务的超时重试次数（不包括首次）   | xxx.ribbon.MaxAutoRetries                                    |
| 具体服务的超时重试机器数（不包括首台） | xxx.ribbon.MaxAutoRetriesNextServer                          |
| 更换 Feign 的客户端（池化连接对象）    | feign.okhttp（新起之秀） / .httpClient（老牌 HC），两者性能差距不大，默认为  JDK 的 HttpUrlConnection |
| 日志调用配置                           | feign.client.config.default.loggerLevel：full                |
| 拦截器配置                             | 实现 RequestInterceptor#apply()                              |
| 自定义解码器                           | 实现 feign.codec.Decoder#decode()                            |
| 降级配置                               | @FeignClient fallback：继承接口自定义 Handler，或者实现 FallBackFactory#create() |

#### 1.1.1.4. SpringBoot 自动装配和自定义 starter？

##### 1、总

Starter 是什么：
1. Starter 就是一个 jar 包，写一个 @Configuration 的配置类，把这些 Bean 的定义都包含在其中，然后在 Starter 包下的 `META-INF/spring.factories` 中写入该配置类，SpringBoot 程序在启动时，就会按照约定来加载该配置类。
2. 开发人员只需要将相应的 Starter 包依赖进应用中，然后进行相关的属性配置，就可以进行代码开发，而不需要再单独对 Bean 进行配置。

##### 2、分

原理：
1. 当启动 SpringBoot 应用时，会先创建 SpringApplication 的对象，在对象的构造方法中，会进行某些参数的初始化工作，最主要的是判断当前应用的类型（比如 Servlet 类型），以及通过 SPI 的方式加载整个应用的 `spring.factories` 文件中的初始化器和监听器的 Class。
2. SpringApplication 对象创建完成之后，开始执行 `run()` 方法，来完成整个启动，启动过程中最主要的有两个方法，第一个叫做 `prepareContext()`，第二个叫做 `refreshContext()`，在这两个步骤中完成了自动装配的核心功能，而其他方法的处理逻辑包含了上下文对象的创建、Banner 的打印、异常报告期的准备等各个准备工作，方便后续来进行调用。
3. 在 `prepareContext()` 方法中，主要完成了对上下文对象的初始化工作，包括比如 `Enviroment`对象属性值的设置，在整个过程中，有一个非常重要的方法 `load()`，`load()` 主要完成一件事，那就是将**启动类**作为BeanDefinition 注册到 Registry 中，方便后续在进行 BeanFactoryPostProcessor 调用执行时，找到对应的启动类来完成 `@SpringBootApplication` 和 `@EnableAutoConfiguration` 等注解的解析工作。
4. 在 `refreshContext()` 方法中，会进行整个容器的刷新过程，会调用 Spring 中的启动流程，即`AbstractApplicationContext#refresh()`，有 13 个关键方法，来完成整个 Spring 应用的启动，其中会调用 `invokeBeanFactoryPostProcessor()` 方法，主要是对`ConfigurationClassPostProcessor` 的处理，会先调用实现 `BeanDefinitionRegistryPostProcessor` 接口的 `postProcessBeanDefinitionRegistry()` 方法，然后再调用自己实现的 `postProcessBeanFactory()` 方法，处理各种包括 @PropertySource、@ComponentScan、@Import、@ImportResource、@Bean 等注解。
5. 其中，在解析 @Import 注解时，会有一个 `getImports()` 的方法，会从**启动类**开始递归解析注解，把所有包含 @Import 注解都收集到，然后在 `processImport()` 方法中，对 Import 导入的类进行分类，这里主要起识别作用的是 `ImportSelect` 的实现类  `AutoConfigurationImportSelect`，来调用 `selectImports()` 方法使用 SPI 的方式，来获取并加载 `spring.factories` 中的 `EnableAutoConfiguration` 自动装配配置类的 Class，完成自动装配。
6. 接着，还调用子类上下文 `ServletWebServerApplicationContext#onRefresh` 方法，来拉起嵌入式的 Tomcat 容器。
7. 最后，就是实例化 Bean，即 `finishBeanFactoryInitialization()` 方法，主要是实例化所有剩下非懒加载的单例 Bean，并完成它们的初始化和依赖注入，通过遍历所有 beanNames，然后挨个判断是走 FactoryBean 的流程，还是走 BeanFactory 的流程，其中主要步骤总结起来分为 3 步，分别是 NewInstance 实例化、Populate 属性赋值 和 Initialization 初始化，从而完成自动装配配置 Bean 的注入。

##### 3、总

自定义 Starter：

1. 创建 Starter 项目，定义 Starter 需要的 Properties 配置类，比如数据库连接信息等。
2. 然后编写自动配置类，自动配置类就是获取配置，根据配置来自动装配 Bean。
3. 编写 `META-INF/spring.factories` 文件，以让 SpringBoot 在启动时加载自动配置类。
4. 然后在项目中，引入自定义 Starter 的 Maven 依赖，增加对应的配置值后，然后即可直接使用。

#### 1.1.1.5. SpringMVC 请求前改写 @RequestBody 参数？

##### 1、总

有三种方法，分别是 (Filter + HttpServletRequestWrapper) + HandlerInterceptor + WebMvcInterceptorConfig、RequestBodyAdvice + @ControllerAdvice 和 Controller AOP。

| Spring MVC 请求扩展点                 | 执行顺序 | 方法                                                         | 作用                                                         |
| ------------------------------------- | -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Filter                                | 1        | doFilter(ServletRequest, ServletResponse)                    | 根据 params、header 过滤非法请求、包装 HttpServletRequest，不做改写 Body |
| HandlerInterceptor                    | 2        | preHandle(HttpServletRequest, HttpServletResponse, Object)   | Handler Method 处理前拦截                                    |
| RequestBodyAdvice + @ControllerAdvice | 3        | afterBodyRead(Object, HttpInputMessage, MethodParameter, Type, Class) | HandlerAdapter#handleInternal 执行过程时解析 Method 参数     |
| Controller AOP                        | 4        | @Before、@Around                                             | point.proceed(args) 前改写 args 参数                         |

##### 2、分

###### HandlerInterceptor | 实现最麻烦

**实现最麻烦**，需要在 Filter 处添加 RequestWrapper 包装，然后在包装类里替换掉 Reader 读取的输入流，最后在 MVC 的拦截器 preHandle 实现 body 替换。

```java
@Component
public class UserFilter implements Filter {
    @Override
    public void init(FilterConfig filterConfig) throws ServletException {
        System.err.println("init");
    }

    @Override
    public void destroy() {
        System.err.println("destroy");
    }

    @Override
    public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException {
        ((ResponseFacade) response).addHeader("sign", request.getParameter("sign"));
        System.err.println("doFilter");
        
        // 1、结合UserBodyWrapper包装request
        chain.doFilter(new UserBodyWrapper((HttpServletRequest) request), response);
    }
}

public class UserBodyWrapper extends HttpServletRequestWrapper {

    private String body;

    public String getBody() {
        return body;
    }

    public void setBody(String body) {
        this.body = body;
    }

    public UserBodyWrapper(HttpServletRequest request) throws IOException {
        super(request);

        // 2、先读取一次输入流, 获取body内容
        this.body = IOUtils.toString(request.getInputStream(), StandardCharsets.UTF_8);
    }

    @Override
    public ServletInputStream getInputStream() throws IOException {
        ByteArrayInputStream bais = new ByteArrayInputStream(body.getBytes(StandardCharsets.UTF_8));

        // 3、以后读取输入流时, 只读取内存中的这个body
        return new ServletInputStream() {
            @Override
            public boolean isFinished() {
                return false;
            }

            @Override
            public boolean isReady() {
                return false;
            }

            @Override
            public void setReadListener(ReadListener listener) {

            }

            @Override
            public int read() throws IOException {
                return bais.read();
            }
        };
    }

    @Override
    public BufferedReader getReader() throws IOException {
        // 4、以后读取输入流时, 只读取内存中的这个body
        return new BufferedReader(new InputStreamReader(getInputStream()));
    }
}

public class UserHandlerInterceptor implements HandlerInterceptor {

    @Override
    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {
        System.err.println("preHandle");

        UserBodyWrapper userBodyWrapper = (UserBodyWrapper) request;
        String body = userBodyWrapper.getBody();
		
        // 5、获取body，改写body
        User user = JSONObject.parseObject(body, User.class);
        user.setUsername("测试添加前缀" + user.getUsername());
        ((UserBodyWrapper) request).setBody(JSONObject.toJSONString(user));
        
        return true;
    }

    @Override
    public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception {
        System.err.println("postHandle");
    }

    @Override
    public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception {
        System.err.println("afterCompletion");
    }
}

@Configuration
public class WebMvcInterceptorConfig implements WebMvcConfigurer {

    @Override
    public void addInterceptors(InterceptorRegistry registry) {
        // 6、添加WebMvc拦截器配置类
        registry.addInterceptor(new UserHandlerInterceptor()).addPathPatterns("/**");
    }
}
```

###### RequestBodyAdvice | 时机最适合

```java
@ControllerAdvice
public class UserRequestBodyAdvice implements RequestBodyAdvice {

    @Override
    public boolean supports(MethodParameter methodParameter, Type targetType, Class<? extends HttpMessageConverter<?>> converterType) {
        System.err.println("supports");
        return true;
    }

    @Override
    public HttpInputMessage beforeBodyRead(HttpInputMessage inputMessage, MethodParameter parameter, Type targetType, Class<? extends HttpMessageConverter<?>> converterType) throws IOException {
        System.err.println("beforeBodyRead");
        return inputMessage;
    }

    // 1、在 HandlerAdpater#resolveArgments 时，HttpMessageConverter读取完body后，进行回调改写body
    @Override
    public Object afterBodyRead(Object body, HttpInputMessage inputMessage, MethodParameter parameter, Type targetType, Class<? extends HttpMessageConverter<?>> converterType) {
        System.err.println("afterBodyRead");

        JSONObject jsonObject = JSONObject.parseObject(JSONObject.toJSONString(body));
        Long id = jsonObject.getLong("id");
        String username = jsonObject.getString("username");
        String password = jsonObject.getString("password");

        try {
            Constructor constructor = ((Class) targetType).getConstructor(Long.class, String.class, String.class);
            return constructor.newInstance(id, username + "测试添加后缀", password);
        } catch (Exception e) {
            e.printStackTrace();
        }

        return body;
    }

    @Override
    public Object handleEmptyBody(Object body, HttpInputMessage inputMessage, MethodParameter parameter, Type targetType, Class<? extends HttpMessageConverter<?>> converterType) {
        System.err.println("handleEmptyBody");
        return body;
    }
}
```

###### Controller AOP | 实现简单强大

```java
@Aspect
@Component
public class AspectJConfig {

    @Pointcut("execution(* com.jsonyao.cs.controller.*.*(..))")
    private void pointcut() {

    }

    @Around("pointcut()")
    public Object around(ProceedingJoinPoint point) throws Throwable {
        System.err.println("around");
        long start = System.currentTimeMillis();
        
        // 1、业务方法调用前，改写body
        Object[] args = point.getArgs();
        User user = User.class.cast(args[0]);
        user.setUsername(user.getUsername() + "测试AOP后缀");
        
        Object res = point.proceed(new Object[] {user});
        
        long end = System.currentTimeMillis();
        System.err.println("执行结果: " + res + ", 消耗时间: " + (end - start));
        return res;
    }
}
```

##### 3、总

综上，HandlerInterceptor  实现最麻烦，但处理位置比较通用，适合一些公共的设置，RequestBodyAdvice 回调的位置**最适合改写 @RequestBody**，适合统一对 @RequestBody 进行设置，Controller AOP 更强大，实现也简单，适合一些通用方法级别的拦截。

#### 1.1.1.6. Spring MVC 拦截器和过滤器的区别？

##### 1、总

拦截器 HandlerInterceptor，过滤器 Filter，虽然可以对请求进行一定处理，但：

1. **实现人不同**：HandlerInterceptor 属于 Spring#Web 包下，Filter 属于 Tomcat#javax 包下。
2. **配置方式不同**：HandlerInterceptor 在 Spring#WebMvcConfigurer 中配置，而 Filter 在 web.xml 中配置。
3. **处理时机不同**：HandlerInterceptor 作用更强大，方法处理前的拦截、处理后的拦截、返回前的拦截，而 Filter 则是在进入DispatcherServlet 前进行一定的请求过滤处理。

##### 2、分

![1645947452956](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1645947452956.png)

Spring MVC 客户端请求的生命周期管理：

1. **Filter#doFilter** 执行链过滤客户端请求。
2. DispatcherServlet，表示中心控制器，在客户端发出请求后，经过 Web 容器（比如 Tomcat）后，会打到 DispatcherServlet 上，并由其来处理请求。
3. HandlerMapping，表示处理器映射，DispatcherServlet 收到请求后，会调用 HandlerMapping，HandlerMapping 会根据请求 url 去查找对应的 Handler，即一个 Handler Method 对象，指的是 url 对应 Controller 中的对应方法。
4. HandlerExecutionChain，表示处理器执行链，Handler 解析完 url  后，会返回一个处理器执行链给DispatcherServlet，其中就包括一堆 HandlerInterceptor。
5. HandlerAdapter，表示处理器适配器，DispatcherServlet 会按照规则去匹配对应的 HandlerAdapter。
6. 再由对应的 Handler Method 去处理请求，其中就包括调用去执行我们编写的 Controller 业务逻辑。
   - 但在执行前，会先调用 **HandlerInterceptor#preHandle** 进行  handler 方法处理前的拦截。
7. Controller 代理对象，则会去访问数据库，填充好模型后，再把 ModelAndView 返回给 HandlerAdapter。
   - 其中，在 HandlerAdpater#resolveArgments 时，HttpMessageConverter读取完 body 前/后，可以对 **RequestBodyAdvice#beforeBodyRead/afterBodyRead** 进行回调，从而改写 body。
   - 而我们平常使用的 @ResponseBoday 的方法，返回的 ModelAndView 为空，也就是不会返回视图给客户端，而是经过 `RequestResponseBodyMethodProcessor`，把 JSON 串写入到 Response 的 Body 中。
8. HandlerAdapter 收到后，再将 ModelAndView  传递给 DispatcherServlet。
9. DispatcherServlet 收到后，则调用视图解析器 ViewResolver，来解析 ModelAndView。
10. ViewResolver 会解析逻辑视图名，根据逻辑的 View 找到实例的 View，并返回给 DispatcherServlet。
11. DispatcherServlet 收到后，则根据 ViewResolver 解析出的 View，调用对应的实际视图，结合 Model 进行渲染。
    - 但在执行前，会先调用 **HandlerInterceptor#postHandle** 进行 handler 方法处理后的拦截。
12. 最后，DispatcherServlet 再将渲染后的 View 作为结果，响应给客户端。
    - 在执行后，返回结果前，会调用 **HandlerInterceptor#afterCompletion** 进行响应前的拦截。

##### 3、总

以上，就是我对 Spring MVC 拦截器和过滤器的一个理解，请问有什么细节需要补充的吗？

#### 1.1.1.7. Kafka 的高可靠保证？

##### 1、总

对于 Kafka 高可靠，需要从 3 个角度进行考虑，分别是生产端、Broker 端以及消费端的高可靠保证。

##### 2、分

1. **生产者端**：通过 @Transactional 事务管理 + 本地消息表打标 + `acks=all`（或者 `acks=1`）  + 定时重发 + 失败补偿来保证，即保证本地消息表和业务表属于同一个事务，业务落库时消息也落库，使用 `acks=all` 保证消息成功写入所有 ISR，使用定时重发 + 失败补偿，可以保证消息最终会写入 Broker。
2. **Broker 端**：通过 ISR 机制来保证，配合生产端 `acks=all` 来保证每次写入消息，必须在 ISR 集合中的所有 Broker 写入成功后才认为消息写入成功，响应 ACK 给生产端。
3. **消费端**：通过 `enable-auto-commit=false` 关闭自动 ACK，`ack-mode=manual` 打开手工 ACK，在处理消息完毕后，才进行一次手工 ACK `acknowledgement.acknowledge()`，避免处理异常后，不能再次重复消费的情况。

##### 3、总

以上，就是我对 Kafka 高可靠保证的一个理解，请问有什么细节需要补充的吗？

#### 1.1.1.8. RabbitMQ 的高可靠保证？

##### 1、总

与 Kafka 高可靠类似，对于 RabbitMQ 高可靠，也需要从 3 个角度进行考虑，分别是生产端、Broker 端以及消费端的高可靠保证。

##### 2、分

1. **生产者端**：通过 @Transactional 事务管理 + 本地消息表打标 +  `publisher-confirms=true` 以及实现 ConfirmCallback 函数开启 Broker 消息 Confirm 机制  + 定时重发 + 失败补偿来保证，即保证本地消息表和业务表属于同一个事务，业务落库时消息也落库，使用 Broker Confirm 机制，保证消息成功写入 Broker，使用定时重发 + 失败补偿，可以保证消息最终会写入 Broker。
2. **Broker 端**：通过 `druable=true` 开启持久化队列 + 生产端消息设置 `deliveryMode=2` 开启持久化消息 ，来保证消息写入后，如果未被消费前，会保存在 Broker 中，防止 MQ 丢消息。
3. **消费端**：通过 `acknowledge-mode=mannul` 关闭自动 ACK，打开手工 ACK，在处理消息完毕后，才 `channel.basicAck` 进行一次手工 ACK，避免处理异常后，RabbitMQ 立即把消息删除，丢失消息的情况。

##### 3、总

以上，就是我对 RabbitMQ 高可靠保证的一个理解，请问有什么细节需要补充的吗？

#### 1.1.1.9. JDK 的新特性、新 JVM？

| JDK 版本 | 新特性                                                       |
| -------- | ------------------------------------------------------------ |
| JDK 5    | 自动拆装箱、 Foreach 循环、枚举类、泛型、JUC                 |
| JDK 6    | 对脚本语言 Ruby、Groovvy、JS 的支持                          |
| JDK 7    | switch 支持 String，开始转移永久代，静态变量、字符串常量池转移到了堆中 |
| JDK 8    | 函数式接口，Lambda 表达式，Stream API、接口支持 default 方法、HashMap 和 ConcurrentHasMap 性能提升、元空间完全取代永久代 |
| JDK 9    | 集合添加 List.of(xxx, xxx) 等工厂方法 、接口支持 private 方法，默认使用 G1垃圾收集器 |
| JDK 10   | G1 多线程并行 Full GC，降低 G1 STW 时间                      |
| JDK 11   | 新增 ZGC，比 G1 更细粒度的内存管理，采用并行回收策略         |
| JDK 12   | 新增 Shenandoah GC 算法、优化 G1 将垃圾分为强制部分和可选部分，强制部分会被回收，可选部分可能不会被回收，提高 GC 效率 |
| JDK 13   | ZGC 优化，将标记长时间空闲的堆内存返还给操作系统，只要保证堆大小不会小于 -Xms 即可 |
| JDK 14   | 删除 CMS 、弃用  Parallel Scavenge + SerialOld 的 GC 组合、将 ZGC 应用到 MacOS 和 Win 中 |
| JDK 15   | 新增隐藏类、密封类（避免抽象类被滥用）                       |
| JDK 16   | ZGC 性能优化，此版本相当于是对 JDK 14 和 15 的一些特性进行了正式的引入 |
| JDK 17   | 正式引入密封类 sealed class，限制抽象类的实现                |

#### 1.2.1.1. JDK 8 VS JDK 7？

| JDK 8 新特性                          | 解释                                                         |
| ------------------------------------- | ------------------------------------------------------------ |
| 函数式接口，Lambda 表达式，Stream API | 在需要一个函数，但又不想费神去命名一个函数时使用，也就是匿名函数，同时，还可以把函数作为参数，传递进某个方法中 |
| 接口支持 default 方法                 | 接口的默认实现                                               |
| 元空间完全取代永久代                  | 元空间使用本地内存，永久代使用 JVM 内存，从而根本上解决了永久代溢出的问题 |
| HashMap 性能提升                      | 拉链 >= 8 时会准换为红黑树，<= 6 时转换为链表、弃用哈希种子、扩容时无需重新 rehash，只需要分高低位转移链表即可、改用尾插法解决并发插入时的 CPU 100% 问题 |
| ConcurrentHasMap 性能提升             | 拉链 >= 8 时会准换为红黑树，<= 6 时转换为链表、弃用哈希种子、Node+CAS+synchronzied+TreeBin 读写锁，替换掉 Segment+HashEntry+ ReentrantLock 分段锁、采用 CAS+synchronzied 实现渐进式并发扩容 |

#### 1.2.1.2. JDK 8 函数式接口的实现原理？

##### 1、总

1. 函数式接口，指**有且只有一个抽象方法的接口**，接口中的 static 方法、default 方法、Object 方法都不算抽象方法，且有个专门的注解 `@FunctionInterface`，但它不是必须的，如果接口符合以上函数式编程的语义，那么加不加这个注解都不影响，加上只是为了编译器进行检查而已，但如果不符合语义，又加上了该注解，那么编译器则会报错。
2. 有且只有一个抽象方法的原因是，由于函数式接口 ` () -> {} ` 的写法，主要是为了简化代码，相当于是一个**匿名内部类的匿名函数**，如果有多个抽象方法，编译器则不知道是重写哪个方法了，所以只能有且只有一个抽象方法。

##### 2、分

在 JDK 8 中，函数式接口主要分为 4 类，分别是**供给型、消费型、断言型和方法型**，其中它们的方法又可以和定义的 default 方法进行**连用**。

| 接口           | 方法               | 说明                                          |
| -------------- | ------------------ | --------------------------------------------- |
| Supplier< T >  | T get();           | 供给型，无参，返回一个函数返回的一个泛型对象  |
| Consumer< T >  | void accept(T t);  | 消费型，传入一个泛型对象，但没有返回值        |
| Predicate< T > | boolean test(T t); | 断言型，传入一个泛型对象，但返回 boolean 类型 |
| Function< T >  | R apply(T t);      | 方法型，传入一个泛型对象，返回另一个泛型结果  |

##### 3、总

以上就是我对 JDK 8 函数式接口的理解，它是 Lambda 表达式和 Stream API 的基础，使用这种方式来编码，既简洁又高效。

#### 1.2.1.3. 线程池核心参数，以及选取原则？

##### 1、总

对于这个问题，我打算从线程池的概念、线程复用原理、线程淘汰原理、构造参数、数据结构、生命周期、工作原理和调优原则这几个方面进行回答。

##### 2、分

1. **线程池的概念**：线程池，ThreadPoolExecutor，允许使用多个线程之一，来执行每个提交的任务，通过**线程复用**，来降低线程创建和销毁所带来的开销，同时任务到达时，可以无需等待线程的创建就能立即执行，从而提高任务的处理速度。

2. **线程复用原理**：通过设计一个**任务队列**，来承放并缓冲更多的执行任务，使得**本已存在的线程**，在处理完它们手上的任务后，可以立马从任务队列的**另一端取出执行任务**，接着继续往下执行，周而复始，从而不用每次都构建一个新的线程再执行，实现线程复用。

3. **线程池的构造参数**：但是，这种线程复用太过于简单暴力，为了让线程池稳定可控，还需要其他参数进行优化：

   - 考虑到，那些本已存在的线程应该有个上限，就需要指定一个 `corePoolSize` 核心线程数，核心线程的意思就是，默认情况下没有保活时间，不会被回收，在不超 `corePoolSize` 上限且收到新任务时会被创建。
   - 考虑到，在任务过多，核心线程处理不过来的情况，就需要指定一个 `maximumPoolSize` 最大线程数。
   - 考虑到，在任务峰值过后，非核心线程可能会空闲一段时间，但仍然占据系统资源的情况，就需要指定一个 `keepAliveTime` 非核心线程的最大空闲时间以及 `TimeUnit` 时间单位。
     - 这也是**线程淘汰**的原理所在，如果在从任务队列中取任务的时间，超过了指定的 `keepAliveTime` 还没取到任务，则可以认为队列中没有多余的任务了，也就是轮训任务队列的这个线程空闲了 `keepAliveTime` 这么久的时间，那么就要对这个线程进行淘汰处理，以节省系统资源。
   - 考虑到，任务需要装入任务队列，就需要指定一个 `BlockingQueue` 阻塞队列接口的具体实现。
   - 考虑到，在构建核心或者非核心线程时，可能需要对线程本身进行一些，比如线程名称等参数设置的情况，就需要指定一个 `ThreadFactory` 的具体实现。
   - 考虑到，任务队列和最大线程都超上限，即线程池超负载时，任务还源源不断到来的情况，就需要指定一种 `RejectedExecutionHandler` 的具体实现，来执行相应的拒绝策略逻辑。

4. **线程池的数据结构**：

   - 根据以上分析，可以容易得到，如果构建一个好点的线程池，就至少需要持有核心线程数、最大线程数、非核心线程的最大空闲时间、任务队列、线程工厂、拒绝策略程序的引用。
   - 另外，JDK 在实现方面，还抽象了一个 `Worker` 类，通过使用 `Worker` 自己持有的 `Thread` 实例，在轮训任务队列时进行任务消费。
   - 同时还持有了一个 `AtomicInteger` 原子类型的 `ctl  ` 线程池控制位，来管理线程池的生命周期及有效线程的数量。

5. **线程池的生命周期**：线程池控制位 ctl，JDK 把 Integer 的高 3 位作为**线程池的状态**，低 29 位作为**有效线程的数量**，前者一个存在 5 种状态，规定：

   -  **-1 为 RUNNING 运行态**，此状态下能够接收新提交的任务，同时还能处理任务队列中的任务。
   -  **0 为 SHUTDOWN 关闭态**，此状态下不会再接受新提交的任务，但还可以继续处理任务队列中的任务。
   -  **1 为 STOP 停止态**，此状态下不会再接收新提交的任务，也不能继续处理任务队列中的任务，并且还会尝试中断正在处理任务的线程。
   -  **2 为 TIDYING 整理态**，此状态下所有的线程都已终止了，此时有效工作数量为 0。
   -  **3 为 TERMINATED 终止态**，在 TIDYING 整理态回调完 `terminated()` 钩子函数以后，会进行此状态。

   ![1645968027493](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1645968027493.png)

6. **线程池的工作原理**：

   1. 首先根据线程控制位 `ctl`，判断当前有效线程数是否小于设定的核心线程数，如果是小于，那么就可以把当前任务作为首个任务 `firstTask`，去**创建核心线程**并执行任务。
   2. 如果发现超出了核心线程数，或者并发下核心线程创建失败了，那么就在检查完线程池还在运行状态后，就尝试**把任务投递到任务队列中**。
   3. 如果任务投递成功，则还要做线程池状态的双重检查，防止线程池突然被关闭，如果发现线程池确实不再是运行态了，那么就需要把刚才投递成功的任务给取出来，再**执行拒绝策略程序**；而如果发现还是运行态，则视有效线程数量是否为 0，来决定是否需要**补充非核心线程**，以保证任务队列中的任务不会永远停留在内存中。
      1. 注意这里**补充非核心线程**的操作，它可以保证 `corePoolSize=0 & maximumPoolSize > 0` 且任务队列还没达到上限时，仍能生成 1 个非核心线程去消费任务队列，避免队列内存溢出的发生，关于这里的细节就可以说说我们的一段生产事故了~
      2. 我们系统某个模块中常常需要使用别的模块的数据源，比如在 POS 模块中需要去查询 PRICE 库，此时，由于主线程默认绑定的是 POS 数据源，要想使用 PRICE 数据源的话，就需要开辟子线程去做切换，所以就为每个模块抽象了数据源切换的工具类。
      3. 由于正常都是调用一次切换方法，也就是把 Lambda 表达式提交给线程池，主线程再去做 `FutureTask.get()` 的监听，拿到结果后返回，这是正常情况，因此我们一开始也没怎么留意。
      4. 但问题就来了，有一次某段代码对连续进行了两次的切换，就发现接口超时，以及往后的切换都超时了，那时排查了很久，最后在这个切换工具类里看到，生产上跑的参数为 `corePoolSize=0、maximumPoolSize=32、workQueue=LinkedBlockingQueue`，按照原理，本以为是错误的配置，会导致任务都阻塞在队列中直到 OOM 发生的，但改成一次切换又可以了，这就很奇怪了，然后就去研究源码，发现刚才上面所说的那段**补充非核心线程**的逻辑，也就是即使 `corePoolSize=0`，只要最大线程数不为 0，那么还是会保证至少创建一个线程去消费任务队列的，所以切换一次就可以拿到任务的运行结果。
      5. 那么为什么切换两次就不行了呢？这是因为，在第一次切换时，是主线程在做 `FutureTask.get()`，阻塞的是主线程，而如果还做一次连续切换，那么就会导致线程池中仅有的 1 个线程也做了 `FutureTask.get()`，从而进入了阻塞，但线程池认为有效线程数还是 0，所以新任务进来也不会去创建新线程消费，会直到 OOM 的发生为止~
      6. 最后，更正了合理的构造参数 `corePoolSize=Runtime.getRuntime().availableProcessors() + 1`，`maximumPoolSize=2 * Runtime.getRuntime().availableProcessors()`，`keepAliveTime=120s`，`LinkedBlockingQueue=1024` 保证不会发生内存溢出，`Google#ThreadFactoryBuilder`来设构建线程工厂、在线程创建时设定名称前缀和递增线程号，`RejectedExecutionHandler=CallerRunsPolicy` 保证任务不丢失，同时设置 `allowCoreThreadTimeOut=true` 允许核心线程空闲时也会被回收。
   4. 如果任务投递失败了，则**尝试补充非核心线程**，如果因为线程池不是为运行态，或者超出了最大核心线程数，导致非核心线程补充失败的话，那么就需要**执行拒绝策略程序**，因为此时要么是 SHUTDOWN 关闭状态不能接受新任务了，要么就是任务队列满了需要拒绝添加新任务了。

   ![1645967491556](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1645967491556.png)

7. **线程池的调优原则**：

   - **线程池大小的设置**：需要先确定任务的类型，分为 CPU 密集型、IO 密集型以及混合型的任务：

      | 类型       | 概念                                                         | 目的                                                         | 合理的经验公式                                               |
      | ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
      | CPU 密集型 | 任务需要大量的运算，中间没有阻塞，CPU 一直全速运行           | 需要尽可能少的线程数量，以减少线程上下切换的次数，提高 CPU 的利用率 | CPU 核数 + 1                                                 |
      | IO 密集型  | 任务大量时间都花在 IO 的阻塞上，希望 CPU 尽可能去调度其他任务，而不是在等待阻塞的线程、浪费 CPU 资源，所以需要更多的线程数以供 CPU 调度 | 需要尽可能多的线程数，但过多的线程也会带来过多的上下文切换   | CPU 核数 * 2                                                 |
      | 混合型     | 既有 CPU 密集型的特点，又有 IO 密集型的特点                  | 需要看平均线程等待时间，和平均线程运行时间，来决定对应的线程数，可通过 Github#PoolSize Calculator 工具类进行粗略的计算 | CPU 核心数 * 目标 CPU 利用率 * （1 + 平均线程等待时间 / 平均线程运行时间），可见，平均线程等待时间越长，平均线程运行时间越短，则需要的线程就越多 |

      => 但这些只是经验公式，最优的参数还需要根据实际环境不断压测、调优才能得到。

   - **任务队列的设置**：控制任务队列容量，实际上就是在考量**内存占用**和**任务的排队策略**，常用的阻塞队列实现有：

      | 实现类              | 特性                                                         | 排队策略   | 优点                                                         | 缺点                                                         |
      | ------------------- | ------------------------------------------------------------ | ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
      | SynchronousQueue    | 无界同步队列，容量为 0，不存储任何元素，每个插入操作都会阻塞等到另一个线程进行相应的删除操作才会恢复（利用 CAS 自旋 + LockSupport 方式实现阻塞） | 直接交接   | 最小的内存花销                                               | 当任务到达速度大于处理速度时，如果搭配无界池，可能会出现无限线程增长的问题 |
      | LinkedBlockingQueue | 单向链表有界阻塞队列，容量可选，空参构造时为 Integer.MAX_VALUE，相当于无界队列 | "无界"队列 | 核心线程繁忙时，任务会在队列中排队，适合用于平滑瞬间爆发的流量 | 如果任务到达速度大于处理速度，可能会导致任务队列元素无限增长，占用较大的内存花销 |
      | ArrayBlockingQueue  | 数组有界阻塞队列，初始时必须先指定容量大小，一旦指定，就不能再修改了 | 有界队列   | 与最大线程数一起使用，可以防止资源被耗尽                     | 任务队列初始化好了后，就难以再动态的调整和控制了             |

   - **拒绝策略程序的设置**：当线程池被关闭，或者任务队列和线程都已经饱和时，新提交的任务会走到拒绝策略程序的处理逻辑中，默认的拒绝策略程序都是定义在 ThreadPoolExecutor 的内部类中：

      | 实现类              | 特性                                                         | 适用场景                                                     |
      | ------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
      | AbortPolicy         | 默认的拒绝策略程序，在任务被拒绝时，会抛出 RejectedExecutionException 异常 | 可以阻止系统正常运行下去                                     |
      | CallerRunsPolicy    | 调用 Runnable#run，当前主线程会自己去运行任务                | 不会造成任务的丢失，可以让线程池有一定的缓冲时间             |
      | DiscardPolicy       | 任务会被简单的丢弃掉                                         | 允许任务丢失时，这是最好的一种丢弃策略                       |
      | DiscardOldestPolicy | 丢弃任务队列头部的任务，然后重新执行一开始的工作流程         | 会丢弃最老的一个任务，也就是可能马上就被执行的任务，然后重新提交当前任务 |

##### 3、总

以上，就是我对线程池核心参数及原理的一个理解，请问有什么细节需要补充的吗？

#### 1.2.1.4. 分布式锁的实现方案，以及红锁的缺点？

##### 1、总

分布式锁指的是，在不同的系统或者同一个系统的不同主机之间，共享访问某个资源时，用来互斥地防止彼此干扰保证一致性的锁实现，其实现方式有：

- **基于数据库实现**：通过乐观锁，或者唯一索引实现。
- **基于分布式缓存实现**： 典型的有，通过 Redis 实现。
- **基于分布式一致性算法实现**：典型的有，通过 ZK 实现。

##### 2、分

###### 1）基于数据库实现 | 负担大

- **基于乐观锁实现**：原理是，根据版本号，来判断更新之前有没有其他线程更新过，如果被更新过，则获取锁失败。
- **基于唯一索引实现**：原理是，在表上建立唯一索引，当想要获得锁时，向表中插入一条记录，释放锁时则删除这条记录。
  - **缺点**：
    1. **锁没有失效时间**，解锁失败会导致死锁，此时该唯一索引所有 insert 都会返回失败，其他线程无法再获取到锁。
    2. **不可重入**，同一线程在没有释放锁之前无法再获取到锁。

###### 2）基于分布式缓存实现 | 锁失效

- **基于 Redis 单机实现**：使用 `SET NX EX` 指令加锁，保证**原子性**地给锁设置**过期时间，防止死锁**，使用 LUA 脚本 `redis.call` 指令，保证 key 值判断与删除键指令原子性执行，且防止由于 STW 时间过长，锁被其他进程误删。

  - **缺点**：会出现由于**时钟漂移** 或者 任务执行时间过长，导致的锁被提前释放的问题。
- **基于 Redisson 实现**：它是一个 Redis 的客户端，其分布式锁的实现原理是，让获得锁的线程开启一个定时守护线程，每隔 expireTime / 3 的时间就去检查一下，该线程持有的锁是否还存在，如果存在，则对锁的过期时间重新设置为 expireTime，完成守护线程对**锁的续约**，防止锁由于过期提前释放。

  - **缺点**：这些只是在 Redis 单机实现的分布式锁，加锁时只作用在一个 Redis 节点上，即使通过了 Sentinel 保证了高可用，但由于 Redis 是**异步复制**的，如果在 Master 节点获取到锁后，在未完成数据同步的情况下，发生了故障转移，那么其他客户端上的线程依然可以获取到锁，丧失了锁的安全性。

- **基于 RedLock 算法实现**：红锁算法的原理是，

  1. 先获取当前时间 `t1`，然后按顺序依次尝试从 n 个 Redis 实例，使用相同的 key 和具有唯一性的 value（例如 UUID）来获取锁，当向 Redis 请求获取锁时，除了设置锁的失效时间 `expire`，还应该设置超时时间 `timeout`，且这个超时时间 **<** 锁的失效时间 `expire` ，这样可以避免 Redis 已经挂掉的情况下，客户端不用一直等待响应结果，而是尽快地去尝试另外一个 Redis 实例来获取锁。
  2. 客户端通过使用当前时间 `t3` 减去开始获取锁时间 `t1`，就得到获取锁花费的总时间 `T`，当且仅当从过半数（N/2+1 个）的 Redis 节点都取到锁，并且获取锁花费的总时间 `T` **<** 锁失效时间 `expire` 时，锁才算获取成功，如果获取到了锁，那么 key 的真正有效时间 `real_expire` 等于锁失效时间 `expire` **减去**锁花费的总时间 `T`。
  3. 如果获取锁失败，客户端则应该在所有的 Redis 实例上使用 **Lua 脚本进行解锁**，原因是可能存在某个节点加锁成功后，**返回客户端时**的响应包丢失了，即客户端到服务器的通信是正常的，但反方向却是有问题的，虽然对客户端而言，由于响应超时导致加锁失败，但是对 Redis节点而言，`SET` 指令执行成功，意味着加锁成功，因此，释放锁时，客户端也应该对当时获取锁失败的那些 Redis 节点同样发起解锁请求。

  - **缺点**：
    1. **性能过重**：使用 RedLock 需要维护那么多的 Redis 实例，提升了系统的维护成本。
    2. **仍然不安全**：RedLock 严重依赖系统时钟，如果 Master 系统时间发生回调，则会导致它持有的锁提前过期释放，还是不能保证锁的安全性，这个是基于时间来实现自动释放的分布式锁，都无法解决的问题。

###### 3）基于分布式一致性算法实现 | 强一致

基于 ZK 实现：可以利用**顺序临时节点**的特性，结点在创建时，会自动在结点名后加一个数字后缀，以保证有序，同时，如果客户端连接失效，则还会立即删除结点，再利用 **watcher 监视器**的特性，注册某个结点的监视器，当节点状态发生改变时，watcher 被触发时，ZK 会向客户端发送一条通知。其分布式锁的实现原理是，

1. 创建一个锁目录 lock，希望获得锁的线程 A 在 lock 目录下，将创建**顺序临时结点**。
2. A 先获取锁目录下所有的子结点，判断是否存在序号比自己小的结点，如果不存在，则说明当前线程的顺序号最小，则线程 A 获得锁。
3. 当另外一个线程 B 获取锁时，判断到 B 自己不是最小的结点，存在有更小的线程 A 结点，则设置  watcher 监听器，只监听比自己**次小**的结点 A。
4. 当线程 A 处理完业务后，会删除结点 A，释放掉分布式锁，然后线程 B 监听到节点状态变更事件后，判断自己已经是最小的结点了，则成功获得锁。

##### 3、总

以上就是我对分布式锁一些实现方案的理解，总结一下就是，

- **基于数据库实现**：
  - **优点**：直接使用数据库，使用简单。
  - **缺点**：但这样会增加数据库的负担。
- **基于分布式缓存实现**：
  - **优点**：属于 AP 模型，性能高，实现起来较为方便，在允许偶发性的锁失效情况发生，不影响系统正常使用时，可以采用分布式缓存来实现锁。
  - **缺点**：通过过期时间实现的锁超时机制不是十分可靠，当业务必须要数据的**强一致性**，不允许重复获得锁时，比如金融场景的重复下单与重复转账场景下，就不能使用分布式缓存来实现锁了，此时可以使用 CP 模型来实现，比如 Zookeeper。
- **基于分布式一致性算法实现**：
  - **优点**：不依靠过期时间来释放锁，可靠性高，当系统要求高可靠性时，可以采用分布式一致性算法来实现锁。
  - **缺点**：性能比不上分布式缓存实现的锁，因为 ZK 需要频繁的创建和删除结点。

#### 1.2.1.5. Redis 数据结构？

##### 1、总

对于这个问题，我打算先介绍 Redis 对象 `RedisObject`，

1. 其数据结构包含一个 4 字节的 `type` 对象类型属性，分为 STRING、HASH、LIST、SET 和 ZSET。
2. 一个 4 字节的 `encoding` 对象编码属性，包括 INT、EMBSTR、RAW、HT、LINKEDLIST、ZIPLIST、INTSET 和 SKIPLIST。
3. 以及一个 `ptr` piont 指针，指向底层实现的数据结构。

然后我再按照上面所说的对象类型（String、Hash、List、Set、ZSet）按顺序进行介绍。

##### 2、分

###### 1）String

1. 首先是 String，常见的 API 有， SET、SET NX EX、GET、APPEND、STRLEN、SETRANGE、GETRANGE、MSET、MGET、INCRBY、DECRBY 等命令，适合存储帖子、评论、热点数据等缓存，其底层的数据结构分为 3 种编码，分别为 INT、EMBSTR 和 RAW：

2. **INT**：只能存储 long 类型的整数，`ptr` 指针指向对应的整数值。

   ![1631946982581](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631946982581.png)

3. **EMBSTR**：（< 3.2 版本时）在字符串值小于等于 39 字节时会使用此编码，其优点是，它是对 `SDS` 的一个小优化，通过将 `RedisObject` 对象头和 `SDS` 存放在一起，采用**连续空间保存**，只需要一次内存分配，避免了它们各自进行空间分配，提高了字符串的内存分配效率，同时还可以减少内存碎片和 `ptr` 指针的占用，节约内存，提高空间的利用率。

   - `SDS`：是 Redis 自己实现一个字符串数据结构，通过持有 `len` 来标识字符串长度，`free` 来记录空闲字符的个数，`buf` 则指向真实的字符数组，能够在 O（1）内获取字符串长度，具有空间预分配、惰性释放内存，以减少分配次数的特点。

   - **缺点**：EMBSTR 是**只读**的形式，要修改时，只能转换为 RAW 编码，才能进行修改。

   ![1631878253168](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631878253168.png)

4. **RAW**：（< 3.2 版本时）在字符串值大于 39 字节时会使用此编码，`ptr` 指针指向一个 `SDS` 数据结构，也就是以 `SDS` 的形式存储，主要为了解决长度计算和追击字符效率的问题。

   ![1631947051873](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631947051873.png)

###### 2）Hash

1. 然后就是 Hash，常见的 API 有，HSET、HGET、HEXSITS、HDEL、HLEN、 HSTRLEN、HINCRBY、HMSET、HMGET、HKEYS、HVALUES 等命令，适合存储结构化的对象数据，其底层的数据结构分为 ZIPLIST 和 HT。

2. **ZIPLIST**：压缩列表，（< 3.2 版本时）在元素大小小于 64 字节且元素个数少于 512 个时会使用此编码，由数组实现，通过 `zlbytes` 记录列表总字节数、`zltail_offset` 记录最后一个节点的偏移量快速定位结尾、`zllength` 记录列表元素个数、`entries` 数组存储列表节点、`zlen` 标志列表结束，以及每个`entry` 节点都持有前一个节点的长度 `prevlous_entry_length`，在 HASH 取值时，可以通过**向后或者向前遍历**找到对应的键和值，其优点是由于内存是连续的，可以减少很多内存碎片和指针内存的占用，进而**节约内存**。

   - **缺点**：ZIPLIST 存在级联更新的问题，由于 `prevlous_entry_length` 存储的长度取决于前一个节点的长度，当前一个节点大于 254 字节时，这个值会从 1 字节变为 5 字节，从而导致后面值可能也会发生变更的连锁反应，此时的时间复杂度非常差为 O（n^2），但好在这种大范围的级联更新出现的概率并不大。

   ![1631950146881](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631950146881.png)

3. **HT**：哈希表，（< 3.2 版本时）在元素大小大于等于 64 字节，或者元素个数大于等于 512 个时会使用此编码，基于 2 张哈希表实现，使用拉链法解决哈希冲突，相对于 Java#HashMap，其特点在于，除了在负载因子大于等于 1  （没有执行 `BGSAVE` 或者 `BGREWARITEAOF` 时）或者 5 （执行 `BGSAVE` 或者 `BGREWARITEAOF` 时）发生 2 倍实际使用长度的扩容外，还会在负载因子小于 0.1 时发生 1 倍实际使用长度的缩容， 并且采用的是渐进式的 `rehash` 机制，即定时执行，或者在客户端每次增删改查操作完成后，让其进行一次 `rehash` 操作，把 ht[0] 对应 `rehashindex` 位置的值搬 ht[1] 上，然后 `rehashIndex+1`，这样避免了集中式扩容带来的性能压力。

   - **缺点**：在 `rehash` 期间，需要同时持有两张哈希表，对内存占用稍大，如果本身内存都不足时，突然的 `rehash` 会使得 Redis 执行缓存淘汰策略，抛弃大量的 Key。

   ![1631950298985](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631950298985.png)

###### 3）List

1. 接着就是 List，常见的 API 有：LPUSH、LPOP、RPUSH、RPOP、LREM、LINSERT、LSET、LINDEX、LRANGE、LTRIM、BLPOP、BRPOP、RPOPLPUSH、BRPOPLPUSH，适合用作评论列表、商品列表、发布与订阅等功能，其底层的数据结构分为 ZIPLIST、LINKEDLIST 和 QUICKLIST 编码。 

2. **ZIPLIST**：压缩列表，（< 3.2 版本时）在元素大小小于 64 字节且元素个数少于 512 个时会使用此编码，由数组实现，通过 `zlbytes` 记录列表总字节数、`zltail_offset` 记录最后一个节点的偏移量快速定位结尾、`zllength` 记录列表元素个数、`entries` 数组存储列表节点、`zlen` 标志列表结束，以及每个`entry` 节点都持有前一个节点的长度 `prevlous_entry_length`，在 LIST 取值时，可以通过**向后或者向前遍历**找到对应的值，其优点是由于内存是连续的，可以减少很多内存碎片和指针内存的占用，进而**节约内存**。

   - **缺点**：ZIPLIST 存在级联更新的问题，由于 `prevlous_entry_length` 存储的长度取决于前一个节点的长度，当前一个节点大于 254 字节时，这个值会从 1 字节变为 5 字节，从而导致后面值可能也会发生变更的连锁反应，此时的时间复杂度非常差为 O（n^2），但好在这种大范围的级联更新出现的概率并不大。

   ![1631952569099](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631952569099.png)

3. **LINKEDLIST**：双向链表，（< 3.2 版本时）在元素大小大于等于 64 字节，或者元素个数大于等于 512 个时会使用此编码，它是 Redis 自己实现的一条双向链表，包含头节点、尾结点、前驱和后继，可以很方便的进行向后或者向前遍历，同时持有 `len` 长度计数器，可以 O（1）内获取到 LIST 的长度 。

   - **缺点**：每个节点都有自己的前后指针，指针这部分所占用的内存较多，且每个节点是单独进行内存分配，当节点过多时，造成的内存碎片会比较多，影响内存管理的效率。

   ![1631952812216](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631952812216.png)

4. **QUICKLIST**：快速列表，大于 3.2 版本后，LIST 统一采用此格式进行存储，是 ZIPLIST 和 LINKEDLIST 的混合体，它将 LINKEDLIST 按段切分，每一段使用 ZIPLIST 来紧凑存储，多个 ZIPLIST 之间使用双向指针串接起来，缓解了 LINKEDLIST 指针内存浪费和内存碎片多的问题，同时解决 ZIPLIST 数据量过大时导致的性能变差问题。

   - **缺点**：ZIPLIST 节点太小的话（比如只存 1 个元素时），快速列表会退化成普通的链表，起不到应有的节省内存的作用，而 ZIPLIST 节点太大的话（比如只有一个 ZIPLIST 节点时），快速列表会退化成压缩列表，还是会出现数据量过大时导致的性能变差问题。
   - 因此，快速列表内部默认定义的单个 ZIPLIST 节点大小为 `8k 字节`，可以由参数 `list-max-ziplist-size` 来控制，其作用是，在分配结点时，如果发现当前 ZIPLIST 节点超过了这个大小，则会重新分配一个 ZIPLIST 节点。

   ![1631953074136](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631953074136.png)

###### 4）Set

1. 再然后就是 Set，常见的 API 有：SADD、SPOP、SREM、SRANGMEMBER、SISMEMBER、SCARD、SMEMBERS、SINTER、SINTERSTORE、SUNION、SUNIONSTORE、SDIFF、SDIFFSTORE，适合用于求交集、并集、差集，比如朋友关系，其底层的数据结构分为 INTSET 和 HT 编码。 

2. **INTSET**：整数集合，（< 3.2 版本时）只存储 long 范围内的整数值，且在元素个数 < 512 个时会使用此编码，持有对应整数的编码类型 `encoding` 总是使用容纳数字的**最小编码进行存储**，以节约内存、集合元素数量 `length` 可以在 O（1） 内获取到对应长度、以及元素使用数组  `contents`  来进行**连续存储**，可以减少内存碎片和指针内存的占用，以节约内存。

   - **缺点**：编码类型只能升级不能降级，在大数字删除后，整数集合还是会使用大类型存储小数字，造成空间的浪费。

   ![1631954240528](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631954240528.png)

3. **HT**：哈希表，（< 3.2 版本时）在元素大小大于等于 64 字节，或者元素个数大于等于 512 个时会使用此编码，基于 2 张哈希表实现，使用拉链法解决哈希冲突，相对于 Java#HashMap，其特点在于，除了在负载因子大于等于 1  （没有执行 `BGSAVE` 或者 `BGREWARITEAOF` 时）或者 5 （执行 `BGSAVE` 或者 `BGREWARITEAOF` 时）发生 2 倍实际使用长度的扩容外，还会在负载因子小于 0.1 时发生 1 倍实际使用长度的缩容， 并且采用的是渐进式的 `rehash` 机制，即定时执行，或者在客户端每次增删改查操作完成后，让其进行一次 `rehash` 操作，把 ht[0] 对应 `rehashindex` 位置的值搬 ht[1] 上，然后 `rehashIndex+1`，这样避免了集中式扩容带来的性能压力。

   - **缺点**：在 `rehash` 期间，需要同时持有两张哈希表，对内存占用稍大，如果本身内存都不足时，突然的 `rehash` 会使得 Redis 执行缓存淘汰策略，抛弃大量的 Key。

   ![1631954355626](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631954355626.png)

###### 5）ZSet

1. 最后就是 ZSet，常见的 API 有：ZADD、ZREM、ZSCORE、ZINCRBY、ZRNAGE（指定偏移，score 从小到大，分数相等，则再按值顺序排序）、 ZRERANGE（指定偏移，score 从大到小，分数相等，则再按值逆序排序）、ZRNAGEBYSOCRE（指定分数）、 ZRERANGEBYSOCRE（指定分数）、ZRANK、ZRERANK、ZCARD、ZCOUNT（区间个数统计） 等命令，可以在去重后进行排序，适合排名的场景，其底层的数据结构分为 ZIPLIST、SKIPLIST 编码。 

2. **ZIPLIST**：压缩列表，（< 3.2 版本时）在元素大小小于 64 字节且元素个数少于 **128** 个时会使用此编码，由数组实现，通过 `zlbytes` 记录列表总字节数、`zltail_offset` 记录最后一个节点的偏移量快速定位结尾、`zllength` 记录列表元素个数、`entries` 数组存储列表节点、`zlen` 标志列表结束，以及每个`entry` 节点都持有前一个节点的长度 `prevlous_entry_length`，在 ZSet 取值时，可以通过**向后或者向前遍历**找到对应的值，其优点是由于内存是连续的，可以减少很多内存碎片和指针内存的占用，进而**节约内存**。

   - **缺点**：ZIPLIST 存在级联更新的问题，由于 `prevlous_entry_length` 存储的长度取决于前一个节点的长度，当前一个节点大于 254 字节时，这个值会从 1 字节变为 5 字节，从而导致后面值可能也会发生变更的连锁反应，此时的时间复杂度非常差为 O（n^2），但好在这种大范围的级联更新出现的概率并不大。

   ![1631954526032](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631954526032.png)

3. **SKIPLIST**：（< 3.2 版本时）在元素大小大于等于 64 字节且元素个数大于等于 **128** 个时会使用此编码，由跳跃表 + 哈希表来实现，通过在有序链表上维护每级 25% 概率生成的索引，从而达到 O（logn）访问元素的目的，通过累加查找路径中的 `SPAN` 跨度，来计算当前节点所处的排名，通过哈希表存储跳跃表节点，以实现在 O（1）内完成根据名称找到对应的得分。

   - **缺点**：由于新节点插入的 LEVEL 是随机的，导致老节点的查找路径可能发生变化，**缓存友好性不如**红黑树，而红黑树则是插入新节点后，大部分老节点仍然处于原查找路径上。
   - **为什么 Redis#ZSet 使用跳跃表而不是红黑树**？
     1. **范围查找效率高**：
        1. 对于 `ZRANGE` 和 `ZREVRANGE` 命令的范围查找，如果使用哈希表，则只能做单值查找，不适合做范围查找。
        2. 如果使用红黑树，需要中序遍历 [ 范围最小的后继，范围最大的前驱 ]，效率低且实现复杂。
        3. 而使用 skiplilst 只需要 O（logn）定位头尾结点，然后遍历链表即可，简单又高效。
     2. **内存占用少**：Redis skiplist 索引的默认生成概率为 25%，即每个结点平均只包含 **1.33** 个指针，内存占用比红黑树的 2 个指针要少。
     3. **实现与调试容易**：使用 skiplist 比红黑树更容易实现与调试。

   ![1631955510770](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1631955510770.png)

##### 3、总

以上就是我对 Redis 常用数据结构的一些理解，请问还有什么细节需要补充的吗？

#### 1.2.1.6. 提升技术的方法？

1. **看视频**：入门。
2. **做实验**：踩坑。
3. **看源码**：找答案。
4. **看书**：系统性总结。

#### 1.2.1.7. 技术不足的地方？

某些技术缺乏实际场景应用，久而久之细节就忘却了，以及有许多知识点学完了，但没做系统性整理，比如 ES、Docker、K8S 这些。

#### 2.1.1.1. 项目自我介绍？

##### 1、自我介绍

面试官你好，我叫姚超松，2019 毕业于广东工业大学，读的是电子信息工程专业，毕业时秋招进了美的集团的美云智数事业部，的供应商部门，主要工作内容是负责功能模块的全栈开发，以及架构的优化。

下面我打算以时间倒序的方式，介绍一下我的项目经历：

1. 第一个，也是最近在做的一个项目是，美的集团的 GSRM 产品，也就是供应商关系管理，包括寻源、资质审查、现场评审、供方生效、退出、失效、画像等业务，技术的话用到了 SpringCloud 来拆的微服务，Sharding JDBC 和 MySQL 做的分库分表，中间件的话用到了 Redis 和 Kafka ，以及 MongoDB 做的一个接口日志收集，我在里面主要负责迭代现场评审模块，和供应商画像模块，以及一些外部接口对接的工作。
2. 再往前一点的话，就是一个自研的 Sass 产品，包括抽象了美的 QMS 业务的品质云，抽象了 MES 业务的进销存云，抽象了 SRM 业务的 SRM 云等等，技术的话，重构之前用到的是一个 SpringBoot 做的应用，数据库用的是 MySQL，中间件的话用到了 Redis 和集团的 ESB 企业总线，我在里面主要负责品质和进销存的需求开发，和数据中台的建设，以及后期品质云 SpringCloud 微服务的落地。
3. 再往前的话，就是推广到比亚迪的一个外部产品的迭代，在 SRM 2.0 的基础上级为 SRM 3.0，主要是增加了服务和资产类的交付、验收、结算等业务，技术的话，用到了 SpringMVC + Dubbo 做的应用，数据库用的是 Oracle，中间件的话用到了 Redis 和 RabbitMQ，我在里面主要负责交付单模块的开发，以及和云平台接口对接的工作。 

以上，就是我的自我介绍，请问有什么细节需要补充的吗？

##### 2、项目细节

###### 1）GSRMC

1. **微服务模块**：

   | 模块名 | 模块业务 | 备注                                                         |
   | ------ | -------- | ------------------------------------------------------------ |
   | BASE   | 基础数据 | 字典、物料、采购分类、研发分类等主数据                       |
   | POS    | 生命周期 | 寻源、资质审查、现场评审、供方生效、信息变更、黑名单管理、质保金管理、供方退出、失效等供应商主数据 |
   | PERF   | 考核绩效 | 供方送货不合格、交期、品质、服务考核 -> 考核会影响绩效（品质、成本、综合水平绩效） -> 绩效影响分级 -> 绩效（大头）与分级影响比例（QUO） -> 也可按规则分配比例（也叫配额） -> ERP根据比例决定供货数量 |
   | QUO    | 比例管理 | 绩效（大头）与分级影响比例（QUO） -> 也可按规则分配比例（也叫配额） -> ERP根据比例决定供货数量 |
   | BID    | 集采招标 | BID 招标定价，给物料定价 -> 一揽子价格                       |
   | PRICE  | 价格管理 | 生效之后，研发工程师在 PLM 分解POM，核价员估价，给定价做参考、线下定价以及BID 招标定价，给物料定价 -> 一揽子价格 |
   | CON    | 电子合同 | 电子合同模块，签署框架协议，合作时要遵守的规则               |
   | -      | -        | 还有ESB 交易数据、JOBS/TASK 定时任务、MIP 审批流程、SYNC 数据同步、INDEX 指标工作台等非核心业务模块 |

2. **主要业务流程**：

   1. 研发工程师在 PLM 根据业务，创建新物料 ITEM，跟 SRM 的采购分类进行关联。
   2. 然后选定采购分类和需求图纸等信息，在供应商库中做**寻源匹配**。
   3. 寻源完毕后，送样到供应商 GSC 进行初步**估价**。
   4. 然后 SRM 根据**资质**和价格进行筛选。
   5. 筛选通过后，PLM 进行试用，SRM 对供应商生产环境进行**现场评审**。
   6. 评审通过后生成供应商品类 ASL，此后就可以按照比例进行批量供货了。
   7. 其中， SRM 可以对供方送货不合格、交期、品质、服务等进行考核 -> 考核会影响**绩效**（品质、成本、综合水平绩效） -> 绩效影响分级 -> 绩效（大头）与分级影响比例（QUO） -> 也可按规则分配**比例**（也叫配额） -> ERP根据比例决定供货数量。
   8. 送货完毕后，SRM 需要对品类进行**核价**，包括招标转定价、意向价、直接定价、线下议价等方式。
   9. 最后走价格审批，把价格同步到 ERP，每月 25 号对上个月进行结算。

   => 总的来说，SRM 就是负责供应商从生到死的生命周期管理，以及物料价格和供货数量管理，也就是寻源到生效、需求到生产、定价到付款三个方向的业务。

3. **主要用到的技术组件**：

   | 组件                    | 作用                                                         | 部署           |
   | ----------------------- | ------------------------------------------------------------ | -------------- |
   | Vue + I-View            | 前端                                                         | 8 Apache       |
   |                         | 负载均衡                                                     | F5、Nginx      |
   | SpringCloud             | Eureka 做服务注册，Feign 做服务通信，Ribbon 做负载均衡，Sleuth+Zipkin 做链路追踪，Config 做配置中心、Stream+Bus 做消息驱动 | 126 应用服务器 |
   | Sharding JDBC + MyBatis | Sharding JDBC 分表，MyBatis ORM 映射                         | 126 应用服务器 |
   | MySQL                   | 数据库                                                       | 8 主 8 从      |
   | MongoDB                 | 文档数据库                                                   | 3 主备         |
   | Redis、Redisson         | 缓存中间件                                                   | 3 主备 + 哨兵  |
   | Kafka、ZK               | 消息中间件                                                   | 3 集群架构     |

4. **分表与分片规则的设置**：

   | 分表                   | 数据量     | 表注释             | 数据库 | 分片规则               |
   | ---------------------- | ---------- | ------------------ | ------ | ---------------------- |
   | srm_sys_items_submeter | 3280.19 w+ | 物料表             | BASE   | ${organization_code}   |
   | srm_sys_item_cates_sub | 1613.38 w+ | 物料采购分类关系表 | BASE   | ${organization_code}   |
   | srm_sys_item_keycs     | 2153.11w+  | 物料研发分类关系表 | BASE   | ${organization_code}   |
   | srm_po_receive_det     | 3.0813 E+  | 采购接收表         | PRICE  | ${period_month / year} |
   | srm_po_line_locations  | 1.0220 E+  | 一揽子价格表       | PRICE  | ${bu_code}             |

5. **服务器（虚拟机）的配置**：

   - **微服务**：基本每个虚机部署 2~3 个模块，平常 CPU 10%，内存 60%，所以一共注册了 126 个服务。
   - **数据库**：监控显示，32 C、128 G、3 T，平均响应时间 15ms，平均 30% CPU 利用率，内存利用率 75%。

   | 模块           | 数量              | CPU           | 内存               | 硬盘  |
   | -------------- | ----------------- | ------------- | ------------------ | ----- |
   | 前端 Apache    | 8                 | 4 C           | 16 G               | 50 G  |
   | Eureka         | 3                 | 1 C           | 1 G                | 10 G  |
   | BASE           | 13                | 8 C           | 16 G               | 100 G |
   | POS            | 19                | 8 C           | 16 G               | 100 G |
   | PERF           | 7                 | 8 C           | 16 G               | 100 G |
   | QUO            | 6                 | 8 C           | 16 G               | 100 G |
   | BID            | 3                 | 8 C           | 16 G               | 100 G |
   | PRICE          | 13                | 8 C           | 16 G               | 100 G |
   | CON            | 6                 | 8 C           | 16 G               | 100 G |
   | ESB            | 5                 | 8 C           | 16 G               | 100 G |
   | JOBS           | 9                 | 8 C           | 16 G               | 100 G |
   | TASK           | 5                 | 8 C           | 16 G               | 100 G |
   | MIP            | 4                 | 8 C           | 16 G               | 100 G |
   | SYNC           | 5                 | 8 C           | 16 G               | 100 G |
   | INDEX          | 3                 | 8 C           | 16 G               | 100 G |
   | 其他模块       | 25                | 8 C           | 16 G               | 100 G |
   | MySQL          | 16（8 主 8 从）   | 32 C          | 128 G              | 3 T   |
   | MongoDB        | 3                 | 8 C           | 16 G               | 500 G |
   | Redis          | 3                 | 8 C           | 32 G               | 100 G |
   | Kafka          | 3                 | 8 C           | 16 G               | 100 G |
   | 报表导出 OSS   | 文件数： 500 / 天 | 数据量：10 G  | 增长量：300 M / 天 | -     |
   | 供应商附件 OSS | 文件数：10 w / 年 | 数据量：200 G | 增长量：200 G / 年 | -     |

6. **应用的 JVM 参数**：

   ```bash
   nohup java -Duser.timezone=Asia/Shanghai 
   -Xms2048m -Xmx2048m 
   -XX:OnOutOfMemoryError="sh kkp.sh" 
   -XX:+HeapDumpOnOutOfMemoryError
   -XX:+PrintGCDetails -Xloggc:/apps/svr/srmpos/logs/gc.log
   -Dsun.net.inetaddr.ttl=30 -Dsun.net.inetaddr.negative.ttl=10   
   -jar srm-pos-2.0.0-SNAPSHOT.jar 
   --spring.cloud.bootstrap.location=/apps/svr/srmpos/bootstrap.yml 
   -Xbootclasspath/a:/apps/svr/srmpos/: > /apps/svr/srmpos/logs/srmpos_$(date +%Y-%m-%d).log &
   ```

   `kkp.sh`：

   ```bash
   pids=`ps -ef|grep srm-pos|grep -v grep|awk '{print $2}'`
   if [ "$pids" != "" ]; then
    echo "kill -9 $pids"
    kill -9 $pids
   fi
   ```

###### 2）云化项目

1. **微服务模块**：（品质云）

   | 模块名 | 模块业务     | 备注                                                |
   | ------ | ------------ | --------------------------------------------------- |
   | BASE   | 基础数据模块 | 字典、IDM 用户、权限、系统附件、企业、客户、组织等  |
   | OEM    | 代工生产模块 | OEM 供方成品下线、抽检、出库等                      |
   | APP    | 综合模块     | SPC 过程检验，PQC  半成品检验、OQC 成品出货检验等   |
   | JOBS   | 定时任务模块 | 包括数据中台、PSI、QMS、GSC、GSRM、MES 等的数据同步 |

2. **品质云主要业务流程**：

   1. 供方原材料上线，MES 进行来料检测后，会走到各制程工序。
   2. 在每个制程工序的 CTQ 品质关键点上，会进行相应的 SPC 过程检验。
   3. 然后，还在工序的检验岗位上，进行对应的 PQC 半成品检验。
   4. 最后，在成品准备入库出货前，则进行 OQC 成品出货检验。
   5. 其中，对于 OEM 供应商，由于是代工生产，成品下线时，还需要进行对应的彩箱、大箱、地台板条码的绑定操作后，才能进行后面的入库出货操作。

   => 总的来说，品质云就是实时管控供方生产的**品质质量**。

3. **进销存云主要业务流程**：

   1. 接收到 GSC 供应商门户 的采购订单，就在进销存为供方生成对应的销售订单。
   2. 然后根据物料 BOM 信息，生成后续的生产订单。
   3. 在生产前，获取原材料时，可以直接在原材料仓进行扣减，也可以发起原材料的采购订单，进行补充原材料。
   4. 在生产完工后，会创建生产入库单，把成品送入成品仓，更新供方成品库存。
   5. 其中，对于 OEM 供应商，由于是代工生产，成品下线时，品质会把彩箱、大箱条码传入进销存，进销存再做对应的地台板绑定、销售出库、以及推送物流平台。

   => 总的来说，进销存就是用来实时管控供方的**库存情况**。

4. **数据中台主要业务流程**：数据中台，主要承担物料、ASL、寻源品质进销存的一些统计工作。

5. **主要用到的技术组件**：

   | 组件             | 作用                                                         | 部署         |
   | ---------------- | ------------------------------------------------------------ | ------------ |
   | Vue + Element UI | 前端                                                         | 8 Nginx      |
   |                  | 负载均衡                                                     | F5、Nginx    |
   | SpringCloud      | 微服务拆分后，Eureka 做服务注册，Feign 做服务通信，Ribbon 做负载均衡，Sleuth+Skywalking+ELK 做链路追踪，Config 做配置中心，Gateway 做服务网关 | 应用服务器   |
   | SpringBoot       | 微服务拆分前，还是单体应用                                   | 3 应用服务器 |
   | MyBatis          | MyBatis ORM 映射                                             | 3 应用服务器 |
   | MySQL            | 数据库                                                       | 1主          |
   | Redis、Jedis     | 缓存中间件                                                   | 3 主 3从集群 |

6. **表数据量**：

   | 分表                        | 数据量    | 表注释         | 系统 | 分片规则               |
   | --------------------------- | --------- | -------------- | ---- | ---------------------- |
   | qc_oem_order_line           | 1200 w+   | 成品下线表     | QC   | 手工按日期分片备份旧表 |
   | qc_oqc_item_standard_value  | 1500 w+   | 物料抽检标准表 | QC   | 手工按日期分片备份旧表 |
   | oem_external_mes_colorcode  | 722.26 w+ | MES 条码表     | QC   | 无分片                 |
   | psi_base_packing_relation   | 315.28 w+ | 箱包关系表     | PSI  | 无分片                 |
   | psi_prd_pallet_box_relation | 250.03 w+ | 板箱关系表     | PSI  | 无分片                 |
   | psi_base_barcode            | 314.67 w+ | 下线条码表     | PSI  | 无分片                 |
   | psi_sales_stock_out         | 840.58 w+ | 销售出库表     | PSI  | 无分片                 |
   | cloud_gsrm_item             | 38.61 w+  | 物料表         | DC   | 无分片                 |
   | cloud_gsrm_asl              | 31.63 w+  | ASL 物料表     | DC   | 无分片                 |
   | mcc_company_info            | 1321      | 企业统计表     | DC   | 无分片                 |
   | mcc_deliver_receive_sum     | 2.24 w+   | 企业下线出库表 | DC   | 无分片                 |
   | mcc_order_quotation_sum     | 342       | 企业寻源报价表 | DC   | 无分片                 |

7. **服务器（虚拟机）的配置**：

   - **微服务**：基本每个虚机部署 2~3 个模块，平常 CPU 10%，内存 60%，所以一共注册了 14 个服务。
   - **数据库**：监控显示，16 C、64 G、3 T，平均响应时间 15ms，平均 30% CPU 利用率，内存利用率 75%。

   | 模块             | 数量             | CPU          | 内存              | 硬盘  |
   | ---------------- | ---------------- | ------------ | ----------------- | ----- |
   | 前端 Nginx       | 3                | 4 C          | 16 G              | 50 G  |
   | Eureka           | 3                | 1 C          | 1 G               | 10 G  |
   | BASE             | 3                | 8 C          | 16 G              | 100 G |
   | APP              | 3                | 8 C          | 16 G              | 100 G |
   | OEM              | 3                | 8 C          | 16 G              | 100 G |
   | JOBS             | 2                | 8 C          | 16 G              | 100 G |
   | MySQL            | 1 主             | 16 C         | 64 G              | 3 T   |
   | Redis            | 6                | 8 C          | 16 G              | 100 G |
   | 文件存储 FASTDFS | 文件数：1 w / 年 | 数据量：20 G | 增长量：20 G / 年 | -     |

8. **应用的 JVM 参数**：

   ```bash
   -Xmx1000m -Xms1000m -Xss256k -Dgen_data_center_id=0 -Dgen_machine_id=2
   ```

   单体时，翻车更改为：

   ```bash
   -Xmx5000m -Xms5000m -Xmn1000m ...
   ```

###### 3）比亚迪 SRM

1. **微服务模块**：

   | 模块名 | 模块业务       | 备注               |
   | ------ | -------------- | ------------------ |
   | APP    | 前端控制器模块 | Controller，6 台   |
   | MID    | 业务处理模块   | Service，7 台      |
   | RPORT  | 报表模块       | 报表导出，6 台     |
   | NGINX  | 前端模块       | WEX5、Jquery，2 台 |

2. **主要业务流程**：

   1. 定时同步云平台 SAP 需要的基础信息，比如采购订单、采购金额、采购数量等到 SRM，按照履行模板和规则，生成相应的履行计划和付款计划。
   2. 通过履行计划触发，生成交付、验收、结算对应节点的订单与待办信息，通知对应的责任人进行审批或者验收。
   3. 通过迁移线下交付单到线上，完成各类的无纸化交付，交付单审批通过后，会推进下一个节点的履行计划和付款计划。
   4. 通过付款计划触发，生成发货款、到货款、调试款、验收款、上线款、完工款等多种款项的待办，通知对应的责任人进行审批或者结算。

   => 总的来说，就是通过交付单推进履行计划和付款计划，来数字化线下供方结算与付款流程。

3. **主要用到的技术组件**：

   | 组件                | 作用             | 部署          |
   | ------------------- | ---------------- | ------------- |
   | JQuery + Element UI | 前端             | 2 Nginx       |
   |                     | 负载均衡         | 2 Nginx       |
   | Spring MVC + Tomcat | 后端应用         | 19 应用服务器 |
   | MyBatis             | MyBatis ORM 映射 | 19 应用服务器 |
   | Oracle              | 数据库           | 1 主          |
   | Redis、Jedis        | 缓存中间件       | 3 主备 + 哨兵 |
   | RabbitMQ            | 消息中间件       | 3 镜像集群    |

4. **表数据量**：

   | 分表          | 数据量     | 表注释              | 系统   | 分片规则 |
   | ------------- | ---------- | ------------------- | ------ | -------- |
   | SAP_CDHDR     | 6.39 E+    | PO 单增量表         | 云平台 | 无需分片 |
   | SAP_EKPO      | 4019.80 w+ | PO 行表             | 云平台 | 无需分片 |
   | SAP_ZMMSSBM03 | 1544.27 w+ | PO 描述表           | 云平台 | 无需分片 |
   | SAP_EKKO      | 1147.68 w+ | PO 币种表           | 云平台 | 无需分片 |
   | SAP_ZEKKO     | 965.70 w+  | PO 寻源表           | 云平台 | 无需分片 |
   | PR / PO       | 1948.13 w+ | 采购需求 / 采购订单 | SRM    | 无需分片 |

5. **服务器（虚拟机）的配置**：

   - **微服务**：基本每个虚机部署 2~3 个模块，平常 CPU 10%，内存 60%，所以一共注册了 21 个服务。
   - **数据库**：监控显示，32 C、128 G、3 T，平均响应时间 15ms，平均 30% CPU 利用率，内存利用率 75%。

   | 模块             | 数量             | CPU          | 内存              | 硬盘  |
   | ---------------- | ---------------- | ------------ | ----------------- | ----- |
   | 前端 Nginx       | 2                | 4 C          | 16 G              | 50 G  |
   | APP              | 6                | 8 C          | 16 G              | 100 G |
   | MID              | 7                | 8 C          | 16 G              | 100 G |
   | RPORT            | 6                | 8 C          | 16 G              | 100 G |
   | Oracle           | 1                | 32 C         | 128 G             | 3 T   |
   | Redis            | 3                | 8 C          | 16 G              | 100 G |
   | 文件存储 FASTDFS | 文件数：1 w / 年 | 数据量：20 G | 增长量：20 G / 年 | -     |

6. **应用的 JVM 参数**：

   ```bash
   -Xmx1000m -Xms1000m -Xss256k -Dgen_data_center_id=0 -Dgen_machine_id=2
   ```

   单体时，翻车更改为：

   ```bash
   -Xmx5000m -Xms5000m -Xmn1000m ...
   ```

##### 3、项目亮点

| 系统       | 亮点清单          | 关键词                                                       |
| ---------- | ----------------- | ------------------------------------------------------------ |
| GSRMC      | PLM 配套接口      | Around 切面注解、Kafka + MongoDB 、MD5  + AES、多线程 + Future、Redis、Jmeter |
|            | 供应商画像同步    | 缓存型线程池、工厂 + 建造者 + 责任链、EsJob + Kafka + MongoDB、线程池调优 |
|            | 线程池调优        | 线程池源码、调优原则                                         |
|            | 死锁问题排查      | 意向锁、间隙锁、可重复读、并发                               |
|            | 内存溢出排查      | MAT、POI、ArrayList                                          |
| Sass 产品  | 条码销售出库      | Druid 监控、SQL 调优、Redis                                  |
|            | 品质云微服务拆分  | SpringCloud                                                  |
|            | 校验失败问题      | 分布式锁、QMS Client Json 对比、问题排查                     |
| 比亚迪 SRM | 大文件上传        | FastDFS、Redis List                                          |
|            | 邮件发送组件      | 线程池 + RabbitMQ + 定时任务                                 |
|            | 发版平台 CPU 过高 | Tomcat Manager、ELK、CPU 95%                                 |

STAR 法则，是情境（Situation）、任务（Task）、行动（Action）、结果（Result）四项的缩写，是一种讲述自己故事的方式，或者说，是一个清晰、条理的作文模板，合理熟练运用该法则，可以轻松的描述事情的逻辑方式，表现出分析与问题的逻辑性、条理性和逻辑性。

1. Situation：情境，本次事件是在什么情况下发生的。
2. Task：任务，在本次事件中，主要负责什么任务。
3. Action：行动，在本次事件中，针对这些情况的分析，采用了什么样的行动。
4. Result：结果，本次事件最后的结果是怎么样的，以及学到了什么。

| 举例 | 内容（大一辩论比赛获得冠军）                                 |
| ---- | ------------------------------------------------------------ |
| S    | 系里一共有 5 支队伍，实例...，我们小组...                    |
| T    | 熟悉辩论流程，掌握辩论技巧，获得系冠军                       |
| A    | 自己主动整理资料，组织小组学习流程，编制训练题，小组训练，根据每个人的特点，分配任务（要尽量详细，包括当中遇到的困难是什么，怎么解决的） |
| R    | 获得系辩论赛冠军                                             |

###### 1）PLM 配套接口 | 分表、并发、Redis

1. **背景 Situation**：供应链体系管理专员反馈，他们在 PLM 建送样申请单时，由于物料配套没有**自动匹配**，导致经常选错配套人员，然后就要撤回、修改、重新提交，影响业务流程效率。

2. **任务 Task**：所以他们希望在 PLM 上，根据物料就可以**自动匹配** SRM 品类分工中的**配套专员信息**，方便他们走业务流程。

3. **行动 Action**：

   1. 接口的实现逻辑大体是这样的，根据 ITEM_CODE + ORG_CODE 找对应分表的采购分类，找不到就根据 ITEM_CODE 找全部库存组织分表的采购分类，再找不到的话就认为要去查物料试用表了，以当前作为试用 P 编码找到对应转正后的编码，然后重走一遍上面逻辑，找到后就合并按照 BU_CODE + 特征名称 + 特征值 + 研发分类 ID，查找研发分类中的采购分类，然后根据采购分类查找品类分工表中的配套人员信息，去重后返回即可。

      ![1644903196224](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1644903196224.png)

   2. 接口实现比较复杂，其中需要优化的点有：

      - 1）第一，接口日志要做统一的收集。
      - 2）第二，接口要做安全性校验。
      - 3）第三，接口要查询所有的库存组织的采购分类分表。
      - 4）第四，接口需要频繁按照 ORG_CODE 查找 BU_CODE。

4. **结果 Result**：所以我当时优化的手段就是：

   - 1）第一，写了一个 Around 类型的切面注解，对公共的日志收集逻辑进行了提取，然后把收集到的日志投递到 Kafka 中，再由消费者插入到 MongoDB，这种做法的好处是，使得日志收集逻辑解耦了业务代码，同时异步收集的话经过测试，对比同步插入还有一定的性能提升。
   - 2）第二，安全性校验这边统一采用的是 MD5 对整个请求体生成摘要，SRM 需要重新生成一遍，然后与传过来的比对是否一致，一致才认为请求体没被修改过，然后再根据缓存中的 AES 秘钥进行解密，得到真正的请求 JSON 串。
   - 3）第三，使用线程池多线程并发的方式，查询每个采购分类分表，通过 FutureTask 监听汇总所有的采购分类编码，对比串行查询有一定的性能提升。
   - 4）第四，由于 ORG_CODE 与 BU_CODE 是那些不怎么会改变的数据，所以通过使用了 Redis 对这些关系进行了远端缓存（使用 Guava Cache 重启可能会导致缓存雪崩，且数据量过大还会占用应用内存，本身也才 2 G，所以放 Redis 中，虽然增多了一次 I/O 这也是可以接受的），使得多次请求只会获取一次库存事业部关系，对比批量查询时有一定的性能提升。 

###### 2）供应商画像同步 | 架构、设计模式、Kafka

1. **背景 Situation**：
   1. 业务方需要一个页面，可观地展示出供应商各维度的一个画像，包括抬头展示它的**基本信息**，词云浓缩它的**肖像标签**，动画展示它自美的引入以后所走的一些**历程**，对接天眼查展示对应它的一些企业经营风险，柱状图展示它历年的**招投标**、**红黄牌**以及**采购金额**的数量，折线图展示它历年的**考核**和**绩效**的趋势，饼图展示它的**分级**和**供货编码**的占比情况，用来横向、纵向辅助对比找出优质供方。
   2. 允许数据延迟，可以每隔 15 天同步一次。
2. **任务 Task**：这样，要做的东西就有，在 TASK 模块的定时任务触发时，要一一查找每家供应商的 POS 供方生命周期库的基本信息、红黄牌、获奖以及供货编码信息，查找 BID 招标库的历年招标信息，查找 PERF 考核绩效库的历年考核、绩效以及分级信息，查找 PRICE 价格库的采购金额信息，对接天眼查找告警风险，最后再根据规则组装出要展示的肖像标签。
3. **行动 Action**：当时有几种方案，
   - 1）第一种就是，1 个定时器，负责同步所有供应商的上述所有维度的信息，优点是实现简单，不用做其他的架构设计，缺点是如果同步实现的话一个事务完成所有工作，任意一处异常则全局回滚，如果异步实现的话，由于是单点触发定时任务，还是会有单台机器负担过重的问题。
   - 2）第二种就是，10 个定时器，每个定时器只负责同步所有供应商的某个维度的信息，优点是可以分为 10 台机器分别触发定时，避免了单台机器负担过重，缺点是一个定时器一个事务，还是会出现任意一处一处则全局回滚，以及定时器过多，管理麻烦。
   - 3）第三种就是，1 个定时器 + 分布式消费，定时器只负责找出要生成画像的供应商编码，封装成消息，扔到 Kafka 上，自己不负责画像的生成，而是交给消费者去进行处理，消费者每次消费一个消息，相当于生成一个供应商的画像，这样做的好处就是，定时逻辑和画像生成逻辑解耦，1 个定时器即可完成任务，要管理维护的地方少，然后就是，画像生成的效率取决于 Partition 和机器的数量，能够充分利用集群多实例部署的优点，还有就是，即使某次供应商画像生成失败了，可以不进行手工 ACK，下次再从 Kafka 里拿消息出来进行重复消费即可。
4. **结果 Result**：所以，当时就采用了第三种方案，在实现时又有几个优化点：
   - 1）第一就是，还是没能解决由于一个供应商所有维度处于同一个事务，出现异常时的全局回滚问题，解决方案就是，通过线程池的方式异步实现，每个线程一个事务，只完成一个维度的信息同步，这样即使某个回滚了也不影响全局。
   - 2）第二就是，由于一共有 10 个维度的信息要同步，就需要 10 个 Callable 任务给线程去执行，所以就抽象了公共的接口。
   - 3）第三就是，由于任务较多，所以就采用了工厂 + 建造者 + 责任链来串联式地组织任务，而且到了后期，由于是责任链的组织方式，就可以根据业务组合出很多种画像的同步方案出来，体现了灵活性。
   - 4）第四就是，由于任务与任务间，经常有重复的信息要查询，所以就对这个接口分为了同步式实现和异步式实现的抽象类，业务只需要继承抽象类实现对应的业务即可，同步式实现主要是为了在一开始给执行链设置公共上下文，避免重复查询，异步式实现则是交给线程池去执行，在要拿那些”重复“信息时只需要去上下文中获取即可。
   - 5）第五就是，某次消费异常，也就是执行链中某个节点异常，其堆栈信息需要对其进行合理记录，方便后面排查问题，解决方案就是，通过如果有任务异常后，那么就收集放到执行链上下文中，返回时再统一地打包成对象，存放到 MongoDB 上。
   - 6）最后，这样设计和实现，用 Kafka + 线程池，保证了分布式高性能消费，用 Kafka + 异常时不手工 ACK 来重复生成画像，保证可靠性，用 MongoDB 记录异常日志，保证异常排查的便捷性。

###### 3）线程池调优 | 线程池、源码、调优

1. 我们系统某个模块中常常需要使用别的模块的数据源，比如在 POS 模块中需要去查询 PRICE 库，此时，由于主线程默认绑定的是 POS 数据源，要想使用 PRICE 数据源的话，就需要开辟子线程去做切换，所以就为每个模块抽象了数据源切换的工具类。
2. 由于正常都是调用一次切换方法，也就是把 Lambda 表达式提交给线程池，主线程再去做 `FutureTask.get()` 的监听，拿到结果后返回，这是正常情况，因此我们一开始也没怎么留意。
3. 但问题就来了，有一次某段代码对连续进行了两次的切换，就发现接口超时，以及往后的切换都超时了，那时排查了很久，最后在这个切换工具类里看到，生产上跑的参数为 `corePoolSize=0、maximumPoolSize=32、workQueue=LinkedBlockingQueue`，按照原理，本以为是错误的配置，会导致任务都阻塞在队列中直到 OOM 发生的，但改成一次切换又可以了，这就很奇怪了，然后就去研究源码，发现刚才上面所说的那段**补充非核心线程**的逻辑，也就是即使 `corePoolSize=0`，只要最大线程数不为 0，那么还是会保证至少创建一个线程去消费任务队列的，所以切换一次就可以拿到任务的运行结果。
4. 那么为什么切换两次就不行了呢？这是因为，在第一次切换时，是主线程在做 `FutureTask.get()`，阻塞的是主线程，而如果还做一次连续切换，那么就会导致线程池中仅有的 1 个线程也做了 `FutureTask.get()`，从而进入了阻塞，但线程池认为有效线程数还是 0，所以新任务进来也不会去创建新线程消费，会直到 OOM 的发生为止~
5. 最后，更正了合理的构造参数 `corePoolSize=Runtime.getRuntime().availableProcessors() + 1`，`maximumPoolSize=2 * Runtime.getRuntime().availableProcessors()`，`keepAliveTime=120s`，`LinkedBlockingQueue=1024` 保证不会发生内存溢出，`Google#ThreadFactoryBuilder`来设构建线程工厂、在线程创建时设定名称前缀和递增线程号，`RejectedExecutionHandler=CallerRunsPolicy` 保证任务不丢失，同时设置 `allowCoreThreadTimeOut=true` 允许核心线程空闲时也会被回收。

###### 4）死锁问题排查 | 间隙锁、可重复读、并发

1. **背景 Situation**：在现场评审优化了一版上线后，DBA 反馈说，POS 数据库因为有大量死锁，导致数据库不断地重启，并且也把问题的 SQL 给发了出来，是一条删除语句导致的：

   ```sql
   DELETE FROM srm_reiew_audit_evidence WHERE locale_review_id = 3 AND REVIEW_TYPE = 3;
   ```

2. **任务 Task**：然后我们很快就定位到代码，找到了以下逻辑：方法传入一个 `List<DTO>`，先是获取第一行的  locale_review_id `${localeReviewId}`，对满足 locale_review_id= `${localeReviewId}` 的都进行删除，完成删除后，再批量插入 locale_review_id= `${localeReviewId}` 的 `List<DTO>` 数据：

   ```java
   int flag=0;
   Long localeReviewId=new Long("1");
   for (AuditEvidenceDto auditEvidenceDto : list) {
       if (auditEvidenceDto.getEvidenceId()!=null){
           flag=1;
           localeReviewId=auditEvidenceDto.getLocaleReviewId();
       }
   }
   HashMap<String,Object>params =new HashMap<>();
   if (flag==1){
       params.put("localeReviewId",localeReviewId);
       params.put("categoryCode",list.get(0).getCategoryCode());
       params.put("reviewType",list.get(0).getReviewType());
       auditEvidenceService.deleteFlag(params);
   }
   if(!"Y".equals(list.get(0).getAttribute1())){
       list.sort(Comparator.comparing(AuditEvidenceDto::getFileType));
       auditEvidenceService.batchInsertAuditEvidence(list);
   
   }
   ```

3. **行动 Action**：

   1. 对于这种写法，首先，在代码里做业务逻辑操作，肯定是不对的，当时事态紧急，也没有做过多的分析，最直接的解决思路就是，找到对应的同事，让他把 Controller  `delete` 和 `insert` 操作都放入一个 Service 中，用同一个事务管理，做一个紧急发版看下能否解决。

   2. 发了新版后，好像问题是解决了，但是结合 `show engine innodb status`，就分析出在并发场景下，还是会死锁的问题，步骤是：

      1. 先在表中先插入几条数据，然后给 locale_review_id 加普通索引（而在表全新，且没有数据时，有没有索引都会加一个行 X 锁，然后也会出现下面的情况）。
      2. 然后，事务 1 对 locale_review_id=3 的记录进行删除，则会先获取一个表级的 IX 锁，然后由于记录不存在，而对 （负无穷，13）、（13，正无穷）上一个间隙锁。
      3. 同理，事务 2 再对 locale_review_id=3 的记录进行删除，也会先获取一个表级的 IX 锁，然后由于记录不存在，而对 （负无穷，13）、（13，正无穷）上一个间隙锁，由于间隙锁不冲突，所以事务 2 不会等待。
      4. 接着，事务 1 再插入  locale_review_id=3 的记录时，由于已经存在事务 2 的间隙锁，所以需要等待它释放，才能获取到插入意图的间隙锁。
         - 在插入行之前，会设置一种称为**插入意图的间隙锁**，表示插入的意图，即如果插入到同一索引间隙中的多个事务未插入到间隙内的同一位置，则它们无需相互等待，是不冲突的。
         - 而如果发生重复键错误，则会在重复索引记录上设置**共享锁**，并且如果另一个会话已经拥有排他锁，那么如果有多个会话尝试插入同一行，则使用共享锁可能会导致**死锁**，比如说后面分析的那种情况。
      5. 同理，事务 2 再插入  locale_review_id=3 的记录时，由于已经存在事务 1 的间隙锁，所以需要等待它释放，才能获取到插入意图的间隙锁。
      6. 此时，就发生了死锁，即事务 1 持有间隙锁，同时等待事务 2 的间隙锁，事务 2 也持有间隙锁，同时等待事务 1 的间隙锁，如果没有外力作用，它们都将无法推进下去，而在 MySQL 5.6.26 上模拟则是，自动回滚了事务 2 的事务，让事务 1 顺利得推进下去，成功插入 locale_review_id=3 的记录。

      ```sql
      -- 使得 show engine innodb status; 能展示锁的信息
      set GLOBAL innodb_status_output_locks=ON;
      -- 查看事务锁持有情况
      show engine innodb status;
      ```

      | 步骤 | 事务 1                                                       | 事务 2                                                       |
      | ---- | ------------------------------------------------------------ | ------------------------------------------------------------ |
      | 1    | SET AUTOCOMMIT = 0; START TRANSACTION;                       | SET AUTOCOMMIT = 0; START TRANSACTION;                       |
      | 2    | DELETE FROM srm_reiew_audit_evidence WHERE locale_review_id = 3 AND REVIEW_TYPE = 3; |                                                              |
      | 3    |                                                              | DELETE FROM srm_reiew_audit_evidence WHERE locale_review_id = 3 AND REVIEW_TYPE = 3; |
      | 4    | INSERT INTO test.srm_reiew_audit_evidence (EVIDENCE_ID, locale_review_id, FILE_TYPE,...) VALUES (3, 3, '3'...); 等待中 |                                                              |
      | 5    |                                                              | INSERT INTO test.srm_reiew_audit_evidence (EVIDENCE_ID, locale_review_id, FILE_TYPE,...) VALUES (3, 3, '3'...); |
      | 6    |                                                              | DEAD LOCK；Auto Roll Back                                    |
      | 7    | 插入成功                                                     |                                                              |

   3. 所以，这种方案也不是万全之策，然后我们重新考虑新的方案，其思路是打算先把一开始的死锁给复现出来，再做解决方案的分析。

   4. 不久后，场景就被模拟出来了，那是 `delete` + `insert` 在不同事务中导致死锁的发生，具体的步骤是：

      1. 事务 1 执行删除，但未提交，由于 locale_review_id=3 的记录已经存在，所以在 locale_review_id=3 行上了排他锁，此时该行的主键 ID 也是等于 3，即 ID=3。
      2. 然后，事务 2 打算插入 ID=3 的记录，由于 ID=3 的记录已经存在，发生了主键重复错误，所以需要请求 ID=3 行的共享锁，而又由于 ID=3 行上存在排他锁，所以事务 2 进入等待。
      3. 同理，事务 3 打算插入 ID=3 的记录，由于 ID=3 的记录已经存在，发生了主键重复错误，所以需要请求 ID=3 行的共享锁，而又由于 ID=3 行上存在排他锁，所以事务 3 也进入等待。
      4. 接着，事务 1 提交，ID=3 的行记录被标记为删除状态（这些标识为删除状态的记录，会后续由后台的 Purge 操作进行物理删除，但是，此时还是会在索引中存放一段时间），ID=3 上的排他锁被释放。
      5. 然后，事务 2、3 就成功抢到了共享锁，打算执行插入操作，由于已经存在事务 3 的共享锁，所以需要等待它释放，才能获取到插入意图的间隙锁，事务 2 进入等待。
      6. 同理，由于已经存在事务 2 的共享锁，所以需要等待它释放，才能获取到插入意图的间隙锁，事务 3 也进入等待。
      7. 此时，就发生了死锁，即事务 2 持有共享锁，同时等待事务 3 的共享锁，事务 3 也持有共享锁，同时等待事务 2 的共享锁，如果没有外力作用，它们都将无法推进下去，而在 MySQL 5.6.26 上模拟则是，自动回滚了事务 3 的事务，让事务 2 顺利得推进下去，成功插入 locale_review_id=3 的记录。

      | 步骤 | 事务 1                                                       | 事务 2                                                       | 事务 3                                                       |
      | ---- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
      | 1    | SET AUTOCOMMIT = 0; START TRANSACTION;                       | SET AUTOCOMMIT = 0; TRANSACTION;                             | SET AUTOCOMMIT = 0; START TRANSACTION;                       |
      | 2    | DELETE FROM srm_reiew_audit_evidence WHERE locale_review_id = 3 AND REVIEW_TYPE = 3; |                                                              |                                                              |
      | 3    |                                                              | INSERT INTO test.srm_reiew_audit_evidence (EVIDENCE_ID, locale_review_id, FILE_TYPE,...) VALUES (3, 3, '3'...); 等待中 |                                                              |
      | 4    |                                                              |                                                              | INSERT INTO test.srm_reiew_audit_evidence (EVIDENCE_ID, locale_review_id, FILE_TYPE,...) VALUES (3, 3, '3'...); 等待中 |
      | 5    | COMMIT;                                                      |                                                              |                                                              |
      | 6    |                                                              |                                                              | DEAD LOCK；Auto Roll Back                                    |
      | 7    |                                                              | 插入成功                                                     |                                                              |

   5. 面对以上的分析情况，解决方案有两个，第一个方案是，在后面批量插入前，先把 ID 给置为 NULL，利用自增机制去设置 ID，避免两个 `insert` 语句同时插入同一个位置，但问题是，`delete` 在释放排他锁后，两个 `insert` 语句都会执行成功，会比正确结果多出来一条记录，所以此方案放弃。

   6. 第二个方案是，把 `delete` + `insert` 的操作，转换为 `update` 操作，则可以避免上述死锁的发生，通过。

4. **结果 Result**：最后我们就是通过第二个方案，把这段有问题的代码，又重新优化了一遍，解决了这种死锁的问题。

###### 6）内存溢出排查 | MAT、POI、ArrayList

1. **背景 Situation**：生产的一台 POS 服务器宕机了，由于配置了 `-XX:+HeapDumpOnOutOfMemoryError`，所以崩溃时导出了对应的 hprof 文件。

2. **任务 Task**：导入 hprof 到 MAT，分析应用崩溃的原因。 

3. **行动 Action**：

   1. MAT -> Leak Suspects 做内存泄露分析，然后就得到了怀疑报告：说，一个 Tomcat 的线程使用了 78.58% 的堆大小，共 1.2 GB。

      ![1646412309344](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646412309344.png)

      ![1646413204478](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646413204478.png)

   2. 然后，查看 List Objects -> with outgoing references，观察一个占据了 22.68% 内存的 ArrayList 的保留集，发现它引用了 27.69 w 个元素，估计是某个 SQL 查了太多元素导致。

      ![1646413486175](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646413486175.png)

   3. 所以，就继续查看堆栈信息，定位问题代码，是一个 Service 实现类的 `setData()` 方法。

      ![1646413368891](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646413368891.png)

   4. 原来，是调用了 Controller 的 `downloadItemDailyCapacityTemplate()` 方法，看了下业务含义，大概的意思是，在导出日常产能预测模板时，由于把产能表每日信息的查询结果，放入到了上面所说的那个 27.69 w 个元素的 ArrayList 中，然后再遍历这个列表，调用 POI#API 在内存中生成 excel，但由于堆内存只配置了 `-Xmx=2048m`，所以导致了内存溢出！

      ![1646413877943](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646413877943.png)

4. **结果 Result**：因此，解决方案是，持有这么大的 ArrayList 时，不应该把 excel 还写入内存中，而是遍历过程中，先把 excel 一点一点写入到磁盘，释放掉这个 ArrayList 后，再读取磁盘的 excel 文件的字节流，写入到 response 输出流中，或者直接扩大堆内存大小 `-Xmx=4096m `，从而解决堆内存溢出的问题。 

###### 7）条码销售出库 | 索引调优、生产问题

1. **背景 Situation**：OEM 供应商反馈，系统响应扫码速度过慢，专门用手机计时，记录延迟有 4.55 s，扫码枪扫完一整板的货，系统才开始对异常条码报错，导致如果一板出现问题条码的话，就需要这一整板重新扫过，才能找出问题条码，影响了供方出货的效率。

2. **任务 Task**：当时，OEM 供方有 2 个，日班和夜班一共有 2 条产线，根据视频中的操作速度，大概是 4~5 个 / 1s，所以也就是并发数为 5 * 2 = 10 的 QPS，分摊到 3 台服务器的话，每台服务器需要让接口满足 300 ms 以下的延迟，因此，在不增加服务器的情况下，尽量让接口的延迟足够低。

3. **行动 Action**：根据供方提供的截图，找到对应的接口，然后在测试环境模拟跑一遍接口，把接口路过的 SQL 日志都收集起来，统一分析，最终发现有几处索引是没有添加的，分别是：

   1. **psi_base_barcode**：条码表，314.67w+，16 c 64 G MySQL， 优化前 Druid SQL 监控【执行时间】显示 1000 ms 左右，优化后 Druid SQL 监控显示 35 ms 左右。

      ```sql
      ALTER TABLE psi_base_barcode ADD INDEX psi_base_barcode_n2(CUS_COMPANY_CODE, PRODUCT_CODE);
      ```

   2. **psi_base_packing_relation**：箱包关系表，315.28w，16 c 64 G MySQL， 优化前 Druid SQL 监控【执行时间】显示 650 ms 左右，优化后 Druid SQL 监控显示 32 ms 左右。

      ```sql
      -- 4、判断条码是否已出库：增加【CARTON_BARCODE】和【BARCODE】索引有提升
      SELECT COUNT(1)
      FROM psi_sales_stock_out
      WHERE (CARTON_BARCODE = '331100007920401110191W' OR BARCODE = '331100007920401110191W');
      ```

   3. **psi_sales_stock_out**：已出库条码表，840.58w，16 c 64 G MySQL，优化前 Druid SQL 监控【执行时间】显示  2290ms 左右，优化后 Druid SQL 监控显示 40 ms 左右。

      ```sql
      -- Add Index： explain => index_merge，Using union(psi_sales_stock_out_n1,psi_sales_stock_out_n2); Using where
      ALTER TABLE psi_sales_stock_out ADD INDEX psi_sales_stock_out_n1(CARTON_BARCODE);
      ALTER TABLE psi_sales_stock_out ADD INDEX psi_sales_stock_out_n2(BARCODE);
      ```

4. **结果 Result**：最终通过使用 Druid 监控，发现关键 SQL 的耗时已经满足要求了，psi_base_barcode 提效 96.51%，psi_base_packing_relation 提效 95.08%，psi_sales_stock_out 提效 98.25%。

###### 8）品质云微服务拆分 | 微服务、架构、设计

1. **背景 Situation**：

   1. 当时，OEM 供方有 2 个，日班和夜班一共有 2 条产线，不能停线的那种，此时如果品质这块有新需求要发，就需要工厂停线，等我们发版、验证通过了才能继续生产，对于大家来说其实都不好。
   2. 而且，由于是单体， OEM 生产功能与其他零部件供方的 SPC、PQC、OQC 的代码统统在一块，导致系统的负载比较高，常常是能看到 3 台服务器，每台 CPU 和内存都过 60%，有时遇到生产高峰以及定时器触发的话，会出现半夜电话报障说，应用宕机了，需要紧急重启的情况。

2. **任务 Task**：所以出于上面说的诉求，以及加上对未来供方不断上线，当时就规划了对各单体云，进行微服务拆分，共用一个注册中心和配置中心，其中就由我来负责品质云的微服务落地。

3. **行动 Action**：

   1. 我这边拆了 4 个业务模块（2 个月左右），分别是 BASE 基础数据模块，主要是一些字典、用户权限、企业、客户、组织等基础业务。
   2. OEM 代工生产模块，也就是上面所说的主要诉求点，包括成品下线、抽检、出库等业务，把这块生产线相关的业务剥离出去，可以做针对性的优化以及灵活的发版处理。
   3. 而剩余的其他业务，都统统放 APP 综合模块，与单体时的业务基本保持不变，主要是零部件供应商的 SPC 过程检验、PQC 半成品检验、OQC 成品出货检验等业务。
   4. JOBS 定时任务模块，则是后来继续剥离的一个模块，剥离这个模块可以保证生产高峰期时，在夜间也能平滑运行，不会受到大量定时器触发，导致的负载突然增高的影响，主要是一些与外围系统数据同步等业务的处理。

4. **结果 Result**：中间遇到的难点有：

   - 1）**服务的依赖边界问题**：

     1. 在拆分时，一开始我是直接把下线扫码、抽检、出库给拆开，SPC、PQC、OQC 也都拆开，然后就发现拆得粒度过小，出现了很多相互的依赖调用，就又要去考虑，如何代价尽可能小地，去避免带来新的分布式事务和性能问题。
     2. 而最后，是根据业务诉求，以及拆分后的风险与成本的评估，才认为最好的方案应该是，下线扫码、抽检、出库这三个生产线相关的业务合回一个 OEM 模块，其他的 SPC、PQC、OQC 等不是本次诉求的业务又合回一个 APP 模块。

   - 2）**Feign 远程调用时，遇到的响应结果被统一包装的问题**：

     1. 一开始单体时的系统，是有对响应结果做 `Result` 统一包装的，其原理是实现了 `HandlerMethodReturnValueHandler#handleReturnValue()` 方法，这是在 `invocableMethod.invokeForRequest()` 方法处理完 Handler Method 并拿到结果后，调用 `this.returnValueHandlers.handleReturnValue` 对返回结果进行的一个后置处理。

        ```java
        // 统一包装
        public class ResponseBodyWrapperHandler implements HandlerMethodReturnValueHandler {
        	@Override
        	public void handleReturnValue(Object returnValue,
        			MethodParameter returnType, ModelAndViewContainer mavContainer,
        			NativeWebRequest webRequest) throws Exception {
        		_logger.info("return data is " + returnValue);
        		if (returnValue instanceof Void) {
        			returnValue = Result.DefaultSuccessResult;
        		} else {
        			if (!(returnValue instanceof Result)) {
        				returnValue = Result.build(true, ResultCode.SUCCESS_CODE, "", returnValue);
        			}
        		}
        		handler.handleReturnValue(returnValue, returnType, mavContainer, webRequest);
        	}
        }
        ```

     2. 这个包装主要是为了打印原始的响应结果，以及自定义 `retCode` 响应码和 `retMsg` 响应语，但在使用 Feign 调用后，就出现，如果使用那边 Controller 的方法返回值类型，作为这边 Fegin 接口的返回值类型，则收到的就只是一个 null，因为中间做了一层 `Result` 的包装，此时需要对其进行解码，解码的话是实现了 `feign.codec.Decoder#decode()` 方法，通过注入并使用`com.fasterxml.jackson.databind.ObjectMapper` 进行读取 body 并修改，从而实现包装的统一解码工作，避免了在业务代码中，写过多的包装解码代码。

        ```java
        // 自定义Feign解码器
        @Component
        class FeignResultDecoder implements Decoder {
        
            @Autowired
            private ObjectMapper objectMapper;
        
            @Override
            public Object decode(Response response, Type type) throws IOException, DecodeException, FeignException {
                if (response.body() == null)
                    throw new DecodeException(response.status(),  "接口没有返回有效的数据, url: " + response.request().url(), response.request());
        
                // 解析body, 得到统一包装Result实例
                Result result = objectMapper.readValue(Util.toString(response.body().asReader(Util.UTF_8)), Result.class);
                if (ResultCode.ERROR_CODE.equals(result.getRetCode()))
                    throw new DecodeException(response.status(), "接口返回错误: " + result.getRetMsg() + ", url: " + response.request().url(), response.request());
        
                // 重新解析Result#data实例并返回
                return objectMapper.readValue(objectMapper.writeValueAsString(result.getData()), TypeFactory.defaultInstance().constructType(type));
            }
        }
        ```

   - 3）**网关路由问题**：由于主要是后端进行的微服务拆分，前端基本保持不变，所以就需要在收到请求后，对原始 url 进行截断，找到对应的关键字进行服务路由，当时采用的是 Gateway 做了服务网关，其好处就是高性能、对开发友好，只需要在代码中更改路由规则即可 、以及对接注册中心，服务地址不需要经常在网关上配来配去。

   - 4）**过滤器问题**：单体时的过滤器是实现 `javax` 包下的 `Filter` 类来实现的，主要是对一些 token 做校验、给上下文设置当前用户信息等操作，但由于 Feign 调用使用的是 HTTP 请求，也会经过这一层层的 `Filter` 校验，在有了服务网关后，就在网关上做了前置的过滤，主要是实现 `GatewayFilter` 和 `GlobalFilter` 接口来完成对应的过滤，这样，服务间调用就无需走那一层层的 `Filter` 校验了，只需要在网关中被校验一次即可，从而保证远程调用时的性能问题。

###### 9）token 校验失败问题 | 分布式锁、QMS Client Json 对比、问题排查

1. **背景 Situation**：OQC 零部件供方在成品出货报告新建完毕后，需要把报告推送给 QMS，在接口联调时没有问题，但放到生产上就偶尔能推得过去，偶尔推不过去，导致供方有些成品无法继续出货，货物堆在了仓库外面，需要马上解决这个问题。
2. **任务 Task**：
   1. 根据日志记录，推不过去的原因是，品质这边的客户端，收到了 QMS 的一个异常返回结果 `token校验失败`，这就很奇怪了，token 如果校验失败的话，那么为什么有时候能够成功呢，不应该一直都是失败的吗？
   2. 然后就翻了日志，与 QMS 收到的 token 进行比对，发现确实是 token 的问题。
   3. 这个 token 呢，是根据 DTO 对象转成 json 字符串后，使用内存中 QMS 的一个序列号 `appSec` 进行 MD5 加盐，生成摘要而得到的，由于其他参数都是写"死"的，也就是问题出在，用于生成摘要的 DTO 对象与传输过去的 DTO 对象不一致！
3. **行动 Action**：
   1. 然后，我扒出了有问题的推送报文，与 QMS 收到的报文进行对比，用了在线 Json 比对工具 `www.bejson.com`，结果放入后就明显看到，QMS 收到了多余的 `NULL串` ，从而造成两边生成的 token 不同。
   2. 果然，看回代码就发现，发送给 QMS 的 json 串和用于生成摘要的 json 串的生成逻辑不一样，前者是交给 `forest.httpClient` 做 json 转换，而后者是直接用了 `objectMapper#setSerializationInclusion(JsonInclude.Include.NON_NULL)` 和 `ObjectMapper#writeValueAsString()` 做的生成。
   3. 在这个工具类里，ObjectMapper 实例作为成员变量，如果先调 ``ObjectMapper#writeValueAsString()` 填充好了 NULL 序列化器，那么这个 ObjectMapper 每次都会输出 `NULL串` ，而如果先调的是 `objectMapper#setSerializationInclusion(JsonInclude.Include.NON_NULL)` 设置了普通序列化器，那么这个 ObjectMapper 每次都不会输出 `NULL串`，这也是为什么 token 时对时不对的原因所在。
   4. 所以，解决方法就是，在工具类中分开两者的使用，持有一个生成 `NULL串` 的 ObjectMapper，再持有一个生成正常串的 ObjectMapper，在生成 token 时使用后者，从而解决 token 不正确的问题。
4. **结果 Result**：最后，再使用分布式锁做一个单点定时，每隔 3 分钟把推送失败的报告重新推送过去，并设置 3 次的最大重推次数，保证这个推送接口的可靠性，和业务的连续性。

###### 10）大文件上传 | FastDFS、Redis List

1. **背景 Situation**：在建广告类交付单时，要求上传大视频，以让审批人去浏览理解当前交付的广告内容。

2. **任务 Task**：当时系统文件存储用的是 FastDFS，主要是用来存储中小文件，比如用户头像、图片、附件等内容，对文件大小也是做了 200 M 的限制，当时如果要实现大视频的话，一来文件接收上限要放宽，违反了组件用来上传小文件的约定，二来，会有大文件不能续点上传的问题，所以最直接的思路就是，对上传过来的文件进行分片上传，接收到所有分片后再合并成进行大文件存储，具体的做法是：

3. **行动 Action**：

   1. 前端方面，用的是 WebUploader 组件对文件进行分片，其原理是，在获取到文件后，先计算它的一个 MD5 文件摘要，然后对其按照每 200 M 作为一个 chunk 分片，并使用序号顺序标记每个 chunk，再调用后端提供的一个接口进行上传。
   2. 后端这边，接口则接收文件名、文件 MD5 摘要值、有无分片、最大分片号、当前分片号、MultipartFile，以及一些其他辅助信息，接收到后，先是用 MD5 判断当前这个大文件是否曾经上传过，如果上传过就不再处理了，再判断 MD5 + 分片号是否存在了，存在则不再处理了，从而解决即使分片报文重复，或者前一次中断然后本次重新上传的问题，实现续点上传。
   3. 然后，调用框架 API 保存好分片，这个框架主要是对FastDFS 做了封装，把文件存到特定的位置，然后返回文件存储对象的元数据。
   4. 接着，把【分片号 + 文件ID 】顺序放入用的 MD5 值做 key 的 Redis List 中。
   5. 最后，在收到最后一个分片时，则取出 Redis 中的所有【分片号 + 文件ID 】，对分片号做一次排序，从而解决分片不按顺序到达的问题，再顺序合并所有分片成为一个大文件，调用框架 API 保存好这个大文件，拿到它的元数据存储到文件关系表中，这样，下次在下载时，则可以根据文件 ID 找到当时的元数据，再去拿存储服务器上对应路径下的大文件即可，从而完成一次大视频文件的上传和下载。

4. **结果 Result**：总的来说，就是用了 WebUploader 前端组件进行文件分片，后端则是共用同一个接口，先对分片进行缓存，然后再顺序合并成大文件，解决大文件续点上传的问题。

   ![1646294342544](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646294342544.png)

   ![1646294375795](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646294375795.png)

###### 11）邮件发送组件 | 设计、RabbitMQ

1. **背景 Situation**：交付单触发待办提醒时，需要发送邮件通知对应的责任人进行处理，此时需要对接比亚迪的一个邮件服务器，进行邮件发送。
2. **任务 Task**：由于邮件发送不需要非常的及时，所以就采用了异步发送的发送，以提高性能。
3. **行动 Action**：
   1. 当时采用的方案是异步发送，接口把邮件对象丢给线程池，就返回响应客户端其他信息了，再由子线程去把邮件落库。
   2. 然后定时触发，捞取对应未发送和发送失败的邮件，进行批量发送，直连式地把消息发送到一个 `SCC_COMMON_EXCHANGE` 的 `SCC_MAIL_QUEUE` 的队列中，在收到 Broker ACK Confirm 后，则标记为发送成功，否则继续被下轮定时器触发捞取出来重新发送。
   3. 最后，由比亚迪那边的消费者进行消费，发送到邮件服务器中，完成异步邮件发送。
4. **结果 Result**：基于上述的方案，我封装了一个异步邮件发送工具类，在每次遇到需要异步发送邮件时，则调用那个工具类的对应方法即可，既方便，又解耦了业务代码，然后还拥有不错的性能。

###### 12）发版平台 CPU 占用过高 | Tomcat Manager、ELK、CPU 95%

1. **背景 Situation**：我们团队这次的迭代，需要在云平台开发几个接口，然后在和 SRM 联调测试时，为了不用等比亚迪 SAP 测试环境的发版窗口，就需要有我们自己的一个云平台测试环境，方便快速发版快速验证。

2. **任务 Task**：那时由我负责来负责搭建，但由于是在内网环境进行开发，访问不了外网，就拉取不到 Jenkins 的依赖，当然也没有云原生这些基础设施，所以采取了 Tomcat Manager + ELK 部署的方式来发版，前者是 Tomcat 的后台，可以开发完代码后导出 war 包，然后在浏览器上把 war 包进行上传部署，后者是用于在 Kibana 上观察接口的异常日志。

3. **行动 Action**：

   1. 就这样，一个简单易用的发版平台就搭建好了，不过没几天问题就来了，我们发现每一次上传 war 包到启动完成，所花的时间好像都会比上一次的要久，最后一次 war 包上传则是浏览器一直在转了。

   2. 当时为了不阻塞测试进度，就到发版平台的服务器上看，先是使用 `top` 命令看到 118243 的 Tomcat 进程，居然占用了 95.3% 的 `%CPU`：

      ![1646305025603](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646305025603.png)

   3. 然后使用 `top -Hp 118243 ` ，查看 Tomcat 进程的所有线程，发现最高的一个线程 `118332` 占用了 73.3% `%CPU`：

      ![1646305182797](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646305182797.png)

   4. 然后使用 `printf %x 118332`，获取对应的 16 进制线程号，用于后面的 jstack 文件搜索：

      ![1646305395917](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646305395917.png)

   5. 然后就是 `jstack -l 118243 > thread-dump.txt `，导出对应 `118243` 进程的线程快照，打开搜索对应的 `1ef96` 16 进制线程号（这个是当时真实的线程号，上面是后期模拟的） ，就定位到了一堆 `ParallelGC` 线程，原来是 GC 太多而导致的 CPU 飚高，那么为什么一个发版的 Tomcat，会有这么多 GC 线程呢？会不会是项目代码哪里写的有问题啊？

      ![1646305627751](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646305627751.png)

   6. 然后我就使用 `jmap -dump:format=b,file=heap-dump.hproft 118243`，以 `hprof` 格式导出 `118243`  Tomcat 进程的堆内存快照，再导入到 MAT -> Leak Suspects 做内存泄露分析，然后就得到了怀疑报告：说，有 18 个 `ParallelWebappClassLoader`  共占据了 44.61% 堆空间，咦？为什么要 18 个这种类加载器呢？

      ![1646306496272](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646306496272.png)

      ![1646306148994](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646306148994.png)

   7. 然后我就想起 Tomcat 的类加载过程，是打破了双亲委派机制的：

      1. **ext -> bootstarp模型**：保证了 JRE 核心类库不会被重复加载，满足了加载JVM共同类库的需求。
      2. **ext -> webapp模型**：实现了每个 web 应用只加载自己的类库（WEB-INF/classes -> WEB-INF/lib），从而实现了应用间的类库隔离的需求。
      3. **webapp -> share -> common模型**：实现了所有 web 应用之间、web 与 Tomcat 之间，能够加载相同的类库，避免指定的类库不会被重复加载，满足了加载其他共同类库的需求。

      ![1626090700749](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1626090700749.png)

   8. 也就是一个 `ParallelWebappClassLoader` 对应一个 Web App 项目，但用来发版的 Tomcat 应该就只有一个项目呀，所以我就怀疑应该是每次发版没把上一版的项目卸载掉所导致的。

4. **结果 Result**：所以我就让组内的成员，大家在下次发版前，先用 Tomcat Manager 把上一个项目卸载掉，再发新的 war 包上去，这样做了后，就没碰到过这样的情况出现了，应该算是蒙对了吧~哈哈

#### 2.1.1.2. 分片广播式定时的设计及原理？

##### 1、总

除了用 Kafka 去实现分片广播式定时，我还知道以下解决方案：

##### 2、分

1. 基于 **Redis ZSet** 有序集合实现：

   - 1）添加定时任务，把每个画像消息作为一个定时任务元素，定义好任务数据结构（表任务 ID、开始时间戳、循环条件等），然后使用 `ZADD` 命令把定时任务加入有序集合中，分数为开始时间戳。
   - 2）取定时任务，则是通过应用轮训去取，每隔 1 秒去消费有序集合，其中要注意的点是，由于应用是集群部署的，所以消费时要先获取分布式锁，获取到的才能进行消费。
   - 3）获取集合最低的分数，判断分数是否比当前时间戳小，是的话则执行后删除元素，然后根据循环条件来决定是否需要将其重新加入集合中，在下次循环之前，释放分布式锁，否的话则直接释放分布式锁。

   => 优点是，Redis 保证任务顺序执行，但可能会存在一定延迟，缺点是，每次轮训都发生所有应用来抢占分布式锁，网络带宽可能会受到影响。

2. 基于 **DB** 实现：

   - 1）添加定时任务，把每个画像消息作为一条记录，带上开始时间戳、循环条件丢到表里。
   - 2）取定时任务，由应用去查找该表最新的一条记录，通过乐观锁的方式，更新版本号，更新成功的则代表获取到此行的分布式锁，然后去执行此行的任务，执行完毕后删除任务，根据循环条件来决定是否需要将其重新加入表中。

   => 优点是，实现简单，没有多余的组件，缺点是，对单表压力大，相当于基于数据库实现的每个任务作为一个分布式锁。

3. 基于开源组件实现：EsJob、Xxl-job 等。

##### 3、总

以上就是我对除 Kafka 消息方案以外的一些实现分片广播式定时的方案，请指教一下方案是否合理？

#### 2.1.1.3. 目前生产上的机器规模？

##### 1、GSRMC

- **微服务**：基本每个虚机部署 2~3 个模块，平常 CPU 10%，内存 60%，所以一共注册了 126 个服务。
- **数据库**：监控显示，32 C、128 G、3 T，平均响应时间 15ms，平均 30% CPU 利用率，内存利用率 75%。

| 模块           | 数量              | CPU           | 内存               | 硬盘  |
| -------------- | ----------------- | ------------- | ------------------ | ----- |
| 前端 Apache    | 8                 | 4 C           | 16 G               | 50 G  |
| Eureka         | 3                 | 1 C           | 1 G                | 10 G  |
| BASE           | 13                | 8 C           | 16 G               | 100 G |
| POS            | 19                | 8 C           | 16 G               | 100 G |
| PERF           | 7                 | 8 C           | 16 G               | 100 G |
| QUO            | 6                 | 8 C           | 16 G               | 100 G |
| BID            | 3                 | 8 C           | 16 G               | 100 G |
| PRICE          | 13                | 8 C           | 16 G               | 100 G |
| CON            | 6                 | 8 C           | 16 G               | 100 G |
| ESB            | 5                 | 8 C           | 16 G               | 100 G |
| JOBS           | 9                 | 8 C           | 16 G               | 100 G |
| TASK           | 5                 | 8 C           | 16 G               | 100 G |
| MIP            | 4                 | 8 C           | 16 G               | 100 G |
| SYNC           | 5                 | 8 C           | 16 G               | 100 G |
| INDEX          | 3                 | 8 C           | 16 G               | 100 G |
| 其他模块       | 25                | 8 C           | 16 G               | 100 G |
| MySQL          | 16（8 主 8 从）   | 32 C          | 128 G              | 3 T   |
| MongoDB        | 3                 | 8 C           | 16 G               | 500 G |
| Redis          | 3                 | 8 C           | 32 G               | 100 G |
| Kafka          | 3                 | 8 C           | 16 G               | 100 G |
| 报表导出 OSS   | 文件数： 500 / 天 | 数据量：10 G  | 增长量：300 M / 天 | -     |
| 供应商附件 OSS | 文件数：10 w / 年 | 数据量：200 G | 增长量：200 G / 年 | -     |

##### 2、云化项目

- **微服务**：基本每个虚机部署 2~3 个模块，平常 CPU 10%，内存 60%，所以一共注册了 14 个服务。
- **数据库**：监控显示，16 C、64 G、3 T，平均响应时间 15ms，平均 30% CPU 利用率，内存利用率 75%。

| 模块             | 数量             | CPU          | 内存              | 硬盘  |
| ---------------- | ---------------- | ------------ | ----------------- | ----- |
| 前端 Nginx       | 3                | 4 C          | 16 G              | 50 G  |
| Eureka           | 3                | 1 C          | 1 G               | 10 G  |
| BASE             | 3                | 8 C          | 16 G              | 100 G |
| APP              | 3                | 8 C          | 16 G              | 100 G |
| OEM              | 3                | 8 C          | 16 G              | 100 G |
| JOBS             | 2                | 8 C          | 16 G              | 100 G |
| MySQL            | 1 主             | 16 C         | 64 G              | 3 T   |
| Redis            | 6                | 8 C          | 16 G              | 100 G |
| 文件存储 FASTDFS | 文件数：1 w / 年 | 数据量：20 G | 增长量：20 G / 年 | -     |

##### 3、比亚迪 SRM

- **微服务**：基本每个虚机部署 2~3 个模块，平常 CPU 10%，内存 60%，所以一共注册了 21 个服务。
- **数据库**：监控显示，32 C、128 G、3 T，平均响应时间 15ms，平均 30% CPU 利用率，内存利用率 75%。

| 模块             | 数量             | CPU          | 内存              | 硬盘  |
| ---------------- | ---------------- | ------------ | ----------------- | ----- |
| 前端 Nginx       | 2                | 4 C          | 16 G              | 50 G  |
| APP              | 6                | 8 C          | 16 G              | 100 G |
| MID              | 7                | 8 C          | 16 G              | 100 G |
| RPORT            | 6                | 8 C          | 16 G              | 100 G |
| Oracle           | 1                | 32 C         | 128 G             | 3 T   |
| Redis            | 3                | 8 C          | 16 G              | 100 G |
| 文件存储 FASTDFS | 文件数：1 w / 年 | 数据量：20 G | 增长量：20 G / 年 | -     |

#### 2.1.1.4. 生产慢问题修复的例子？

见项目自我介绍里的，线程池调优，以及 CPU 95% 的例子。

##### 1、线程池调优 | 线程池、源码、调优

1. 我们系统某个模块中常常需要使用别的模块的数据源，比如在 POS 模块中需要去查询 PRICE 库，此时，由于主线程默认绑定的是 POS 数据源，要想使用 PRICE 数据源的话，就需要开辟子线程去做切换，所以就为每个模块抽象了数据源切换的工具类。
2. 由于正常都是调用一次切换方法，也就是把 Lambda 表达式提交给线程池，主线程再去做 `FutureTask.get()` 的监听，拿到结果后返回，这是正常情况，因此我们一开始也没怎么留意。
3. 但问题就来了，有一次某段代码对连续进行了两次的切换，就发现接口超时，以及往后的切换都超时了，那时排查了很久，最后在这个切换工具类里看到，生产上跑的参数为 `corePoolSize=0、maximumPoolSize=32、workQueue=LinkedBlockingQueue`，按照原理，本以为是错误的配置，会导致任务都阻塞在队列中直到 OOM 发生的，但改成一次切换又可以了，这就很奇怪了，然后就去研究源码，发现刚才上面所说的那段**补充非核心线程**的逻辑，也就是即使 `corePoolSize=0`，只要最大线程数不为 0，那么还是会保证至少创建一个线程去消费任务队列的，所以切换一次就可以拿到任务的运行结果。
4. 那么为什么切换两次就不行了呢？这是因为，在第一次切换时，是主线程在做 `FutureTask.get()`，阻塞的是主线程，而如果还做一次连续切换，那么就会导致线程池中仅有的 1 个线程也做了 `FutureTask.get()`，从而进入了阻塞，但线程池认为有效线程数还是 0，所以新任务进来也不会去创建新线程消费，会直到 OOM 的发生为止~
5. 最后，更正了合理的构造参数 `corePoolSize=Runtime.getRuntime().availableProcessors() + 1`，`maximumPoolSize=2 * Runtime.getRuntime().availableProcessors()`，`keepAliveTime=120s`，`LinkedBlockingQueue=1024` 保证不会发生内存溢出，`Google#ThreadFactoryBuilder`来设构建线程工厂、在线程创建时设定名称前缀和递增线程号，`RejectedExecutionHandler=CallerRunsPolicy` 保证任务不丢失，同时设置 `allowCoreThreadTimeOut=true` 允许核心线程空闲时也会被回收。

##### 2、发版平台 CPU 占用过高 | Tomcat Manager、ELK、CPU 95%

1. **背景 Situation**：我们团队这次的迭代，需要在云平台开发几个接口，然后在和 SRM 联调测试时，为了不用等比亚迪 SAP 测试环境的发版窗口，就需要有我们自己的一个云平台测试环境，方便快速发版快速验证。

2. **任务 Task**：那时由我负责来负责搭建，但由于是在内网环境进行开发，访问不了外网，就拉取不到 Jenkins 的依赖，当然也没有云原生这些基础设施，所以采取了 Tomcat Manager + ELK 部署的方式来发版，前者是 Tomcat 的后台，可以开发完代码后导出 war 包，然后在浏览器上把 war 包进行上传部署，后者是用于在 Kibana 上观察接口的异常日志。

3. **行动 Action**：

   1. 就这样，一个简单易用的发版平台就搭建好了，不过没几天问题就来了，我们发现每一次上传 war 包到启动完成，所花的时间好像都会比上一次的要久，最后一次 war 包上传则是浏览器一直在转了。

   2. 当时为了不阻塞测试进度，就到发版平台的服务器上看，先是使用 `top` 命令看到 118243 的 Tomcat 进程，居然占用了 95.3% 的 `%CPU`：

      ![1646305025603](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646305025603.png)

   3. 然后使用 `top -Hp 118243 ` ，查看 Tomcat 进程的所有线程，发现最高的一个线程 `118332` 占用了 73.3% `%CPU`：

      ![1646305182797](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646305182797.png)

   4. 然后使用 `printf %x 118332`，获取对应的 16 进制线程号，用于后面的 jstack 文件搜索：

      ![1646305395917](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646305395917.png)

   5. 然后就是 `jstack -l 118243 > thread-dump.txt `，导出对应 `118243` 进程的线程快照，打开搜索对应的 `1ef96` 16 进制线程号（这个是当时真实的线程号，上面是后期模拟的） ，就定位到了一堆 `ParallelGC` 线程，原来是 GC 太多而导致的 CPU 飚高，那么为什么一个发版的 Tomcat，会有这么多 GC 线程呢？会不会是项目代码哪里写的有问题啊？

      ![1646305627751](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646305627751.png)

   6. 然后我就使用 `jmap -dump:format=b,file=heap-dump.hproft 118243`，以 `hprof` 格式导出 `118243`  Tomcat 进程的堆内存快照，再导入到 MAT -> Leak Suspects 做内存泄露分析，然后就得到了怀疑报告：说，有 18 个 `ParallelWebappClassLoader`  共占据了 44.61% 堆空间，咦？为什么要 18 个这种类加载器呢？

      ![1646306496272](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646306496272.png)

      ![1646306148994](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646306148994.png)

   7. 然后我就想起 Tomcat 的类加载过程，是打破了双亲委派机制的：

      1. **ext -> bootstarp模型**：保证了 JRE 核心类库不会被重复加载，满足了加载JVM共同类库的需求。
      2. **ext -> webapp模型**：实现了每个 web 应用只加载自己的类库（WEB-INF/classes -> WEB-INF/lib），从而实现了应用间的类库隔离的需求。
      3. **webapp -> share -> common模型**：实现了所有 web 应用之间、web 与 Tomcat 之间，能够加载相同的类库，避免指定的类库不会被重复加载，满足了加载其他共同类库的需求。

      ![1626090700749](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1626090700749.png)

   8. 也就是一个 `ParallelWebappClassLoader` 对应一个 Web App 项目，但用来发版的 Tomcat 应该就只有一个项目呀，所以我就怀疑应该是每次发版没把上一版的项目卸载掉所导致的。

4. **结果 Result**：所以我就让组内的成员，大家在下次发版前，先用 Tomcat Manager 把上一个项目卸载掉，再发新的 war 包上去，这样做了后，就没碰到过这样的情况出现了，应该算是蒙对了吧~哈哈

#### 2.1.1.5. MySQL MVCC？

##### 1、总

MVCC，Multi-Version Concurrency Control，多版本并发控制，可以实现数据库读写冲突时的无锁并发访问，可以做到在读时不阻塞写，写时不阻塞读，提高了数据库并发的读写性能，同时还可以解决 MySQL 不可重复读、幻读等事务并发问题。

##### 2、分

1. 首先，需要讲一下 MySQL 中的当前读和快照读：

   - 1）**当前读**：非 MVCC 实现，读取的是记录的最新版本，读取时，要保证其他并发事务不能修改当前记录，因此会对读取的记录进行加锁。
     - 举例：select lock in share mode（共享锁）, select for update（排他锁），update、insert、delete（排他锁）、串行化事务隔离级别等。
   - 2）**快照读**：MySQL 实现 MVCC 模型的中一个具体的非阻塞读功能，可以避免读时的加锁操作，从而降低开销、提高并发性能，但快照读可能读到的不一定是数据的最新版本，而是之前的历史版本，但前提是，事务使用非串行化的隔离级别，否则串行化级别下的快照读会退化成当前读。
     - 举例：不加锁的 select。

2. 然后就是实现原理，MySQL 是由 undo log + 版本链 + Read View 来实现 MVCC 的：

3. 第一个是 **undo log**，回滚日志，属于逻辑日志，记录的是 sql 执行相关的信息，是 MySQL 中事务原子性和隔离性实现的基础，其实现原理是：

   - 1）在 MySQL InnoDB 中，当事务对数据库进行修改时，InnoDB 会生成对应的 undo log。
   - 2）如果事务执行失败或调用了 rollback，导致事务需要回滚时，就可以根据 undo log 的内容做与之前相反的工作，把数据回滚到修改之前的样子，以实现事务原子性操作。
   - 3）对于每个 insert，回滚时会执行 delete。对于每个 delete，回滚时会执行 insert。对于每个 update，回滚时会执行一个相反的 update，把数据改回去。
   - 4）而 MVCC 则是采用 undo log 来记录旧版本，链首为最新的旧记录，链尾为最早的旧记录。

4. 第二个是**版本链**，在 InnoDB 中，每次修改版本都会在版本链中记录，通过 undo log + trix_id + roll_pointer 来实现，undo log 原理如上所说，

   ![1630673126935](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630673126935.png)

   - 1）**trx_id**：当前版本的事务ID，用来存储每次对某条记录进行修改时的事务ID，其中事务 ID 则是，当每个事务开启时，都会被分配一个ID，而这个ID是递增的，因此越新的事务其ID值越大。
   - 2）**roll_pointer**：回滚指针，由于每次对记录修改时，都会把老版本写入到 undo 日志中，使用回滚指针 roll_pointer 来指向这条记录上一个版本的位置，通过它来获得上一个版本的记录信息，其中，插入操作的 undo 日志是没有 roll_pointer 的，因为它没有老版本。

5. 第三个是 Read View，它是事务进行快照读操作时产生的读视图，是数据库当前的一个快照，记录了系统当前活跃事务ID 集合（即还没有提交的事务 ID），用来做可见性判断，即当某个事务执行快照读时，会对该记录创建一个Read View 读视图， 并把它作为条件，来判断当前事务能够看到哪个版本的数据，其中可能是当前版本的数据，也有可能是该行记录 undo log 里面某个版本的老数据，具体原理如下：

   ![1630674963257](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1630674963257.png)

   1. **trx_id == creator_trx_id**：可以访问这个版本，这个版本的事务 ID 等于当前创建 Read View的事务 ID，即自己能够读到自己的版本。
   2. **trx_id < min_trx_id**：可以访问这个版本，这个版本的事务 ID 小于最小活跃事务 ID，说明这个版本已经被提交过了，对于当前做可见性判断的事务来说，是可以看见的。
   3. **trx_id > max_trx_id**：不可以访问这个版本，这个版本的事务 ID 大于下一个事务 ID，说明这个版本记录是在该 Read View 生成之后产生的，已经超出了版本链范围，而快照读只能读取版本链中的版本，因此该版本对于当前做可见性判断的事务来说，是不应该看见的。
   4. **min_trx_id <= trx_id <= max_trx_id**：
      - 如果这个版本的事务 ID 为 m_ids 中的某个值，则不可以访问这个版本的，因为m_ids 都是活跃的、还没提交的事务，说明该版本记录还没有提交，对于当前做可见性判断的事务来说，是不应该看见的。
      - 如果这个版本的事务 ID 不为 m_ids 中的某个值，则可以访问这个版本，因为没在 m_ids 里，又小于等于 max_trx_id，说明该版本记录已提交了，对于当前做可见性判断的事务来说，是可以看见的。

   => 因此，可见性判断总的来说就是，要当前事务能看到的版本应该是自己创建的或者已经提交了的，这也是实现 MVCC 的原理所在。

##### 3、总

以上，就是我对 MySQL MVCC 的一个理解，请问有什么细节需要补充的吗？

#### 2.1.1.6. Dubbo 配置的注意点有哪些？

##### 1、Provider 多配置 Consumer 参数

1. 作为服务提供⽅，自己⽐消费⽅更清楚服务的这些接口参数的取值，比如超时时间、重试次数、负载均衡策略等。
2. 由于配置覆盖策略存在，在 Provider 端配置后，Consumer 端不配置则会使⽤ Provider 端配置，即 Provider 配置可以作为 Consumer 的缺省值。
3. 而如果 Provider 不配置，Consumer 也不直接配置，则会使⽤ Consumer 端的全局设置，这对于 Provider 是不可控的，并且往往是不合理的。
4. 因此，Provider 端尽量多配置、完善这些接口参数，让 Provider 实现者⼀开始就思考 Provider 端的服
   务特点和服务质量等问题。

![1636882183556](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1636882183556.png)

##### 2、Provider 合理配置性能参数

比如，**threads**（服务线程池⼤⼩）、**executes**（服务提供者并发请求的上限）。

##### 3、服务使用固定端口

使⽤固定端⼝来暴露服务，不要使⽤随机端⼝，这样在注册中⼼推送延迟的情况下，消费端仍然能够通过缓存列表，调⽤到原地址+原端口的服务，保证调⽤成功。

##### 4、推荐使用 XML 进行配置

XML 配置优先级高于 properties 和 yml，且标签的配置方式更加容易阅读和理解。

##### 5、应用配置负责人参数

配置 `dubbo:application#owner` 负责人参数，这些可以在运维平台上看到，以便于在发现问题时，找到对应服务负责⼈。

#### 2.1.1.7. Dubbo 提供者、消费者服务启动原理？

##### 1、总

1. 首先是 Dubbo 与 Spring 的融合原理，基于 dubbo.jar 内的 `META-INF/spring.handlers` 配置，Spring 在遇到 Dubbo XML 配置的名称空间 `http\://dubbo.apache.org/schema/dubbo` 时，会回调 `DubboNamespaceHandler` 。
2. 而 `http\://dubbo.apache.org/schema/dubbo/dubbo.xsd` 则定义了 Dubbo XML 的标签语法，所有 dubbo 的标签，都统一用 `DubboNamespaceHandler#DubboBeanDefinitionParser` 进行解析，基于一对一属性映射，将 XML 标签解析为 Bean 对象。
3. Provider 在 `ApplicationListener<ContextRefreshedEvent>#onApplicationEvent` 回调时，或者 Consumer 在接口注入 `FactoryBean#getObject` 时，将 Bean 对象转换 URL 格式，所有 Bean 属性转成 Dubbo#URL，然后将 Dubbo#URL 传给 Protocol 扩展点，基于扩展点的 SPI 机制，根据 Dubbo#URL 的协议头，进行不同协议的服务暴露或者引用。

##### 2、分

###### 1）Provider 服务发布原理

1. `ServiceConfig` 在 XML 解析后，拿到对外提供实现类 `XML#ref` 配置。
2. 然后，Provider 在 `ApplicationListener<ContextRefreshedEvent>#onApplicationEvent` 回调时，通过 `ProxyFactory#getInvoker（）` 为 `XML#ref` ，生成一个 interfaceClass 的 javasist 动态代理 Wrapper 包装类 Invoker 实例，可以动态代理调用 `XML#ref` 实现类的方法，到这一步就完成了具体实现类到 `Invoker` 的转化。
3. 接下来，就是 `Invoker` 转换到 `Exporter` 的过程，是服务暴露的关键过程，其转换分为两种类型：
   - 1）**暴露本地服务**：
     1. 指服务暴露和引用都在同一个 JVM 里，自己调用自己接口，没必要进行远程通信。
     2. 会调用 `InjvmProtocol#export（）` 把 invoker 转换为 `InjvmExporter`，并存进 exporters 缓存中。
     3. 调用则是获取 exporters 缓存中的 `InjvmExporter`，进行一个普通的动态代理到实现类的方法。
   - 2）**暴露远程服务**：
     1. 指服务暴露给远程客户端IP和端口号，以实现远程通信。
     2. 会调用 `DubboProtocol#export（）`  把 invoker 转换为 `DubboExporter`，然后打开 Netty 服务器暴露服务，并存进 exporters 缓存中。
     3. 然后调用 `RegistryProtocol#register（）`，使用 Curator 客户端建立 ZK 连接，注册 provider 持久化结点、当前服务的非持久化结点、configurators 非持久化结点，并设置监听器，当配置发生变更时，则会回调监听器的 `notify（）` 方法，来修改 invoker 信息。
     4. 调用则是通过 `Netty#channelRead()` 方法，当有数据请求时，则调用 handler 和线程池进行接收、处理，最终获取缓存中的 `DubboExporter`，动态代理到实现类的方法。

![1637067703250](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1637067703250.png)

###### 2）Consumer 服务引用原理

1.  Consumer 在接口注入 `FactoryBean#getObject` 时，调用 `Protocol#refer（）`生成 `Invoker` 实例，是服务消费的关键，其分类 3 种类型：
   - **本地服务引用**：如果为本地暴露服务，则 `InjvmProtocol` 生成本地执行的 `InjvmInvoker` 。
   - **直连服务引用**：如果为直连服务，则 `DubboProtocol` 创建 Netty 客户端，连接 url 服务，构建 `DubboInvoker` 。
   - **远程服务引用**：
     1. 如果为非直连的远程服务引用，则会调用 `RegistryProtocol` 进行 Consumer 注册到 ZK 并订阅，拉取相关 url 和 配置信息，当有发生变更时，则触发监听器的回调函数，调用 `DubboProtocol#refer()` 生成 `DubboInvoker` ，并加入集群，最后默认伪装返回一个 `FailoverClusterInvoker`。
2. 最后利用动态代理，把 `Invoker` 转换为客户端需要的接口实现类。
3. 当应用对代理对象进行方法调用时，则是动态代理到对应 `Invoker#invoke` 方法，调用本地方法或者发起远程请求。

![1637149757299](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1637149757299.png)

###### 3）Consumer 远程调用 Provider 原理

1. 消费方使用的接口动态代理实现类 proxy，调用其对应的 `Invoker`，发起真正的远程服务调用。

2. 而提供方的实现类，则会被封装成为一个 `AbstractProxyInvoker` ，然后生成一个 `Exporter` ，当服务提供方收到一个请求后，则会找到对应的 `Exporter` 实例，并调用它所对应的 `AbstractProxyInvoker` 实例，从而真正调用了服务提供者的代码。

3. 整个**消费方调用提供方**的调用链架构如下：

   ![1637150417647](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1637150417647.png)

   1. **服务引用**：通过 Javassist 反向代理，代理调用 InvokerInvocationHandler#invoke 方法。
   2. **服务本地调用、降级、缓存**。
   3. **集群容错与负载均衡**：非 mock Invoker 筛选，Invoker 目录查找，根据容错策略、负载均衡策略，挑选唯一的 Invoker。
   4. **服务过滤链、监听器、包装类 SPI 扩展**。
   5. **服务协议**：根据协议，使用不同的 Invoker 调用不同的网络传输底层。
   6. **网络传输**：抽象 Netty、Mina 等统一接口，把消息序列化后发送到网络，传输给 Server 端。
   7. **消息接收与异步处理**：Server 端接收到消息后，经过反序列化后，交由线程池异步处理。
   8. **服务协议**：根据协议，选择不同的 Exporter 进行调用。
   9. **服务过滤链、监听器、包装类 SPI 扩展**。
   10. **服务调用**：最后调用真正的接口实现类，得到方法执行结果。

4. 而**提供方响应消费方**的顺序为：

   1. **服务响应**：在得到方法执行结果后，通过网络传输底层，序列化后发送响应报文给客户端。
   2. **结果接收**：客户端接收到响应报文后，交由线程池异步处理程序，经过反序列化后塞回到 Future 对象中，完成一次服务调用。

##### 3、总

以上，就是我对 Dubbo Provider 和 Consumer 启动过程以及远程调用过程的一个理解，请问有什么细节需要补充的吗？

#### 2.1.1.8. SpringBoot 的启动原理？

1. 当启动 SpringBoot 应用时，会先创建 SpringApplication 的对象，在对象的构造方法中，会进行某些参数的初始化工作，最主要的是判断当前应用的类型（比如 Servlet 类型），以及通过 SPI 的方式加载整个应用的 `spring.factories` 文件中的初始化器和监听器的 Class。
2. SpringApplication 对象创建完成之后，开始执行 `run()` 方法，来完成整个启动，启动过程中最主要的有两个方法，第一个叫做 `prepareContext()`，第二个叫做 `refreshContext()`，在这两个步骤中完成了自动装配的核心功能，而其他方法的处理逻辑包含了上下文对象的创建、Banner 的打印、异常报告期的准备等各个准备工作，方便后续来进行调用。
3. 在 `prepareContext()` 方法中，主要完成了对上下文对象的初始化工作，包括比如 `Enviroment`对象属性值的设置，在整个过程中，有一个非常重要的方法 `load()`，`load()` 主要完成一件事，那就是将**启动类**作为BeanDefinition 注册到 Registry 中，方便后续在进行 BeanFactoryPostProcessor 调用执行时，找到对应的启动类来完成 `@SpringBootApplication` 和 `@EnableAutoConfiguration` 等注解的解析工作。
4. 在 `refreshContext()` 方法中，会进行整个容器的刷新过程，会调用 Spring 中的启动流程，即`AbstractApplicationContext#refresh()`，有 13 个关键方法，来完成整个 Spring 应用的启动，其中会调用 `invokeBeanFactoryPostProcessor()` 方法，主要是对`ConfigurationClassPostProcessor` 的处理，会先调用实现 `BeanDefinitionRegistryPostProcessor` 接口的 `postProcessBeanDefinitionRegistry()` 方法，然后再调用自己实现的 `postProcessBeanFactory()` 方法，处理各种包括 @PropertySource、@ComponentScan、@Import、@ImportResource、@Bean 等注解。
5. 其中，在解析 @Import 注解时，会有一个 `getImports()` 的方法，会从**启动类**开始递归解析注解，把所有包含 @Import 注解都收集到，然后在 `processImport()` 方法中，对 Import 导入的类进行分类，这里主要起识别作用的是 `ImportSelect` 的实现类  `AutoConfigurationImportSelect`，来调用 `selectImports()` 方法使用 SPI 的方式，来获取并加载 `spring.factories` 中的 `EnableAutoConfiguration` 自动装配配置类的 Class，完成自动装配。
6. 接着，还调用子类上下文 `ServletWebServerApplicationContext#onRefresh` 方法，来拉起嵌入式的 Tomcat 容器。
7. 最后，就是实例化 Bean，即 `finishBeanFactoryInitialization()` 方法，主要是实例化所有剩下非懒加载的单例 Bean，并完成它们的初始化和依赖注入，通过遍历所有 beanNames，然后挨个判断是走 FactoryBean 的流程，还是走 BeanFactory 的流程，其中主要步骤总结起来分为 3 步，分别是 NewInstance 实例化、Populate 属性赋值 和 Initialization 初始化，从而完成自动装配配置 Bean 的注入。

#### 2.1.1.9. Spring MVC 中过滤器和拦截器的区别？

##### 1、总

拦截器 HandlerInterceptor，过滤器 Filter，虽然可以对请求进行一定处理，但：

1. **实现人不同**：HandlerInterceptor 属于 Spring#Web 包下，Filter 属于 Tomcat#javax 包下。
2. **配置方式不同**：HandlerInterceptor 在 Spring#WebMvcConfigurer 中配置，而 Filter 在 web.xml 中配置。
3. **处理时机不同**：HandlerInterceptor 作用更强大，方法处理前的拦截、处理后的拦截、返回前的拦截，而 Filter 则是在进入DispatcherServlet 前进行一定的请求过滤处理。

##### 2、分

![1645947452956](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1645947452956.png)

Spring MVC 客户端请求的生命周期管理：

1. **Filter#doFilter** 执行链过滤客户端请求。
2. DispatcherServlet，表示中心控制器，在客户端发出请求后，经过 Web 容器（比如 Tomcat）后，会打到 DispatcherServlet 上，并由其来处理请求。
3. HandlerMapping，表示处理器映射，DispatcherServlet 收到请求后，会调用 HandlerMapping，HandlerMapping 会根据请求 url 去查找对应的 Handler，即一个 Handler Method 对象，指的是 url 对应 Controller 中的对应方法。
4. HandlerExecutionChain，表示处理器执行链，Handler 解析完 url  后，会返回一个处理器执行链给DispatcherServlet，其中就包括一堆 HandlerInterceptor。
5. HandlerAdapter，表示处理器适配器，DispatcherServlet 会按照规则去匹配对应的 HandlerAdapter。
6. 再由对应的 Handler Method 去处理请求，其中就包括调用去执行我们编写的 Controller 业务逻辑。
   - 但在执行前，会先调用 **HandlerInterceptor#preHandle** 进行  handler 方法处理前的拦截。
7. Controller 代理对象，则会去访问数据库，填充好模型后，再把 ModelAndView 返回给 HandlerAdapter。
   - 其中，在 HandlerAdpater#resolveArgments 时，HttpMessageConverter读取完 body 前/后，可以对 **RequestBodyAdvice#beforeBodyRead/afterBodyRead** 进行回调，从而改写 body。
   - 而我们平常使用的 @ResponseBoday 的方法，返回的 ModelAndView 为空，也就是不会返回视图给客户端，而是经过 `RequestResponseBodyMethodProcessor`，把 JSON 串写入到 Response 的 Body 中。
8. HandlerAdapter 收到后，再将 ModelAndView  传递给 DispatcherServlet。
9. DispatcherServlet 收到后，则调用视图解析器 ViewResolver，来解析 ModelAndView。
10. ViewResolver 会解析逻辑视图名，根据逻辑的 View 找到实例的 View，并返回给 DispatcherServlet。
11. DispatcherServlet 收到后，则根据 ViewResolver 解析出的 View，调用对应的实际视图，结合 Model 进行渲染。
    - 但在执行前，会先调用 **HandlerInterceptor#postHandle** 进行 handler 方法处理后的拦截。
12. 最后，DispatcherServlet 再将渲染后的 View 作为结果，响应给客户端。
    - 在执行后，返回结果前，会调用 **HandlerInterceptor#afterCompletion** 进行响应前的拦截。

##### 3、总

以上，就是我对 Spring MVC 拦截器和过滤器的一个理解，请问有什么细节需要补充的吗？

#### 2.1.2.0. 算法题 | 实际问题中树的深度优先遍历

思路就是，把对应的数据结构，转换为树结点的数据结构，然后进行常规的深度优先遍历即可，比如二叉树的**先序遍历**：

##### 1、递归法 | O（n）

- **思路**：递归遍历，中 -> 左 -> 右，因此在一开始来到中时，就把节点的值加入集合中即可。
- **结论**：时间，0ms，100%，36.4mb，92.56%，非常简单。

```java
/**
 * Definition for a binary tree node.
 * public class TreeNode {
 *     int val;
 *     TreeNode left;
 *     TreeNode right;
 *     TreeNode() {}
 *     TreeNode(int val) { this.val = val; }
 *     TreeNode(int val, TreeNode left, TreeNode right) {
 *         this.val = val;
 *         this.left = left;
 *         this.right = right;
 *     }
 * }
 */
class Solution {
    public List<Integer> preorderTraversal(TreeNode root) {
        List<Integer> res = new LinkedList<>();
        f(root, res);
        return res;
    }
    
    private void f(TreeNode root, List<Integer> res) {
        if(root == null) {
            return;
        }

        res.add(root.val);
        f(root.left, res);
        f(root.right, res);
    }
}
```

##### 2、迭代法 | O（n）

- **思路**：迭代法中，先序遍历实现最简单，因为当前使用的 while 循环可以认为是“中”，然后模拟系统栈把右、左一次压栈即可（可以认为是方法倒过来压栈）。
- **结论**：时间，0ms，100%，36.4mb，93.30%，注意添加和使用前都做个判空，防止空指针和节省空间的使用。

```java
/**
 * Definition for a binary tree node.
 * public class TreeNode {
 *     int val;
 *     TreeNode left;
 *     TreeNode right;
 *     TreeNode() {}
 *     TreeNode(int val) { this.val = val; }
 *     TreeNode(int val, TreeNode left, TreeNode right) {
 *         this.val = val;
 *         this.left = left;
 *         this.right = right;
 *     }
 * }
 */
class Solution {
    public List<Integer> preorderTraversal(TreeNode root) {
        LinkedList<TreeNode> stack = new LinkedList<>();
        if(root != null) {
            stack.push(root);
        }
        
        List<Integer> res = new LinkedList<>();
        while(!stack.isEmpty()) {
            root = stack.pop();
            if(root != null) {
                res.add(root.val);
                if(root.right != null) {
                    stack.push(root.right);
                }
                if(root.left != null) {
                    stack.push(root.left);
                }
            }
        }

        return res;
    }
}
```

#### 2.1.2.1. 算法题 | Linux#uniq 指令、Java BufferedReader#ready 和 readLine

这道题的难点在于，Linux#uniq 指令的作用，和 Java BufferedReader 的 API。

##### 1、Linux#uniq 文档编辑命令

Linux#uniq，输入一个文件，检查文本文件中重复出现的行列，并输出到另一个文件，-c 代表在每列旁边显示该行重复出现的次数，-d 代表仅显示重复出现的行列，-u 代表仅显示只出现一次的行列。

##### 2、Java BufferedReader API

BufferedReader 可以从字符输入流中读取文本，并缓冲字符，以便有效地读取字符，使用时可以通过构造函数指定缓冲区大小或者也可以只使用默认大小 8192，同时建议使用 BufferedReader 包装 Reader 实例，以提高效率。

```java
private static void printLinuxCmd(File file) throws IOException {
    // 使用 BufferedReader 包装 Reader 实例
    BufferedReader reader = new BufferedReader(new FileReader(file));
    Set<String> res = new HashSet<>();

    // 允许读取：ready()
    String str;
    while (reader.ready()) {
        // 按行读取：readLine()
        str = reader.readLine();
        if(str.contains("abc")) {
            res.add(str);
        }
    }
    
    // 排序后去重
    List<String> reslist = res.stream().sorted(Comparator.comparingInt(Object::hashCode)).collect(Collectors.toList());
    System.err.println(reslist);
    
    // 底层是去关闭输入流
    reader.close();
}
```

#### 3.1.1.1. sychrozied 锁升级流程？

##### 1、总

sychrozied 锁状态一共有 4 种，级别由低到高分别为：无锁、偏向锁、轻量级锁和重量级锁。

1. 在 JDK 1.6 之前，sychrozied  锁是一个重量级锁，效率比较低下。
2. 在 JDK 1.6 之后，为了提高锁的获取和释放效率，就对 synchronized 的实现进行了优化，引入了偏向锁和轻量级锁。
3. 从此 synchronized 锁就有了以上 4 种状态，并且它们会随着竞争的情况逐渐升级，但不可降级。

对于 sychrozied  优化后的，锁执行过程总结如下：

![1629617805708](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629617805708.png)

![1629617826584](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629617826584.png)

1. **确认是否为可偏向状态**：线程抢锁时，JVM 首先检测内置锁对象 Mark Word 中的 biased_lock（偏向锁标识）是否设置成 1，lock（锁标志位）是否为 01，如果都满足，则确认内置锁对象为**可偏向状态**。
2. **确认线程 ID**：在内置锁对象确认为可偏向状态后， JVM 会检查 Mark Word 中的线程 ID 是否为抢锁线程的 ID。
3. **同线程 ID，直接执行临界区代码**：如果是，则表示抢锁线程处于偏向锁状态，然后抢锁线程快速获得锁，开始执行临界区代码。
4. **竞争锁成功，升级为偏向锁**：如果 Mark Word 中的线程 ID 并未指向抢锁线程，则通过 CAS 操作去竞争锁。
   - 如果竞争成功，则将 Mark Word 中的线程 ID 设置为抢锁线程，偏向标志位设置为 1，锁标志位设置为 01，然后执行临界区代码，此时内置锁对象为**偏向锁状态**。
5. **发生锁竞争失败，偏向锁升级为轻量级锁**：如果 CAS 操作竞争失败，说明发生了竞争，此时 JVM 会去检查原先持有锁的线程是否存活。
   - 如果原线程已经死亡，则设置 Mark Word 为抢到锁的线程 ID，内置锁继续保持为偏向锁。
   - 如果原线程没有死亡，且其堆栈中存在该 sychrozied 锁的所记录，则需要撤销偏向锁，进而升级为**轻量级锁**（继续由原持有锁线程持有）。
6. **其他线程继续 CAS 竞争，以获取轻量级锁**：如果仍然有其他线程继续竞争轻量级锁，则在 JVM 将在替换锁对象 Mark Word 中的 ptr_to_lock_record 过程中，使用 CAS 替换为抢锁线程的锁记录指针。
   - 如果替换成功，则代表抢锁线程获得了**轻量级锁**。
   - 如果替换失败，表明存在其他线程竞争锁，则 JVM 接着尝试使用 CAS + 自旋方式来替换。
     - 如果替换成功，代表抢锁成功，则锁对象继续保持为**轻量级锁状态**。
7. **CAS + 自旋失败，轻量级锁升级为重量级锁**：如果 CAS + 自旋替换失败，即超过了最大自旋次数，轻量级锁则升级为重量级锁，此后等待锁的线程都需要进入**阻塞状态**。

=> 总的来说：

1. **偏向锁**是在没有发生锁争用的情况下使用的。
2. 一旦有了第二个线程争用锁，偏向锁则会升级为**轻量级锁**。
3. 如果锁争用很激烈，轻量级锁的 CAS + 自旋到达自旋阈值后，轻量级锁则会升级为**重量级锁**。

##### 2、分

###### 1）无锁

![1629597556442](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629597556442.png)

无锁，即无线程竞争，在 java 对象刚创建时，还没有任何线程来竞争，对象处于无锁状态，此时，偏向标志位为0，锁状态标志位为 01。

###### 2）偏向锁

![1629597869808](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629597869808.png)

偏向锁，是指一段同步代码，一直被同一个线程所访问，偏向锁状态下的 Mark Word，会记录 synchronized 锁偏爱的线程 ID，从而让 synchronized 锁认定该线程为“熟人”，从而该线程可以自动获取锁，降低获取锁的代价。

- **主要解决无竞争下的锁性能问题**：由于线程 ID 被记录在锁对象的 Mark Word 中（CAS设置），以后该线程获取锁时，只需要判断一下线程 ID 和标志位，就可以直接进入同步块，连 CAS 操作都不需要，省去了大量有关锁申请的操作，消除了无锁竞争情况下的同步原语，从而提高了程序的性能。
- **偏向锁的膨胀**：
  1. 如果偏向锁已经被 A 占据，一旦有第二个线程 B 来争抢这个对象，由于偏向锁不会主动释放，所以 B 看到的 synchronized 锁是偏向状态，表明已经存在了竞争，则 JVM 会去检查原来持有该对象锁的占有线程A 是否依然存活。
  2. 如果发现 A 已经挂了，则将锁对象变为无锁状态，然后重新偏向 B 线程。
  3. 如果发现 A 依然存活，则会进一步检查 A 的调用堆栈是否有锁记录持有该偏向锁。
  4. 如果存在锁记录，表明原来的线程 A 仍然在使用该偏向锁，即 A 和 B 此时发生了锁竞争，则 JVM 会撤销原来的偏向锁，将偏向锁膨胀（INFLATING）为轻量级锁。
- **偏向锁的撤销**：如果锁对象经常被多个线程竞争，那么偏向锁就是多余的，并且撤销偏向锁的过程也会带来一些性能开销。
  1. 在一个安全点停止拥有锁的线程。
  2. 遍历线程的栈帧，检查是否存在锁记录，如果存在，则需要清空锁记录（锁记录是线程私有的，每个线程都有自己的一份锁记录），使其变为无锁状态，并修复锁记录指向的 Mark Word，清除其线程 ID。
  3. 将当前锁升级为轻量级锁。
  4. 唤醒当前线程。
- **偏向锁的撤销条件**：
  - 多个线程竞争偏向锁。
  - 调用偏向锁对象的 `hashCode（）`方法，或者 `System.identityHashCode（）`方法，计算对象的HashCode 之后，将哈希码放置到了 Mark Word 中，synchronized 锁变成无锁状态，偏向锁会被撤销。

=> 经验表明，大部分情况下，一个同步代码块的线程都是同一个线程，总体来说，使用偏向锁带来的好处大于偏向锁撤销和膨胀带来的代价。如果某些临界区存在两个，或者两个以上的线程竞争，偏向锁反而会降低性能，此时可以在启动 JVM 时，把偏向锁的默认功能关闭。

###### 3）轻量级锁

![1629598045639](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629598045639.png)

轻量级锁，也被成为非阻塞同步锁、或者叫乐观锁，是指当锁处于偏向锁，又被另一个线程企图抢占时，偏向锁就会升级为轻量级锁，而企图抢占的线程会通过自旋的形式尝试获取锁，不会阻塞抢锁线程，以提高性能，其中，哪个线程先占有锁对象，锁对象的 Mark Word 就会指向哪个线程栈帧中的锁记录。

- **引入轻量级锁的主要目的**：在多线程竞争不激烈的情况下，通过 CAS 机制竞争锁，减少重量级锁产生的性能损耗，尽可能不动用操作系统层面的互斥锁，在应用层面上通过自旋来解决线程同步的问题。

  - **自旋原理**：如果持有锁的线程能在很短时间内释放锁资源，竞争等待锁的线程则不需要进行内核态和用户态的切换进入阻塞状态，它们只需要 CPU 自旋，等待持有锁的线程释放锁后即可立即获取锁，避免了用户线程和内核切换的消耗，从而提高性能。
  - **自旋锁分类**：
    - **普通自旋锁**：指当有线程来竞争锁时，抢锁线程会在原地循环等待，而不是被阻塞，直到那个占有锁的线程释放锁之后，这个抢锁线程才可以获得锁。
      - JDK 1.6 的轻量级锁使用的是普通自旋锁，需要使用 `-XX：+UseSpinning` 选项手工开启。
      - 默认情况下，自旋次数为 10 次，可以通过 `-XX：PreBlockSpin` 选项来进行更改。
      - 然而，线程自旋需要消耗 CPU，如果一直获取不到锁，那么线程也不能一直占用 CPU 自旋做无用功，因此需要设定一个自旋等待的最大时间。
    - **适应性自旋锁**：
      - JVM 对于自旋周期的选择，JDK 1.7 引入了适应性自旋锁（自动开启），指的是自旋的时间不是固定式的，而是由前一次在同一个锁上的自旋时间，以及锁的拥有者状态来决定的，解决的是锁竞争时间不确定的问题，使得竞争程度趋于稳定。
      - **自旋成功了则下次自旋的次数就会更多**：如果抢锁线程在同一个锁对象上之前成功获得过锁（竞争力强，适合竞争），JVM 则会认为这次自旋很可能再次成功，此次允许自旋等待持续相对更长的时间。
      - **自旋失败了则下次自旋的次数就会减少**：如果对于某个锁，抢锁线程很少成功获得过（竞争力弱，不适合竞争），JVM 则减少自旋时间甚至省略自旋过程，以避免浪费 CPU 资源。

- **轻量级锁的抢占过程**：

  1. 在抢锁线程进入临界区之前，如果该 synchronized 锁没有被锁定，JVM 首先将在抢锁线程的栈帧中建立一个锁记录（Lock Record），用于存储对象目前 Mark Word 的拷贝。

     ![1629601733727](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629601733727.png)

  2. 然后，抢锁线程将使用 CAS 自旋操作，尝试将内置锁对象头的 Mark Word 的 ptr_to_lock_record（锁记录指针），更新为抢锁线程栈帧中锁记录的地址，如果这个更新执行成功了，这个线程就拥有了对象锁。

  3. 接着，JVM 将 Mark Word 中的 lock 标记位改为 00，代表该内置锁对象处于轻量级锁状态。

  4. 抢锁成功之后，JVM 会将 Mark Word 中原来的锁对象信息（如哈希码等），保存在抢锁线程锁记录中的Displaced Mark Word 字段中，再将抢锁线程中锁记录的 owner 指针指向锁对象。

     ![1629602512221](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629602512221.png)

- **轻量级锁的膨胀**：

  - 如果临界区代码执行耗时较长，在其执行期间，其他线程都在原地 CAS + 自旋等待替换ptr_to_lock_record，导致一直空耗 CPU，带来很大的性能损耗。
  - 而轻量级锁的本意，是为了减少多线程进入操作系统层面互斥锁的概率，并不是要替代其互斥锁。
  - 因此，在内置锁争用激烈的场景下，轻量级锁会膨胀为基于操作系统内核互斥锁实现的重量级锁。

###### 4）重量级锁

![1629599089661](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629599089661.png)

重量级锁，也称为同步锁，是指当锁处于轻量级锁，如果持有锁的线程执行的时间，超过了自旋等待的最大时间仍然没有释放锁，自旋等待锁的线程不会一直自旋下去，而是会进入阻塞状态，该锁也膨胀为重量级锁。

- 锁对象的 Mark Word 会再次发生变化，指向一个监视器对象，该监视器对象会用集合的形式来登记和管理排队的线程。

  - **监视器**：是一个同步工具，相当于一个许可证，拿到许可证的线程即可进入临界区进行操作，没有拿到的则需要阻塞等待。在 JVM 中，每个对象都关联一个监视器，监视器和对象一起创建、销毁，保证同一时间只有一个线程可以访问被保护的临界区代码。监视器也可以说是同步机制，主要特点是：
    - **同步**：监视器所保护的临界区代码互斥地执行，一个监视器是一个运行许可，任一线程进入临界区代码都需要获得这个许可，离开时会把许可归还。
    - **协作**：监视器提供 Signal 机制，允许正持有许可的线程，暂时放弃许可进入阻塞等待状态，等待其他线程发送 Signal 去唤醒；其他拥有许可的线程可以发送 Signal，唤醒正在阻塞等待的线程，让它可以重新获得许可并启动执行。

- 重量级锁通过监视器的方式，保障了任何时间，只允许一个线程通过受到监视器保护的临界区代码。在Hotspot 虚拟机中，监视器由 C++ 类 ObjectMonitor 实现：

  ![1629615372409](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1629615372409.png)

  - **_recursions**：线程的重入次数。
  - **_owner**：标识拥有该 Monitor 的线程，即获得锁的线程。
  - **cxq**：竞争队列，所有请求锁的线程首先会被放入这个竞争队列中。cxq 由 Node 及其 next 指针逻辑构成（单向链表），并不存在一个队列的数据结构，只是一个虚拟队列：
    1. 在线程进入 cxq 前，抢锁线程会先尝试通过 CAS 自旋获取锁，如果获取不到，则会进入 cxq 队列，显然抢锁操作这对于那些已经进入了 cxq 队列的线程是不公平的，因此 synchronized 同步块所使用的重量级锁是**非公平锁**。
    2. 每次新加入的 Node 会在 cxq 的队头进行，通过 CAS 改变第一个结点的指针为新增结点，同时设置新增结点的 next 指向后续结点。
    3. 从 cxq 取出元素时，会从队尾获取。由于只有 owner 线程才能从队尾取出元素，即线程出列操作无争用，因此 cxq 是无锁结构。
  - **_EntryList**： 候选竞争队列，由于 cxq 会被线程并发访问，为了降低对 cxq 队尾的争用，在 owner 线程释放锁时，JVM 会从 cxq 中迁移线程到 EntryList 中，并会指定 EntryList 中某个线程（一般为 Head）为OnDeck Thread（Ready Thread），因此 EntryList 中的线程是作为候选竞争线程而存在的。
    - **OnDeck Thread**：
      - JVM 不直接把锁传递给 Owner Thread，而是把锁竞争的权利交给 OnDeck Thread，On Deck 需要重新竞争锁，这种行为称为竞争切换，虽然牺牲了一些公平性，但极大提升了系统的吞吐量。
      - OnDeck Thread 获取到锁资源后将会变为 Owner Thread，无法获得锁的 OnDeck Thread 则会依然留在 EntryList 中。
      - 另外，在 OnDeck Thread 成为 Owner 的过程中，还要一个**不公平**的事情：后来的新抢锁线程可能会直接通过 CAS 自旋成为 Owner 而获得锁。
  - **_WaitSet**：等待队列，某个拥有 ObjectMonitor 的线程（owner 线程）在调用 Object.wait（）方法之后将被阻塞，然后该线程将被放置在 _WaitSet 链表中，直到某个时刻通过 Object.notify（）或者Object.notifyAll（）唤醒后，该线程才会重新进入 EntryList 中继续候选竞争锁。

- **重量级锁性能低**：重量级锁使用了操作系统底层的互斥锁（Mutex Lock），会导致线程在用户态和内核态之间的频繁切换，从而带来较大的性能损耗。

  - 处于 cxq、EntryList、WaitSet 中的线程都处于阻塞状态，线程的阻塞或者唤醒，都需要操作系统来帮忙。比如在 Linux 内核中采用 pthread_mutex_lock 系统调用（互斥锁）来实现，因此，进程需要从用户态切换到内核态。
  - 进程从用户态切换到内核态，这种切换需要消耗很多时间，有可能比用户执行代码的时间还要长，而由于轻量级锁使用 CAS 进行自旋抢锁，都处于用户态下，进程不存在用户态和内核态之间的切换，因此轻量级锁开销较小，而重量级锁使用了 Linux 内核态下的互斥锁，会造成较大的性能开销。

##### 3、总

适用场景总结：

| 锁       | 优点                                                         | 缺点                                             | 适用场景                                 |
| -------- | ------------------------------------------------------------ | ------------------------------------------------ | ---------------------------------------- |
| 偏向锁   | 加锁和解锁不需要额外的消耗，和执行非同步方法比，仅存在纳秒级的差距 | 如果线程间存在锁竞争，则会带来额外的锁撤销消耗   | 适用于只有一个线程访问临界区的场景       |
| 轻量级锁 | 竞争的线程不会阻塞，提高程序的响应速度                       | 抢不到锁竞争的线程会使用CAS 自旋等待，会消耗 CPU | 锁占用时间很短，追求响应时间，但吞吐量低 |
| 重量级锁 | 线程竞争不使用自旋，不会消耗 CPU                             | 使用系统互斥锁，线程阻塞，响应时间缓慢           | 吞吐量高，追求吞吐量，但锁占用时间较长   |

=> 以上，就是我对 sychrozied 的一个理解，请问有什么细节需要补充的吗？

#### 3.1.1.2. Java 线程同步方式？

##### 1、总

1. 线程同步，指当有一个线程对内存进行操作时，其他线程都不可以对这个内存进行操作，直到此线程完成操作后，其他线程才能对这个内存进行操作。
2. 在 Java 中，实现线程同步的基础是，操作系统的 mutex 互斥锁、volatile 保证内存可见性和 CAS 比较交换，从上层表现形式来看，可以划分为以下几种同步方式：

##### 2、分

1. **锁式**：指的是，互斥访问同步代码块，拿到互斥量的线程进行同步代码块执行，拿不到互斥量的线程则进入阻塞状态，比如 synchronized 内置锁、Lock 接口的实现类比如 ReentrantLock 重入锁、ReentrantReadWriteLock 读写锁。
2. **事件通知式**：指的是，通过事件通知来协调线程，安全地访问同一块内存，阻塞后的线程需要等待其他线程来唤醒，比如 Object#wait()、notify()、notifyAll()，LockSupport#park()、unpark()，Lock.Condition#await()、signal()。
3. **资源控制式**：指的是，通过控制资源数量，来决定线程是否能够访问临界区或者内存，拿到资源的则可以访问，拿不到的则进入阻塞或者排队等待，比如 Semaphore 信号量。
4. **排队等待式**：指的是，通过队列来实现线程的有序访问，比如 AQS CLH 自旋锁队列、BlockingQueue 阻塞队列。

##### 3、总

以上，就是我对 Java 线程同步方式的一个理解，请问有什么细节需要补充的吗？

#### 3.1.1.3. 你为什么值这么多？

##### 1、总

对于这个问题，我认为我个人对于技术的研究，其广度和深度都是十分符合当前条件。

##### 2、分

1. 首先是广度，
   - 1）开发使用方面，本人有全栈开发经验，对于前后端的实现角度，都有自己一些独特的理解，应用开发上自然也能驾驭得不错。
   - 2）应用架构方面，本人对主流开源框架基本所有了解、掌握，在架构设计时能够有足够多的灵感来源，基本能够覆盖较多的需求实现。
   - 3）系统架构方面，本人对于一些系统部署相关的，做了较多实验，也从中得到了不错的经验和心得，比如 LVS、Haproxy、Nginx、Redis、RabbitMQ、Kafka、ELK、Docker、K8s、Cloud Foundry、Mesos、Marathon，以及一些性能调优方面的等等。
2. 然后是深度，
   - 1）java 语言方面的原理，个人研究过集合和并发框架源码，能够在日常开发中保证需求完成的同时，还能保持代码的高质量。
   - 2）应用框架方面的原理，也是能研究都尽量研究了，比如 java 写的 Spring、Spring MVC、Spring Boot、Spring Cloud、Dubbo、MyBatis 源码等，对于非 java 写的比如 MySQL、Redis、RabbitMQ、Kafka 的一些常见原理也是有所掌握，能够在日常工作中解决掉突发的线上问题。

##### 3、总

综上，我认为这些原因都能很好的回答这个问题，希望能够得到您的认可。

#### 3.2.1.1. epoll 详解，以及与 select 的区别？

##### 1、总

1. `select`、`poll`、`epoll` 都是 Linux 为 I/O 多路复用模型提供的函数。
2. 其中，I/O多路复用，是指可以通过一种机制，让一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。

##### 2、分

###### 1）select

```c++
int select (int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);
```

1. `select` 函数监视的文件描述符分 3 类，分别是 `readfds`、`writefds` 和 `exceptfds`。
2. 调用后，`select` 函数会阻塞，直到有描述副就绪（比如有数据 可读、可写、或者有 except），或者超时（`timeout` 指定等待时间，如果立即返回设为 null 即可），则函数返回。
3. 当 `select` 函数返回后，可以通过遍历 `fdset` ，来找到就绪的描述符。

- **优点**：`select` 目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个优点。
- **缺点**：`select` 的一个缺点是，单个进程能够监视的文件描述符的数量存在最大限制，在 Linux 上一般为1024，可以通过修改宏定义，甚至重新编译内核的方式来提升这一限制，但是这样也会造成效率的降低。

###### 2）poll

```c++
int poll (struct pollfd *fds, unsigned int nfds, int timeout);

struct pollfd {
    int fd; /* file descriptor */
    short events; /* requested events to watch */
    short revents; /* returned events witnessed */
};
```

- **特点**：
  1. 不同与 `select` 使用三个位图来表示三个 `fdset` 的方式，`poll` 只使用一个 `pollfd` 的指针来实现。
  2. `pollfd` 结构包含了要监视的 event，和 发生的 event，不再使用 `select` “参数-值” 的传递方式。
  3. 同时，`pollfd` 并没有最大数量限制，但数量过大后性能也还是会下降。 
  4. 和 `select` 函数一样，`poll` 返回后，需要轮询 `pollfd` 来获取就绪的描述符。
- **缺点**：
  1. 从上面看，`select` 和 `poll` 都需要在返回后，通过遍历文件描述符来获取已经就绪的 socket。
  2. 事实上，同时连接的大量客户端，在一时刻可能只有很少的处于就绪状态，因此随着监视的描述符数量的增长，其效率也会线性下降。

###### 3）epoll

`epoll` 是在 Linux 2.6 内核中提出的，是之前的 `select` 和`poll` 的增强版本。

```c++
int epoll_create(int size)；// 创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；
int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
```

1. 在 `select` / `poll` 中，进程只有在调用一定的方法后，内核才对所有监视的文件描述符进行扫描。
2. 而 `epoll` 事先通过 `epoll_ctl()` 来注册一个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似 `callback` 的回调机制，迅速激活这个文件描述符，当进程调用 `epoll_wait()` 时便得到通知，从而去掉了对文件描述符的遍历操作。

- **优点**：
  1. **监视的描述符数量不受限制**：`epoll` 所支持的 fd 上限，是最大可以打开文件的数目，这个数字一般远大于 2048。比如，在 1GB 内存的机器上大约为10 万左右，具体数目可以 `cat /proc/sys/fs/file-max` 察看，一般来说这个数目和系统内存关系很大。
     - `select` 的最大缺点就是，进程打开的 fd 是有数量限制的，这对于连接数量比较大的服务器来说，根本不能满足。
  2. **I/O 效率不会随着监视 fd 的数量的增长而下降**：`epoll` 不同于 `select` 和 `poll` 轮询的方式，而是通过每个 fd 定义的回调函数来实现的，只有就绪的 fd 才会执行回调函数。
- **局限**：如果没有大量的 idle -connection 或者 dead-connection，`epoll` 的效率并不会比 `select` / `poll` 高很多，但是当遇到大量的 idle- connection，就会发现 `epoll` 的效率大大高于 `select` / `poll`。

##### 3、总

以上，就是我对 select、poll、epoll 的一个理解，请问有什么细节需要补充的吗？

#### 3.2.1.2. Netty 采用了哪种 I/O 多路复用模式？

##### 1、总

1. Netty 采用的是 Reactor 模式，应用于同步 I/O 的操作。
2. 然后， Reactor 根据实现方式的不同，其线层模型又分为 3 种，分别是单 Reactor 单线程模式、单 Reactor 多线程模式、主从 Reactor 多线程模式，其中，Netty 采用的是主从 Reactor 多线程模式。

##### 2、分

首先是 I/O 设计模式，分为 Reactor 模式和 Proactor 模式，

###### 1）Reactor 模式

Reactor 模式，应用于同步 I/O 场景，以 read 操作为例，Reactor 中的具体步骤为：

1. 应用程序注册 `读就绪` 事件和相关联的事件处理器。
2. 事件分离器等待事件的发生。
3. 当发生 `读就绪` 事件时，事件分离器调用第一步注册的事件处理器。
4. 事件处理器首先执行实际的读取操作，然后根据读取到的内容进行进一步的处理。

![1646831826891](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646831826891.png)

**Reactor 优点**：

1. 响应快，可以不必被单个同步时间所阻塞，可以最大程度地避免复杂了多线程及同步问题，以及多线程 / 进程切换的开销。
2. 扩展性好，可以方便的通过增加 Reactor 实例个数，来充分利用 CPU 资源。
3. 复用性好，Reactor 模型本身与具体事件处理逻辑无关，具有很高的复用性。

###### 2）Proactor 模式

Reactor 和 Proactor 模式的主要区别就是，真正的读取和写入操作是由谁来完成的，Reactor 中需要应用程序自己读取或者写入数据，而 Proactor 模式中，应用程序不需要进行实际的读写过程，只需要从缓存区读取或者写入即可，操作系统会写入缓存区或者从缓存区读取并写入到真正的 I/O 设备中。

Proactor 模式 read 操作过程为：

1. 应用程序初始化一个异步读取操作，然后注册相应的事件处理器，此时事件处理器不关注 `读取就绪` 事件，而是关注 `读完成` 事件，这是区别于 Reactor 的关键点 1。
2. 事件分离器等待 `读完成` 事件。
3. 在事件分离器等待 `读完成` 时，操作系统调用内核线程完成读取操作，并将读取的内容放入用户传递过来的缓存区中，这是区别于 Reactor 的关键点 2，在 Proactor 中，应用程序**需要传递缓存区**。
4. 事件分离器捕获到 `读完成` 事件后，激活应用程序注册的事件处理器，事件处理器直接从缓存区读取数据，而不需要进行实际的读取操作。
5. 而 Proactor 中的 write 操作和 read 操作类似，即感兴趣的事件是 `写完成` 事件。

![1646831843451](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646831843451.png)

###### 3）单 Reactor 单线程模式

然后，就是 Reactor 第一种线程模型，单 Reactor 单线程模式，

![1646831937063](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646831937063.png)

1. `selector.select()` 是多路复用模型 NIO#API ，可以实现一个阻塞对象监听多路的连接请求。
2. `Reactor` 对象通过 `selector.select()` 监控客户端请求事件，收到事件后通过 `dispatch()` 进行分发。
3. 如果是建立连接请求事件，则由 `Acceptor` 通过 `serverSocketChannel.accept()` 处理连接请求，然后创建一个 `Handler` 对象，来处理连接完成后的后续业务处理。
4. 如果不是建立连接事件，则 `Reactor` 会分发调用连接对应的 `Handler` 来响应。
5. `Handler` 会完成 `read()` -> `doBusiness()` 业务处理 -> `send()` 一个完整的业务流程。

- **优点**：服务器使用一个线程，通过多路复用搞定所有 I/O 操作（包括连接、读、写等），编码简单，清晰明了，但如果客户端连接数量较多，将无法支撑。
- **缺点**：
  1. **性能问题**：只有一个线程，无法完全发挥多核 CPU 的性能，Handler 在处理某个连接上业务时，整个进程无法处理其它连接事件，容易导致性能瓶颈。
  2. **可靠性问题**：线程意外终止，或者进入死循环，会导致整个系统通信模块不可用，不能接收和处理外部消息，造成节点故障。

###### 4）单 Reactor 多线程模式

![1646889343446](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646889343446.png)

1. `Reactor` 对象通过 `selector.select()` 监控客户端请求事件，收到事件后，通过 `dispatch()` 进行分发。
2. 如果是建立连接请求，则由 `Acceptor` 通过 `serverSocketChannel.accept()` 处理连接请求，然后创建一个 `Handler` 对象，来处理完成连接后的各种事件。
3. 如果不是连接请求，则由 `Reactor` 对象分发调用连接对应的 `Handler` 来处理。
4. 此时，`Handler` 只负责读取和响应事件，不做具体的业务处理，通过 `read()` 读取数据后，会分发给后面的 `Worker` 线程池的某个线程进行处理业务。
5. `Worker` 线程池会分配独立线程完成真正的业务，并将结果返回给 `Handler`。
6. `Handler` 收到线程池处理完的结果后，通过 `send()` 将结果返回给 Client。

- **优点**：可以充分利用多核 CPU 的处理能力。
- **缺点**：
  1. 多线程在需要数据共享时，可能实现比较复杂。
  2. Reactor 单线程处理完所有的监听、连接、读、写事件，在高并发场景下，容易出现性能瓶颈。

###### 5）主从 Reactor 多线程模式

![1646889441630](D:\MyData\yaocs2\AppData\Roaming\Typora\typora-user-images\1646889441630.png)

1. `Reactor` 主线程 `MainReactor` 对象，通过 `selector.select()` 监听连接事件，收到事件后，通过 `Acceptor` 处理连接事件。
2. 当 `Acceptor` 处理连接事件后，`MainReactor` 轮训式地将连接分配给 `SubReactor`，其中，`Reactor` 主线程可以对应多个 `Reactor` 子线程，即 `MainReactor` 可以关联多个 `SubReactor`。
3. `SubReactor` 将连接加入到连接队列进行监听，并创建 `Handler` 进行各种事件处理。
4. 当有新事件发生时，`SubReactor` 就会调用对应的 `Handler` 进行处理。
5. `Handler` 通过 `read()` 读取数据，分发给后面线程池中的 `Worker` 线程进行处理。
6. `Worker` 线程池会分配独立的 worker 线程进行业务处理，并返回结果。
7. `Handler` 收到处理结果后，再通过 `send()` 将结果返回给 Client。

- **优点**：
  1. 父线程与子线程的数据交互简单职责明确，父线程只需要接收新连接，子线程完成后续的业务处理。
  2. 父线程与子线程的数据交互简单，Reactor 主线程只需要把新连接传给子线程，子线程无需返回数据。
- **缺点**：编程复杂度较高。

=> 这种模型在许多项目中广泛使用，包括 Nginx 主从 Reactor 多进程模型，Memcached 主从多线程，Netty 主从多线程模型的支持。

##### 3、总

因此，总结的话，Reactor 模式线程模型可以比喻为，

| Reactor 线程模型    | 比喻                                           | 优点                                                         | 缺点                           |
| ------------------- | ---------------------------------------------- | ------------------------------------------------------------ | ------------------------------ |
| 单 Reactor 单线程   | 前台接待员和服务员都是同一个人，全程为顾客服务 | 主线程负责所有的连接、读写、业务处理，编程简单               | 性能问题、可靠性问题           |
| 单 Reactor 多线程   | 1 个前台接待员，多个服务员，接待员只负责接待   | 主线程负责所有的连接、读写，Worker 负责业务处理              | 高并发场景下，容易出现性能瓶颈 |
| 主从 Reactor 多线程 | 多个前台接待员，多个服务生                     | MainReactor 负责连接、SubReactor 负责读写、Worker 负责业务处理，充分发挥多核 CPU 的优势，满足高并发场景 | 编程复杂                       |

=> 以上，就是我对 Netty 多路复用模式 的一个理解，请问有什么细节需要补充的吗？

#### 3.2.1.3. RocketMQ 底层文件原理？



#### 3.2.1.4. RabbitMQ 底层文件原理？



#### 3.2.1.5. Kafka 底层文件原理？



#### 3.2.1.6. ThreadLocal 底层原理？



#### 3.2.1.7. HashMap 底层原理？



#### 3.2.1.8. 红黑树是稳定的吗，链表和红黑树性能稳定的情况？



#### 3.2.1.9. 设计模式了解过吗？线程池这种是什么设计模式？



#### 3.2.2.0. 动态代理实现原理，如果给你实现的话你会怎么做？



#### 3.2.2.1. 线上 CPU 100% 如何解决，除了 MAT 还有其他方式吗？



#### 3.2.2.2. Redis 集群架构，以及哨兵模式和集群模式使用上的区别？



#### 3.2.2.3. 算法题 | 100 个数实现随机取两次，不能拿重复？



#### 4.1.1.1. 项目介绍，分表规则、服务器配置、JVM 配置？



#### 4.1.1.2. 线程池参数？



#### 4.1.1.3. Spring MVC 的处理过程？



#### 4.1.1.4. Spring AOP 的原理？

讲了大概原理，但具体是哪个 BeanPostProcessor 忘了，最后还举了 @Transactional 注解的例子。



#### 4.1.1.5. Spring MVC 拦截器和过滤器的区别？



#### 4.1.1.6. JDK 8 的内存分布情况，年轻代和老年代默认的比值，以及 Eden 区的比值，以及为什么？



#### 4.1.1.7. 项目中的垃圾收集器用了哪些？

只回答了老年代的 CMS，忘了年轻代具体用了哪个了，而且 CMS 的具体流程也会没回答上来。

#### 4.1.1.8. 线上怎么确定是老年代还是年轻代的 GC 时间过长？

回到了 jmap 的 HeapDump + MAT 分析，其实可以看 Skywalking 的。

#### 4.1.1.9. 线上出现慢 SQL 是怎么解决的？

回答了 Druid 监控拿去 SQL，然后进行 Explain 本地调优，但很多细节也忘了，其实可以继续说下针对不同 SQL 的不同调优策略的。

#### 4.1.2.0. 讲一下为什么要用 Https？



#### 4.1.2.1. Docker 和 K8S 有了解过吗？

回答了个人实验用过，但生产上没有，也没让继续介绍了。

#### 4.1.2.2. 算法题 | n 的二进制 1 的个数

用了暴力递归 O（logn），其实可以用 Integer.count(n)，O（1）时间的。

#### 4.1.2.3. 算法题 | 1T 的字符串数字 ID，怎么在一台 2C4G 的机器上去重？

回答了去重后分片，再去重，但说不行，因为没有递归出口。

然后给出了按位的值去生成每个文件夹，每个文件夹只存一个ID，最后再收集所有文件夹的ID，输出到一个文件里，那个文件就是最终去重后了的答案了。

#### 4.1.2.4. 是否有参加过开源项目，或者 ACM 比赛获奖什么的？

回答了没有，但有提交过 BUG 验证给 Oracle 官网的 JDK 组织，讲的是提交红黑树在删除后调整，判断红黑树是否为合法的红黑树时，没有判断两个连续红节点的情况，最后 Oracle 反馈说需要提供详细的例子证明，但我没有一个很好的数据状态，事件就不了了之了。

#### 5.1.1.1. List 与 Set 的区别？



#### 5.1.1.2. ArrayList 扩容机制？



#### 5.1.1.3. ArrayList 与 LinkedList 的区别？



#### 5.1.1.4. 如何保证 List 的线程安全？



#### 5.1.1.5. HashMap 的数据结构？



#### 5.1.1.6. HashMap 的扩容机制，以及为什么是 2^n？



#### 5.1.1.7. JDK 7 与 JDK 8 中 HashMap 的区别？



#### 5.1.1.8. HashMap 如何保证线程安全？



#### 5.1.1.9. JDK 7 与 JDK 8 中 ConcurrentHashMap 的区别？



#### 5.1.2.0. CAS 的缺点，以及什么是总线风暴？



#### 5.1.2.1. volatile 关键字的实现原理？



#### 5.1.2.2. 线程池的数据结构，实现原理，以及线程数如何选择？

- **关键字**：

1. ThreadPoolExecutor、核心线程数、最大线程数、空闲线程最大存活时间、存活时间单位、工作队列、线程工厂、拒绝策略程序。
2. FutureTask、Runnable、Callable、RunnableFuture、CAS、WaitQueue。



#### 5.1.2.3. MySQL 主键索引、二级索引、联合索引查找数据的原理？



#### 5.1.2.4. MySQL 联合索引失效原理？



#### 5.1.2.5. MySQL 3 个二级索引好还是 1 个 3 字段的联合索引好？



#### 5.1.2.6. MySQL 二级索引和联合索引是不是建的越多越好？



#### 5.1.2.7. 介绍一下 B+ 树，以及 MySQL 为什么采用 B+ 树？



#### 5.1.2.8. 分布式 ID 的实现方案？

雪花算法、发号服务器、Leaf 雪花算法以及分段发号原理

#### 5.1.2.9. MySQL 隐藏主键生成原理？



#### 5.1.3.0. MySQL 事务 ACID 特性，以及实现原理？



#### 5.1.3.1. MySQL 持久性保证原理？

Buffer Pool、redo log。

#### 5.1.3.2. MySQL Buffer Pool 满了以后会怎么样？

页面置换算法，以及 Redis  LRU、LFU 等内存淘汰策略。

#### 5.1.3.3. Redis 键过期策略？

定期扫描，惰性删除，键+守护线程。

#### 5.1.3.4. Redis 常用数据结构，以及 ZSet 底层原理？



#### 5.1.3.5. Spring @Transactional 以及 AOP 的原理？



#### 5.2.1.1. 自我介绍，以及项目亮点？



#### 5.2.1.2. Kafka 高可靠保证？



#### 5.2.1.3.  线程池调优？



#### 6.1.1.1. Kafka 高可靠保证，如果一个 Broker 宕机了，还能保证高可靠吗？



#### 6.1.1.2. 1T 字符串数字 ID，怎么在 2C4G 的机器上做去重并且排序？

写入判断有就不写入，保证去重，收集时顺序收集，保证有序

#### 6.1.1.3. Netty 有了解过吗？

回答了怎么用，但具体原理记不清了

#### 6.1.1.4. 自动打包有了解过吗？

回答了 Jenkins

#### 6.1.1.5. 低代码平台，或者说代码生成器，有了解过吗，给你设计你怎么设计？



#### 6.1.1.6. 一张表每月一条记录实现员工日常打卡记录？

主键，员工号，年，月，打卡信息字段（使用每一位二进制位，来代表每一天的打卡信息，或上2^n）实现

#### 6.1.1.7. 怎么用程序模拟浏览器上的手工业务操作，且会经常掉线？

答了抓包参数+程序发起连接+异常捕捉+重新连接获取token+递归处理业务+递归出口为执行次数或者时间

#### 7.1.1.1. CountDownLatch 和 AQS 讲一下？



#### 7.1.1.2. 分布式锁的实现方案？



#### 7.1.1.3. AOP 的实现原理？



#### 7.1.1.4. 责任链模式的特点，以及与适配器模式的区别？



#### 7.2.1.1. 项目亮点讲一下？



#### 7.2.1.2. Kafka 分区日志存储原理？



#### 7.2.1.3. Kafka 有进行过调优吗？



#### 7.2.1.4. ZK 与 Redis 分布式锁的区别？



#### 7.2.1.5. 设计一个注解，实现对返回值（比如手机号码）进行脱敏，基于 Spring、Spring MVC 如何实现？



#### 7.2.1.6.  设置一个配置中心，以满足配置动态刷新的需求？



#### 7.2.1.7. k8s 架构图？



#### 7.2.1.8. 阻塞队列有哪些？



#### 7.2.1.9. ArrayBlockingQueue 与 LinkedBlockingQueue 的异同，比如在锁的角度呢？



#### 7.2.2.0. 有参与过 JVM 调优吗，只有过 MAT 吗，OGLIB 有听过没？



#### 7.2.2.1. CMS 和 G1 有什么差别？



#### 7.2.2.2. 反射的 API 里有哪些类？



#### 7.3.1.1. 自我介绍？



#### 7.3.1.2. 为什么要离职？



#### 7.3.1.3. 根据已有的经验，谈谈你对未来 G 系统迭代方向的一些看法？



#### 8.1.1.1. 项目亮点？



#### 8.1.1.2. 谈一下你对微服务的一个理解？



#### 8.1.1.3. 分库分表方案是怎么设计的？



#### 8.1.1.4. 分布式锁的实现方案？



#### 8.2.1.1. 讲一下 Eureka 源码？

服务注册、服务发现、服务续约、服务下线、服务剔除

#### 8.2.1.2. 服务下线后，TCP 会立马感应到吗？



#### 8.2.1.3. 设计一个负载均衡算法，实现实时动态感知？

Google 论文：60分以下的会在续约时剔除掉，客户端通过超时降级+重试+挑选优质服务实例，其中90+分挑选优质服务实例则从随机选择一个，保证唯一解，60-90分的淘汰选择，从而保证类实时感知